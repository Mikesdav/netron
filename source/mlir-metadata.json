[
  {
    "name": "affine.apply",
    "summary": "affine apply operation",
    "description": "The `affine.apply` operation applies an [affine mapping](#affine-maps)\n    to a list of SSA values, yielding a single SSA value. The number of\n    dimension and symbol operands to `affine.apply` must be equal to the\n    respective number of dimensional and symbolic inputs to the affine mapping;\n    the affine mapping has to be one-dimensional, and so the `affine.apply`\n    operation always returns one value. The input operands and result must all\n    have ‘index’ type.\n\n    An operand that is a valid dimension as per the [rules on valid affine\n    dimensions and symbols](#restrictions-on-dimensions-and-symbols)\n    cannot be used as a symbolic operand.\n\n    Example:\n\n    ```mlir\n    #map = affine_map<(d0, d1) -> (d0 floordiv 8 + d1 floordiv 128)>\n    ...\n    %1 = affine.apply #map (%s, %t)\n\n    // Inline example.\n    %2 = affine.apply affine_map<(i)[s0] -> (i + s0)> (%42)[%n]\n    ```",
    "inputs": [
      { "name": "mapOperands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "affine.delinearize_index",
    "summary": "delinearize an index",
    "description": "The `affine.delinearize_index` operation takes a single index value and\n    calculates the multi-index according to the given basis.\n\n    Example:\n\n    ```\n    %indices:3 = affine.delinearize_index %linear_index into (%c16, %c224, %c224) : index, index, index\n    ```\n\n    In the above example, `%indices:3` conceptually holds the following:\n\n    ```\n    #map0 = affine_map<()[s0] -> (s0 floordiv 50176)>\n    #map1 = affine_map<()[s0] -> ((s0 mod 50176) floordiv 224)>\n    #map2 = affine_map<()[s0] -> (s0 mod 224)>\n    %indices_0 = affine.apply #map0()[%linear_index]\n    %indices_1 = affine.apply #map1()[%linear_index]\n    %indices_2 = affine.apply #map2()[%linear_index]\n    ```\n\n    In other words, `%0:3 = affine.delinearize_index %x into (B, C)` produces\n    `%0 = {%x / (B * C), (%x mod (B * C)) / C, %x mod C}`.\n\n    The basis may either contain `N` or `N-1` elements, where `N` is the number of results.\n    If there are N basis elements, the first one will not be used during computations,\n    but may be used during analysis and canonicalization to eliminate terms from\n    the `affine.delinearize_index` or to enable conclusions about the total size of\n    `%linear_index`.\n\n    If the basis is fully provided, the delinearize_index operation is said to \"have\n    an outer bound\". The builders assume that an `affine.delinearize_index` has\n    an outer bound by default, as this is how the operation was initially defined.\n\n    That is, the example above could also have been written\n    ```mlir\n    %0:3 = affine.delinearize_index %linear_index into (244, 244) : index, index\n    ```\n\n    Note that, for symmetry with `getPaddedBasis()`, if `hasOuterBound` is `true`\n    when one of the `OpFoldResult` builders is called but the first element of the\n    basis is `nullptr`, that first element is ignored and the builder proceeds as if\n    there was no outer bound.\n\n    Due to the constraints of affine maps, all the basis elements must\n    be strictly positive. A dynamic basis element being 0 or negative causes\n    undefined behavior.\n\n    As with other affine operations, lowerings of delinearize_index may assume\n    that the underlying computations do not overflow the index type in a signed sense\n    - that is, the product of all basis elements is positive as an `index` as well.",
    "inputs": [
      { "name": "linear_index", "type": "Index" },
      { "name": "dynamic_basis", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "multi_index", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "static_basis", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$linear_index `into`\n    custom<DynamicIndexList>($dynamic_basis, $static_basis, \"{}\", \"::mlir::AsmParser::Delimiter::Paren\")\n    attr-dict `:` type($multi_index)"
  },
  {
    "name": "affine.for",
    "summary": "for operation",
    "description": "Syntax:\n\n    ```\n    operation   ::= `affine.for` ssa-id `=` lower-bound `to` upper-bound\n                    (`step` integer-literal)? `{` op* `}`\n\n    lower-bound ::= `max`? affine-map-attribute dim-and-symbol-use-list | shorthand-bound\n    upper-bound ::= `min`? affine-map-attribute dim-and-symbol-use-list | shorthand-bound\n    shorthand-bound ::= ssa-id | `-`? integer-literal\n    ```\n\n    The `affine.for` operation represents an affine loop nest. It has one region\n    containing its body. This region must contain one block that terminates with\n    [`affine.yield`](#affineyield-mliraffineyieldop). *Note:* when\n    `affine.for` is printed in custom format, the terminator is omitted. The\n    block has one argument of [`index`](Builtin.md/#indextype) type that\n    represents the induction variable of the loop.\n\n    The `affine.for` operation executes its body a number of times iterating\n    from a lower bound to an upper bound by a stride. The stride, represented by\n    `step`, is a positive constant integer which defaults to \"1\" if not present.\n    The lower and upper bounds specify a half-open range: the range includes the\n    lower bound but does not include the upper bound.\n\n    The lower and upper bounds of a `affine.for` operation are represented as an\n    application of an affine mapping to a list of SSA values passed to the map.\n    The [same restrictions](#restrictions-on-dimensions-and-symbols) hold for\n    these SSA values as for all bindings of SSA values to dimensions and\n    symbols.\n\n    The affine mappings for the bounds may return multiple results, in which\n    case the `max`/`min` keywords are required (for the lower/upper bound\n    respectively), and the bound is the maximum/minimum of the returned values.\n    There is no semantic ambiguity, but MLIR syntax requires the use of these\n    keywords to make things more obvious to human readers.\n\n    Many upper and lower bounds are simple, so MLIR accepts two custom form\n    syntaxes: the form that accepts a single 'ssa-id' (e.g. `%N`) is shorthand\n    for applying that SSA value to a function that maps a single symbol to\n    itself, e.g., `()[s]->(s)()[%N]`. The integer literal form (e.g. `-42`) is\n    shorthand for a nullary mapping function that returns the constant value\n    (e.g. `()->(-42)()`).\n\n    Example showing reverse iteration of the inner loop:\n\n    ```mlir\n    #map57 = affine_map<(d0)[s0] -> (s0 - d0 - 1)>\n\n    func.func @simple_example(%A: memref<?x?xf32>, %B: memref<?x?xf32>) {\n      %N = dim %A, 0 : memref<?x?xf32>\n      affine.for %i = 0 to %N step 1 {\n        affine.for %j = 0 to %N {   // implicitly steps by 1\n          %0 = affine.apply #map57(%j)[%N]\n          %tmp = call @F1(%A, %i, %0) : (memref<?x?xf32>, index, index)->(f32)\n          call @F2(%tmp, %B, %i, %0) : (f32, memref<?x?xf32>, index, index)->()\n        }\n      }\n      return\n    }\n    ```\n    `affine.for` can also operate on loop-carried variables (`iter_args`) and\n    return the final values after loop termination. The initial values of the\n    variables are passed as additional SSA operands to the `affine.for`\n    following the operands for the loop's lower and upper bounds. The\n    operation's region has equivalent arguments for each variable representing\n    the value of the variable at the current iteration.\n\n    The region must terminate with an `affine.yield` that passes all the current\n    iteration variables to the next iteration, or to the `affine.for`'s results\n    if at the last iteration. For `affine.for`'s that execute zero iterations, the\n    initial values of the loop-carried variables (corresponding to the SSA\n    operands) will be the op's results.\n\n    For example, to sum-reduce a memref:\n\n     ```mlir\n    func.func @reduce(%buffer: memref<1024xf32>) -> (f32) {\n      // Initial sum set to 0.\n      %sum_0 = arith.constant 0.0 : f32\n      // iter_args binds initial values to the loop's region arguments.\n      %sum = affine.for %i = 0 to 10 step 2\n          iter_args(%sum_iter = %sum_0) -> (f32) {\n        %t = affine.load %buffer[%i] : memref<1024xf32>\n        %sum_next = arith.addf %sum_iter, %t : f32\n        // Yield current iteration sum to next iteration %sum_iter or to %sum\n        // if final iteration.\n        affine.yield %sum_next : f32\n      }\n      return %sum : f32\n    }\n    ```\n\n    ```mlir\n    %res:2 = affine.for %i = 0 to 128 iter_args(%arg0 = %init0, %arg1 = %init1)\n               -> (index, index) {\n      %y0 = arith.addi %arg0, %c1 : index\n      %y1 = arith.addi %arg1, %c2 : index\n      affine.yield %y0, %y1 : index, index\n    }\n    ```\n    If the `affine.for` defines any values, a yield terminator must be\n    explicitly present. The number and types of the \"affine.for\" results must\n    match the initial values in the `iter_args` binding and the yield operands.",
    "inputs": [
      { "name": "lowerBoundOperands", "type": "Variadic" },
      { "name": "upperBoundOperands", "type": "Variadic" },
      { "name": "inits", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "lowerBoundMap", "type": "AffineMapAttr" },
      { "name": "upperBoundMap", "type": "AffineMapAttr" },
      { "name": "step", "type": "IndexAttr" }
    ]
  },
  {
    "name": "affine.if",
    "summary": "if-then-else operation",
    "description": "Syntax:\n\n    ```\n    operation  ::= `affine.if` if-op-cond `{` op* `}` (`else` `{` op* `}`)?\n    if-op-cond ::= integer-set-attr dim-and-symbol-use-list\n    ```\n\n    The `affine.if` operation restricts execution to a subset of the loop\n    iteration space defined by an integer set (a conjunction of affine\n    constraints). A single `affine.if` may end with an optional `else` clause.\n\n    The condition of the `affine.if` is represented by an\n    [integer set](#integer-sets) (a conjunction of affine constraints),\n    and the SSA values bound to the dimensions and symbols in the integer set.\n    The [same restrictions](#restrictions-on-dimensions-and-symbols) hold for\n    these SSA values as for all bindings of SSA values to dimensions and\n    symbols.\n\n    The `affine.if` operation contains two regions for the \"then\" and \"else\"\n    clauses.  `affine.if` may return results that are defined in its regions.\n    The values defined are determined by which execution path is taken.  Each\n    region of the `affine.if` must contain a single block with no arguments,\n    and be terminated by `affine.yield`.  If `affine.if` defines no values,\n    the `affine.yield` can be left out, and will be inserted implicitly.\n    Otherwise, it must be explicit.  If no values are defined, the else block\n    may be empty (i.e. contain no blocks).\n\n    Example:\n\n    ```mlir\n    #set = affine_set<(d0, d1)[s0]: (d0 - 10 >= 0, s0 - d0 - 9 >= 0,\n                                     d1 - 10 >= 0, s0 - d1 - 9 >= 0)>\n    func.func @reduced_domain_example(%A, %X, %N) : (memref<10xi32>, i32, i32) {\n      affine.for %i = 0 to %N {\n         affine.for %j = 0 to %N {\n           %0 = affine.apply #map42(%j)\n           %tmp = call @S1(%X, %i, %0)\n           affine.if #set(%i, %j)[%N] {\n              %1 = affine.apply #map43(%i, %j)\n              call @S2(%tmp, %A, %i, %1)\n           }\n        }\n      }\n      return\n    }\n    ```\n\n    Example with an explicit yield (initialization with edge padding):\n\n    ```mlir\n    #interior = affine_set<(i, j) : (i - 1 >= 0, j - 1 >= 0,  10 - i >= 0, 10 - j >= 0)> (%i, %j)\n    func.func @pad_edges(%I : memref<10x10xf32>) -> (memref<12x12xf32) {\n      %O = alloc memref<12x12xf32>\n      affine.parallel (%i, %j) = (0, 0) to (12, 12) {\n        %1 = affine.if #interior (%i, %j) {\n          %2 = load %I[%i - 1, %j - 1] : memref<10x10xf32>\n          affine.yield %2\n        } else {\n          %2 = arith.constant 0.0 : f32\n          affine.yield %2 : f32\n        }\n        affine.store %1, %O[%i, %j] : memref<12x12xf32>\n      }\n      return %O\n    }\n    ```",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "condition", "type": "IntegerSetAttr" }
    ]
  },
  {
    "name": "affine.linearize_index",
    "summary": "linearize an index",
    "description": "The `affine.linearize_index` operation takes a sequence of index values and a\n    basis of the same length and linearizes the indices using that basis.\n\n    That is, for indices `%idx_0` to `%idx_{N-1}` and basis elements `b_0`\n    (or `b_1`) up to `b_{N-1}` it computes\n\n    ```\n    sum(i = 0 to N-1) %idx_i * product(j = i + 1 to N-1) B_j\n    ```\n\n    In other words, `%0 = affine.linearize_index [%z, %y, %x] by (Z, Y, X)`\n    gives `%0 = %x + %y * X + %z * X * Y`, or `%0 = %x + X * (%y + Y * (%z))`.\n\n    The basis may either have `N` or `N-1` elements, where `N` is the number of\n    inputs to linearize_index. If `N` inputs are provided, the first one is not used\n    in computation, but may be used during analysis or canonicalization as a bound\n    on `%idx_0`.\n\n    If all `N` basis elements are provided, the linearize_index operation is said to\n    \"have an outer bound\".\n\n    As a convenience, and for symmetry with `getPaddedBasis()`, if the first\n    element of a set of `OpFoldResult`s passed to the builders of this operation is\n    `nullptr`, that element is ignored.\n\n    If the `disjoint` property is present, this is an optimization hint that,\n    for all `i`, `0 <= %idx_i < B_i` - that is, no index affects any other index,\n    except that `%idx_0` may be negative to make the index as a whole negative.\n    In addition, `disjoint` is an assertion that all bases elements are non-negative.\n\n    Note that the outputs of `affine.delinearize_index` are, by definition, `disjoint`.\n\n    As with other affine ops, undefined behavior occurs if the linearization\n    computation overflows in the signed sense.\n\n    Example:\n\n    ```mlir\n    %linear_index = affine.linearize_index [%index_0, %index_1, %index_2] by (2, 3, 5) : index\n    // Same effect\n    %linear_index = affine.linearize_index [%index_0, %index_1, %index_2] by (3, 5) : index\n    ```\n\n    In the above example, `%linear_index` conceptually holds the following:\n\n    ```mlir\n    #map = affine_map<()[s0, s1, s2] -> (s0 * 15 + s1 * 5 + s2)>\n    %linear_index = affine.apply #map()[%index_0, %index_1, %index_2]\n    ```",
    "inputs": [
      { "name": "multi_index", "type": "Variadic" },
      { "name": "dynamic_basis", "type": "Variadic" },
      { "name": "disjoint", "type": "UnitProp" }
    ],
    "outputs": [
      { "name": "linear_index", "type": "Index" }
    ],
    "attributes": [
      { "name": "static_basis", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "(`disjoint` $disjoint^)? ` `\n    `[` $multi_index `]` `by`\n    custom<DynamicIndexList>($dynamic_basis, $static_basis, \"{}\", \"::mlir::AsmParser::Delimiter::Paren\")\n    attr-dict `:` type($linear_index)"
  },
  {
    "name": "affine.load",
    "summary": "affine load operation",
    "description": "Syntax:\n\n    ```\n    operation ::= ssa-id `=` `affine.load` ssa-use `[` multi-dim-affine-map-of-ssa-ids `]` `:` memref-type\n    ```\n\n    The `affine.load` op reads an element from a memref, where the index\n    for each memref dimension is an affine expression of loop induction\n    variables and symbols. The output of `affine.load` is a new value with the\n    same type as the elements of the memref. An affine expression of loop IVs\n    and symbols must be specified for each dimension of the memref. The keyword\n    `symbol` can be used to indicate SSA identifiers which are symbolic.\n\n    Example 1:\n\n    ```mlir\n    %1 = affine.load %0[%i0 + 3, %i1 + 7] : memref<100x100xf32>\n    ```\n\n    Example 2: Uses `symbol` keyword for symbols `%n` and `%m`.\n\n    ```mlir\n    %1 = affine.load %0[%i0 + symbol(%n), %i1 + symbol(%m)] : memref<100x100xf32>\n    ```",
    "inputs": [
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "affine.max",
    "summary": "max operation",
    "description": "The `affine.max` operation computes the maximum value result from a multi-result\n    affine map.\n\n    Example:\n\n    ```mlir\n    %0 = affine.max (d0) -> (1000, d0 + 512) (%i0) : index\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "affine.min",
    "summary": "min operation",
    "description": "Syntax:\n\n    ```\n    operation ::= ssa-id `=` `affine.min` affine-map-attribute dim-and-symbol-use-list\n    ```\n\n    The `affine.min` operation applies an [affine mapping](#affine-expressions)\n    to a list of SSA values, and returns the minimum value of all result\n    expressions. The number of dimension and symbol arguments to `affine.min`\n    must be equal to the respective number of dimensional and symbolic inputs to\n    the affine mapping; the `affine.min` operation always returns one value. The\n    input operands and result must all have 'index' type.\n\n    Example:\n\n    ```mlir\n    %0 = affine.min affine_map<(d0)[s0] -> (1000, d0 + 512, s0)> (%arg0)[%arg1]\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "affine.parallel",
    "summary": "multi-index parallel band operation",
    "description": "The `affine.parallel` operation represents a hyper-rectangular affine\n    parallel band, defining zero or more SSA values for its induction variables.\n    It has one region capturing the parallel band body. The induction variables\n    are represented as arguments of this region. These SSA values always have\n    type index, which is the size of the machine word. The strides, represented\n    by steps, are positive constant integers which defaults to \"1\" if not\n    present. The lower and upper bounds specify a half-open range: the range\n    includes the lower bound but does not include the upper bound. The body\n    region must contain exactly one block that terminates with `affine.yield`.\n\n    The lower and upper bounds of a parallel operation are represented as an\n    application of an affine mapping to a list of SSA values passed to the map.\n    The same restrictions hold for these SSA values as for all bindings of SSA\n    values to dimensions and symbols. The list of expressions in each map is\n    interpreted according to the respective bounds group attribute. If a single\n    expression belongs to the group, then the result of this expression is taken\n    as a lower(upper) bound of the corresponding loop induction variable. If\n    multiple expressions belong to the group, then the lower(upper) bound is the\n    max(min) of these values obtained from these expressions. The loop band has\n    as many loops as elements in the group bounds attributes.\n\n    Each value yielded by `affine.yield` will be accumulated/reduced via one of\n    the reduction methods defined in the AtomicRMWKind enum.  The order of\n    reduction is unspecified, and lowering may produce any valid ordering.\n    Loops with a 0 trip count will produce as a result the identity value\n    associated with each reduction (i.e. 0.0 for addf, 1.0 for mulf).  Assign\n    reductions for loops with a trip count != 1 produces undefined results.\n\n    Note: Calling `AffineParallelOp::build` will create the required region and\n    block, and insert the required terminator if it is trivial (i.e. no values\n    are yielded).  Parsing will also create the required region, block, and\n    terminator, even when they are missing from the textual representation.\n\n    Example (3x3 valid convolution):\n\n    ```mlir\n    func.func @conv_2d(%D : memref<100x100xf32>, %K : memref<3x3xf32>) -> (memref<98x98xf32>) {\n      %O = memref.alloc() : memref<98x98xf32>\n      affine.parallel (%x, %y) = (0, 0) to (98, 98) {\n        %0 = affine.parallel (%kx, %ky) = (0, 0) to (2, 2) reduce (\"addf\") -> f32 {\n          %1 = affine.load %D[%x + %kx, %y + %ky] : memref<100x100xf32>\n          %2 = affine.load %K[%kx, %ky] : memref<3x3xf32>\n          %3 = arith.mulf %1, %2 : f32\n          affine.yield %3 : f32\n        }\n        affine.store %0, %O[%x, %y] : memref<98x98xf32>\n      }\n      return %O : memref<98x98xf32>\n    }\n    ```\n\n    Example (tiling by potentially imperfectly dividing sizes):\n\n    ```mlir\n    affine.parallel (%ii, %jj) = (0, 0) to (%N, %M) step (32, 32) {\n      affine.parallel (%i, %j) = (%ii, %jj)\n                              to (min(%ii + 32, %N), min(%jj + 32, %M)) {\n        call @f(%i, %j) : (index, index) -> ()\n      }\n    }\n    ```",
    "inputs": [
      { "name": "mapOperands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "reductions", "type": "TypedArrayAttrBase" },
      { "name": "lowerBoundsMap", "type": "AffineMapAttr" },
      { "name": "lowerBoundsGroups", "type": "I32ElementsAttr" },
      { "name": "upperBoundsMap", "type": "AffineMapAttr" },
      { "name": "upperBoundsGroups", "type": "I32ElementsAttr" },
      { "name": "steps", "type": "I64SmallVectorArrayAttr" }
    ]
  },
  {
    "name": "affine.prefetch",
    "summary": "affine prefetch operation",
    "description": "The `affine.prefetch` op prefetches data from a memref location described\n    with an affine subscript similar to affine.load, and has three attributes:\n    a read/write specifier, a locality hint, and a cache type specifier as shown\n    below:\n\n    ```mlir\n    affine.prefetch %0[%i, %j + 5], read, locality<3>, data : memref<400x400xi32>\n    ```\n\n    The read/write specifier is either 'read' or 'write', the locality hint\n    specifier ranges from locality<0> (no locality) to locality<3> (extremely\n    local keep in cache). The cache type specifier is either 'data' or 'instr'\n    and specifies whether the prefetch is performed on data cache or on\n    instruction cache.",
    "inputs": [
      { "name": "memref", "type": "AnyMemRef" },
      { "name": "indices", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "isWrite", "type": "BoolAttr" },
      { "name": "localityHint", "type": "ConfinedAttr" },
      { "name": "isDataCache", "type": "BoolAttr" },
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "affine.store",
    "summary": "affine store operation",
    "description": "Syntax:\n\n    ```\n    operation ::= `affine.store` ssa-use, ssa-use `[` multi-dim-affine-map-of-ssa-ids `]` `:` memref-type\n    ```\n\n    The `affine.store` op writes an element to a memref, where the index\n    for each memref dimension is an affine expression of loop induction\n    variables and symbols. The `affine.store` op stores a new value which is the\n    same type as the elements of the memref. An affine expression of loop IVs\n    and symbols must be specified for each dimension of the memref. The keyword\n    `symbol` can be used to indicate SSA identifiers which are symbolic.\n\n    Example 1:\n\n    ```mlir\n    affine.store %v0, %0[%i0 + 3, %i1 + 7] : memref<100x100xf32>\n    ```\n\n    Example 2: Uses `symbol` keyword for symbols `%n` and `%m`.\n\n    ```mlir\n    affine.store %v0, %0[%i0 + symbol(%n), %i1 + symbol(%m)] : memref<100x100xf32>\n    ```",
    "inputs": [
      { "name": "value", "type": "AnyType" },
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "affine.vector_load",
    "summary": "affine vector load operation",
    "description": "The `affine.vector_load` is the vector counterpart of\n    [affine.load](#affineload-mliraffineloadop). It reads a slice from a\n    [MemRef](Builtin.md/#memreftype), supplied as its first operand,\n    into a [vector](Builtin.md/#vectortype) of the same base elemental type.\n    The index for each memref dimension is an affine expression of loop induction\n    variables and symbols. These indices determine the start position of the read\n    within the memref. The shape of the return vector type determines the shape of\n    the slice read from the memref. This slice is contiguous along the respective\n    dimensions of the shape. Strided vector loads will be supported in the future.\n    An affine expression of loop IVs and symbols must be specified for each\n    dimension of the memref. The keyword `symbol` can be used to indicate SSA\n    identifiers which are symbolic.\n\n    Example 1: 8-wide f32 vector load.\n\n    ```mlir\n    %1 = affine.vector_load %0[%i0 + 3, %i1 + 7] : memref<100x100xf32>, vector<8xf32>\n    ```\n\n    Example 2: 4-wide f32 vector load. Uses `symbol` keyword for symbols `%n` and `%m`.\n\n    ```mlir\n    %1 = affine.vector_load %0[%i0 + symbol(%n), %i1 + symbol(%m)] : memref<100x100xf32>, vector<4xf32>\n    ```\n\n    Example 3: 2-dim f32 vector load.\n\n    ```mlir\n    %1 = affine.vector_load %0[%i0, %i1] : memref<100x100xf32>, vector<2x8xf32>\n    ```\n\n    TODOs:\n    * Add support for strided vector loads.\n    * Consider adding a permutation map to permute the slice that is read from memory\n    (see [vector.transfer_read](../Vector/#vectortransfer_read-mlirvectortransferreadop)).",
    "inputs": [
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyVectorOfNonZeroRank" }
    ],
    "attributes": [
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "affine.vector_store",
    "summary": "affine vector store operation",
    "description": "The `affine.vector_store` is the vector counterpart of\n    [affine.store](#affinestore-mliraffinestoreop). It writes a\n    [vector](Builtin.md/#vectortype), supplied as its first operand,\n    into a slice within a [MemRef](Builtin.md/#memreftype) of the same base\n    elemental type, supplied as its second operand.\n    The index for each memref dimension is an affine expression of loop\n    induction variables and symbols. These indices determine the start position\n    of the write within the memref. The shape of the input vector determines the\n    shape of the slice written to the memref. This slice is contiguous along the\n    respective dimensions of the shape. Strided vector stores will be supported\n    in the future.\n    An affine expression of loop IVs and symbols must be specified for each\n    dimension of the memref. The keyword `symbol` can be used to indicate SSA\n    identifiers which are symbolic.\n\n    Example 1: 8-wide f32 vector store.\n\n    ```mlir\n    affine.vector_store %v0, %0[%i0 + 3, %i1 + 7] : memref<100x100xf32>, vector<8xf32>\n    ```\n\n    Example 2: 4-wide f32 vector store. Uses `symbol` keyword for symbols `%n` and `%m`.\n\n    ```mlir\n    affine.vector_store %v0, %0[%i0 + symbol(%n), %i1 + symbol(%m)] : memref<100x100xf32>, vector<4xf32>\n    ```\n\n    Example 3: 2-dim f32 vector store.\n\n    ```mlir\n    affine.vector_store %v0, %0[%i0, %i1] : memref<100x100xf32>, vector<2x8xf32>\n    ```\n\n    TODOs:\n    * Add support for strided vector stores.\n    * Consider adding a permutation map to permute the slice that is written to memory\n    (see [vector.transfer_write](../Vector/#vectortransfer_write-mlirvectortransferwriteop)).",
    "inputs": [
      { "name": "value", "type": "AnyVectorOfNonZeroRank" },
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "affine.yield",
    "summary": "Yield values to parent operation",
    "description": "The `affine.yield` yields zero or more SSA values from an affine op region and\n    terminates the region. The semantics of how the values yielded are used\n    is defined by the parent operation.\n    If `affine.yield` has any operands, the operands must match the parent\n    operation's results.\n    If the parent operation defines no values, then the `affine.yield` may be\n    left out in the custom syntax and the builders will insert one implicitly.\n    Otherwise, it has to be present in the syntax to indicate which values are\n    yielded.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "amdgpu.dpp",
    "summary": "AMDGPU DPP operation",
    "description": "This operation represents DPP functionality in a GPU program.\n     DPP provides the following operations:\n    - Full crossbar in a group of four (`quad_perm`)\n    - Wavefront shift left by one lane (`wave_shl`)\n    - Wavefront shift right by one lane (`wave_shr`)\n    - Wavefront rotate right by one lane (`wave_ror`)\n    - Wavefront rotate left by one lane (`wave_rol`)\n    - Row shift left by 1–15 lanes (`row_shl`)\n    - Row shift right by 1–15 lanes (`row_shr`)\n    - Row rotate right by 1–15 lanes (`row_ror`)\n    - Reverse within a row (`row_mirror`)\n    - Reverse within a half-row (`row_half_mirror`)\n    - Broadcast the 15th lane of each row to the next row (`row_bcast`)\n    - Broadcast lane 31 to rows 2 and 3 (`row_bcast`)",
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "$old $src $kind (`(` $permArgument^ `)`)? attr-dict `:` type($result)"
  },
  {
    "name": "amdgpu.ext_packed_fp8",
    "summary": "Extend a fp8 value to a float or a vector of packed fp8 values to two floats",
    "description": "Extend one or two 8-bit floats in `source[index]` to a 32-bit float or\n    two floats and return them.\n\n    This rather unusual signature arises from the fact that AMD GPUs cannot\n    easily work with sub 32-bit quantities, so the compiler intrinsics for\n    extending 8-bit floats (which are, currently, the only way to work with\n    this operation) take packed vectors of 4 such floats.\n\n    If the passed-in vector has fewer than four elements, or the input is scalar,\n    the remaining values in the <4 x i8> will be filled with\n    undefined values as needed.",
    "assemblyFormat": "attr-dict $source `[` $index `]` `:` type($source) `to` type($res)"
  },
  {
    "name": "amdgpu.fat_raw_buffer_cast",
    "summary": "Create a raw buffer fat pointer that matches `memref`",
    "description": "Wraps the memory pointed to by `source` as a raw buffer fat pointer, or,\n    in LLVM terms, a `ptr addrspace(7)`, returning a memref that has the same\n    sizes and layout but the `#amdgpu.address_space<fat_raw_buffer>`\n    address space.\n\n    This memref can be used with standard memref operations like `memref.load`,\n    `memref.store`, and `memref.atomicrmw`, which will be lowered to the relevant\n    buffer intrinsics. (`vector.masked_load/store` will work once there's backend\n    support for lowering them, and then this document will be updated)\n\n    If `validBytes` is given, it is the number of bytes that will be valid as\n    an offset to `out`. If it is not provided, this will be inferred from\n    the size of the memref during lowering. This size is\n    max_{d = 0 upto rank(source)} (sizes[d] * strides[d]) * sizeof(element type).\n\n    The flags of the buffer descriptor will be set up to enable raw usage -\n    for example, stride = 0, add_tid = 0, and so on. The `boundsCheck`\n    property determines if bounds checking is enabled or not (on architectures\n    where this can be controlled - that is, on RDNA chips).\n\n    If `cacheSwizzleStride` is provided, L1 cache swizzling will be enabled\n    on architectures that support it. This swizzling, unlike the main swizzling\n    mode (whose usage makes a buffer non-raw) does not affect index calculation,\n    but does affect cache behavior. Mixing access between cache-swizzled raw\n    buffers and other forms of memory access, like ordinary pointer loads or\n    unswizzled buffer pointers can cause incorrect behavior and must be avoided.\n\n    This operation preserves the sizes, strides, and offset of the input\n    memref - they'll be added in by `memref.load` later. However, if\n    `resetOffset` is set, that offset will be added to the base pointer.\n    If the value of the memref's offset is not uniform (independent of the lane/thread ID),\n    this will lead to substantially decreased performance due to the need for\n    a waterfall loop on the base address of the buffer resource.",
    "assemblyFormat": "$source oilist (`validBytes` `(` $validBytes `)`\n      | `cacheSwizzleStride` `(` $cacheSwizzleStride `)`\n      | `boundsCheck` `(` $boundsCheck `)`\n      | `resetOffset` $resetOffset )\n    attr-dict `:` type($source) `to` type($result)"
  },
  {
    "name": "amdgpu.gather_to_lds",
    "summary": "MLIR wrapper for CDNA Gather to LDS instructions",
    "description": "The `amdgpu.gather_to_lds` op is a wrapper around the `global_load_lds` instructions.\n\n    Operands:\n    * `$src`: global memory (including fat buffer) memref to read from.\n    * `$srcIndices`: indices into `$src` to read from for this thread.\n    * `$dst`: LDS memory memref to write to.\n    * `$dstIndices`: base indices into `$dst` to write to for the subgroup of this thread.\n      The elements gathered by the subgroup will be written contiguously in order of lane ID\n      starting at `$dst[$dstIndices]`. Byte-sized (ex. i8) or short-sized (ex. i16)\n      types will be zero-padded/extended to 32 bits before being written. 96-bit types\n      (ex. vector<3xf32>) will be zero-padded to 128 bits before being written. Only the\n      offsets held by lane 0 are used.\n    * `$transferType`: type of the data to be transferred by each thread. This is used to determine\n      the size of the data to be transferred and the number of threads in the subgroup.\n      The transfer type must be a scalar type or a vector type with a single element type.\n\n    The `$dst`, along with its indices, points to the memory location the subgroup of this thread\n    will write to.\n\n    Note: only supported on gfx9 and gfx10.",
    "assemblyFormat": "$src `[` $srcIndices `]` `,` $dst `[` $dstIndices `]` attr-dict `:` $transferType `,` type($src) `,` type($dst)"
  },
  {
    "name": "amdgpu.lds_barrier",
    "summary": "Barrier that includes a wait for LDS memory operations.",
    "description": "`amdgpu.lds_barrier` is both a barrier (all workitems in a workgroup must reach\n    the barrier before any of them may proceed past it) and a wait for all\n    operations that affect the Local Data Store (LDS) issued from that wrokgroup\n    to complete before the workgroup may continue. Since the LDS is per-workgroup\n    memory, this barrier may be used, for example, to ensure all workitems have\n    written data to LDS before any workitem attempts to read from it.\n\n    Note that `lds_barrier` does **not** force reads to or from global memory\n    to complete before execution continues. Therefore, it should be used when\n    operations on global memory can be issued far in advance of when their results\n    are used (for example, by writing them to LDS).\n\n    WARNING: On architectures that do not support the BackOffBarrier feature,\n    (those which will implement this barrier by emitting inline assembly),\n    use of this operation will impede the usabiliity of memory watches (including\n    breakpoints set on variables) when debugging.",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "amdgpu.memory_counter_wait",
    "summary": "Wait for specified hardware counters",
    "description": "Wait for the specified counters to be less-than or equal-to the provided\n    values before continuing.\n\n    Counters can lower to different instructions on different architectires,\n    including clamping to the some HW supported max value or combining multiple\n    counters into one.",
    "assemblyFormat": "oilist( `load` `(` $load `)` | `store` `(` $store `)` | `ds` `(` $ds `)` | `exp` `(` $exp `)` ) attr-dict"
  },
  {
    "name": "amdgpu.mfma",
    "summary": "MLIR wrapper for CDNA mfma instructions",
    "description": "The `amdgpu.mfma` op is an MLIR wrapper around intrinsics\n    for various `mfma` instructions in the CDNA architecture, which perform\n    multiple outer products in order to allow fast matrix multiplication.\n\n    The wrapper will select an appropriate `mfma` instruction, if one is available,\n    based on the provided `m`, `k`, `n`, and `nBlks` attributes, along with the\n    types of the source and destination arguments.\n\n    For information on the layouts of the input and output matrices (which are stored\n    in `sourceA`, `sourceB`, `destC`, and `destD`), see the CDNA ISA documentation.\n\n    The `cbsz`, `abid`, and `blgp` parameters control how the lanes of the wave\n    are permuted when matrix data is being loaded: `blgp` can be any number of\n    fixed permutations, `cbsz` specifies the log_2 of the number of chunks the lanes\n    holding sourceA are split into, and `abid` selects one of those chunks.\n\n    Note, this wrapper allows specifying `vector<4Kxi8>` arguments to MFMA\n    intrinsics that take an integer type of width `4K`. For example,\n    one can provide a vector<4xi8> as an argument to an MFMA instruction that\n    logically takes 4 i8s but whose intrinsics are specified to take an i32.\n    In these cases, the bytes in the vector will be concatenated in little-endian\n    order (that is, v[0] will go to arg[7:0], v[1] to arg[15:8] and so on).\n\n    The negateA, negateB, and negateC flags are only supported for double-precision\n    operations on gfx94x.\n\n    Example:\n    ```mlir\n      %0 = amdgpu.mfma 16x16x16 %matA * %matB + %matC\n        : vector<4xf16>, vector<4xf16>, vector<4xf32>\n\n      %1 = amdgpu.mfma 32x32x1 %matD * %matE + %matF\n        { abid = 1 : i32, cbsz = 1 : i32, blocks = 2 : i32 }\n        blgp = bcast_second_32 : f32, f32, vector<32xf32>\n    ```",
    "assemblyFormat": "custom<MNKDimensionList>($m, $n, $k) $sourceA `*` $sourceB `+` $destC\n    attr-dict\n    `blgp` `=` $blgp\n    `:` type($sourceA) `,` type($sourceB) `,` type($destC)"
  },
  {
    "name": "amdgpu.packed_scaled_trunc",
    "summary": "Round two floats into a packed vector of floats",
    "description": "Scale and round the inputs `source` (which is undefined if not\n    specified) into the low or high word (bottom two or top two) elements\n    of the returned vector, keeping the other two elements of `existing`\n    unchanged if present (or undefined if it was not passed in).\n\n    The reason for this odd signature is that AMD GPUs cannot easily work with\n    sub-registers, and so the conversion intrinsics take 32-bit wide\n    packed vectors of float values.",
    "assemblyFormat": "attr-dict $source `into` ($existing^):(`undef`)? `[` $index `]`\n    `,` $scale\n    `:` type($source) `to` type($res) (`into` type($existing)^)?"
  },
  {
    "name": "amdgpu.packed_stoch_round_fp8",
    "summary": "Round float stochiastically into a packed vector of 8-bit floats",
    "description": "Round the input `source`, adding in `stochiasticParam`, and place it into\n    the `storeIndex`th element of `res`.\n\n    If `existing` is passed in, elements of `res` other than the one at `storeIndex`\n    are copied from `existing`.\n\n    The reason for this odd signature is that AMD GPUs cannot easily work with\n    sub-registers, and so the conversion intrinsics (which are currently the\n    only way to work with 8-bit float types) take packed vectors of 4 8-bit\n    values.",
    "assemblyFormat": "attr-dict $source `+` $stochiasticParam\n    `into` ($existing^):(`undef`)? `[` $storeIndex `]`\n    `:` type($source) `to` type($res) (`into` type($existing)^)?"
  },
  {
    "name": "amdgpu.packed_trunc_2xfp8",
    "summary": "Round two floats into a packed vector of 8-bit floats",
    "description": "Round the inputs `sourceA` and `sourceB` (which is undefined if not\n    specified) into the low or high word (bottom two or top two) elements\n    of the returned vector, keeping the other two elements of `existing`\n    unchanged if present (or undefined if it was not passed in).\n\n    The reason for this odd signature is that AMD GPUs cannot easily work with\n    sub-registers, and so the conversion intrinsics (which are currently the\n    only way to work with 8-bit float types) take packed vectors of 4 8-bit\n    values.",
    "assemblyFormat": "attr-dict $sourceA `,` ($sourceB^):(`undef`)?\n    `into` ($existing^):(`undef`)? `[` `word` $wordIndex `]`\n    `:` type($sourceA) `to` type($res) (`into` type($existing)^)?"
  },
  {
    "name": "amdgpu.permlane_swap",
    "summary": "AMDGPU permlane swap op",
    "description": "High-level wrapper on `rocdl.permlane{16,32}.swap` variants for permutations\n    on rows of lanes in a subgroup.\n\n    Supports arbitrary int/float/vector types, which will be repacked to i32 and\n    one or more `rocdl.permlane_swap` ops during lowering.\n    Supported lane permutations:\n    - Swap the data between odd and even rows of 16 lanes\n    - Swap the data between the first 32 lanes and the last 32 lanes\n\n    Example:\n    ```mlir\n    %0 = amdgpu.permlane_swap %src 16 : f16\n    %1 = amdgpu.permlane_swap %src 32 { fetch_inactive = true, bound_ctrl = true } : f16\n    ```\n\n    Operands:\n    * `$src`: Vector register to permute across lanes of the subgroup.\n    * `$row_length`: The length of a row to permute in number of lanes (valid values are 16 and 32).\n    * `$fetch_inactive`: Optional. Used to dertermine behavior of a fetch from a disabled lane.\n      `fetch_inactive = false`: If the source lane is disabled, use `bound_ctrl` to determine the source value.\n      `fetch_inactive = true`: If the source lane is disabled, fetch the source value anyway (ignoring `bound_ctrl`).\n    * `$bound_ctrl`: Optional. Used to determine what a thread should do if its source operand is from\n      a disabled lane: use the value zero, or disable the write.\n      `bound_ctrl = false`: Do not write when source is from a disabled lane\n      `bound_ctrl = true`: Use zero as input if source is from a disabled lane\n\n    Note: Lowering is only supported on gfx950 and up.",
    "inputs": [
      { "name": "src", "type": "AnyIntegerOrFloatOr1DVector" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyIntegerOrFloatOr1DVector" }
    ],
    "attributes": [
      { "name": "row_length", "type": "I32Attr" },
      { "name": "fetch_inactive", "type": "DefaultValuedAttr" },
      { "name": "bound_ctrl", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$src $row_length attr-dict `:` type($result)"
  },
  {
    "name": "amdgpu.raw_buffer_atomic_cmpswap",
    "summary": "Raw Buffer Atomic compare-and-swap",
    "description": "The `amdgpu.raw_buffer_atomic_cmpswap` op is a wrapper around the\n    buffer-based atomic compare-and-swap min available on AMD GPUs.\n\n    The index into the buffer is computed as for `memref.store` with the addition\n    of `indexOffset` (which is used to aid in emitting vectorized code) and,\n    if present `sgprOffset` (which is added after bounds checks and includes\n    any non-zero offset on the memref type).\n\n    All indexing components are given in terms of the memref's element size, not\n    the byte lengths required by the intrinsic.\n\n    Out of bounds atomic operations are ignored in hardware.\n\n    See `amdgpu.raw_buffer_load` for a description of how the underlying\n    instruction is constructed.",
    "assemblyFormat": "attr-dict $src `,` $cmp `->` $memref `[` $indices `]`\n      (`sgprOffset` $sgprOffset^)? `:`\n      type($value) `->` type($memref) `,` type($indices)"
  },
  {
    "name": "amdgpu.raw_buffer_atomic_fadd",
    "summary": "Raw Buffer Floating-point Atomic Add (MI-* only)",
    "description": "The `amdgpu.raw_buffer_atomic_fadd` op is a wrapper around the\n    buffer-based atomic floating point addition available on the MI-* series\n    of AMD GPUs.\n\n    The index into the buffer is computed as for `memref.store` with the addition\n    of `indexOffset` (which is used to aid in emitting vectorized code) and,\n    if present `sgprOffset` (which is added after bounds checks and includes\n    any non-zero offset on the memref type).\n\n    All indexing components are given in terms of the memref's element size, not\n    the byte lengths required by the intrinsic.\n\n    Out of bounds atomic operations are ignored in hardware.\n\n    See `amdgpu.raw_buffer_load` for a description of how the underlying\n    instruction is constructed.",
    "assemblyFormat": "attr-dict $value `->` $memref `[` $indices `]`\n      (`sgprOffset` $sgprOffset^)? `:`\n      type($value) `->` type($memref) `,` type($indices)"
  },
  {
    "name": "amdgpu.raw_buffer_atomic_fmax",
    "summary": "Raw Buffer Floating-point Atomic Max (non-GFX9)",
    "description": "The `amdgpu.raw_buffer_atomic_fmax` op is a wrapper around the\n    buffer-based atomic floating point max available on AMD GPUs (except GFX9).\n\n    The index into the buffer is computed as for `memref.store` with the addition\n    of `indexOffset` (which is used to aid in emitting vectorized code) and,\n    if present `sgprOffset` (which is added after bounds checks and includes\n    any non-zero offset on the memref type).\n\n    All indexing components are given in terms of the memref's element size, not\n    the byte lengths required by the intrinsic.\n\n    Out of bounds atomic operations are ignored in hardware.\n\n    See `amdgpu.raw_buffer_load` for a description of how the underlying\n    instruction is constructed.",
    "assemblyFormat": "attr-dict $value `->` $memref `[` $indices `]`\n      (`sgprOffset` $sgprOffset^)? `:`\n      type($value) `->` type($memref) `,` type($indices)"
  },
  {
    "name": "amdgpu.raw_buffer_atomic_smax",
    "summary": "Raw Buffer Signed Integer Atomic Max",
    "description": "The `amdgpu.raw_buffer_atomic_smax` op is a wrapper around the\n    buffer-based atomic signed integer max available on AMD GPUs.\n\n    The index into the buffer is computed as for `memref.store` with the addition\n    of `indexOffset` (which is used to aid in emitting vectorized code) and,\n    if present `sgprOffset` (which is added after bounds checks and includes\n    any non-zero offset on the memref type).\n\n    All indexing components are given in terms of the memref's element size, not\n    the byte lengths required by the intrinsic.\n\n    Out of bounds atomic operations are ignored in hardware.\n\n    See `amdgpu.raw_buffer_load` for a description of how the underlying\n    instruction is constructed.",
    "assemblyFormat": "attr-dict $value `->` $memref `[` $indices `]`\n      (`sgprOffset` $sgprOffset^)? `:`\n      type($value) `->` type($memref) `,` type($indices)"
  },
  {
    "name": "amdgpu.raw_buffer_atomic_umin",
    "summary": "Raw Buffer Unsigned Integer Atomic Min",
    "description": "The `amdgpu.raw_buffer_atomic_umin` op is a wrapper around the\n    buffer-based atomic signed integer min available on AMD GPUs.\n\n    The index into the buffer is computed as for `memref.store` with the addition\n    of `indexOffset` (which is used to aid in emitting vectorized code) and,\n    if present `sgprOffset` (which is added after bounds checks and includes\n    any non-zero offset on the memref type).\n\n    All indexing components are given in terms of the memref's element size, not\n    the byte lengths required by the intrinsic.\n\n    Out of bounds atomic operations are ignored in hardware.\n\n    See `amdgpu.raw_buffer_load` for a description of how the underlying\n    instruction is constructed.",
    "assemblyFormat": "attr-dict $value `->` $memref `[` $indices `]`\n      (`sgprOffset` $sgprOffset^)? `:`\n      type($value) `->` type($memref) `,` type($indices)"
  },
  {
    "name": "amdgpu.raw_buffer_load",
    "summary": "Raw Buffer load, exposing GCN features",
    "description": "The `amdgpu.raw_buffer_load` op is a wrapper around the buffer load intrinsics\n    available on AMD GPUs, including extensions in newer GPUs.\n\n    The index into the buffer is computed as for `memref.load` with the additon\n    of `indexOffset` and `sgprOffset` (which **may or may not** be considered\n    in bounds checks and includes any offset present on the memref type if it's\n    non-zero).\n\n    All indices and offsets are in units of the memref's data type and are\n    converted to bytes during lowering.\n\n    When a load is out of bounds, the instruction returns zero.\n    Partially-out of bounds have chipset-dependent behavior: whether reading\n    2 elements starting at index 7 of a `memref<8xf32>` returns the last element\n    in the first vector component depends on the architecture.\n\n    The memref struct is converted into a buffer resource (a V#) and the arguments\n    are translated to intrinsic arguments as follows:\n    - The base address of the buffer is the base address of the memref\n    - The stride is 0 to enable raw mode\n    - The number of records is the size of the memref, in bytes\n      In the case of dynamically-shaped memrefs, this is computed at runtime\n      as max_d (size(d) * stride(d)) * sizeof(elementType(memref))\n    - The offset enable bit is 1, the index enable bit is 0.\n    - The thread ID addition bit is off\n    - If `boundsCheck` is false and the target chipset is RDNA, OOB_SELECT is set\n      to 2 to disable bounds checks, otherwise it is 3\n    - The cache coherency bits are off",
    "assemblyFormat": "attr-dict $memref `[` $indices `]`\n      (`sgprOffset` $sgprOffset^)? `:`\n      type($memref) (`,` type($indices)^)? `->` type($value)"
  },
  {
    "name": "amdgpu.raw_buffer_store",
    "summary": "Raw Buffer Store, exposing GCN features",
    "description": "The `amdgpu.raw_buffer_store` op is a wrapper around the buffer store\n    intrinsics available on AMD GPUs, including extensions in newer GPUs.\n\n    The store index is computed as in `memref.store` with the addition of\n    `indexOffset` (which is included for uniformity with atomics and may be useful\n    when writing vectorized code) and `sgprOffset` (which is added after bounds\n    checks and implicitly includes the offset of the memref type if non-zero).\n    All index components are in terms of the elements of the memref, not bytes,\n    and are scaled up appropriately.\n\n    Out of bounds stores are ignored in hardware.\n    Wthether a vector write that includes some in-bounds and soeme out-of-bounds\n    components is partically completed is chipset-dependent.\n\n    See `amdgpu.raw_buffer_load` for a description of how the underlying\n    instruction is constructed.",
    "assemblyFormat": "attr-dict $value `->` $memref `[` $indices `]`\n      (`sgprOffset` $sgprOffset^)? `:`\n      type($value) `->` type($memref) (`,` type($indices)^)?"
  },
  {
    "name": "amdgpu.scaled_ext_packed",
    "summary": "Extend a vector of packed floating point values",
    "description": "Extend and scale two packed floats in `source[index]` to two floats and\n    return them.\n\n    This rather unusual signature arises from the fact that AMD GPUs cannot\n    easily work with sub 32-bit quantities, so the compiler intrinsics for\n    extending 8-bit floats (which are, currently, the only way to work with\n    this operation) take packed vectors of 2 such floats.\n\n    If the passed-in vector has fewer than two elements, or the input is scalar,\n    the remaining values in the <2 x i8> will be filled with\n    undefined values as needed.",
    "assemblyFormat": "attr-dict $source `[` $index `]` `,` $scale `:` type($source) `to` type($res)"
  },
  {
    "name": "amdgpu.scaled_ext_packed816",
    "summary": "Extend a vector of packed floating point values",
    "description": "The scales applied to the input microfloats are stored in two bytes which\n    come from the `scales` input provided in a *half* of the wave identified\n    by `firstScaleLane`. The pair of bytes used is selected by\n    `firstScaleByte`. The 16 vectors in consecutive lanes starting from\n    `firstScaleLane` (which we'll call the scale vectors) will be used by both\n    halves of the wave (with lane L reading from L % 16'th scale vector), but\n    each half will use a different byte.\n\n    When the block size is 32, `firstScaleByte` can be either 0 or 2,\n    selecting halves of the scale vectors. Lanes 0-15 will read from\n    `firstScaleByte` and lanes 16-31 will read from `firstScaleByte` + 1.\n    For example:\n    ```mlir\n    // Input: 8-element vector of F8E4M3FN, converting to F32\n    // Lanes 0-15 read from byte 0, lanes 16-31 read from byte 1\n    %result = amdgpu.scaled_ext_packed816 %source scale(%scales)\n      blockSize(32) firstScaleLane(0) firstScaleByte(0)\n      : vector<8xf8E4M3FN>, vector<4xf8E8M0FNU> -> vector<8xf32>\n\n    // Input: 16-element vector of F6E2M3FN, converting to F16\n    // Lanes 0-15 read from byte 2, lanes 16-31 read from byte 3\n    %result = amdgpu.scaled_ext_packed816 %source scale(%scales)\n      blockSize(32) firstScaleLane(1) firstScaleByte(2)\n      : vector<16xf6E2M3FN>, vector<4xf8E8M0FNU> -> vector<16xf16>\n    ```\n\n    However, when the block size is 16, `firstScaleByte` can be 0 or 1.\n    Lanes 0-15 read from the `firstScaleByte`th element of the scale vectors,\n    while lanes 16-31 read from `firstScaleByte` + 2.\n    For example:\n    ```mlir\n    // Input: 8-element vector of F8E5M2, converting to BF16\n    // Lanes 0-15 read from byte 0, lanes 16-31 read from byte 2 (0+2)\n    %result = amdgpu.scaled_ext_packed816 %source scale(%scales)\n      blockSize(16) firstScaleLane(0) firstScaleByte(0)\n      : vector<8xf8E5M2>, vector<4xf8E8M0FNU> -> vector<8xbf16>\n\n    // Input: 16-element vector of F6E3M2FN, converting to F32\n    // Lanes 0-15 read from byte 1, lanes 16-31 read from byte 3 (1+2)\n    %result = amdgpu.scaled_ext_packed816 %source scale(%scales)\n      blockSize(16) firstScaleLane(1) firstScaleByte(1)\n      : vector<16xf6E3M2FN>, vector<4xf8E8M0FNU> -> vector<16xf32>\n    ```\n\n    Note: the layout for the scales generally mirrors how the WMMA\n    instructions use for matix scales. These selection operands allows\n    one to choose portions of the matrix to convert.\n\n    Available on gfx1250+.",
    "assemblyFormat": "attr-dict $source\n    `scale` `(` $scale `)`\n    `blockSize` `(` $blockSize `)`\n    `firstScaleLane` `(` $firstScaleLane`)`\n    `firstScaleByte` `(` $firstScaleByte `)`\n    `:` type($source) `,` type($scale) `->` type($res)"
  },
  {
    "name": "amdgpu.scaled_mfma",
    "summary": "MLIR wrapper for CDNA scaled mfma instructions",
    "description": "The `amdgpu.scaled_mfma` op is an MLIR wrapper around intrinsics\n    for various scaled versions of `mfma` instructions in the CDNA architecture, which\n    perform multiple outer products in order to allow fast matrix multiplication.\n\n    The wrapper will select an appropriate `mfma` instruction, if one is available,\n    based on the provided `m`, `k`, `n`, and `nBlks` attributes, along with the\n    types of the source and destination arguments.\n\n    Note, this wrapper allows specifying `vector<4Kxi8>` arguments to MFMA\n    intrinsics that take an integer type of width `4K`. For example,\n    one can provide a `vector<4xi8>` as an argument to an MFMA instruction that\n    logically takes 4 i8s but whose intrinsics are specified to take an i32.\n    In these cases, the bytes in the vector will be concatenated in little-endian\n    order (that is, v[0] will go to arg[7:0], v[1] to arg[15:8] and so on).\n\n    This wrapper takes inspiration from `amdgpu.mfma`, but has some key differences:\n    - `amdgpu.scaled_mfma` operates on fp4 (f4E2M1FN), fp6 (f6E2M3FN and f6E3M2FN) and\n      fp8 (f8E4M3FN and f8E5M2) types using either M=N=16, K=128 or M=N=32, K=64 as\n      their tile size.\n    - `amdgpu.scaled_mfma` does not support broadcasting. So, `cbsz`, `abid`, and `blgp`\n      are omitted from this wrapper.\n    - The `negateA`, `negateB`, and `negateC` flags in `amdgpu.mfma` are only supported\n      for double-precision operations on gfx94x and so are not included here.\n\n    Example:\n    ```mlir\n      %0 = amdgpu.scaled_mfma 32x32x64 (%arg0[0] * %arg1) * (%arg0[1] * %arg1) + %arg2\n        : vector<4xf8E8M0FNU>, vector<32xf6E2M3FN>, f8E8M0FNU, vector<32xf6E2M3FN>, vector<16xf32>\n    ```",
    "assemblyFormat": "custom<MNKDimensionList>($m, $n, $k) ` `\n    `(` $scalesA `[` $scalesIdxA `]` `*` $sourceA `)` `*`\n    `(` $scalesB `[` $scalesIdxB `]` `*` $sourceB `)` `+` $destC\n    attr-dict\n    `:` type($scalesA) `,` type($sourceA) `,` type($scalesB) `,` type($sourceB) `,` type($destC)"
  },
  {
    "name": "amdgpu.sched_barrier",
    "summary": "Barrier that limits the backend scheduler of instruction movement",
    "description": "`amdgpu.sched_barrier` serves as a barrier that could be\n    configured to restrict movements of instructions through it as\n    defined by sched_barrier_opts.",
    "assemblyFormat": "`allow` `=` $opts attr-dict"
  },
  {
    "name": "amdgpu.swizzle_bitmode",
    "summary": "AMDGPU ds_swizzle op, bitmode variant",
    "description": "High-level wrapper on bitmode `rocdl.ds_swizzle` op, masks are represented\n    as separate fields so user won't need to do manual bitpacking.\n\n    Supports arbitrary int/float/vector types, which will be repacked to i32 and\n    one or more `rocdl.ds_swizzle` ops during lowering.",
    "outputs": [
      { "name": "result", "type": "AnyIntegerOrFloatOr1DVector" }
    ],
    "assemblyFormat": "$src $and_mask $or_mask $xor_mask attr-dict `:` type($result)"
  },
  {
    "name": "amdgpu.transpose_load",
    "summary": "MLIR wrapper for CDNA Transpose Load instructions",
    "description": "The `amdgpu.transpose_load` op is a wrapper around the `ds_read_tr` instructions.\n    The transpose load op represents a subgroup load from LDS memory,\n    where the subgroup of threads collectively reads a matrix from the source\n    memref, with each thread reading a vector of the matrix, and gets a transposed matrix\n    in as the result. That is, each thread reads a vector of the col-major matrix at different\n    indices, and the thread's read result is a vector of the corresponding row of the transposed\n    matrix.\n\n    This op is a direct wrapper around the ROCDL `ds_read_tr` family intrinsics. Please refer\n    to the CDNA4 ISA documentation for more details about its exact semantics.\n\n    Format example:\n    ```\n    %0 = amdgpu.transpose_load %src[%srcIndices] : memref<128x256xf16> -> vector<4xf16>\n    ```\n    Operands:\n    * `$src`: LDS memref to read from.\n    * `$srcIndices`: indices into `$src` to read from for this thread.\n    * `$result`: target register this transpose load instruction will write to.\n\n    Note: Lowering is only supported on gfx950 and up.",
    "assemblyFormat": "$src `[` $srcIndices `]` attr-dict `:` type($src) `->` type($result)"
  },
  {
    "name": "amdgpu.wmma",
    "summary": "MLIR wrapper for wmma instructions",
    "description": "The `amdgpu.wmma` op is an MLIR wrapper around intrinsics for various `wmma`\n    instructions in the AMDGPU architecture, which perform matrix multiplication.\n\n    On gfx11/RDNA3, wmma intrinsics have M=N=K=16 dimensions.\n\n    On gfx12/RDNA4, wmma intrinsics have M=N=16 dimensions and support K=16 for\n    all element types, and K=32 for i4 sources.\n\n    On gfx1250, wmma intrinsics have M=N=16 and K dimensions of 4, 32, 64, or 128,\n    depending on the element types.\n\n    On gfx11/RDNA3, emitting f16->f16 (or bf16->bf16) wmma the output is a 16xf16\n    (or 16xbf16) vector containing only 8 valid values:\n      - If `subwordOffset` is 0, then the output is stored at indices 0, 2, 4, ..., 14.\n      - If `subwordOffset` is 1, then the output is stored at indices 1, 3, 5, ..., 15.\n    On gfx12/RDNA4 and gfx1250, the result is instead returned as vector where all\n    the values are valid and the `subwordOffset` must be `0`, as it cannot be used.\n\n    `unsignedA` and `unsignedB` flag that the `int8` LLVM inputs are unsigned.\n\n    The `clamp` flag is used to saturate the output of type T to `numeric_limits<T>::max()`\n    in case of overflow.\n\n    Example:\n    ```mlir\n      %0 = amdgpu.wmma 16x16x16 %matA * %matB + %matC : vector<8xf16>, vector<8xf16>, vector<8xf16>\n\n      %1 = amdgpu.wmma 16x16x64 %matD * %matE + %matF : vector<32xi8>, vector<8xf32>, vector<8xf32>\n\n      %2 = amdgpu.wmma 16x16x128 %matG * %matH + %matI : vector<64xf4E2M1FN>, vector<64xf4E2M1FN>, vector<8xf32>\n\n      %3 = amdgpu.wmma 16x16x4 %matJ * %matK + %matL : vector<2xf32>, vector<2xf32>, vector<8xf32>\n    ```",
    "assemblyFormat": "custom<MNKDimensionList>($m, $n, $k) $sourceA `*` $sourceB `+` $destC\n    attr-dict\n    `:` type($sourceA) `,` type($sourceB) `,` type($destC)"
  },
  {
    "name": "amx.tile_load",
    "summary": "tile load operation",
    "description": "Loads a tile from memory defined by a `base` and `indices`, with the\n    shape defined by the 2-dim vector type of the result.\n    The tile's rows are populated by reading contiguous elements starting\n    at the `base`. For each tile row, the `base` is incremented by `stride`\n    number of elements.\n\n    The tile is loaded using the following indexing scheme:\n\n    ```\n    for row in enumerate(tile_rows):\n      mem_row = base[i0, i1, ..., iN + row * stride]\n      for col in enumerate(tile_cols):\n        tile[row, col] = mem_row[col]\n    ```\n\n    If the `stride` is not provided, then the `base` buffer must be at least\n    2-dimensional, and the `stride` is automatically inferred and corresponds\n    to the stride of the buffer's second innermost dimension.\n\n    The operation is eventually lowered into the \"tileloadd\" instruction\n    with the corresponding tile configuration.\n\n    With the write memory effect, each `amx.tile_load` operation serves as\n    a compilation hint to use a separate tile register.\n\n    Example:\n\n    ```mlir\n      // Tile load from a 2-D memref with implicit stride.\n      %0 = amx.tile_load %arg0[%c0, %c0] : memref<?x?xi8> into !amx.tile<16x64xi8>\n\n      // Tile load from a 1-D memref with explicit stride.\n      %0 = amx.tile_load %arg0[%c0], %stride : memref<?xi8> into !amx.tile<16x64xi8>\n    ```",
    "inputs": [
      { "name": "base", "type": "Arg" },
      { "name": "indices", "type": "Variadic" },
      { "name": "stride", "type": "Optional" }
    ],
    "outputs": [
      { "name": "res", "type": "AnyAMXTile" }
    ],
    "assemblyFormat": "$base `[` $indices `]` (`,` $stride^ )? attr-dict`:` type($base) `into` qualified(type($res))"
  },
  {
    "name": "amx.tile_mulf",
    "summary": "tile multiplication operation (floating-point)",
    "description": "Multiplies a \"m x k\" tile with a \"k x n\" tile and accumulates the results\n    into a \"m x n\" destination tile. Supports \"f32 <- bf16 x bf16\" (with\n    pairs of \"bf16\").\n    \n    The operation is eventually lowered into the \"tdpbf16ps\" instruction with\n    the corresponding tile configuration.\n\n    Example:\n\n    ```mlir\n      %0 = amx.tile_mulf %a, %b, %c\n        : !amx.tile<16x32xbf16>, !amx.tile<16x32xbf16>, !amx.tile<16x16xf32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "AMXTileF16OrBF16" },
      { "name": "rhs", "type": "AMXTileF16OrBF16" },
      { "name": "acc", "type": "AMXTileF32" }
    ],
    "outputs": [
      { "name": "res", "type": "AMXTileF32" }
    ],
    "assemblyFormat": "$lhs `,` $rhs `,` $acc attr-dict `:` qualified(type($lhs)) `,` qualified(type($rhs)) `,` qualified(type($acc))"
  },
  {
    "name": "amx.tile_muli",
    "summary": "tile multiplication operation (integer)",
    "description": "Multiplies a \"m x k\" tile with a \"k x n\" tile and accumulates the results\n    into a \"m x n\" destination tile. Supports all \"si32 <- s/ui8 x s/ui8\"\n    combinations (4 bytes packed into dwords in the columns of both the\n    source operand tiles; the zero or sign extension is specified with\n    the attributes and default to sign extended).\n    \n    The operation is eventually lowered into one of the \"tdpbssd\",\n    \"tdpbsud\", \"tdpbusd\", or \"tdpbuud\" instructions with the corresponding\n    tile configuration.\n\n    Example:\n\n    ```mlir\n      %0 = amx.tile_muli %a zext, %b zext, %c\n        : !amx.tile<16x64xi8>, !amx.tile<16x64xi8>, !amx.tile<16x16xi32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "AMXTileI8" },
      { "name": "rhs", "type": "AMXTileI8" },
      { "name": "acc", "type": "AMXTileI32" }
    ],
    "outputs": [
      { "name": "res", "type": "AMXTileI32" }
    ],
    "attributes": [
      { "name": "isZextLhs", "type": "UnitAttr" },
      { "name": "isZextRhs", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$lhs (`zext` $isZextLhs^)? `,` $rhs (`zext` $isZextRhs^)? `,` $acc attr-dict `:` qualified(type($lhs)) `,` qualified(type($rhs)) `,` qualified(type($acc))"
  },
  {
    "name": "amx.tile_store",
    "summary": "tile store operation",
    "description": "Stores a tile to memory defined by a `base` and `indices`, with the\n    shape defined by the 2-dim vector type of the value.\n    The tile's rows are written contiguously to the buffer starting at\n    the `base`. For each tile row, the `base` is incremented by `stride`\n    number of elements.\n\n    The tile is stored using the following indexing scheme:\n\n    ```\n    for row in enumerate(tile_rows):\n      mem_row = base[i0, i1, ..., iN + row * stride]\n      for col in enumerate(tile_cols):\n        mem_row[col] = tile[row, col]\n    ```\n\n    If the `stride` is not provided, then the `base` buffer must be at least\n    2-dimensional, and the `stride` is automatically inferred and corresponds\n    to the stride of the buffer's second innermost dimension.\n\n    The operation is eventually lowered into the \"tilestored\" instruction\n    with the corresponding tile configuration.\n\n    Example:\n\n    ```mlir\n      // Tile store to a 2-D memref with implicit stride.\n      amx.tile_store %arg1[%c0, %c0], %0 : memref<?x?xi8>, !amx.tile<16x64xi8>\n\n      // Tile store to a 1-D memref with explicit stride.\n      amx.tile_store %arg1[%c0], %0, %stride : memref<?xi8>, !amx.tile<16x64xi8>\n    ```",
    "inputs": [
      { "name": "base", "type": "Arg" },
      { "name": "indices", "type": "Variadic" },
      { "name": "val", "type": "AnyAMXTile" },
      { "name": "stride", "type": "Optional" }
    ],
    "assemblyFormat": "$base `[` $indices `]` `,` $val (`,` $stride^ )?attr-dict `:` type($base) `,` qualified(type($val))"
  },
  {
    "name": "amx.tile_zero",
    "summary": "tile zero operation",
    "description": "Zeroes the destination tile, with the shape defined by the 2-dim\n    vector type of the result.\n    \n    The operation is eventually lowered into the \"tilezero\" instruction\n    with the corresponding tile configuration.\n    \n    With the write memory effect, each `amx.tile_zero` operation serves as\n    a compilation hint to use a separate tile register.\n\n    Example:\n\n    ```mlir\n      %0 = amx.tile_zero : !amx.tile<16x16xbf16>\n    ```",
    "outputs": [
      { "name": "res", "type": "AnyAMXTile" }
    ],
    "assemblyFormat": "attr-dict `:` qualified(type($res))"
  },
  {
    "name": "arith.addf",
    "summary": "floating point addition operation",
    "description": "The `addf` operation takes two operands and returns one result, each of\n    these is required to be the same type. This type may be a floating point\n    scalar type, a vector whose element type is a floating point type, or a\n    floating point tensor.\n\n    Example:\n\n    ```mlir\n    // Scalar addition.\n    %a = arith.addf %b, %c : f64\n\n    // SIMD vector addition, e.g. for Intel SSE.\n    %f = arith.addf %g, %h : vector<4xf32>\n\n    // Tensor addition.\n    %x = arith.addf %y, %z : tensor<4x?xbf16>\n    ```\n\n    TODO: In the distant future, this will accept optional attributes for fast\n    math, contraction, rounding mode, and other controls.",
    "assemblyFormat": "$lhs `,` $rhs (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "arith.addi",
    "summary": "integer addition operation",
    "description": "Performs N-bit addition on the operands. The operands are interpreted as\n    unsigned bitvectors. The result is represented by a bitvector containing the\n    mathematical value of the addition modulo 2^n, where `n` is the bitwidth.\n    Because `arith` integers use a two's complement representation, this operation\n    is applicable on both signed and unsigned integer operands.\n\n    The `addi` operation takes two operands and returns one result, each of\n    these is required to be the same type. This type may be an integer scalar type,\n    a vector whose element type is integer, or a tensor of integers.\n\n    This op supports `nuw`/`nsw` overflow flags which stands for\n    \"No Unsigned Wrap\" and \"No Signed Wrap\", respectively. If the `nuw` and/or\n    `nsw` flags are present, and an unsigned/signed overflow occurs\n    (respectively), the result is poison.\n\n    Example:\n\n    ```mlir\n    // Scalar addition.\n    %a = arith.addi %b, %c : i64\n\n    // Scalar addition with overflow flags.\n    %a = arith.addi %b, %c overflow<nsw, nuw> : i64\n\n    // SIMD vector element-wise addition.\n    %f = arith.addi %g, %h : vector<4xi32>\n\n    // Tensor element-wise addition.\n    %x = arith.addi %y, %z : tensor<4x?xi8>\n    ```",
    "assemblyFormat": "$lhs `,` $rhs (`overflow` `` $overflowFlags^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "arith.addui_extended",
    "summary": "extended unsigned integer addition operation returning sum and overflow bit",
    "description": "Performs (N+1)-bit addition on zero-extended operands. Returns two results:\n    the N-bit sum (same type as both operands), and the overflow bit\n    (boolean-like), where `1` indicates unsigned addition overflow, while `0`\n    indicates no overflow.\n\n    Example:\n\n    ```mlir\n    // Scalar addition.\n    %sum, %overflow = arith.addui_extended %b, %c : i64, i1\n\n    // Vector element-wise addition.\n    %d:2 = arith.addui_extended %e, %f : vector<4xi32>, vector<4xi1>\n\n    // Tensor element-wise addition.\n    %x:2 = arith.addui_extended %y, %z : tensor<4x?xi8>, tensor<4x?xi1>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "SignlessIntegerOrIndexLike" },
      { "name": "rhs", "type": "SignlessIntegerOrIndexLike" }
    ],
    "outputs": [
      { "name": "sum", "type": "SignlessIntegerOrIndexLike" },
      { "name": "overflow", "type": "BoolLike" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($sum) `,` type($overflow)"
  },
  {
    "name": "arith.andi",
    "summary": "integer binary and",
    "description": "The `andi` operation takes two operands and returns one result, each of\n    these is required to be the same type. This type may be an integer scalar\n    type, a vector whose element type is integer, or a tensor of integers. It\n    has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar integer bitwise and.\n    %a = arith.andi %b, %c : i64\n\n    // SIMD vector element-wise bitwise integer and.\n    %f = arith.andi %g, %h : vector<4xi32>\n\n    // Tensor element-wise bitwise integer and.\n    %x = arith.andi %y, %z : tensor<4x?xi8>\n    ```",
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($result)"
  },
  {
    "name": "arith.bitcast",
    "summary": "bitcast between values of equal bit width",
    "description": "Bitcast an integer or floating point value to an integer or floating point\n    value of equal bit width. When operating on vectors, casts elementwise.\n\n    Note that this implements a logical bitcast independent of target\n    endianness. This allows constant folding without target information and is\n    consitent with the bitcast constant folders in LLVM (see\n    https://github.com/llvm/llvm-project/blob/18c19414eb/llvm/lib/IR/ConstantFold.cpp#L168)\n    For targets where the source and target type have the same endianness (which\n    is the standard), this cast will also change no bits at runtime, but it may\n    still require an operation, for example if the machine has different\n    floating point and integer register files. For targets that have a different\n    endianness for the source and target types (e.g. float is big-endian and\n    integer is little-endian) a proper lowering would add operations to swap the\n    order of words in addition to the bitcast.",
    "assemblyFormat": "$in attr-dict `:` type($in) `to` type($out)"
  },
  {
    "name": "arith.ceildivsi",
    "summary": "signed ceil integer division operation",
    "description": "Signed integer division. Rounds towards positive infinity, i.e. `7 / -2 = -3`.\n\n    Divison by zero, or signed division overflow (minimum value divided by -1)\n    is undefined behavior. When applied to `vector` and `tensor` values, the\n    behavior is undefined if _any_ of its elements are divided by zero or has a\n    signed division overflow.\n\n    Example:\n\n    ```mlir\n    // Scalar signed integer division.\n    %a = arith.ceildivsi %b, %c : i64\n    ```",
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($result)"
  },
  {
    "name": "arith.ceildivui",
    "summary": "unsigned ceil integer division operation",
    "description": "Unsigned integer division. Rounds towards positive infinity. Treats the\n    leading bit as the most significant, i.e. for `i16` given two's complement\n    representation, `6 / -2 = 6 / (2^16 - 2) = 1`.\n\n    Division by zero is undefined behavior. When applied to `vector` and\n    `tensor` values, the behavior is undefined if _any_ elements are divided by\n    zero.\n\n    Example:\n\n    ```mlir\n    // Scalar unsigned integer division.\n    %a = arith.ceildivui %b, %c : i64\n    ```",
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($result)"
  },
  {
    "name": "arith.cmpf",
    "summary": "floating-point comparison operation",
    "description": "The `cmpf` operation compares its two operands according to the float\n    comparison rules and the predicate specified by the respective attribute.\n    The predicate defines the type of comparison: (un)orderedness, (in)equality\n    and signed less/greater than (or equal to) as well as predicates that are\n    always true or false.  The operands must have the same type, and this type\n    must be a float type, or a vector or tensor thereof.  The result is an i1,\n    or a vector/tensor thereof having the same shape as the inputs. Unlike cmpi,\n    the operands are always treated as signed. The u prefix indicates\n    *unordered* comparison, not unsigned comparison, so \"une\" means unordered or\n    not equal. For the sake of readability by humans, custom assembly form for\n    the operation uses a string-typed attribute for the predicate.  The value of\n    this attribute corresponds to lower-cased name of the predicate constant,\n    e.g., \"one\" means \"ordered not equal\".  The string representation of the\n    attribute is merely a syntactic sugar and is converted to an integer\n    attribute by the parser.\n\n    Example:\n\n    ```mlir\n    %r1 = arith.cmpf oeq, %0, %1 : f32\n    %r2 = arith.cmpf ult, %0, %1 : tensor<42x42xf64>\n    %r3 = \"arith.cmpf\"(%0, %1) {predicate: 0} : (f8, f8) -> i1\n    ```",
    "inputs": [
      { "name": "lhs", "type": "FloatLike" },
      { "name": "rhs", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "BoolLike" }
    ],
    "attributes": [
      { "name": "predicate", "type": "Arith_CmpFPredicateAttr" },
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$predicate `,` $lhs `,` $rhs (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($lhs)"
  },
  {
    "name": "arith.cmpi",
    "summary": "integer comparison operation",
    "description": "The `cmpi` operation is a generic comparison for integer-like types. Its two\n    arguments can be integers, vectors or tensors thereof as long as their types\n    match. The operation produces an i1 for the former case, a vector or a\n    tensor of i1 with the same shape as inputs in the other cases.\n\n    Its first argument is an attribute that defines which type of comparison is\n    performed. The following comparisons are supported:\n\n    -   equal (mnemonic: `\"eq\"`; integer value: `0`)\n    -   not equal (mnemonic: `\"ne\"`; integer value: `1`)\n    -   signed less than (mnemonic: `\"slt\"`; integer value: `2`)\n    -   signed less than or equal (mnemonic: `\"sle\"`; integer value: `3`)\n    -   signed greater than (mnemonic: `\"sgt\"`; integer value: `4`)\n    -   signed greater than or equal (mnemonic: `\"sge\"`; integer value: `5`)\n    -   unsigned less than (mnemonic: `\"ult\"`; integer value: `6`)\n    -   unsigned less than or equal (mnemonic: `\"ule\"`; integer value: `7`)\n    -   unsigned greater than (mnemonic: `\"ugt\"`; integer value: `8`)\n    -   unsigned greater than or equal (mnemonic: `\"uge\"`; integer value: `9`)\n\n    The result is `1` if the comparison is true and `0` otherwise. For vector or\n    tensor operands, the comparison is performed elementwise and the element of\n    the result indicates whether the comparison is true for the operand elements\n    with the same indices as those of the result.\n\n    Note: while the custom assembly form uses strings, the actual underlying\n    attribute has integer type (or rather enum class in C++ code) as seen from\n    the generic assembly form. String literals are used to improve readability\n    of the IR by humans.\n\n    This operation only applies to integer-like operands, but not floats. The\n    main reason being that comparison operations have diverging sets of\n    attributes: integers require sign specification while floats require various\n    floating point-related particularities, e.g., `-ffast-math` behavior,\n    IEEE754 compliance, etc\n    ([rationale](../Rationale/Rationale.md#splitting-floating-point-vs-integer-operations)).\n    The type of comparison is specified as attribute to avoid introducing ten\n    similar operations, taking into account that they are often implemented\n    using the same operation downstream\n    ([rationale](../Rationale/Rationale.md#specifying-comparison-kind-as-attribute)). The\n    separation between signed and unsigned order comparisons is necessary\n    because of integers being signless. The comparison operation must know how\n    to interpret values with the foremost bit being set: negatives in two's\n    complement or large positives\n    ([rationale](../Rationale/Rationale.md#specifying-sign-in-integer-comparison-operations)).\n\n    Example:\n\n    ```mlir\n    // Custom form of scalar \"signed less than\" comparison.\n    %x = arith.cmpi slt, %lhs, %rhs : i32\n\n    // Generic form of the same operation.\n    %x = \"arith.cmpi\"(%lhs, %rhs) {predicate = 2 : i64} : (i32, i32) -> i1\n\n    // Custom form of vector equality comparison.\n    %x = arith.cmpi eq, %lhs, %rhs : vector<4xi64>\n\n    // Generic form of the same operation.\n    %x = \"arith.cmpi\"(%lhs, %rhs) {predicate = 0 : i64}\n        : (vector<4xi64>, vector<4xi64>) -> vector<4xi1>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "SignlessIntegerOrIndexLike" },
      { "name": "rhs", "type": "SignlessIntegerOrIndexLike" }
    ],
    "outputs": [
      { "name": "result", "type": "BoolLike" }
    ],
    "attributes": [
      { "name": "predicate", "type": "Arith_CmpIPredicateAttr" }
    ],
    "assemblyFormat": "$predicate `,` $lhs `,` $rhs attr-dict `:` type($lhs)"
  },
  {
    "name": "arith.constant",
    "summary": "integer or floating point constant",
    "description": "The `constant` operation produces an SSA value equal to some integer or\n    floating-point constant specified by an attribute. This is the way MLIR\n    forms simple integer and floating point constants.\n\n    Example:\n\n    ```\n    // Integer constant\n    %1 = arith.constant 42 : i32\n\n    // Equivalent generic form\n    %1 = \"arith.constant\"() {value = 42 : i32} : () -> i32\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "value", "type": "TypedAttrInterface" }
    ],
    "assemblyFormat": "attr-dict $value"
  },
  {
    "name": "arith.divf",
    "summary": "floating point division operation",
    "assemblyFormat": "$lhs `,` $rhs (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "arith.divsi",
    "summary": "signed integer division operation",
    "description": "Signed integer division. Rounds towards zero. Treats the leading bit as\n    sign, i.e. `6 / -2 = -3`.\n\n    Divison by zero, or signed division overflow (minimum value divided by -1)\n    is undefined behavior. When applied to `vector` and `tensor` values, the\n    behavior is undefined if _any_ of its elements are divided by zero or has a\n    signed division overflow.\n\n    Example:\n\n    ```mlir\n    // Scalar signed integer division.\n    %a = arith.divsi %b, %c : i64\n\n    // SIMD vector element-wise division.\n    %f = arith.divsi %g, %h : vector<4xi32>\n\n    // Tensor element-wise integer division.\n    %x = arith.divsi %y, %z : tensor<4x?xi8>\n    ```",
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($result)"
  },
  {
    "name": "arith.divui",
    "summary": "unsigned integer division operation",
    "description": "Unsigned integer division. Rounds towards zero. Treats the leading bit as\n    the most significant, i.e. for `i16` given two's complement representation,\n    `6 / -2 = 6 / (2^16 - 2) = 0`.\n\n    Division by zero is undefined behavior. When applied to `vector` and\n    `tensor` values, the behavior is undefined if _any_ elements are divided by\n    zero.\n\n    Example:\n\n    ```mlir\n    // Scalar unsigned integer division.\n    %a = arith.divui %b, %c : i64\n\n    // SIMD vector element-wise division.\n    %f = arith.divui %g, %h : vector<4xi32>\n\n    // Tensor element-wise integer division.\n    %x = arith.divui %y, %z : tensor<4x?xi8>\n    ```",
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($result)"
  },
  {
    "name": "arith.extf",
    "summary": "cast from floating-point to wider floating-point",
    "description": "Cast a floating-point value to a larger floating-point-typed value.\n    The destination type must to be strictly wider than the source type.\n    When operating on vectors, casts elementwise.",
    "inputs": [
      { "name": "in", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "out", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$in (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($in) `to` type($out)"
  },
  {
    "name": "arith.extsi",
    "summary": "integer sign extension operation",
    "description": "The integer sign extension operation takes an integer input of\n    width M and an integer destination type of width N. The destination\n    bit-width must be larger than the input bit-width (N > M).\n    The top-most (N - M) bits of the output are filled with copies\n    of the most-significant bit of the input.\n\n    Example:\n\n    ```mlir\n    %1 = arith.constant 5 : i3      // %1 is 0b101\n    %2 = arith.extsi %1 : i3 to i6  // %2 is 0b111101\n    %3 = arith.constant 2 : i3      // %3 is 0b010\n    %4 = arith.extsi %3 : i3 to i6  // %4 is 0b000010\n\n    %5 = arith.extsi %0 : vector<2 x i32> to vector<2 x i64>\n    ```",
    "assemblyFormat": "$in attr-dict `:` type($in) `to` type($out)"
  },
  {
    "name": "arith.extui",
    "summary": "integer zero extension operation",
    "description": "The integer zero extension operation takes an integer input of\n    width M and an integer destination type of width N. The destination\n    bit-width must be larger than the input bit-width (N > M).\n    The top-most (N - M) bits of the output are filled with zeros.\n\n    Example:\n\n    ```mlir\n      %1 = arith.constant 5 : i3      // %1 is 0b101\n      %2 = arith.extui %1 : i3 to i6  // %2 is 0b000101\n      %3 = arith.constant 2 : i3      // %3 is 0b010\n      %4 = arith.extui %3 : i3 to i6  // %4 is 0b000010\n\n      %5 = arith.extui %0 : vector<2 x i32> to vector<2 x i64>\n    ```",
    "assemblyFormat": "$in attr-dict `:` type($in) `to` type($out)"
  },
  {
    "name": "arith.floordivsi",
    "summary": "signed floor integer division operation",
    "description": "Signed integer division. Rounds towards negative infinity, i.e. `5 / -2 = -3`.\n\n    Divison by zero, or signed division overflow (minimum value divided by -1)\n    is undefined behavior. When applied to `vector` and `tensor` values, the\n    behavior is undefined if _any_ of its elements are divided by zero or has a\n    signed division overflow.\n\n    Example:\n\n    ```mlir\n    // Scalar signed integer division.\n    %a = arith.floordivsi %b, %c : i64\n\n    ```",
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($result)"
  },
  {
    "name": "arith.fptosi",
    "summary": "cast from floating-point type to integer type",
    "description": "Cast from a value interpreted as floating-point to the nearest (rounding\n    towards zero) signed integer value. When operating on vectors, casts\n    elementwise.",
    "assemblyFormat": "$in attr-dict `:` type($in) `to` type($out)"
  },
  {
    "name": "arith.fptoui",
    "summary": "cast from floating-point type to integer type",
    "description": "Cast from a value interpreted as floating-point to the nearest (rounding\n    towards zero) unsigned integer value. When operating on vectors, casts\n    elementwise.",
    "assemblyFormat": "$in attr-dict `:` type($in) `to` type($out)"
  },
  {
    "name": "arith.index_cast",
    "summary": "cast between index and integer types",
    "description": "Casts between scalar or vector integers and corresponding 'index' scalar or\n    vectors. Index is an integer of platform-specific bit width. If casting to\n    a wider integer, the value is sign-extended. If casting to a narrower\n    integer, the value is truncated.",
    "assemblyFormat": "$in attr-dict `:` type($in) `to` type($out)"
  },
  {
    "name": "arith.index_castui",
    "summary": "unsigned cast between index and integer types",
    "description": "Casts between scalar or vector integers and corresponding 'index' scalar or\n    vectors. Index is an integer of platform-specific bit width. If casting to\n    a wider integer, the value is zero-extended. If casting to a narrower\n    integer, the value is truncated.",
    "assemblyFormat": "$in attr-dict `:` type($in) `to` type($out)"
  },
  {
    "name": "arith.maximumf",
    "summary": "floating-point maximum operation",
    "description": "Returns the maximum of the two arguments, treating -0.0 as less than +0.0.\n    If one of the arguments is NaN, then the result is also NaN.\n\n    Example:\n\n    ```mlir\n    // Scalar floating-point maximum.\n    %a = arith.maximumf %b, %c : f64\n    ```",
    "assemblyFormat": "$lhs `,` $rhs (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "arith.maxnumf",
    "summary": "floating-point maximum operation",
    "description": "Returns the maximum of the two arguments.\n    If the arguments are -0.0 and +0.0, then the result is either of them.\n    If one of the arguments is NaN, then the result is the other argument.\n\n    Example:\n\n    ```mlir\n    // Scalar floating-point maximum.\n    %a = arith.maxnumf %b, %c : f64\n    ```",
    "assemblyFormat": "$lhs `,` $rhs (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "arith.maxsi",
    "summary": "signed integer maximum operation",
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($result)"
  },
  {
    "name": "arith.maxui",
    "summary": "unsigned integer maximum operation",
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($result)"
  },
  {
    "name": "arith.minimumf",
    "summary": "floating-point minimum operation",
    "description": "Returns the minimum of the two arguments, treating -0.0 as less than +0.0.\n    If one of the arguments is NaN, then the result is also NaN.\n\n    Example:\n\n    ```mlir\n    // Scalar floating-point minimum.\n    %a = arith.minimumf %b, %c : f64\n    ```",
    "assemblyFormat": "$lhs `,` $rhs (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "arith.minnumf",
    "summary": "floating-point minimum operation",
    "description": "Returns the minimum of the two arguments.\n    If the arguments are -0.0 and +0.0, then the result is either of them.\n    If one of the arguments is NaN, then the result is the other argument.\n\n    Example:\n\n    ```mlir\n    // Scalar floating-point minimum.\n    %a = arith.minnumf %b, %c : f64\n    ```",
    "assemblyFormat": "$lhs `,` $rhs (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "arith.minsi",
    "summary": "signed integer minimum operation",
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($result)"
  },
  {
    "name": "arith.minui",
    "summary": "unsigned integer minimum operation",
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($result)"
  },
  {
    "name": "arith.mulf",
    "summary": "floating point multiplication operation",
    "description": "The `mulf` operation takes two operands and returns one result, each of\n    these is required to be the same type. This type may be a floating point\n    scalar type, a vector whose element type is a floating point type, or a\n    floating point tensor.\n\n    Example:\n\n    ```mlir\n    // Scalar multiplication.\n    %a = arith.mulf %b, %c : f64\n\n    // SIMD pointwise vector multiplication, e.g. for Intel SSE.\n    %f = arith.mulf %g, %h : vector<4xf32>\n\n    // Tensor pointwise multiplication.\n    %x = arith.mulf %y, %z : tensor<4x?xbf16>\n    ```\n\n    TODO: In the distant future, this will accept optional attributes for fast\n    math, contraction, rounding mode, and other controls.",
    "assemblyFormat": "$lhs `,` $rhs (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "arith.muli",
    "summary": "Integer multiplication operation.",
    "description": "Performs N-bit multiplication on the operands. The operands are interpreted as\n    unsigned bitvectors. The result is represented by a bitvector containing the\n    mathematical value of the multiplication modulo 2^n, where `n` is the bitwidth.\n    Because `arith` integers use a two's complement representation, this operation is\n    applicable on both signed and unsigned integer operands.\n\n    The `muli` operation takes two operands and returns one result, each of\n    these is required to be the same type. This type may be an integer scalar type,\n    a vector whose element type is integer, or a tensor of integers.\n\n    This op supports `nuw`/`nsw` overflow flags which stands for\n    \"No Unsigned Wrap\" and \"No Signed Wrap\", respectively. If the `nuw` and/or\n    `nsw` flags are present, and an unsigned/signed overflow occurs\n    (respectively), the result is poison.\n\n    Example:\n\n    ```mlir\n    // Scalar multiplication.\n    %a = arith.muli %b, %c : i64\n\n    // Scalar multiplication with overflow flags.\n    %a = arith.muli %b, %c overflow<nsw, nuw> : i64\n\n    // SIMD vector element-wise multiplication.\n    %f = arith.muli %g, %h : vector<4xi32>\n\n    // Tensor element-wise multiplication.\n    %x = arith.muli %y, %z : tensor<4x?xi8>\n    ```",
    "assemblyFormat": "$lhs `,` $rhs (`overflow` `` $overflowFlags^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "arith.mulsi_extended",
    "summary": "extended signed integer multiplication operation",
    "description": "Performs (2*N)-bit multiplication on sign-extended operands. Returns two\n    N-bit results: the low and the high halves of the product. The low half has\n    the same value as the result of regular multiplication `arith.muli` with\n    the same operands.\n\n    Example:\n\n    ```mlir\n    // Scalar multiplication.\n    %low, %high = arith.mulsi_extended %a, %b : i32\n\n    // Vector element-wise multiplication.\n    %c:2 = arith.mulsi_extended %d, %e : vector<4xi32>\n\n    // Tensor element-wise multiplication.\n    %x:2 = arith.mulsi_extended %y, %z : tensor<4x?xi8>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "SignlessIntegerOrIndexLike" },
      { "name": "rhs", "type": "SignlessIntegerOrIndexLike" }
    ],
    "outputs": [
      { "name": "low", "type": "SignlessIntegerOrIndexLike" },
      { "name": "high", "type": "SignlessIntegerOrIndexLike" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($lhs)"
  },
  {
    "name": "arith.mului_extended",
    "summary": "extended unsigned integer multiplication operation",
    "description": "Performs (2*N)-bit multiplication on zero-extended operands. Returns two\n    N-bit results: the low and the high halves of the product. The low half has\n    the same value as the result of regular multiplication `arith.muli` with\n    the same operands.\n\n    Example:\n\n    ```mlir\n    // Scalar multiplication.\n    %low, %high = arith.mului_extended %a, %b : i32\n\n    // Vector element-wise multiplication.\n    %c:2 = arith.mului_extended %d, %e : vector<4xi32>\n\n    // Tensor element-wise multiplication.\n    %x:2 = arith.mului_extended %y, %z : tensor<4x?xi8>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "SignlessIntegerOrIndexLike" },
      { "name": "rhs", "type": "SignlessIntegerOrIndexLike" }
    ],
    "outputs": [
      { "name": "low", "type": "SignlessIntegerOrIndexLike" },
      { "name": "high", "type": "SignlessIntegerOrIndexLike" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($lhs)"
  },
  {
    "name": "arith.negf",
    "summary": "floating point negation",
    "description": "The `negf` operation computes the negation of a given value. It takes one\n    operand and returns one result of the same type. This type may be a float\n    scalar type, a vector whose element type is float, or a tensor of floats.\n    It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar negation value.\n    %a = arith.negf %b : f64\n\n    // SIMD vector element-wise negation value.\n    %f = arith.negf %g : vector<4xf32>\n\n    // Tensor element-wise negation value.\n    %x = arith.negf %y : tensor<4x?xf8>\n    ```",
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "arith.ori",
    "summary": "integer binary or",
    "description": "The `ori` operation takes two operands and returns one result, each of these\n    is required to be the same type. This type may be an integer scalar type, a\n    vector whose element type is integer, or a tensor of integers. It has no\n    standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar integer bitwise or.\n    %a = arith.ori %b, %c : i64\n\n    // SIMD vector element-wise bitwise integer or.\n    %f = arith.ori %g, %h : vector<4xi32>\n\n    // Tensor element-wise bitwise integer or.\n    %x = arith.ori %y, %z : tensor<4x?xi8>\n    ```",
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($result)"
  },
  {
    "name": "arith.remf",
    "summary": "floating point division remainder operation",
    "description": "Returns the floating point division remainder.\n    The remainder has the same sign as the dividend (lhs operand).",
    "assemblyFormat": "$lhs `,` $rhs (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "arith.remsi",
    "summary": "signed integer division remainder operation",
    "description": "Signed integer division remainder. Treats the leading bit as sign, i.e. `6 %\n    -2 = 0`.\n\n    Division by zero is undefined behavior. When applied to `vector` and\n    `tensor` values, the behavior is undefined if _any_ elements are divided by\n    zero.\n\n    Example:\n\n    ```mlir\n    // Scalar signed integer division remainder.\n    %a = arith.remsi %b, %c : i64\n\n    // SIMD vector element-wise division remainder.\n    %f = arith.remsi %g, %h : vector<4xi32>\n\n    // Tensor element-wise integer division remainder.\n    %x = arith.remsi %y, %z : tensor<4x?xi8>\n    ```",
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($result)"
  },
  {
    "name": "arith.remui",
    "summary": "unsigned integer division remainder operation",
    "description": "Unsigned integer division remainder. Treats the leading bit as the most\n    significant, i.e. for `i16`, `6 % -2 = 6 % (2^16 - 2) = 6`.\n\n    Division by zero is undefined behavior. When applied to `vector` and\n    `tensor` values, the behavior is undefined if _any_ elements are divided by\n    zero.\n\n    Example:\n\n    ```mlir\n    // Scalar unsigned integer division remainder.\n    %a = arith.remui %b, %c : i64\n\n    // SIMD vector element-wise division remainder.\n    %f = arith.remui %g, %h : vector<4xi32>\n\n    // Tensor element-wise integer division remainder.\n    %x = arith.remui %y, %z : tensor<4x?xi8>\n    ```",
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($result)"
  },
  {
    "name": "arith.scaling_extf",
    "summary": "Upcasts input floats using provided scales values following OCP MXFP Spec",
    "description": "This operation upcasts input floating-point values using provided scale\n    values. It expects both scales and the input operand to be of the same shape,\n    making the operation elementwise. Scales are usually calculated per block\n    following the OCP MXFP spec as described in https://arxiv.org/abs/2310.10537.\n\n    If scales are calculated per block where blockSize != 1, then scales may\n    require broadcasting to make this operation elementwise. For example, let's\n    say the input is of shape `<dim1 x dim2 x ... dimN>`. Given blockSize != 1 and\n    assuming quantization happens on the last axis, the input can be reshaped to\n    `<dim1 x dim2 x ... (dimN/blockSize) x blockSize>`. Scales will be calculated\n    per block on the last axis. Therefore, scales will be of shape\n    `<dim1 x dim2 x ... (dimN/blockSize) x 1>`. Scales could also be of some other\n    shape as long as it is broadcast compatible with the input, e.g.,\n    `<1 x 1 x ... (dimN/blockSize) x 1>`.\n\n    In this example, before calling into `arith.scaling_extf`, scales must be\n    broadcasted to `<dim1 x dim2 x dim3 ... (dimN/blockSize) x blockSize>`. Note\n    that there could be multiple quantization axes. Internally,\n    `arith.scaling_extf` would perform the following:\n\n    ```mlir\n    // Cast scale to result type.\n    %0 = arith.truncf %1 : f32 to f8E8M0FNU\n    %1 = arith.extf %0 : f8E8M0FNU to f16\n\n    // Cast input to result type.\n    %2 = arith.extf %3 : f4E2M1FN to f16\n\n    // Perform scaling\n    %3 = arith.mulf %2, %1 : f16\n    ```\n    It propagates NaN values. Therefore, if either scale or the input element\n    contains NaN, then the output element value will also be a NaN.\n\n    Example:\n\n    ```mlir\n    // Upcast from f4E2M1FN to f32.\n    %a = arith.scaling_extf %b, %c : f4E2M1FN, f8E8M0FNU to f32\n\n    // Element-wise upcast with broadcast (blockSize = 32).\n    %f = vector.broadcast %g : vector<1xf8E8M0FNU> to vector<32xf8E8M0FNU>\n    %h = arith.scaling_extf %i, %f : vector<32xf4E2M1FN>, vector<32xf8E8M0FNU> to vector<32xbf16>\n    ```",
    "assemblyFormat": "$in `,` $scale (`fastmath` `` $fastmath^)? attr-dict `:`\n      type($in) `,` type($scale) `to` type($out)"
  },
  {
    "name": "arith.scaling_truncf",
    "summary": "Downcasts input floating point values using provided scales values following OCP MXFP Spec",
    "description": "This operation downcasts input using the provided scale values. It expects\n    both scales and the input operand to be of the same shape and, therefore,\n    makes the operation elementwise. Scales are usually calculated per block\n    following the OCP MXFP spec as described in https://arxiv.org/abs/2310.10537.\n    Users are required to normalize and clamp the scales as necessary before calling\n    passing them to this operation.  OCP MXFP spec also does the flushing of denorms\n    on the input operand, which should be handled during lowering by passing appropriate\n    fastMath flag to this operation.\n\n    If scales are calculated per block where blockSize != 1, scales may require\n    broadcasting to make this operation elementwise. For example, let's say the\n    input is of shape `<dim1 x dim2 x ... dimN>`. Given blockSize != 1 and\n    assuming quantization happens on the last axis, the input can be reshaped to\n    `<dim1 x dim2 x ... (dimN/blockSize) x blockSize>`. Scales will be calculated\n    per block on the last axis. Therefore, scales will be of shape\n    `<dim1 x dim2 x ... (dimN/blockSize) x 1>`. Scales could also be of some other\n    shape as long as it is broadcast compatible with the input, e.g.,\n    `<1 x 1 x ... (dimN/blockSize) x 1>`.\n\n    In this example, before calling into `arith.scaling_truncf`, scales must be\n    broadcasted to `<dim1 x dim2 x dim3 ... (dimN/blockSize) x blockSize>`. Note\n    that there could be multiple quantization axes. Internally,\n    `arith.scaling_truncf` would perform the following:\n\n    ```mlir\n    // Cast scale to input type.\n    %0 = arith.truncf %1 : f32 to f8E8M0FNU\n    %1 = arith.extf %0 : f8E8M0FNU to f16\n\n    // Perform scaling.\n    %3 = arith.divf %2, %1 : f16\n\n    // Cast to result type.\n    %4 = arith.truncf %3 : f16 to f4E2M1FN\n    ```\n\n    Example:\n\n    ```mlir\n    // Downcast from f32 to f4E2M1FN.\n    %a = arith.scaling_truncf %b, %c : f32, f8E8M0FNU to f4E2M1FN\n\n    // Element-wise downcast with broadcast (blockSize = 32).\n    %f = vector.broadcast %g : vector<1xf8E8M0FNU> to vector<32xf8E8M0FNU>\n    %h = arith.scaling_truncf %i, %f : vector<32xbf16>, vector<32xf8E8M0FNU> to vector<32xf4E2M1FN>\n    ```",
    "assemblyFormat": "$in `,` $scale ($roundingmode^)? (`fastmath` `` $fastmath^)? attr-dict `:`\n      type($in) `,` type($scale) `to` type($out)"
  },
  {
    "name": "arith.select",
    "summary": "select operation",
    "description": "The `arith.select` operation chooses one value based on a binary condition\n    supplied as its first operand.\n\n    If the value of the first operand (the condition) is `1`, then the second\n    operand is returned, and the third operand is ignored, even if it was poison.\n\n    If the value of the first operand (the condition) is `0`, then the third\n    operand is returned, and the second operand is ignored, even if it was poison.\n\n    If the value of the first operand (the condition) is poison, then the\n    operation returns poison.\n\n    The operation applies to vectors and tensors elementwise given the _shape_\n    of all operands is identical. The choice is made for each element\n    individually based on the value at the same position as the element in the\n    condition operand. If an i1 is provided as the condition, the entire vector\n    or tensor is chosen.\n\n    Example:\n\n    ```mlir\n    // Custom form of scalar selection.\n    %x = arith.select %cond, %true, %false : i32\n\n    // Generic form of the same operation.\n    %x = \"arith.select\"(%cond, %true, %false) : (i1, i32, i32) -> i32\n\n    // Element-wise vector selection.\n    %vx = arith.select %vcond, %vtrue, %vfalse : vector<42xi1>, vector<42xf32>\n\n    // Full vector selection.\n    %vx = arith.select %cond, %vtrue, %vfalse : vector<42xf32>\n    ```",
    "inputs": [
      { "name": "condition", "type": "BoolLike" },
      { "name": "true_value", "type": "AnyType" },
      { "name": "false_value", "type": "AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ]
  },
  {
    "name": "arith.shli",
    "summary": "integer left-shift",
    "description": "The `shli` operation shifts the integer value of the first operand to the left\n    by the integer value of the second operand. The second operand is interpreted as\n    unsigned. The low order bits are filled with zeros. If the value of the second\n    operand is greater or equal than the bitwidth of the first operand, then the\n    operation returns poison.\n\n    This op supports `nuw`/`nsw` overflow flags which stands for\n    \"No Unsigned Wrap\" and \"No Signed Wrap\", respectively. If the `nuw` and/or\n    `nsw` flags are present, and an unsigned/signed overflow occurs\n    (respectively), the result is poison.\n\n    Example:\n\n    ```mlir\n    %1 = arith.constant 5 : i8  // %1 is 0b00000101\n    %2 = arith.constant 3 : i8\n    %3 = arith.shli %1, %2 : i8 // %3 is 0b00101000\n    %4 = arith.shli %1, %2 overflow<nsw, nuw> : i8\n    ```",
    "assemblyFormat": "$lhs `,` $rhs (`overflow` `` $overflowFlags^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "arith.shrsi",
    "summary": "signed integer right-shift",
    "description": "The `shrsi` operation shifts an integer value of the first operand to the right\n    by the value of the second operand. The first operand is interpreted as signed,\n    and the second operand is interpreter as unsigned. The high order bits in the\n    output are filled with copies of the most-significant bit of the shifted value\n    (which means that the sign of the value is preserved). If the value of the second\n    operand is greater or equal than bitwidth of the first operand, then the operation\n    returns poison.\n\n    Example:\n\n    ```mlir\n    %1 = arith.constant 160 : i8               // %1 is 0b10100000\n    %2 = arith.constant 3 : i8\n    %3 = arith.shrsi %1, %2 : (i8, i8) -> i8   // %3 is 0b11110100\n    %4 = arith.constant 96 : i8                   // %4 is 0b01100000\n    %5 = arith.shrsi %4, %2 : (i8, i8) -> i8   // %5 is 0b00001100\n    ```",
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($result)"
  },
  {
    "name": "arith.shrui",
    "summary": "unsigned integer right-shift",
    "description": "The `shrui` operation shifts an integer value of the first operand to the right\n    by the value of the second operand. The first operand is interpreted as unsigned,\n    and the second operand is interpreted as unsigned. The high order bits are always\n    filled with zeros. If the value of the second operand is greater or equal than the\n    bitwidth of the first operand, then the operation returns poison.\n\n    Example:\n\n    ```mlir\n    %1 = arith.constant 160 : i8               // %1 is 0b10100000\n    %2 = arith.constant 3 : i8\n    %3 = arith.shrui %1, %2 : (i8, i8) -> i8   // %3 is 0b00010100\n    ```",
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($result)"
  },
  {
    "name": "arith.sitofp",
    "summary": "cast from integer type to floating-point",
    "description": "Cast from a value interpreted as a signed integer to the corresponding\n    floating-point value. If the value cannot be exactly represented, it is\n    rounded using the default rounding mode. When operating on vectors, casts\n    elementwise.",
    "assemblyFormat": "$in attr-dict `:` type($in) `to` type($out)"
  },
  {
    "name": "arith.subf",
    "summary": "floating point subtraction operation",
    "description": "The `subf` operation takes two operands and returns one result, each of\n    these is required to be the same type. This type may be a floating point\n    scalar type, a vector whose element type is a floating point type, or a\n    floating point tensor.\n\n    Example:\n\n    ```mlir\n    // Scalar subtraction.\n    %a = arith.subf %b, %c : f64\n\n    // SIMD vector subtraction, e.g. for Intel SSE.\n    %f = arith.subf %g, %h : vector<4xf32>\n\n    // Tensor subtraction.\n    %x = arith.subf %y, %z : tensor<4x?xbf16>\n    ```\n\n    TODO: In the distant future, this will accept optional attributes for fast\n    math, contraction, rounding mode, and other controls.",
    "assemblyFormat": "$lhs `,` $rhs (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "arith.subi",
    "summary": "Integer subtraction operation.",
    "description": "Performs N-bit subtraction on the operands. The operands are interpreted as unsigned\n    bitvectors. The result is represented by a bitvector containing the mathematical\n    value of the subtraction modulo 2^n, where `n` is the bitwidth. Because `arith`\n    integers use a two's complement representation, this operation is applicable on\n    both signed and unsigned integer operands.\n\n    The `subi` operation takes two operands and returns one result, each of\n    these is required to be the same type. This type may be an integer scalar type,\n    a vector whose element type is integer, or a tensor of integers.\n\n    This op supports `nuw`/`nsw` overflow flags which stands for\n    \"No Unsigned Wrap\" and \"No Signed Wrap\", respectively. If the `nuw` and/or\n    `nsw` flags are present, and an unsigned/signed overflow occurs\n    (respectively), the result is poison.\n\n    Example:\n\n    ```mlir\n    // Scalar subtraction.\n    %a = arith.subi %b, %c : i64\n\n    // Scalar subtraction with overflow flags.\n    %a = arith.subi %b, %c overflow<nsw, nuw> : i64\n\n    // SIMD vector element-wise subtraction.\n    %f = arith.subi %g, %h : vector<4xi32>\n\n    // Tensor element-wise subtraction.\n    %x = arith.subi %y, %z : tensor<4x?xi8>\n    ```",
    "assemblyFormat": "$lhs `,` $rhs (`overflow` `` $overflowFlags^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "arith.truncf",
    "summary": "cast from floating-point to narrower floating-point",
    "description": "Truncate a floating-point value to a smaller floating-point-typed value.\n    The destination type must be strictly narrower than the source type.\n    If the value cannot be exactly represented, it is rounded using the\n    provided rounding mode or the default one if no rounding mode is provided.\n    When operating on vectors, casts elementwise.",
    "assemblyFormat": "$in ($roundingmode^)?\n                          (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($in) `to` type($out)"
  },
  {
    "name": "arith.trunci",
    "summary": "integer truncation operation",
    "description": "The integer truncation operation takes an integer input of\n    width M and an integer destination type of width N. The destination\n    bit-width must be smaller than the input bit-width (N < M).\n    The top-most (N - M) bits of the input are discarded.\n\n    This op supports `nuw`/`nsw` overflow flags which stands for \"No Unsigned\n    Wrap\" and \"No Signed Wrap\", respectively. If the nuw keyword is present,\n    and any of the truncated bits are non-zero, the result is a poison value.\n    If the nsw keyword is present, and any of the truncated bits are not the\n    same as the top bit of the truncation result, the result is a poison value.\n\n    Example:\n\n    ```mlir\n      // Scalar truncation.\n      %1 = arith.constant 21 : i5     // %1 is 0b10101\n      %2 = arith.trunci %1 : i5 to i4 // %2 is 0b0101\n      %3 = arith.trunci %1 : i5 to i3 // %3 is 0b101\n\n      // Vector truncation.\n      %4 = arith.trunci %0 : vector<2 x i32> to vector<2 x i16>\n\n      // Scalar truncation with overflow flags.\n      %5 = arith.trunci %a overflow<nsw, nuw> : i32 to i16\n    ```",
    "inputs": [
      { "name": "in", "type": "SignlessFixedWidthIntegerLike" }
    ],
    "outputs": [
      { "name": "out", "type": "SignlessFixedWidthIntegerLike" }
    ],
    "attributes": [
      { "name": "overflowFlags", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$in (`overflow` `` $overflowFlags^)? attr-dict\n    `:` type($in) `to` type($out)"
  },
  {
    "name": "arith.uitofp",
    "summary": "cast from unsigned integer type to floating-point",
    "description": "Cast from a value interpreted as unsigned integer to the corresponding\n    floating-point value. If the value cannot be exactly represented, it is\n    rounded using the default rounding mode. When operating on vectors, casts\n    elementwise.",
    "assemblyFormat": "$in attr-dict `:` type($in) `to` type($out)"
  },
  {
    "name": "arith.xori",
    "summary": "integer binary xor",
    "description": "The `xori` operation takes two operands and returns one result, each of\n    these is required to be the same type. This type may be an integer scalar\n    type, a vector whose element type is integer, or a tensor of integers. It\n    has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar integer bitwise xor.\n    %a = arith.xori %b, %c : i64\n\n    // SIMD vector element-wise bitwise integer xor.\n    %f = arith.xori %g, %h : vector<4xi32>\n\n    // Tensor element-wise bitwise integer xor.\n    %x = arith.xori %y, %z : tensor<4x?xi8>\n    ```",
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($result)"
  },
  {
    "name": "asuka.abs",
    "summary": "abs",
    "description": "abs",
    "inputs": [
      { "name": "operand", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.add",
    "summary": "add",
    "description": "add",
    "inputs": [
      { "name": "lhs", "type": "AS_AnyTensor" },
      { "name": "rhs", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.avg_pool",
    "summary": "asuka avgpool",
    "inputs": [
      { "name": "operand", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "attributes": [
      { "name": "kernel_size", "type": "DenseI64ArrayAttr" },
      { "name": "stride", "type": "DenseI64ArrayAttr" },
      { "name": "padding", "type": "DenseI64ArrayAttr" },
      { "name": "ceil_mode", "type": "BoolAttr" },
      { "name": "count_include_pad", "type": "BoolAttr" }
    ],
    "assemblyFormat": "$operand `,` `kernel_size` `=` $kernel_size `,` `stride` `=` $stride `,` `padding` `=` $padding `,` `ceil_mode` `=` $ceil_mode `,` `count_include_pad` `=` $count_include_pad attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.block_for",
    "summary": "block for",
    "inputs": [
      { "name": "block_args", "type": "Variadic" },
      { "name": "init_args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "lower_bound", "type": "IndexAttr" },
      { "name": "upper_bound", "type": "IndexAttr" },
      { "name": "step", "type": "IndexAttr" },
      { "name": "block_dims", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "`lb` `=` $lower_bound `,` `ub` `=` $upper_bound `,` `step` `=` $step `,` `args` `=` `[` $block_args `]` `,` `dims` `=` $block_dims `,` `init` `=` `[` $init_args `]` $region attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.block_yield",
    "summary": "block yield in block for op",
    "inputs": [
      { "name": "block_outs", "type": "Variadic" },
      { "name": "iter_outs", "type": "Variadic" }
    ],
    "assemblyFormat": "`block_outs` `=` `[` $block_outs `]` `,` `iter_outs` `=` `[` $iter_outs `]` attr-dict `:` type(operands)"
  },
  {
    "name": "asuka.call",
    "summary": "call",
    "description": "call",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "callee", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$callee `(` $operands `)` attr-dict `:` functional-type($operands, results)"
  },
  {
    "name": "asuka.cmp",
    "summary": "cmp",
    "description": "cmp",
    "inputs": [
      { "name": "lhs", "type": "AS_AnyTensor" },
      { "name": "rhs", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_BoolTensor" }
    ],
    "attributes": [
      { "name": "cmp_type", "type": "Asuka_CmpTypeAttr" }
    ],
    "assemblyFormat": "$cmp_type $lhs `,` $rhs attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.convert",
    "summary": "convert",
    "inputs": [
      { "name": "operand", "type": "AS_TypedTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_TypedTensor" }
    ],
    "attributes": [
      { "name": "dst_type", "type": "TypeAttr" }
    ],
    "assemblyFormat": "$operand `,` `type` `=` $dst_type attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.div",
    "summary": "div",
    "description": "div",
    "inputs": [
      { "name": "lhs", "type": "AS_AnyTensor" },
      { "name": "rhs", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.dot",
    "summary": "asuka dot",
    "description": "dot op\n    for lhs: [x.., m, k]\n    for rhs: [y.., k, n]\n    k is reduce dim, x.. and y.. are batch dim",
    "inputs": [
      { "name": "lhs", "type": "AS_AnyTensor" },
      { "name": "rhs", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.dynamic_block_for",
    "summary": "block for",
    "inputs": [
      { "name": "lower_bound", "type": "Index" },
      { "name": "upper_bound", "type": "Index" },
      { "name": "block_args", "type": "Variadic" },
      { "name": "init_args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "step", "type": "IndexAttr" },
      { "name": "block_dims", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "`lb` `=` $lower_bound `,` `ub` `=` $upper_bound `,` `step` `=` $step `,` `args` `=` `[` $block_args `]` `,` `dims` `=` $block_dims `,` `init` `=` `[` $init_args `]` $region attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.erase_type",
    "summary": "erase type",
    "description": "erase type",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "assemblyFormat": "$inputs attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.exp",
    "summary": "exp",
    "description": "exp",
    "inputs": [
      { "name": "operand", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.exp2",
    "summary": "exp2",
    "description": "exp2",
    "inputs": [
      { "name": "operand", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.kernel",
    "summary": "asuka kernel",
    "description": "kernel",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "asuka.log",
    "summary": "log",
    "description": "log",
    "inputs": [
      { "name": "operand", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.log2",
    "summary": "log2",
    "description": "log2",
    "inputs": [
      { "name": "operand", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.mask",
    "inputs": [
      { "name": "starts", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "attributes": [
      { "name": "sizes", "type": "DenseI64ArrayAttr" },
      { "name": "element_type", "type": "TypeAttr" }
    ],
    "assemblyFormat": "`starts` `=` `[` $starts `]` `,` `sizes` `=` $sizes `,` `type` `=` $element_type $region attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.mask_yield",
    "inputs": [
      { "name": "srcs", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict $srcs `:` type($srcs)"
  },
  {
    "name": "asuka.mul",
    "summary": "mul",
    "description": "mul",
    "inputs": [
      { "name": "lhs", "type": "AS_AnyTensor" },
      { "name": "rhs", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.neg",
    "summary": "neg",
    "description": "neg",
    "inputs": [
      { "name": "operand", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.normalize",
    "summary": "asuka normalize",
    "description": "normalize",
    "inputs": [
      { "name": "operand", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "attributes": [
      { "name": "reduce_dimension", "type": "I64Attr" },
      { "name": "lp", "type": "I64Attr" }
    ],
    "assemblyFormat": "$operand `,` `dim` `=` $reduce_dimension `,` `Lp` `=` $lp attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.parallel_for",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "assemblyFormat": "$inputs $region attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.parallel_yield",
    "inputs": [
      { "name": "srcs", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict $srcs `:` type($srcs)"
  },
  {
    "name": "asuka.permute",
    "inputs": [
      { "name": "operand", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "attributes": [
      { "name": "dims", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand `,` `dims` `=` $dims attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.pow",
    "summary": "pow",
    "description": "pow",
    "inputs": [
      { "name": "lhs", "type": "AS_AnyTensor" },
      { "name": "rhs", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.precise_dot_op",
    "summary": "Dot operation with output precision at least as high as input precision",
    "description": "This operation performs a dot product on two input tensors with the same element type,\n    and ensures the output tensor has a precision (element type) that is at least as high as the inputs.",
    "inputs": [
      { "name": "lhs", "type": "AS_AnyTensor" },
      { "name": "rhs", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "attributes": [
      { "name": "acc_type", "type": "TypeAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs `,` `acc` `=` $acc_type attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.reduce",
    "summary": "asuka reduce",
    "description": "reduce",
    "inputs": [
      { "name": "operand", "type": "AS_AnyTensor" },
      { "name": "init", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "attributes": [
      { "name": "reduce_dimension", "type": "I64Attr" },
      { "name": "reduce_type", "type": "Asuka_ReduceTypeAttr" },
      { "name": "keep_dim", "type": "BoolAttr" }
    ],
    "assemblyFormat": "`(` $operand ( `,` `init` `=` $init^ )? `)` `,` `dim` `=` $reduce_dimension `,` `op` `=` $reduce_type attr-dict `,` `keep_dim` `=` $keep_dim `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.reshape",
    "inputs": [
      { "name": "operand", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.return",
    "summary": "return",
    "description": "return",
    "inputs": [
      { "name": "srcs", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($srcs^ `:` type($srcs))?"
  },
  {
    "name": "asuka.rng",
    "summary": "rng",
    "description": "rng",
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "attributes": [
      { "name": "rng_distribution", "type": "Asuka_RngDistributionAttr" }
    ],
    "assemblyFormat": "`distribution` `=` $rng_distribution attr-dict `:` type($result)"
  },
  {
    "name": "asuka.softmax",
    "summary": "asuka softmax",
    "description": "softmax",
    "inputs": [
      { "name": "operand", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "attributes": [
      { "name": "reduce_dimension", "type": "I64Attr" }
    ],
    "assemblyFormat": "$operand `,` `dim` `=` $reduce_dimension attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.specify_type",
    "summary": "specify type",
    "description": "specify type",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "element_types", "type": "TypeArrayAttr" }
    ],
    "assemblyFormat": "$inputs `with` `type` `=` $element_types attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.split",
    "summary": "rng",
    "description": "split",
    "inputs": [
      { "name": "operand", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "split_dimension", "type": "I64Attr" }
    ],
    "assemblyFormat": "$operand `,` `dim` `=` $split_dimension attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.sub",
    "summary": "sub",
    "description": "sub",
    "inputs": [
      { "name": "lhs", "type": "AS_AnyTensor" },
      { "name": "rhs", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.tanh",
    "summary": "tanh",
    "inputs": [
      { "name": "operand", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.transpose",
    "inputs": [
      { "name": "operand", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "attributes": [
      { "name": "dims", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand `,` `dims` `=` $dims attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.trilu",
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "attributes": [
      { "name": "diagonal", "type": "I64Attr" },
      { "name": "is_upper", "type": "BoolAttr" },
      { "name": "shape", "type": "DenseI64ArrayAttr" },
      { "name": "val", "type": "TypedAttrInterface" }
    ],
    "assemblyFormat": "`diagonal` `=` $diagonal `,` `is_upper` `=` $is_upper `,` `shape` `=` $shape `,` `val` `=` $val attr-dict"
  },
  {
    "name": "asuka.unsqueeze",
    "inputs": [
      { "name": "operand", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "attributes": [
      { "name": "dim", "type": "I64Attr" }
    ],
    "assemblyFormat": "$operand `,` `dim` `=` $dim attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.view",
    "inputs": [
      { "name": "operand", "type": "AS_AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "asuka.zero",
    "outputs": [
      { "name": "result", "type": "AS_AnyTensor" }
    ],
    "attributes": [
      { "name": "shape", "type": "DenseI64ArrayAttr" },
      { "name": "element_type", "type": "TypeAttr" }
    ],
    "assemblyFormat": "`shape` `=` $shape `,` `type` `=` $element_type attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "async.add_to_group",
    "summary": "adds an async token or value to the group",
    "description": "The `async.add_to_group` adds an async token or value to the async group.\n    Returns the rank of the added element in the group. This rank is fixed\n    for the group lifetime.\n\n    Example:\n\n    ```mlir\n    %0 = async.create_group %size : !async.group\n    %1 = ... : !async.token\n    %2 = async.add_to_group %1, %0 : !async.token\n    ```",
    "inputs": [
      { "name": "operand", "type": "Async_AnyValueOrTokenType" },
      { "name": "group", "type": "Async_GroupType" }
    ],
    "outputs": [
      { "name": "rank", "type": "Index" }
    ],
    "assemblyFormat": "$operand `,` $group `:` type($operand) attr-dict"
  },
  {
    "name": "async.await",
    "summary": "waits for the argument to become ready",
    "description": "The `async.await` operation waits until the argument becomes ready, and for\n    the `async.value` arguments it unwraps the underlying value\n\n    Example:\n\n    ```mlir\n    %0 = ... : !async.token\n    async.await %0 : !async.token\n\n    %1 = ... : !async.value<f32>\n    %2 = async.await %1 : !async.value<f32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "Async_AnyValueOrTokenType" }
    ],
    "outputs": [
      { "name": "result", "type": "Optional" }
    ],
    "assemblyFormat": "$operand `:` custom<AwaitResultType>(\n      type($operand), type($result)\n    ) attr-dict"
  },
  {
    "name": "async.await_all",
    "summary": "waits for the all async tokens or values in the group to become ready",
    "description": "The `async.await_all` operation waits until all the tokens or values in the\n    group become ready.\n\n    Example:\n\n    ```mlir\n    %0 = async.create_group %size : !async.group\n\n    %1 = ... : !async.token\n    %2 = async.add_to_group %1, %0 : !async.token\n\n    %3 = ... : !async.token\n    %4 = async.add_to_group %2, %0 : !async.token\n\n    async.await_all %0\n    ```",
    "inputs": [
      { "name": "operand", "type": "Async_GroupType" }
    ],
    "assemblyFormat": "$operand attr-dict"
  },
  {
    "name": "async.call",
    "summary": "async call operation",
    "description": "The `async.call` operation represents a direct call to an async function\n    that is within the same symbol scope as the call. The operands and result\n    types of the call must match the specified async function type. The callee\n    is encoded as a symbol reference attribute named \"callee\".\n\n    Example:\n\n    ```mlir\n    %2 = async.call @my_add(%0, %1) : (f32, f32) -> !async.value<f32>\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "callee", "type": "FlatSymbolRefAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$callee `(` $operands `)` attr-dict `:` functional-type($operands, results)"
  },
  {
    "name": "async.coro.begin",
    "summary": "returns a handle to the coroutine",
    "description": "The `async.coro.begin` allocates a coroutine frame and returns a handle to\n    the coroutine.",
    "inputs": [
      { "name": "id", "type": "Async_CoroIdType" }
    ],
    "outputs": [
      { "name": "handle", "type": "Async_CoroHandleType" }
    ],
    "assemblyFormat": "$id attr-dict"
  },
  {
    "name": "async.coro.end",
    "summary": "marks the end of the coroutine in the suspend block",
    "description": "The `async.coro.end` marks the point where a coroutine needs to return\n    control back to the caller if it is not an initial invocation of the\n    coroutine. It the start part of the coroutine is is no-op.",
    "inputs": [
      { "name": "handle", "type": "Async_CoroHandleType" }
    ],
    "assemblyFormat": "$handle attr-dict"
  },
  {
    "name": "async.coro.free",
    "summary": "deallocates the coroutine frame",
    "description": "The `async.coro.free` deallocates the coroutine frame created by the\n    async.coro.begin operation.",
    "inputs": [
      { "name": "id", "type": "Async_CoroIdType" },
      { "name": "handle", "type": "Async_CoroHandleType" }
    ],
    "assemblyFormat": "$id `,` $handle attr-dict"
  },
  {
    "name": "async.coro.id",
    "summary": "returns a switched-resume coroutine identifier",
    "description": "The `async.coro.id` returns a switched-resume coroutine identifier.",
    "outputs": [
      { "name": "id", "type": "Async_CoroIdType" }
    ],
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "async.coro.save",
    "summary": "saves the coroutine state",
    "description": "The `async.coro.saves` saves the coroutine state.",
    "inputs": [
      { "name": "handle", "type": "Async_CoroHandleType" }
    ],
    "outputs": [
      { "name": "state", "type": "Async_CoroStateType" }
    ],
    "assemblyFormat": "$handle attr-dict"
  },
  {
    "name": "async.coro.suspend",
    "summary": "suspends the coroutine",
    "description": "The `async.coro.suspend` suspends the coroutine and transfers control to the\n    `suspend` successor. If suspended coroutine later resumed it will transfer\n    control to the `resume` successor. If it is destroyed it will transfer\n    control to the the `cleanup` successor.\n\n    In switched-resume lowering coroutine can be already in resumed state when\n    suspend operation is called, in this case control will be transferred to the\n    `resume` successor skipping the `suspend` successor.",
    "inputs": [
      { "name": "state", "type": "Async_CoroStateType" }
    ],
    "successors": [
      {
        "name": "suspendDest"
      },
      {
        "name": "resumeDest"
      },
      {
        "name": "cleanupDest"
      }
    ],
    "assemblyFormat": "$state `,` $suspendDest `,` $resumeDest  `,` $cleanupDest attr-dict"
  },
  {
    "name": "async.create_group",
    "summary": "creates an empty async group",
    "description": "The `async.create_group` allocates an empty async group. Async tokens or\n    values can be added to this group later. The size of the group must be\n    specified at construction time, and `await_all` operation will first\n    wait until the number of added tokens or values reaches the group size.\n\n    Example:\n\n    ```mlir\n    %size = ... : index\n    %group = async.create_group %size : !async.group\n    ...\n    async.await_all %group\n    ```",
    "inputs": [
      { "name": "size", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Async_GroupType" }
    ],
    "assemblyFormat": "$size `:` type($result) attr-dict"
  },
  {
    "name": "async.execute",
    "summary": "Asynchronous execute operation",
    "description": "The `body` region attached to the `async.execute` operation semantically\n    can be executed concurrently with the successor operation. In the followup\n    example \"compute0\" can be executed concurrently with \"compute1\".\n\n    The actual concurrency semantics depends on the dialect lowering to the\n    executable format. Fully sequential execution (\"compute0\" completes before\n    \"compute1\" starts) is a completely legal execution.\n\n    Because concurrent execution is not guaranteed, it is illegal to create an\n    implicit dependency from \"compute1\" to \"compute0\" (e.g. via shared global\n    state). All dependencies must be made explicit with async execute arguments\n    (`async.token` or `async.value`).\n\n   `async.execute` operation takes `async.token` dependencies and `async.value`\n    operands separately, and starts execution of the attached body region only\n    when all tokens and values become ready.\n\n    Example:\n\n    ```mlir\n    %dependency = ... : !async.token\n    %value = ... : !async.value<f32>\n\n    %token, %results =\n      async.execute [%dependency](%value as %unwrapped: !async.value<f32>)\n                 -> !async.value<!some.type>\n      {\n        %0 = \"compute0\"(%unwrapped): (f32) -> !some.type\n        async.yield %0 : !some.type\n      }\n\n    %1 = \"compute1\"(...) : !some.type\n    ```\n\n    In the example above asynchronous execution starts only after dependency\n    token and value argument become ready. Unwrapped value passed to the\n    attached body region as an %unwrapped value of f32 type.",
    "inputs": [
      { "name": "dependencies", "type": "Variadic" },
      { "name": "bodyOperands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "token", "type": "Async_TokenType" },
      { "name": "bodyResults", "type": "Variadic" }
    ]
  },
  {
    "name": "async.func",
    "summary": "async function operation",
    "description": "An async function is like a normal function, but supports non-blocking\n    await. Internally, async function is lowered to the LLVM coroutinue with\n    async runtime intrinsic. It can return an async token and/or async values.\n    The token represents the execution state of async function and can be used\n    when users want to express dependencies on some side effects, e.g.,\n    the token becomes available once every thing in the func body is executed.\n\n    Example:\n\n    ```mlir\n    // Async function can't return void, it always must be some async thing.\n    async.func @async.0() -> !async.token {\n      return\n    }\n\n    // Function returns only async value.\n    async.func @async.1() -> !async.value<i32> {\n      %0 = arith.constant 42 : i32\n      return %0 : i32\n    }\n\n    // Implicit token can be added to return types.\n    async.func @async.2() -> !async.token, !async.value<i32> {\n      %0 = arith.constant 42 : i32\n      return %0 : i32\n    }\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "async.return",
    "summary": "Async function return operation",
    "description": "The `async.return` is a special terminator operation for Async function.\n\n    Example:\n\n    ```mlir\n    async.func @foo() : !async.token {\n      return\n    }\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "async.runtime.add_ref",
    "summary": "adds a reference to async value",
    "description": "The `async.runtime.add_ref` operation adds a reference(s) to async value\n    (token, value or group).",
    "inputs": [
      { "name": "operand", "type": "Async_AnyAsyncType" }
    ],
    "attributes": [
      { "name": "count", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand)"
  },
  {
    "name": "async.runtime.add_to_group",
    "summary": "adds an async token or value to the group",
    "description": "The `async.runtime.add_to_group` adds an async token or value to the async\n    group. Returns the rank of the added element in the group.",
    "inputs": [
      { "name": "operand", "type": "Async_AnyValueOrTokenType" },
      { "name": "group", "type": "Async_GroupType" }
    ],
    "outputs": [
      { "name": "rank", "type": "Index" }
    ],
    "assemblyFormat": "$operand `,` $group attr-dict `:` type($operand)"
  },
  {
    "name": "async.runtime.await",
    "summary": "blocks the caller thread until the operand becomes available",
    "description": "The `async.runtime.await` operation blocks the caller thread until the\n    operand becomes available or error.",
    "inputs": [
      { "name": "operand", "type": "Async_AnyAsyncType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand)"
  },
  {
    "name": "async.runtime.await_and_resume",
    "summary": "awaits the async operand and resumes the coroutine",
    "description": "The `async.runtime.await_and_resume` operation awaits for the operand to\n    become available or error and resumes the coroutine on a thread managed by\n    the runtime.",
    "inputs": [
      { "name": "operand", "type": "Async_AnyAsyncType" },
      { "name": "handle", "type": "Async_CoroHandleType" }
    ],
    "assemblyFormat": "$operand `,` $handle attr-dict `:` type($operand)"
  },
  {
    "name": "async.runtime.create",
    "summary": "creates an async runtime token or value",
    "description": "The `async.runtime.create` operation creates an async dialect token or\n    value. Tokens and values are created in the non-ready state.",
    "outputs": [
      { "name": "result", "type": "Async_AnyValueOrTokenType" }
    ],
    "assemblyFormat": "attr-dict `:` type($result)"
  },
  {
    "name": "async.runtime.create_group",
    "summary": "creates an async runtime group",
    "description": "The `async.runtime.create_group` operation creates an async dialect group\n    of the given size. Group created in the empty state.",
    "inputs": [
      { "name": "size", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Async_GroupType" }
    ],
    "assemblyFormat": "$size `:` type($result) attr-dict"
  },
  {
    "name": "async.runtime.drop_ref",
    "summary": "drops a reference to async value",
    "description": "The `async.runtime.drop_ref` operation drops a reference(s) to async value\n    (token, value or group).",
    "inputs": [
      { "name": "operand", "type": "Async_AnyAsyncType" }
    ],
    "attributes": [
      { "name": "count", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand)"
  },
  {
    "name": "async.runtime.is_error",
    "summary": "returns true if token, value or group is in error state",
    "description": "The `async.runtime.is_error` operation returns true if the token, value or\n    group (any of the async runtime values) is in the error state. It is the\n    caller responsibility to check error state after the call to `await` or\n    resuming after `await_and_resume`.",
    "inputs": [
      { "name": "operand", "type": "Async_AnyAsyncType" }
    ],
    "outputs": [
      { "name": "is_error", "type": "I1" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand)"
  },
  {
    "name": "async.runtime.load",
    "summary": "loads the value from the runtime async.value",
    "description": "The `async.runtime.load` operation loads the value from the runtime\n    async.value storage.",
    "inputs": [
      { "name": "storage", "type": "Async_ValueType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "$storage attr-dict `:` type($storage)"
  },
  {
    "name": "async.runtime.num_worker_threads",
    "summary": "gets the number of threads in the threadpool from the runtime",
    "description": "The `async.runtime.num_worker_threads` operation gets the number of threads\n    in the threadpool from the runtime.",
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "attr-dict `:` type($result)"
  },
  {
    "name": "async.runtime.resume",
    "summary": "resumes the coroutine on a thread managed by the runtime",
    "description": "The `async.runtime.resume` operation resumes the coroutine on a thread\n    managed by the runtime.",
    "inputs": [
      { "name": "handle", "type": "Async_CoroHandleType" }
    ],
    "assemblyFormat": "$handle attr-dict"
  },
  {
    "name": "async.runtime.set_available",
    "summary": "switches token or value to available state",
    "description": "The `async.runtime.set_available` operation switches async token or value\n    state to available.",
    "inputs": [
      { "name": "operand", "type": "Async_AnyValueOrTokenType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand)"
  },
  {
    "name": "async.runtime.set_error",
    "summary": "switches token or value to error state",
    "description": "The `async.runtime.set_error` operation switches async token or value\n    state to error.",
    "inputs": [
      { "name": "operand", "type": "Async_AnyValueOrTokenType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand)"
  },
  {
    "name": "async.runtime.store",
    "summary": "stores the value into the runtime async.value",
    "description": "The `async.runtime.store` operation stores the value into the runtime\n    async.value storage.",
    "inputs": [
      { "name": "value", "type": "AnyType" },
      { "name": "storage", "type": "Async_ValueType" }
    ],
    "assemblyFormat": "$value `,` $storage attr-dict `:` type($storage)"
  },
  {
    "name": "async.yield",
    "summary": "terminator for Async execute operation",
    "description": "The `async.yield` is a special terminator operation for the block inside\n    `async.execute` operation.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "($operands^ `:` type($operands))? attr-dict"
  },
  {
    "name": "bufferization.alloc_tensor",
    "summary": "allocate buffer for a tensor",
    "description": "`bufferization.alloc_tensor` materializes an uninitialized tensor with a\n    given shape (dynamic or static). It always bufferizes to a new buffer\n    allocation of the given shape. The optional `copy` operand specifies the\n    contents of the tensors. If no `copy` operand is specified, reading from the\n    result of an `alloc_tensor` op yields an undefined value.\n\n    If `copy` is specified, no dynamic sizes should be passed, since they are\n    the same as the dynamic sizes of the `copy` operand.\n\n    `alloc_tensor` is a helper op for bufferization. The operation is provided\n    as an anchor that marks the beginning of a new tensor SSA use-def chain. It\n    can be used to control in-place bufferization decisions during One-Shot\n    Bufferize: The bufferized result of a `bufferization.alloc_tensor` does not\n    alias with any other buffer, so it can be used to resolve read-after-write\n    conflicts that would have been introduced by the in-place bufferization of\n    another op.\n\n    The optional `memory_space` attribute specifies the memory space when\n    bufferizing this op. The memory space is inferred from `copy` if specified.\n    If neither `copy` nor `memory_space` is specified, the default memory space\n    is used during bufferization.\n\n    The optional `size_hint` operand specifies the number of non-zero elements\n    for sparse tensors. The value of `size_hint` should be not less than 1 and\n    not larger than the linear size of the corresponding dense tensor type. If\n    this requirement is not met, the behavior of the operator is undefined.\n\n    Both dense and sparse tensor types are supported. The result of a\n    `bufferization.alloc_tensor` is a tensor value that can be used like any\n    other tensor value. In practice, it is often used as the \"out\" operand of\n    another op. Sparse tensor allocations should always be used in a local\n    construction operation and never escape the function boundary directly.\n\n    Example:\n\n    ```mlir\n    %c = bufferization.alloc_tensor(%d1, %d2) : tensor<?x?xf32, #SparseMatrix>\n    %0 = linalg.matmul\n      ins(%a, %b: tensor<?x?xf32, #SparseMatrix>, tensor<?x?xf32, #SparseMatrix>)\n      outs(%c: tensor<?x?xf32, #SparseMatrix>) -> tensor<?x?xf32, #SparseMatrix>\n    return %0 : tensor<?x?xf32, #SparseMatrix>\n    ```\n\n    ```mlir\n    %c = bufferization.alloc_tensor(%d1, %d2) size_hint = %noe\n      : tensor<?x?xf32, #SparseMatrix>\n    ```\n\n    Note: An `alloc_tensor` with a `copy` should also be expressed as an\n    `alloc_tensor` without `copy`, followed by a `copy_tensor`.",
    "inputs": [
      { "name": "dynamic_sizes", "type": "Variadic" },
      { "name": "copy", "type": "Optional" },
      { "name": "size_hint", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "memory_space", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "bufferization.clone",
    "summary": "clone a memref",
    "description": "Clones the data in the input view into an implicitly defined output view.\n\n    Usage:\n\n    ```mlir\n    %arg1 = bufferization.clone %arg0 : memref<?xf32> to memref<?xf32>\n    ```\n\n    Valid implementations of this operation may alias the input and output\n    views or create an actual copy. Mutating the source or result\n    of the clone operation after the clone operation thus leads to undefined\n    behavior.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "assemblyFormat": "$input attr-dict `:` type($input) `to` type($output)"
  },
  {
    "name": "bufferization.dealloc",
    "summary": "deallocates the given memrefs if no alias is retained",
    "description": "This operation deallocates each of the given memrefs if there is no alias\n    to that memref in the list of retained memrefs and the corresponding\n    condition value is set. This condition can be used to indicate and pass on\n    ownership of memref values (or in other words, the responsibility of\n    deallocating that memref). If two memrefs alias each other, only one will be\n    deallocated to avoid double free situations.\n\n    The number of variadic `memref` operands (the memrefs to be deallocated)\n    must equal the number of variadic `condition` operands and correspond to\n    each other element-wise.\n\n    The `memref` operands must be the originally allocated memrefs, however, the\n    `retained` memref operands may be arbitrary memrefs.\n\n    This operation returns a variadic number of `updatedConditions` operands,\n    one updated condition per retained memref. An updated condition indicates\n    the ownership of the respective retained memref. It is computed as the\n    disjunction of all `conditions` operands where the corresponding to\n    `memrefs` operand aliases with the retained memref. If the retained memref\n    has no aliases among `memrefs`, the resulting updated condition is 'false'.\n    This is because all memrefs that need to be deallocated within one basic\n    block should be added to the same `bufferization.dealloc` operation at the\n    end of the block; if no aliasing memref is present, then it does not have to\n    be deallocated and thus we don't need to claim ownership. If the memrefs to\n    be deallocated are split over multiple dealloc operations (e.g., to avoid\n    aliasing checks at runtime between the `memref` operands), then the results\n    have to be manually combined using an `arith.ori` operation and all of them\n    still require the same list of `retained` memref operands unless the\n    (potentially empty) set of aliasing memrefs can be determined statically. In\n    that case, the `updatedCondition` operand can be replaced accordingly (e.g.,\n    by a canonicalizer).\n\n    Example:\n    ```mlir\n    %0:3 = bufferization.dealloc (%a0, %a1 : memref<2xf32>, memref<4xi32>)\n      if (%cond0, %cond1) retain (%r0, %r1, %r2 : memref<?xf32>, memref<f64>,\n      memref<2xi32>)\n    ```\n    Deallocation will be called on `%a0` if `%cond0` is 'true' and neither\n    `%r0`, `%r1`, or `%r2` are aliases of `%a0`. `%a1` will be deallocated when\n    `%cond1` is set to 'true' and none of `%r0`, `%r1`, `%r2`, and `%a0` are\n    aliases.\n\n    Note that this can be an expensive operation if there are many operands that\n    cannot be optimized away. The runtime cost of this operation (assuming that\n    nothing is optimized away) is `O(|memrefs|^2+|memrefs|*|retained|)`. The\n    cost in terms of memory space is `O(|memrefs|+|retained|)`. As a result, it\n    is recommended to place it carefully in the IR such that most operands can\n    be optimized away by running the `buffer-deallocation-simplification` pass.",
    "inputs": [
      { "name": "memrefs", "type": "Variadic" },
      { "name": "conditions", "type": "Variadic" },
      { "name": "retained", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "updatedConditions", "type": "Variadic" }
    ],
    "assemblyFormat": "(` ``(` $memrefs^ `:` type($memrefs) `)` `if` ` ` `(` $conditions `)` )?\n    (`retain` ` ` `(` $retained^ `:` type($retained) `)` )? attr-dict"
  },
  {
    "name": "bufferization.dealloc_tensor",
    "summary": "release underlying storage format of given tensor",
    "description": "`bufferization.dealloc_tensor` is a buffer deallocation in tensor land. This\n    op can be used for manual buffer deallocation. Some bufferizations (such as\n    One-Shot Bufferize) take care of buffer deallocation, in which case this op\n    is usually not needed. Details can be found in the documentation of the\n    respective bufferization passes.\n\n    In case of a dense tensor, this op lowers to a `memref.dealloc` op during\n    bufferization.\n\n    In case of a sparse tensor, this op releases the underlying sparse storage\n    format for a tensor that materialized earlier through a `new` operation, a\n    `convert` operation with annotated destination tensor type (unless the\n    convert is folded away), or a `bufferization.alloc_tensor` operation. The\n    release operation should only be called once for any materialized tensor.\n    After this operation, any subsequent `memref` querying operation on the\n    tensor returns undefined results.\n\n    Example:\n\n    ```mlir\n    bufferization.dealloc_tensor %tensor : tensor<1024x1024xf64, #CSR>\n    ```",
    "inputs": [
      { "name": "tensor", "type": "AnyTensor" }
    ],
    "assemblyFormat": "$tensor attr-dict `:` type($tensor)"
  },
  {
    "name": "bufferization.materialize_in_destination",
    "summary": "copy a tensor",
    "description": "This op indicates that the data of the `source` tensor is guaranteed to\n    materialize in `dest`, which can be a tensor or a memref. In case of a\n    tensor, `source` materializes in the future buffer of `dest` and a the\n    updated destination tensor is returned. If this is not possible, e.g.,\n    because the destination tensor is read-only or because its original\n    contents are still read later, the input IR fails to bufferize. In case of a\n    memref, `source` materializes in `dest`, which is already a buffer. The op\n    has no results in that case.\n\n    `source`, `dest` and `result` (if present) must have the same runtime shape\n    and element type. If the op has a result, the types of `result` and `dest`\n    must match exactly (e.g., including any tensor encodings).\n\n    By default, this op bufferizes to a memcpy from the future buffer of the\n    `source` tensor to the future buffer of the `dest` tensor or to the `dest`\n    buffer. However, transformations such as \"empty tensor elimination\" may\n    rewrite IR such that a computation is performed directly in `dest` and no\n    memcpy is needed.\n\n    If `dest` is a buffer, the `writable` attribute must be specified and the\n    `restrict` keyword can be specified. These attributes have the same meaning\n    as the respective attributes of `bufferization.to_tensor`.\n\n    `writable` indicates that the `dest` buffer is considered writable. It does\n    not make sense to materialize a computation in a read-only buffer, so\n    `writable` is required.\n\n    `restrict` indicates that there is no `bufferization.to_tensor` op and no\n    other `bufferization.materialize_in_destination` op with `dest` (or an alias\n    thereof) and \"restrict\". Only ops with this attribute are considered for\n    \"empty tensor elimination\". As part of empty tensor elimination, a new\n    `to_tensor` op with `dest` may be inserted and the `restrict` attribute is\n    transferred from this op to the new `to_tensor` op. Having \"restrict\" on\n    this op guarantees that performing empty tensor elimination would not create\n    invalid IR (i.e., having multiple `to_tensor restrict` with aliasing\n    buffers).\n\n    Note: `writable` could be removed from this op because it must always be set\n    for memref destinations. This op has that attribute to make clear the\n    requirements on the `dest` operand in the op assembly format.\n\n    Note: If `dest` is a tensor, `tensor.insert_slice` could be used for the\n    same purpose, but since tensor dialect ops only indicate *what* should be\n    computed but not *where*, it could fold away, causing the computation to\n    materialize in a different buffer.",
    "inputs": [
      { "name": "source", "type": "AnyTensor" },
      { "name": "dest", "type": "AnyShaped" }
    ],
    "outputs": [
      { "name": "result", "type": "Optional" }
    ],
    "attributes": [
      { "name": "restrict", "type": "UnitAttr" },
      { "name": "writable", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$source `in` (`restrict` $restrict^)? (`writable` $writable^)? $dest\n        attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "bufferization.to_buffer",
    "summary": "cast a tensor-like type to buffer-like type",
    "description": "An operation that returns the future buffer of a `tensor`.\n\n    ```mlir\n    // Result type is memref<4x?xf32, #layout, 0>\n    %m = bufferization.to_buffer %t : tensor<4x?xf32> to memref<4x?xf32, #layout, 0>\n    ```\n\n    This operation is a specialized variant of the built-in\n    `unrealized_conversion_cast` and is used to make sure that the IR stays\n    valid at any point during the bufferization.\n\n    The `read_only` attribute can optionally be set, indicating to the\n    bufferization that the buffer returned by this op (or an alias created from\n    the returned buffer) will not be written to.",
    "inputs": [
      { "name": "tensor", "type": "Bufferization_TensorLikeTypeInterface" }
    ],
    "outputs": [
      { "name": "buffer", "type": "Bufferization_BufferLikeTypeInterface" }
    ],
    "attributes": [
      { "name": "read_only", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$tensor (`read_only` $read_only^)? attr-dict `:` type($tensor) `to` type($buffer)"
  },
  {
    "name": "bufferization.to_tensor",
    "summary": "create a tensor-like type from a buffer-like type",
    "description": "An operation that creates a tensor from a buffer. The result value is a\n    tensor-like type that must match the corresponding buffer-like operand as\n    per TensorLikeType::verifyCompatibleBufferType(). For builtins (TensorType\n    and BaseMemRefType), this means that shapes and element types match between\n    the tensor and the buffer.\n\n    The opposite of this op is `to_buffer`. Together, these two ops are\n    useful for source/target materializations when doing type conversions\n    involving tensors and buffers.\n\n    Example:\n\n    ```mlir\n    // Produces a value of tensor<4x?xf32> type.\n    %t = bufferization.to_tensor %m : memref<4x?xf32, #layout, 0> to tensor<4x?xf32>\n    ```\n\n    If the `writable` unit attribute is set, the produced tensor is considered\n    \"writable\" during bufferization. Otherwise, every OpOperand that bufferizes\n    to a write to the future buffer of the resulting tensor (or an alias\n    thereof) will bufferize out-of-place to prevent emitting any writes to\n    `memref` during bufferization.\n\n    The `restrict` unit attribute (similar to the C `restrict` keyword)\n    indicates that the produced tensor result is the only way for the tensor\n    IR to gain access to the `memref` operand (or an alias thereof). E.g.,\n    there must be no other `to_tensor` op with the same or with an aliasing\n    `memref` operand.\n\n    Note: Only `to_tensor` ops with the `restrict` unit attribute are supported\n    by One-Shot Bufferize. Other IR is rejected. (To support `to_tensor`\n    without `restrict`, One-Shot Bufferize would have to analyze memref IR.)\n    Ops that have incorrect usage of `restrict` may bufferize incorrectly.\n\n    Example:\n\n    ```\n    %t = bufferization.to_tensor %m restrict writable : memref<4xf32> to tensor<4xf32>\n\n    // %t is writable, so the tensor.insert may bufferize in-place in the\n    // absence of other conflicts.\n    %r = tensor.insert %f into %t[%idx] : tensor<4xf32>\n    ```\n\n    `to_tensor` ops are not bufferized. They are expected to fold away after\n    bufferization. If there are non-bufferizable ops in the IR and\n    `allowUnknownOps` is set, they may be part of the resulting IR and not fold\n    away. However, such IR is no longer bufferizable with One-Shot Bufferize.",
    "inputs": [
      { "name": "buffer", "type": "Arg" }
    ],
    "outputs": [
      { "name": "result", "type": "Bufferization_TensorLikeTypeInterface" }
    ],
    "attributes": [
      { "name": "restrict", "type": "UnitAttr" },
      { "name": "writable", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$buffer (`restrict` $restrict^)? (`writable` $writable^)? attr-dict\n      `:` type($buffer) `to` type($result)"
  },
  {
    "name": "builtin.module",
    "summary": "A top level container operation",
    "description": "A `module` represents a top-level container operation. It contains a single\n    [graph region](../LangRef.md#control-flow-and-ssacfg-regions) containing a single block\n    which can contain any operations and does not have a terminator. Operations\n    within this region cannot implicitly capture values defined outside the module,\n    i.e. Modules are [IsolatedFromAbove](../Traits#isolatedfromabove). Modules have\n    an optional [symbol name](../SymbolsAndSymbolTables.md) which can be used to refer\n    to them in operations.\n\n    Example:\n\n    ```mlir\n    module {\n      func.func @foo()\n    }\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "OptionalAttr" },
      { "name": "sym_visibility", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($sym_name^)? attr-dict-with-keyword $bodyRegion"
  },
  {
    "name": "builtin.unrealized_conversion_cast",
    "summary": "An unrealized conversion from one set of types to another",
    "description": "An `unrealized_conversion_cast` operation represents an unrealized\n    conversion from one set of types to another, that is used to enable the\n    inter-mixing of different type systems. This operation should not be\n    attributed any special representational or execution semantics, and is\n    generally only intended to be used to satisfy the temporary intermixing of\n    type systems during the conversion of one type system to another.\n\n    This operation may produce results of arity 1-N, and accept as input\n    operands of arity 0-N.\n\n    Example:\n\n    ```mlir\n    // An unrealized 0-1 conversion. These types of conversions are useful in\n    // cases where a type is removed from the type system, but not all uses have\n    // been converted. For example, imagine we have a tuple type that is\n    // expanded to its element types. If only some uses of an empty tuple type\n    // instance are converted we still need an instance of the tuple type, but\n    // have no inputs to the unrealized conversion.\n    %result = unrealized_conversion_cast to !bar.tuple_type<>\n\n    // An unrealized 1-1 conversion.\n    %result1 = unrealized_conversion_cast %operand : !foo.type to !bar.lowered_type\n\n    // An unrealized 1-N conversion.\n    %results2:2 = unrealized_conversion_cast %tuple_operand : !foo.tuple_type<!foo.type, !foo.type> to !foo.type, !foo.type\n\n    // An unrealized N-1 conversion.\n    %result3 = unrealized_conversion_cast %operand, %operand : !foo.type, !foo.type to !bar.tuple_type<!foo.type, !foo.type>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "assemblyFormat": "($inputs^ `:` type($inputs))? `to` type($outputs) attr-dict"
  },
  {
    "name": "cf.assert",
    "summary": "Assert operation with message attribute",
    "description": "Assert operation at runtime with single boolean operand and an error\n    message attribute.\n    If the argument is `true` this operation has no effect. Otherwise, the\n    program execution will abort. The provided error message may be used by a\n    runtime to propagate the error to the user.\n\n    Example:\n\n    ```mlir\n    cf.assert %b, \"Expected ... to be true\"\n    ```",
    "inputs": [
      { "name": "arg", "type": "I1" }
    ],
    "attributes": [
      { "name": "msg", "type": "StrAttr" }
    ],
    "assemblyFormat": "$arg `,` $msg attr-dict"
  },
  {
    "name": "cf.br",
    "summary": "Branch operation",
    "description": "The `cf.br` operation represents a direct branch operation to a given\n    block. The operands of this operation are forwarded to the successor block,\n    and the number and type of the operands must match the arguments of the\n    target block.\n\n    Example:\n\n    ```mlir\n    ^bb2:\n      %2 = call @someFn()\n      cf.br ^bb3(%2 : tensor<*xf32>)\n    ^bb3(%3: tensor<*xf32>):\n    ```",
    "inputs": [
      { "name": "destOperands", "type": "Variadic" }
    ],
    "successors": [
      {
        "name": "dest"
      }
    ],
    "assemblyFormat": "$dest (`(` $destOperands^ `:` type($destOperands) `)`)? attr-dict"
  },
  {
    "name": "cf.cond_br",
    "summary": "Conditional branch operation",
    "description": "The `cf.cond_br` terminator operation represents a conditional branch on a\n    boolean (1-bit integer) value. If the bit is set, then the first destination\n    is jumped to; if it is false, the second destination is chosen. The count\n    and types of operands must align with the arguments in the corresponding\n    target blocks.\n\n    The MLIR conditional branch operation is not allowed to target the entry\n    block for a region. The two destinations of the conditional branch operation\n    are allowed to be the same.\n\n    The following example illustrates a function with a conditional branch\n    operation that targets the same block.\n\n    Example:\n\n    ```mlir\n    func.func @select(%a: i32, %b: i32, %flag: i1) -> i32 {\n      // Both targets are the same, operands differ\n      cf.cond_br %flag, ^bb1(%a : i32), ^bb1(%b : i32)\n\n    ^bb1(%x : i32) :\n      return %x : i32\n    }\n    ```",
    "inputs": [
      { "name": "condition", "type": "I1" },
      { "name": "trueDestOperands", "type": "Variadic" },
      { "name": "falseDestOperands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "branch_weights", "type": "OptionalAttr" }
    ],
    "successors": [
      {
        "name": "trueDest"
      },
      {
        "name": "falseDest"
      }
    ],
    "assemblyFormat": "$condition (`weights` `(` $branch_weights^ `)` )? `,`\n    $trueDest (`(` $trueDestOperands^ `:` type($trueDestOperands) `)`)? `,`\n    $falseDest (`(` $falseDestOperands^ `:` type($falseDestOperands) `)`)?\n    attr-dict"
  },
  {
    "name": "cf.switch",
    "summary": "Switch operation",
    "description": "The `cf.switch` terminator operation represents a switch on a signless integer\n    value. If the flag matches one of the specified cases, then the\n    corresponding destination is jumped to. If the flag does not match any of\n    the cases, the default destination is jumped to. The count and types of\n    operands must align with the arguments in the corresponding target blocks.\n\n    Example:\n\n    ```mlir\n    cf.switch %flag : i32, [\n      default: ^bb1(%a : i32),\n      42: ^bb1(%b : i32),\n      43: ^bb3(%c : i32)\n    ]\n    ```",
    "inputs": [
      { "name": "flag", "type": "AnyInteger" },
      { "name": "defaultOperands", "type": "Variadic" },
      { "name": "caseOperands", "type": "VariadicOfVariadic" }
    ],
    "attributes": [
      { "name": "case_values", "type": "OptionalAttr" },
      { "name": "case_operand_segments", "type": "DenseI32ArrayAttr" }
    ],
    "successors": [
      {
        "name": "defaultDestination"
      },
      {
        "name": "caseDestinations"
      }
    ],
    "assemblyFormat": "$flag `:` type($flag) `,` `[` `\\n`\n      custom<SwitchOpCases>(ref(type($flag)),$defaultDestination,\n                            $defaultOperands,\n                            type($defaultOperands),\n                            $case_values,\n                            $caseDestinations,\n                            $caseOperands,\n                            type($caseOperands))\n   `]`\n    attr-dict"
  },
  {
    "name": "chlo._asin_acos_kernel",
    "summary": "AsinAcosKernel operator",
    "description": "Returns `AsinAcosKernel(operand)` element-wise.\n\n    ```\n    If\n      w = _asin_acos_kernel(z)\n      w' = _asin_acos_kernel(I * z)\n    Then\n      asin(z) = complex(atan2(z.real, w.real), sign(z.imag) * w.imag)\n      acos(z) = complex(atan2(w.real, z.real), -sign(z.imag) * w.imag)\n      asinh(z) = complex(sign(z.real) * w'.imag, atan2(z.imag, w'.real))\n      acosh(z) = complex(w.imag, sign(z.imag) * atan2(w.real, z.real))\n    ```\n\n    This op is used as an intermediate value in decompositions and\n    should never be constructed directly by frameworks or consumed by\n    backends.",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.acos",
    "summary": "Acos operator",
    "description": "Returns `Acos(operand)` element-wise.\n\n    $$\n    \\acos(x) = 2 * \\atan(\\sqrt(1 - x^2) / (1 + x)) if x != -1\n             = pi                                  if x == -1\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.acosh",
    "summary": "Acosh operation",
    "description": "Returns `Acosh(operand)` element-wise.\n\n    $$\n    \\acosh(x) = log(x + sqrt(x^2 - 1))      if x >= -1\n    \\acosh(x) = nan                         if x < -1\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.asin",
    "summary": "Asin operator",
    "description": "Returns `Asin(operand)` element-wise.\n\n    $$\n    \\asin(x) = 2 * atan(x / (1 + sqrt(1 - x^2)))\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.asinh",
    "summary": "Asinh operation",
    "description": "Returns `Asinh(operand)` element-wise.\n\n    $$\n    \\asinh(x) = log(x + sqrt(x^2 + 1))\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.atan",
    "summary": "Atan operator",
    "description": "Returns `Atan(operand)` element-wise.\n\n    $$\n    \\atan(x) = \\atan2(x, 1)\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.atanh",
    "summary": "Atanh operator",
    "description": "Returns `Atanh(operand)` element-wise.\n\n    $$\n    \\atanh(x) = 0.5 * log((1 + x) / (1 - x)) if abs(x) <= 1\n              = nan                          otherwise\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.bessel_i1e",
    "summary": "Bessel function of order 1",
    "description": "Returns `bessel_i1e(operand)` element-wise.",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.broadcast_add",
    "summary": "Addition operator (with optional broadcasting)",
    "description": "Returns `lhs + rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_and",
    "summary": "Logical and operator (with optional broadcasting)",
    "description": "Returns `logical_and(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyPredOrIntTensor" },
      { "name": "rhs", "type": "HLO_AnyPredOrIntTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_atan2",
    "summary": "Atan2 operator (with optional broadcasting)",
    "description": "Returns `atan2(lhs/rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_compare",
    "summary": "Compare operator (with optional broadcasting)",
    "description": "Compares `lhs` and `rhs` elementwise according to `comparison_direction`\n    and `compare_type`. If unspecified, `compare_type` is FLOAT for float element\n    types, SIGNED for signed element types and UNSIGNED for unsigned element\n    types.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_comparison_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" },
      { "name": "comparison_direction", "type": "CHLO_ComparisonDirectionAttr" },
      { "name": "compare_type", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_complex",
    "summary": "Complex operator (with optional broadcasting)",
    "description": "Performs element-wise conversion of a pair of real and imaginary values to\n    a complex value.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyFpTensor" },
      { "name": "rhs", "type": "HLO_AnyFpTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_divide",
    "summary": "Division operator (with optional broadcasting)",
    "description": "Returns `lhs / rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_maximum",
    "summary": "Maximum operator (with optional broadcasting)",
    "description": "Returns `max(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_minimum",
    "summary": "Minimum operator (with optional broadcasting)",
    "description": "Returns `min(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_multiply",
    "summary": "Multiplication operator (with optional broadcasting)",
    "description": "Returns `lhs * rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_next_after",
    "summary": "std::nextafter operator (with optional broadcasting)",
    "description": "Returns the next representable value of `lhs` in the direction of `rhs`,\n    element-wise. It can also return a subnormal number.\n\n    Equivalent to the C++ std::nextafter function.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_or",
    "summary": "Logical or operator (with optional broadcasting)",
    "description": "Returns `logical_or(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyPredOrIntTensor" },
      { "name": "rhs", "type": "HLO_AnyPredOrIntTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_polygamma",
    "summary": "Polygamma function (with optional broadcasting)",
    "description": "Returns `Polygamma(operand, operand)` element-wise.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_power",
    "summary": "Power operator (with optional broadcasting)",
    "description": "Returns `lhs ^ rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_remainder",
    "summary": "Remainder operator (with optional broadcasting)",
    "description": "Returns `lhs % rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_select",
    "summary": "Select operator (with optional numpy-style broadcasting)",
    "description": "Constructs an output array from elements of two input arrays, based on the\n    values of a predicate array.\n\n    See https://www.tensorflow.org/xla/operation_semantics#select",
    "inputs": [
      { "name": "pred", "type": "HLO_PredTensor" },
      { "name": "on_true", "type": "HLO_AnyTensor" },
      { "name": "on_false", "type": "HLO_AnyTensor" }
    ],
    "assemblyFormat": "$pred `,` $on_true `,` $on_false attr-dict `:`\n    `(` type($pred) `,` type($on_true) `,` type($on_false) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_shift_left",
    "summary": "Shift left operator (with optional broadcasting)",
    "description": "Returns `lhs << rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_shift_right_arithmetic",
    "summary": "Shift right arithmetic operator (with optional broadcasting)",
    "description": "Returns `lhs >> rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_shift_right_logical",
    "summary": "Shift right logical operator (with optional broadcasting)",
    "description": "Returns `lhs >> rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_subtract",
    "summary": "Subtraction operator (with optional broadcasting)",
    "description": "Returns `lhs - rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_xor",
    "summary": "Logical xor operator (with optional broadcasting)",
    "description": "Returns `logical_xor(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyPredOrIntTensor" },
      { "name": "rhs", "type": "HLO_AnyPredOrIntTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.broadcast_zeta",
    "summary": "Hurwitz zeta function",
    "description": "Returns `Zeta(operand, operand)` element-wise.\n\n    $$\n    \\(\\zeta(x, q) = \\sum_{n=0}^{\\infty} (q + n)^{-x}\\)\n    $$",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyFpTensor" },
      { "name": "rhs", "type": "HLO_AnyFpTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:`\n    `(` type($lhs) `,` type($rhs) `)` `->` type(results)"
  },
  {
    "name": "chlo.conj",
    "summary": "Conj operator",
    "description": "Returns `Conj(operand)` element-wise.\n\n    $$\n    \\conj(x) = (\\real(x), \\neg(\\imag(x)))\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.constant",
    "summary": "Constant operator",
    "description": "Represents a constant value.",
    "outputs": [
      { "name": "output", "type": "HLO_StaticShapeTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "ElementsAttr" }
    ],
    "assemblyFormat": "attr-dict $value"
  },
  {
    "name": "chlo.constant_like",
    "summary": "Constant like operator",
    "description": "Returns a splat constant of the same shape as the operand.",
    "inputs": [
      { "name": "operand", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "TypedAttrInterface" }
    ]
  },
  {
    "name": "chlo.cosh",
    "summary": "Cosh operator",
    "description": "Returns `Cosh(operand)` element-wise.\n\n    $$\n    \\cosh(x) = (e^x + e^-x) / 2\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.digamma",
    "summary": "Digamma function",
    "description": "Returns `Digamma(operand)` element-wise.",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.erf",
    "summary": "Erfc operator",
    "description": "Computes the Gauss error function of `x` element-wise.\n\n    erf(x) = erf_impl(x)            if |x| < 1\n           = 1 - erfc_impl(x)       otherwise",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.erf_inv",
    "summary": "Inverse Erf",
    "description": "Returns `ErfInv(operand)` element-wise.",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.erfc",
    "summary": "Erfc operator",
    "description": "Computes an approximation of the error function complement (1 - erf(x)).\n\n    erfc(x) = erfc_impl(x)           if |x| > 1\n            = 1 - erf_impl(x)        otherwise",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.is_inf",
    "summary": "IsInf predicate",
    "description": "Returns if a value is +/-inf element-wise.",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.is_neg_inf",
    "summary": "IsNegInf predicate",
    "description": "Returns if a value is -inf element-wise.",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.is_pos_inf",
    "summary": "IsPosInf predicate",
    "description": "Returns if a value is +inf element-wise.",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.lgamma",
    "summary": "Lgamma function",
    "description": "Returns `Lgamma(operand)` element-wise.",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.next_after",
    "summary": "std::nextafter operator",
    "description": "Returns the next representable value of `x` in the direction of `y`,\n    element-wise. It can also return a subnormal number.\n\n    Equivalent to the C++ std::nextafter function.",
    "inputs": [
      { "name": "x", "type": "HLO_AnyFpTensor" },
      { "name": "y", "type": "HLO_AnyFpTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_AnyFpTensor" }
    ],
    "assemblyFormat": "$x `,` $y attr-dict `:` type($x) `,` type($y) `->` type(results)"
  },
  {
    "name": "chlo.polygamma",
    "summary": "Polygamma function",
    "description": "Returns `Polygamma(operand, operand)` element-wise.",
    "inputs": [
      { "name": "n", "type": "HLO_AnyFpTensor" },
      { "name": "x", "type": "HLO_AnyFpTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_AnyFpTensor" }
    ],
    "assemblyFormat": "$n `,` $x attr-dict `:` type($n) `,` type($x) `->` type(results)"
  },
  {
    "name": "chlo.ragged_dot",
    "summary": "Computes a matmul over a single ragged dimension",
    "description": "This operation takes three tensor args---lhs, rhs, and group_sizes---and\n    a \"ragged_dot_dimension_numbers\" attribute. Like dot_general, the lhs and\n    rhs are allowed arbitrary batch and contracting dimensions. Additionally,\n    the lhs is required to have one ragged dimension, and the rhs may have at\n    most one group dimension. The op has three modes, depending on the kind of\n    the lhs ragged dimension.\n\n    In mode 1, the shape-signature is `[b,m,k], [g,b,k,n], [b,g] -> [b,m,n]`.\n    Here the ragged dimension is an lhs non-contracting dimension (`m`). The\n    dimensions `b` and `k` represent batch and contracting dimensions\n    respectively. The rhs is required to have a group dimension (`g`).\n\n    In mode 2, the shape-signature is `[b,m,k], [b,k,n], [b,g] -> [g,b,m,n]`.\n    Here the ragged dimension is an lhs/rhs contracting dimension (`k`).\n\n    In mode 3, the shape-signature is `[b,m,k], [b,k,n], [g] -> [b,m,n]`. Here\n    the ragged dimension is an lhs/rhs batch dimension (`b`).",
    "inputs": [
      { "name": "lhs", "type": "HLO_AnyTensor" },
      { "name": "rhs", "type": "HLO_AnyTensor" },
      { "name": "group_sizes", "type": "Arg" },
      { "name": "ragged_dot_dimension_numbers", "type": "CHLO_RaggedDotDimensionNumbers" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_AnyTensor" }
    ],
    "attributes": [
      { "name": "precision_config", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "chlo.sinh",
    "summary": "Sinh operation",
    "description": "Returns `Sinh(operand)` element-wise.\n\n    $$\n    \\sinh(x) = (e^x - e^-x) / 2                     if |x| < 1\n             = e^(x + log(1/2)) - e^(-x + log(1/2)) otherwise.\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.square",
    "summary": "Square operation",
    "description": "Returns `Square(operand)` element-wise.\n\n    $$\n    \\square(x) = complex((x.real - x.imag) * (x.real + x.imag), x.real * x.imag * 2) if x is a complex number\n               = x * x                                                               otherwise\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.tan",
    "summary": "Tan operation",
    "description": "Returns `Tan(operand)` element-wise.\n\n    $$\n    \\tan(x) = \\sin(x) / \\cos(x)\n    $$",
    "inputs": [
      { "name": "operand", "type": "ArgTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "chlo.top_k",
    "summary": "Finds values and indices of the `k` largest elements for the last dimension",
    "description": "If the input is a vector (rank-1), finds the `k` largest entries in the\n    vector and outputs their values and indices as vectors.  Thus `values[j]` is\n    the `j`-th largest entry in `input`, and its index is `indices[j]`.\n\n    For matrices (resp. higher rank input), computes the top `k` entries in each\n    row (resp. vector along the last dimension).  Thus,\n\n    ```\n    values.shape = indices.shape = input.shape[:-1] + [k]\n    ```\n\n    If two elements are equal, the lower-index element appears first.",
    "inputs": [
      { "name": "operand", "type": "Arg" },
      { "name": "k", "type": "Arg" }
    ],
    "outputs": [
      { "name": "values", "type": "HLO_AnyTensor" },
      { "name": "indices", "type": "HLO_AnyTensor" }
    ],
    "assemblyFormat": "`(`$operand `,` `k` `=` $k`)` attr-dict `:`\n    type($operand) `->` `(`type($values)`,` type($indices)`)`"
  },
  {
    "name": "chlo.zeta",
    "summary": "Hurwitz zeta function",
    "description": "Returns `Zeta(operand, operand)` element-wise.\n\n    $$\n    \\(\\zeta(x, q) = \\sum_{n=0}^{\\infty} (q + n)^{-x}\\)\n    $$",
    "inputs": [
      { "name": "x", "type": "HLO_AnyFpTensor" },
      { "name": "q", "type": "HLO_AnyFpTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_AnyFpTensor" }
    ],
    "assemblyFormat": "$x `,` $q attr-dict `:` type($x) `,` type($q) `->` type(results)"
  },
  {
    "name": "complex.angle",
    "summary": "computes argument value of a complex number",
    "description": "The `angle` op takes a single complex number and computes its argument value with a branch cut along the negative real axis.\n\n    Example:\n\n    ```mlir\n         %a = complex.angle %b : complex<f32>\n    ```",
    "inputs": [
      { "name": "complex", "type": "Complex" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyFloat" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$complex (`fastmath` `` $fastmath^)? attr-dict `:` type($complex)"
  },
  {
    "name": "complex.atan2",
    "summary": "complex 2-argument arctangent",
    "description": "For complex numbers it is expressed using complex logarithm\n    atan2(y, x) = -i * log((x + i * y) / sqrt(x**2 + y**2))\n\n    Example:\n\n    ```mlir\n    %a = complex.atan2 %b, %c : complex<f32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Complex" },
      { "name": "rhs", "type": "Complex" }
    ],
    "outputs": [
      { "name": "result", "type": "Complex" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs (`fastmath` `` $fastmath^)? attr-dict `:` type($result)"
  },
  {
    "name": "complex.bitcast",
    "summary": "computes bitcast between complex and equal arith types",
    "description": "Example:\n\n    ```mlir\n         %a = complex.bitcast %b : complex<f32> -> i64\n    ```",
    "inputs": [
      { "name": "operand", "type": "AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "complex.conj",
    "summary": "Calculate the complex conjugate",
    "description": "The `conj` op takes a single complex number and computes the\n    complex conjugate.\n\n    Example:\n\n    ```mlir\n    %a = complex.conj %b: complex<f32>\n    ```",
    "inputs": [
      { "name": "complex", "type": "Complex" }
    ],
    "outputs": [
      { "name": "result", "type": "Complex" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$complex (`fastmath` `` $fastmath^)? attr-dict `:` type($complex)"
  },
  {
    "name": "complex.cos",
    "summary": "computes cosine of a complex number",
    "description": "The `cos` op takes a single complex number and computes the cosine of\n    it, i.e. `cos(x)`, where `x` is the input value.\n\n    Example:\n\n    ```mlir\n    %a = complex.cos %b : complex<f32>\n    ```",
    "inputs": [
      { "name": "complex", "type": "Complex" }
    ],
    "outputs": [
      { "name": "result", "type": "Complex" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$complex (`fastmath` `` $fastmath^)? attr-dict `:` type($complex)"
  },
  {
    "name": "complex.create",
    "summary": "complex number creation operation",
    "description": "The `complex.create` operation creates a complex number from two\n    floating-point operands, the real and the imaginary part.\n\n    Example:\n\n    ```mlir\n    %a = complex.create %b, %c : complex<f32>\n    ```",
    "inputs": [
      { "name": "real", "type": "AnyFloat" },
      { "name": "imaginary", "type": "AnyFloat" }
    ],
    "outputs": [
      { "name": "complex", "type": "Complex" }
    ],
    "assemblyFormat": "$real `,` $imaginary attr-dict `:` type($complex)"
  },
  {
    "name": "complex.eq",
    "summary": "computes whether two complex values are equal",
    "description": "The `eq` op takes two complex numbers and returns whether they are equal.\n\n    Example:\n\n    ```mlir\n    %a = complex.eq %b, %c : complex<f32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Complex" },
      { "name": "rhs", "type": "Complex" }
    ],
    "outputs": [
      { "name": "result", "type": "I1" }
    ],
    "assemblyFormat": "$lhs `,` $rhs  attr-dict `:` type($lhs)"
  },
  {
    "name": "complex.expm1",
    "summary": "computes exponential of a complex number minus 1",
    "description": "complex.expm1(x) := complex.exp(x) - 1\n\n    Example:\n\n    ```mlir\n    %a = complex.expm1 %b : complex<f32>\n    ```",
    "inputs": [
      { "name": "complex", "type": "Complex" }
    ],
    "outputs": [
      { "name": "result", "type": "Complex" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$complex (`fastmath` `` $fastmath^)? attr-dict `:` type($complex)"
  },
  {
    "name": "complex.im",
    "summary": "extracts the imaginary part of a complex number",
    "description": "The `im` op takes a single complex number and extracts the imaginary part.\n\n    Example:\n\n    ```mlir\n    %a = complex.im %b : complex<f32>\n    ```",
    "inputs": [
      { "name": "complex", "type": "Complex" }
    ],
    "outputs": [
      { "name": "imaginary", "type": "AnyFloat" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$complex (`fastmath` `` $fastmath^)? attr-dict `:` type($complex)"
  },
  {
    "name": "complex.log1p",
    "summary": "computes natural logarithm of a complex number",
    "description": "The `log` op takes a single complex number and computes the natural\n    logarithm of one plus the given value, i.e. `log(1 + x)` or `log_e(1 + x)`,\n    where `x` is the input value. `e` denotes Euler's number and is\n    approximately equal to 2.718281.\n\n    Example:\n\n    ```mlir\n    %a = complex.log1p %b : complex<f32>\n    ```",
    "inputs": [
      { "name": "complex", "type": "Complex" }
    ],
    "outputs": [
      { "name": "result", "type": "Complex" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$complex (`fastmath` `` $fastmath^)? attr-dict `:` type($complex)"
  },
  {
    "name": "complex.neq",
    "summary": "computes whether two complex values are not equal",
    "description": "The `neq` op takes two complex numbers and returns whether they are not\n    equal.\n\n    Example:\n\n    ```mlir\n    %a = complex.neq %b, %c : complex<f32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Complex" },
      { "name": "rhs", "type": "Complex" }
    ],
    "outputs": [
      { "name": "result", "type": "I1" }
    ],
    "assemblyFormat": "$lhs `,` $rhs  attr-dict `:` type($lhs)"
  },
  {
    "name": "complex.powi",
    "summary": "complex number raised to signed integer power",
    "description": "The `powi` operation takes a `base` operand of complex type and a `power`\n    operand of signed integer type and returns one result of the same type\n    as `base`. The result is `base` raised to the power of `power`.\n\n    Example:\n\n    ```mlir\n    %a = complex.powi %b, %c : complex<f32>, i32\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Complex" },
      { "name": "rhs", "type": "AnySignlessInteger" }
    ],
    "outputs": [
      { "name": "result", "type": "Complex" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs (`fastmath` `` $fastmath^)? attr-dict `:` type($result) `,` type($rhs)"
  },
  {
    "name": "complex.re",
    "summary": "extracts the real part of a complex number",
    "description": "The `re` op takes a single complex number and extracts the real part.\n\n    Example:\n\n    ```mlir\n    %a = complex.re %b : complex<f32>\n    ```",
    "inputs": [
      { "name": "complex", "type": "Complex" }
    ],
    "outputs": [
      { "name": "real", "type": "AnyFloat" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$complex (`fastmath` `` $fastmath^)? attr-dict `:` type($complex)"
  },
  {
    "name": "complex.rsqrt",
    "summary": "complex reciprocal of square root",
    "description": "The `rsqrt` operation computes reciprocal of square root.\n\n    Example:\n\n    ```mlir\n    %a = complex.rsqrt %b : complex<f32>\n    ```",
    "inputs": [
      { "name": "complex", "type": "Complex" }
    ],
    "outputs": [
      { "name": "result", "type": "Complex" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$complex (`fastmath` `` $fastmath^)? attr-dict `:` type($complex)"
  },
  {
    "name": "complex.sign",
    "summary": "computes sign of a complex number",
    "description": "The `sign` op takes a single complex number and computes the sign of\n    it, i.e. `y = sign(x) = x / |x|` if `x != 0`, otherwise `y = 0`.\n\n    Example:\n\n    ```mlir\n    %a = complex.sign %b : complex<f32>\n    ```",
    "inputs": [
      { "name": "complex", "type": "Complex" }
    ],
    "outputs": [
      { "name": "result", "type": "Complex" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$complex (`fastmath` `` $fastmath^)? attr-dict `:` type($complex)"
  },
  {
    "name": "complex.sin",
    "summary": "computes sine of a complex number",
    "description": "The `sin` op takes a single complex number and computes the sine of\n    it, i.e. `sin(x)`, where `x` is the input value.\n\n    Example:\n\n    ```mlir\n    %a = complex.sin %b : complex<f32>\n    ```",
    "inputs": [
      { "name": "complex", "type": "Complex" }
    ],
    "outputs": [
      { "name": "result", "type": "Complex" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$complex (`fastmath` `` $fastmath^)? attr-dict `:` type($complex)"
  },
  {
    "name": "complex.sqrt",
    "summary": "complex square root",
    "description": "The `sqrt` operation takes a complex number and returns its square root.\n\n    Example:\n\n    ```mlir\n    %a = complex.sqrt %b : complex<f32>\n    ```",
    "inputs": [
      { "name": "complex", "type": "Complex" }
    ],
    "outputs": [
      { "name": "result", "type": "Complex" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$complex (`fastmath` `` $fastmath^)? attr-dict `:` type($complex)"
  },
  {
    "name": "emitc.add",
    "summary": "Addition operation",
    "description": "With the `emitc.add` operation the arithmetic operator + (addition) can\n    be applied.\n\n    Example:\n\n    ```mlir\n    // Custom form of the addition operation.\n    %0 = emitc.add %arg0, %arg1 : (i32, i32) -> i32\n    %1 = emitc.add %arg2, %arg3 : (!emitc.ptr<f32>, i32) -> !emitc.ptr<f32>\n    ```\n    ```c++\n    // Code emitted for the operations above.\n    int32_t v5 = v1 + v2;\n    float* v6 = v3 + v4;\n    ```",
    "inputs": [
      { "name": "lhs", "type": "EmitCType" },
      { "name": "rhs", "type": "EmitCType" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "emitc.apply",
    "summary": "Apply operation",
    "description": "With the `emitc.apply` operation the operators & (address of) and * (contents of)\n    can be applied to a single operand.\n\n    Example:\n\n    ```mlir\n    // Custom form of applying the & operator.\n    %0 = emitc.apply \"&\"(%arg0) : (!emitc.lvalue<i32>) -> !emitc.ptr<i32>\n\n    // Generic form of the same operation.\n    %0 = \"emitc.apply\"(%arg0) {applicableOperator = \"&\"}\n        : (!emitc.lvalue<i32>) -> !emitc.ptr<i32>\n\n    ```",
    "inputs": [
      { "name": "applicableOperator", "type": "Arg" },
      { "name": "operand", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "EmitCType" }
    ],
    "assemblyFormat": "$applicableOperator `(` $operand `)` attr-dict `:` functional-type($operand, results)"
  },
  {
    "name": "emitc.assign",
    "summary": "Assign operation",
    "description": "The `emitc.assign` operation stores an SSA value to the location designated by an\n    EmitC variable. This operation doesn't return any value. The assigned value\n    must be of the same type as the variable being assigned. The operation is\n    emitted as a C/C++ '=' operator.\n\n    Example:\n\n    ```mlir\n    // Integer variable\n    %0 = \"emitc.variable\"(){value = 42 : i32} : () -> !emitc.lvalue<i32>\n    %1 = emitc.call_opaque \"foo\"() : () -> (i32)\n\n    // Assign emitted as `... = ...;`\n    \"emitc.assign\"(%0, %1) : (!emitc.lvalue<i32>, i32) -> ()\n    ```",
    "inputs": [
      { "name": "var", "type": "Res" },
      { "name": "value", "type": "EmitCType" }
    ],
    "assemblyFormat": "$value `:` type($value) `to` $var `:` type($var) attr-dict"
  },
  {
    "name": "emitc.bitwise_and",
    "summary": "Bitwise and operation",
    "description": "With the `emitc.bitwise_and` operation the bitwise operator & (and) can\n    be applied.\n\n    Example:\n\n    ```mlir\n    %0 = emitc.bitwise_and %arg0, %arg1 : (i32, i32) -> i32\n    ```\n    ```c++\n    // Code emitted for the operation above.\n    int32_t v3 = v1 & v2;\n    ```",
    "inputs": [
      { "name": "lhs", "type": "EmitCType" },
      { "name": "rhs", "type": "EmitCType" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "emitc.bitwise_left_shift",
    "summary": "Bitwise left shift operation",
    "description": "With the `emitc.bitwise_left_shift` operation the bitwise operator <<\n    (left shift) can be applied.\n\n    Example:\n\n    ```mlir\n    %0 = emitc.bitwise_left_shift %arg0, %arg1 : (i32, i32) -> i32\n    ```\n    ```c++\n    // Code emitted for the operation above.\n    int32_t v3 = v1 << v2;\n    ```",
    "inputs": [
      { "name": "lhs", "type": "EmitCType" },
      { "name": "rhs", "type": "EmitCType" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "emitc.bitwise_not",
    "summary": "Bitwise not operation",
    "description": "With the `emitc.bitwise_not` operation the bitwise operator ~ (not) can\n    be applied.\n\n    Example:\n\n    ```mlir\n    %0 = emitc.bitwise_not %arg0 : (i32) -> i32\n    ```\n    ```c++\n    // Code emitted for the operation above.\n    int32_t v2 = ~v1;\n    ```",
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "emitc.bitwise_or",
    "summary": "Bitwise or operation",
    "description": "With the `emitc.bitwise_or` operation the bitwise operator | (or)\n    can be applied.\n\n    Example:\n\n    ```mlir\n    %0 = emitc.bitwise_or %arg0, %arg1 : (i32, i32) -> i32\n    ```\n    ```c++\n    // Code emitted for the operation above.\n    int32_t v3 = v1 | v2;\n    ```",
    "inputs": [
      { "name": "lhs", "type": "EmitCType" },
      { "name": "rhs", "type": "EmitCType" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "emitc.bitwise_right_shift",
    "summary": "Bitwise right shift operation",
    "description": "With the `emitc.bitwise_right_shift` operation the bitwise operator >>\n    (right shift) can be applied.\n\n    Example:\n\n    ```mlir\n    %0 = emitc.bitwise_right_shift %arg0, %arg1 : (i32, i32) -> i32\n    ```\n    ```c++\n    // Code emitted for the operation above.\n    int32_t v3 = v1 >> v2;\n    ```",
    "inputs": [
      { "name": "lhs", "type": "EmitCType" },
      { "name": "rhs", "type": "EmitCType" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "emitc.bitwise_xor",
    "summary": "Bitwise xor operation",
    "description": "With the `emitc.bitwise_xor` operation the bitwise operator ^ (xor)\n    can be applied.\n\n    Example:\n\n    ```mlir\n    %0 = emitc.bitwise_xor %arg0, %arg1 : (i32, i32) -> i32\n    ```\n    ```c++\n    // Code emitted for the operation above.\n    int32_t v3 = v1 ^ v2;\n    ```",
    "inputs": [
      { "name": "lhs", "type": "EmitCType" },
      { "name": "rhs", "type": "EmitCType" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "emitc.call",
    "summary": "Call operation",
    "description": "The `emitc.call` operation represents a direct call to an `emitc.func`\n    that is within the same symbol scope as the call. The operands and result type\n    of the call must match the specified function type. The callee is encoded as a\n    symbol reference attribute named \"callee\".\n\n    Example:\n\n    ```mlir\n    %2 = emitc.call @my_add(%0, %1) : (f32, f32) -> f32\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "callee", "type": "FlatSymbolRefAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$callee `(` $operands `)` attr-dict `:` functional-type($operands, results)"
  },
  {
    "name": "emitc.call_opaque",
    "summary": "Opaque call operation",
    "description": "The `emitc.call_opaque` operation represents a C++ function call. The callee\n    can be an arbitrary non-empty string. The call allows specifying order\n    of operands and attributes in the call as follows:\n\n    - integer value of index type refers to an operand;\n    - attribute which will get lowered to constant value in call;\n\n    Example:\n\n    ```mlir\n    // Custom form defining a call to `foo()`.\n    %0 = emitc.call_opaque \"foo\" () : () -> i32\n\n    // Generic form of the same operation.\n    %0 = \"emitc.call_opaque\"() {callee = \"foo\"} : () -> i32\n    ```",
    "inputs": [
      { "name": "callee", "type": "Arg" },
      { "name": "args", "type": "Arg" },
      { "name": "template_args", "type": "Arg" },
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "$callee `(` $operands `)` attr-dict `:` functional-type($operands, results)"
  },
  {
    "name": "emitc.cast",
    "summary": "Cast operation",
    "description": "The `emitc.cast` operation performs an explicit type conversion and is emitted\n    as a C-style cast expression. It can be applied to integer, float, index\n    and EmitC types.\n\n    Example:\n\n    ```mlir\n    // Cast from `int32_t` to `float`\n    %0 = emitc.cast %arg0: i32 to f32\n\n    // Cast from `void` to `int32_t` pointer\n    %1 = emitc.cast %arg1 :\n        !emitc.ptr<!emitc.opaque<\"void\">> to !emitc.ptr<i32>\n    ```",
    "inputs": [
      { "name": "source", "type": "EmitCType" }
    ],
    "outputs": [
      { "name": "dest", "type": "EmitCType" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($dest)"
  },
  {
    "name": "emitc.class",
    "summary": "Represents a C++ class definition, encapsulating fields and methods.",
    "description": "The `emitc.class` operation defines a C++ class, acting as a container\n    for its data fields (`emitc.field`) and methods (`emitc.func`).\n    It creates a distinct scope, isolating its contents from the surrounding\n    MLIR region, similar to how C++ classes encapsulate their internals.\n\n    Example:\n\n    ```mlir\n    emitc.class @modelClass {\n      emitc.field @fieldName0 : !emitc.array<1xf32> = {emitc.opaque = \"input_tensor\"}\n      emitc.func @execute() {\n        %0 = \"emitc.constant\"() <{value = 0 : index}> : () -> !emitc.size_t\n        %1 = get_field @fieldName0 : !emitc.array<1xf32>\n        %2 = subscript %1[%0] : (!emitc.array<1xf32>, !emitc.size_t) -> !emitc.lvalue<f32>\n        return\n      }\n    }\n    // Class with a final specifer\n    emitc.class final @modelClass {\n      emitc.field @fieldName0 : !emitc.array<1xf32> = {emitc.opaque = \"input_tensor\"}\n      emitc.func @execute() {\n        %0 = \"emitc.constant\"() <{value = 0 : index}> : () -> !emitc.size_t\n        %1 = get_field @fieldName0 : !emitc.array<1xf32>\n        %2 = subscript %1[%0] : (!emitc.array<1xf32>, !emitc.size_t) -> !emitc.lvalue<f32>\n        return\n      }\n    }\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "final_specifier", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`final` $final_specifier^)? $sym_name attr-dict-with-keyword $body"
  },
  {
    "name": "emitc.cmp",
    "summary": "Comparison operation",
    "description": "With the `emitc.cmp` operation the comparison operators ==, !=, <, <=, >, >=, <=> \n    can be applied.\n\n    Its first argument is an attribute that defines the comparison operator:\n\n    - equal to (mnemonic: `\"eq\"`; integer value: `0`)\n    - not equal to (mnemonic: `\"ne\"`; integer value: `1`)\n    - less than (mnemonic: `\"lt\"`; integer value: `2`)\n    - less than or equal to (mnemonic: `\"le\"`; integer value: `3`)\n    - greater than (mnemonic: `\"gt\"`; integer value: `4`)\n    - greater than or equal to (mnemonic: `\"ge\"`; integer value: `5`)\n    - three-way-comparison (mnemonic: `\"three_way\"`; integer value: `6`)\n\n    Example:\n    ```mlir\n    // Custom form of the cmp operation.\n    %0 = emitc.cmp eq, %arg0, %arg1 : (i32, i32) -> i1\n    %1 = emitc.cmp lt, %arg2, %arg3 : \n        (\n          !emitc.opaque<\"std::valarray<float>\">,\n          !emitc.opaque<\"std::valarray<float>\">\n        ) -> !emitc.opaque<\"std::valarray<bool>\">\n    ```\n    ```c++\n    // Code emitted for the operations above.\n    bool v5 = v1 == v2;\n    std::valarray<bool> v6 = v3 < v4;\n    ```",
    "inputs": [
      { "name": "lhs", "type": "EmitCType" },
      { "name": "rhs", "type": "EmitCType" }
    ],
    "attributes": [
      { "name": "predicate", "type": "EmitC_CmpPredicateAttr" }
    ],
    "assemblyFormat": "$predicate `,` operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "emitc.conditional",
    "summary": "Conditional (ternary) operation",
    "description": "With the `emitc.conditional` operation the ternary conditional operator can\n    be applied.\n\n    Example:\n\n    ```mlir\n    %0 = emitc.cmp gt, %arg0, %arg1 : (i32, i32) -> i1\n\n    %c0 = \"emitc.constant\"() {value = 10 : i32} : () -> i32\n    %c1 = \"emitc.constant\"() {value = 11 : i32} : () -> i32\n\n    %1 = emitc.conditional %0, %c0, %c1 : i32\n    ```\n    ```c++\n    // Code emitted for the operations above.\n    bool v3 = v1 > v2;\n    int32_t v4 = 10;\n    int32_t v5 = 11;\n    int32_t v6 = v3 ? v4 : v5;\n    ```",
    "inputs": [
      { "name": "condition", "type": "I1" },
      { "name": "true_value", "type": "EmitCType" },
      { "name": "false_value", "type": "EmitCType" }
    ],
    "outputs": [
      { "name": "result", "type": "EmitCType" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "emitc.constant",
    "summary": "Constant operation",
    "description": "The `emitc.constant` operation produces an SSA value equal to some constant\n    specified by an attribute. This can be used to form simple integer and\n    floating point constants, as well as more exotic things like tensor\n    constants. The `emitc.constant` operation also supports the EmitC opaque\n    attribute and the EmitC opaque type. Since folding is supported,\n    it should not be used with pointers.\n\n    Example:\n\n    ```mlir\n    // Integer constant\n    %0 = \"emitc.constant\"(){value = 42 : i32} : () -> i32\n\n    // Constant emitted as `char = CHAR_MIN;`\n    %1 = \"emitc.constant\"() {value = #emitc.opaque<\"CHAR_MIN\">}\n      : () -> !emitc.opaque<\"char\">\n    ```",
    "attributes": [
      { "name": "value", "type": "EmitC_OpaqueOrTypedAttr" }
    ]
  },
  {
    "name": "emitc.declare_func",
    "summary": "An operation to declare a function",
    "description": "The `emitc.declare_func` operation allows to insert a function declaration for an\n    `emitc.func` at a specific position. The operation only requires the \"callee\"\n    of the `emitc.func` to be specified as an attribute.\n\n    Example:\n\n    ```mlir\n    emitc.declare_func @bar\n    emitc.func @foo(%arg0: i32) -> i32 {\n      %0 = emitc.call @bar(%arg0) : (i32) -> (i32)\n      emitc.return %0 : i32\n    }\n\n    emitc.func @bar(%arg0: i32) -> i32 {\n      emitc.return %arg0 : i32\n    }\n    ```\n\n    ```c++\n    // Code emitted for the operations above.\n    int32_t bar(int32_t v1);\n    int32_t foo(int32_t v1) {\n      int32_t v2 = bar(v1);\n      return v2;\n    }\n\n    int32_t bar(int32_t v1) {\n      return v1;\n    }\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$sym_name attr-dict"
  },
  {
    "name": "emitc.div",
    "summary": "Division operation",
    "description": "With the `emitc.div` operation the arithmetic operator / (division) can\n    be applied.\n\n    Example:\n\n    ```mlir\n    // Custom form of the division operation.\n    %0 = emitc.div %arg0, %arg1 : (i32, i32) -> i32\n    %1 = emitc.div %arg2, %arg3 : (f32, f32) -> f32\n    ```\n    ```c++\n    // Code emitted for the operations above.\n    int32_t v5 = v1 / v2;\n    float v6 = v3 / v4;\n    ```",
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "emitc.do",
    "summary": "Do-while operation",
    "description": "The `emitc.do` operation represents a C/C++ do-while loop construct that\n    repeatedly executes a body region as long as a condition region evaluates to\n    true. The operation has two regions:\n\n    1. A body region that contains the loop body\n    2. A condition region that must yield a boolean value (i1)\n\n    The condition is evaluated before each iteration as follows:\n    - The condition region must contain exactly one block with:\n      1. An `emitc.expression` operation producing an i1 value\n      2. An `emitc.yield` passing through the expression result\n    - The expression's body contains the actual condition logic\n\n    The body region is executed before the first evaluation of the \n    condition. Thus, there is a guarantee that the loop will be executed \n    at least once. The loop terminates when the condition yields false.\n\n    The canonical structure of `emitc.do` is:\n\n    ```mlir\n    emitc.do {\n      // Body region (no terminator required).\n      // Loop body operations...\n    } while {\n      // Condition region (must yield i1)\n      %condition = emitc.expression : () -> i1 {\n        // Condition computation...\n        %result = ... : i1  // Last operation must produce i1\n        emitc.yield %result : i1\n      }\n      // Forward expression result\n      emitc.yield %condition : i1  \n    }\n    ```\n\n    Example:\n\n    ```mlir\n    emitc.func @do_example() {\n      %counter = \"emitc.variable\"() <{value = 0 : i32}> : () -> !emitc.lvalue<i32>\n      %end = emitc.literal \"10\" : i32\n      %step = emitc.literal \"1\" : i32\n\n      emitc.do {\n        // Print current value\n        %val = emitc.load %counter : !emitc.lvalue<i32>\n        emitc.verbatim \"printf(\\\"%d\\\\n\\\", {});\" args %val : i32\n\n        // Increment counter\n        %new_val = emitc.add %val, %step : (i32, i32) -> i32\n        \"emitc.assign\"(%counter, %new_val) : (!emitc.lvalue<i32>, i32) -> ()\n      } while {\n        %condition = emitc.expression %counter, %end : (!emitc.lvalue<i32>, i32) -> i1 {\n          %current = emitc.load %counter : !emitc.lvalue<i32>\n          %cmp_res = emitc.cmp lt, %current, %end : (i32, i32) -> i1\n          emitc.yield %cmp_res : i1\n        }\n        emitc.yield %condition : i1\n      }\n      return\n    }\n    ```\n    ```c++\n    // Code emitted for the operation above.\n    void do_example() {\n      int32_t v1 = 0;\n      do {\n        int32_t v2 = v1;\n        printf(\"%d\\n\", v2);\n        int32_t v3 = v2 + 1;\n        v1 = v3;\n      } while (v1 < 10);\n      return;\n    }\n    ```"
  },
  {
    "name": "emitc.expression",
    "summary": "Expression operation",
    "description": "The `emitc.expression` operation returns a single SSA value which is yielded by\n    its single-basic-block region. The operation takes zero or more input operands \n    that are passed as block arguments to the region.\n\n    As the operation is to be emitted as a C expression, the operations within\n    its body must form a single Def-Use tree, or a DAG trivially expandable to\n    one, i.e. a DAG where each operation with side effects is only reachable\n    once from the expression root.\n\n    Input operands can be of both value types (`EmitCType`) and lvalue types\n    (`EmitC_LValueType`).\n\n    Example:\n    ```mlir\n    %r = emitc.expression %a, %b, %c : (i32, i32, i32) -> i32 {\n      %0 = emitc.call_opaque \"foo\"(%a) : (i32) -> i32\n      %1 = emitc.add %b, %c : (i32, i32) -> i32\n      %2 = emitc.mul %0, %1 : (i32, i32) -> i32\n      emitc.yield %2 : i32\n    }\n    ```\n\n    May be emitted as:\n    ```c++\n    int32_t v4 = foo(v1) * (v2 + v3);\n    ```\n\n    When specified, the optional `noinline` indicates that the expression is\n    to be emitted as seen above, i.e. as the rhs of an EmitC SSA value\n    definition. Otherwise, the expression may be emitted inline, i.e. directly\n    at its use.",
    "inputs": [
      { "name": "defs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "EmitCType" }
    ],
    "attributes": [
      { "name": "do_not_inline", "type": "UnitAttr" }
    ]
  },
  {
    "name": "emitc.field",
    "summary": "A field within a class",
    "description": "The `emitc.field` operation declares a named field within an `emitc.class`\n    operation. The field's type must be an EmitC type. \n\n    Example:\n\n    ```mlir\n    // Example with an attribute:\n    emitc.field @fieldName0 : !emitc.array<1xf32>  {emitc.opaque = \"another_feature\"}\n    // Example with no attribute:\n    emitc.field @fieldName0 : !emitc.array<1xf32>\n    // Example with an initial value:\n    emitc.field @fieldName0 : !emitc.array<1xf32> = dense<0.0>\n    // Example with an initial value and attributes:\n    emitc.field @fieldName0 : !emitc.array<1xf32> = dense<0.0> {\n      emitc.opaque = \"input_tensor\"}\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "type", "type": "TypeAttr" },
      { "name": "initial_value", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$sym_name\n       `:` custom<EmitCFieldOpTypeAndInitialValue>($type, $initial_value)\n       attr-dict"
  },
  {
    "name": "emitc.file",
    "summary": "A file container operation",
    "description": "A `file` represents a single C/C++ file.\n\n    `mlir-translate` ignores the body of all `emitc.file` ops\n    unless the `-file-id=id` flag is used. With that flag, all `emitc.file` ops\n    with matching id are emitted.\n\n    Example:\n\n    ```mlir\n    emitc.file \"main\" {\n      emitc.func @func_one() {\n        emitc.return\n      }\n    }\n    ```",
    "attributes": [
      { "name": "id", "type": "Builtin_StringAttr" }
    ],
    "assemblyFormat": "$id attr-dict-with-keyword $bodyRegion"
  },
  {
    "name": "emitc.for",
    "summary": "For operation",
    "description": "The `emitc.for` operation represents a C loop of the following form:\n\n    ```c++\n    for (T i = lb; i < ub; i += step) { /* ... */ } // where T is typeof(lb)\n    ```\n\n    The operation takes 3 SSA values as operands that represent the lower bound,\n    upper bound and step respectively, and defines an SSA value for its\n    induction variable. It has one region capturing the loop body. The induction\n    variable is represented as an argument of this region. This SSA value is a\n    signless integer, or an index. The step is a value of same type.\n\n    This operation has no result. The body region must contain exactly one block\n    that terminates with `emitc.yield`. Calling ForOp::build will create such a\n    region and insert the terminator implicitly if none is defined, so will the\n    parsing even in cases when it is absent from the custom format. For example:\n\n    ```mlir\n    // Index case.\n    emitc.for %iv = %lb to %ub step %step {\n      ... // body\n    }\n    ...\n    // Integer case.\n    emitc.for %iv_32 = %lb_32 to %ub_32 step %step_32 : i32 {\n      ... // body\n    }\n    ```",
    "inputs": [
      { "name": "lowerBound", "type": "IntegerIndexOrOpaqueType" },
      { "name": "upperBound", "type": "IntegerIndexOrOpaqueType" },
      { "name": "step", "type": "IntegerIndexOrOpaqueType" }
    ]
  },
  {
    "name": "emitc.func",
    "summary": "An operation with a name containing a single `SSACFG` region",
    "description": "Operations within the function cannot implicitly capture values defined\n    outside of the function, i.e. Functions are `IsolatedFromAbove`. All\n    external references must use function arguments or attributes that establish\n    a symbolic connection (e.g. symbols referenced by name via a string\n    attribute like SymbolRefAttr). While the MLIR textual form provides a nice\n    inline syntax for function arguments, they are internally represented as\n    “block arguments” to the first block in the region.\n\n    Only dialect attribute names may be specified in the attribute dictionaries\n    for function arguments, results, or the function itself.\n\n    Example:\n\n    ```mlir\n    // A function with no results:\n    emitc.func @foo(%arg0 : i32) {\n      emitc.call_opaque \"bar\" (%arg0) : (i32) -> ()\n      emitc.return\n    }\n\n    // A function with its argument as single result:\n    emitc.func @foo(%arg0 : i32) -> i32 {\n      emitc.return %arg0 : i32\n    }\n\n    // A function with specifiers attribute:\n    emitc.func @example_specifiers_fn_attr() -> i32\n                attributes {specifiers = [\"static\",\"inline\"]} {\n      %0 = emitc.call_opaque \"foo\" (): () -> i32\n      emitc.return %0 : i32\n    }\n\n    // An external function definition:\n    emitc.func private @extern_func(i32)\n                        attributes {specifiers = [\"extern\"]}\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "specifiers", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "emitc.get_field",
    "summary": "Obtain access to a field within a class instance",
    "description": "The `emitc.get_field` operation retrieves the lvalue of a\n     named field from a given class instance.\n\n     Example:\n\n     ```mlir\n     %0 = get_field @fieldName0 : !emitc.array<1xf32>\n     ```",
    "outputs": [
      { "name": "result", "type": "EmitCType" }
    ],
    "attributes": [
      { "name": "field_name", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$field_name `:` type($result) attr-dict"
  },
  {
    "name": "emitc.get_global",
    "summary": "Obtain access to a global variable",
    "description": "The `emitc.get_global` operation retrieves the lvalue of a\n     named global variable. If the global variable is marked constant, assigning\n     to that lvalue is undefined.\n\n     Example:\n\n     ```mlir\n     %x = emitc.get_global @foo : !emitc.array<2xf32>\n     %y = emitc.get_global @bar : !emitc.lvalue<i32>\n     ```",
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "name", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$name `:` type($result) attr-dict"
  },
  {
    "name": "emitc.global",
    "summary": "A global variable",
    "description": "The `emitc.global` operation declares or defines a named global variable.\n    The backing memory for the variable is allocated statically and described by\n    the variable's type, which must be an EmitC type.\n    Optionally, an `initial_value` can be provided.\n    Internal linkage can be specified using the `static_specifier` unit attribute\n    and external linkage can be specified using the `extern_specifier` unit attribute.\n    Note that the default linkage without those two keywords depends on whether\n    the target is C or C++ and whether the global variable is `const`.\n    The global variable can also be marked constant using the `const_specifier`\n    unit attribute. Writing to such constant global variables is\n    undefined.\n\n    The global variable can be accessed by using the `emitc.get_global` to\n    retrieve the value for the global variable.\n\n    Example:\n\n    ```mlir\n    // Global variable with an initial value.\n    emitc.global @x : !emitc.array<2xf32> = dense<0.0>\n    // Global variable with an initial values.\n    emitc.global @x : !emitc.array<3xi32> = dense<[0, 1, 2]>\n    // Global variable with an opaque initial value.\n    emitc.global @x : !emitc.opaque<\"char\"> = #emitc.opaque<\"CHAR_MIN\">\n    // External global variable\n    emitc.global extern @x : !emitc.array<2xf32>\n    // Constant global variable with internal linkage\n    emitc.global static const @x : i32 = 0\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "type", "type": "TypeAttr" },
      { "name": "initial_value", "type": "OptionalAttr" },
      { "name": "extern_specifier", "type": "UnitAttr" },
      { "name": "static_specifier", "type": "UnitAttr" },
      { "name": "const_specifier", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`extern` $extern_specifier^)?\n       (`static` $static_specifier^)?\n       (`const` $const_specifier^)?\n       $sym_name\n       `:` custom<EmitCGlobalOpTypeAndInitialValue>($type, $initial_value)\n       attr-dict"
  },
  {
    "name": "emitc.if",
    "summary": "If-then-else operation",
    "description": "The `emitc.if` operation represents an if-then-else construct for\n    conditionally executing two regions of code. The operand to an if operation\n    is a boolean value. For example:\n\n    ```mlir\n    emitc.if %b  {\n      ...\n    } else {\n      ...\n    }\n    ```\n\n    The \"then\" region has exactly 1 block. The \"else\" region may have 0 or 1\n    blocks. The blocks are always terminated with `emitc.yield`, which can be\n    left out to be inserted implicitly. This operation doesn't produce any\n    results.",
    "inputs": [
      { "name": "condition", "type": "I1" }
    ]
  },
  {
    "name": "emitc.include",
    "summary": "Include operation",
    "description": "The `emitc.include` operation allows to define a source file inclusion via the\n    `#include` directive.\n\n    Example:\n\n    ```mlir\n    // Custom form defining the inclusion of `<myheader>`.\n    emitc.include <\"myheader.h\">\n\n    // Generic form of the same operation.\n    \"emitc.include\" (){include = \"myheader.h\", is_standard_include} : () -> ()\n\n    // Custom form defining the inclusion of `\"myheader\"`.\n    emitc.include \"myheader.h\"\n\n    // Generic form of the same operation.\n    \"emitc.include\" (){include = \"myheader.h\"} : () -> ()\n    ```",
    "inputs": [
      { "name": "include", "type": "Arg" }
    ],
    "attributes": [
      { "name": "is_standard_include", "type": "UnitAttr" }
    ]
  },
  {
    "name": "emitc.literal",
    "summary": "Literal operation",
    "description": "The `emitc.literal` operation produces an SSA value equal to some constant\n    specified by an attribute.\n\n    Example:\n\n    ```mlir\n    %p0 = emitc.literal \"M_PI\" : f32\n    %1 = \"emitc.add\" (%arg0, %p0) : (f32, f32) -> f32\n    ```\n    ```c++\n    // Code emitted for the operation above.\n    float v2 = v1 + M_PI;\n    ```",
    "outputs": [
      { "name": "result", "type": "EmitCType" }
    ],
    "attributes": [
      { "name": "value", "type": "StrAttr" }
    ],
    "assemblyFormat": "$value attr-dict `:` type($result)"
  },
  {
    "name": "emitc.load",
    "summary": "Load an lvalue into an SSA value.",
    "description": "This operation loads the content of a modifiable lvalue into an SSA value. \n    Modifications of the lvalue executed after the load are not observable on \n    the produced value.\n\n    Example:\n\n    ```mlir\n    %1 = emitc.load %0 : !emitc.lvalue<i32>\n    ```\n    ```c++\n    // Code emitted for the operation above.\n    int32_t v2 = v1;\n    ```",
    "inputs": [
      { "name": "operand", "type": "Res" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand)"
  },
  {
    "name": "emitc.logical_and",
    "summary": "Logical and operation",
    "description": "With the `emitc.logical_and` operation the logical operator && (and) can\n    be applied.\n\n    Example:\n\n    ```mlir\n    %0 = emitc.logical_and %arg0, %arg1 : i32, i32\n    ```\n    ```c++\n    // Code emitted for the operation above.\n    bool v3 = v1 && v2;\n    ```",
    "inputs": [
      { "name": "lhs", "type": "EmitCType" },
      { "name": "rhs", "type": "EmitCType" }
    ],
    "assemblyFormat": "operands attr-dict `:` type(operands)"
  },
  {
    "name": "emitc.logical_not",
    "summary": "Logical not operation",
    "description": "With the `emitc.logical_not` operation the logical operator ! (negation) can\n    be applied.\n\n    Example:\n\n    ```mlir\n    %0 = emitc.logical_not %arg0 : i32\n    ```\n    ```c++\n    // Code emitted for the operation above.\n    bool v2 = !v1;\n    ```",
    "assemblyFormat": "operands attr-dict `:` type(operands)"
  },
  {
    "name": "emitc.logical_or",
    "summary": "Logical or operation",
    "description": "With the `emitc.logical_or` operation the logical operator || (inclusive or)\n    can be applied.\n\n    Example:\n\n    ```mlir\n    %0 = emitc.logical_or %arg0, %arg1 : i32, i32\n    ```\n    ```c++\n    // Code emitted for the operation above.\n    bool v3 = v1 || v2;\n    ```",
    "inputs": [
      { "name": "lhs", "type": "EmitCType" },
      { "name": "rhs", "type": "EmitCType" }
    ],
    "assemblyFormat": "operands attr-dict `:` type(operands)"
  },
  {
    "name": "emitc.member",
    "summary": "Member operation",
    "description": "With the `emitc.member` operation the member access operator `.` can be\n    applied.\n\n    Example:\n\n    ```mlir\n    %0 = \"emitc.member\" (%arg0) {member = \"a\"}\n        : (!emitc.lvalue<!emitc.opaque<\"mystruct\">>) -> !emitc.lvalue<i32>\n    %1 = \"emitc.member\" (%arg0) {member = \"b\"}\n        : (!emitc.lvalue<!emitc.opaque<\"mystruct\">>) -> !emitc.array<2xi32>\n    ```",
    "inputs": [
      { "name": "member", "type": "Arg" },
      { "name": "operand", "type": "EmitC_LValueOf" }
    ]
  },
  {
    "name": "emitc.member_of_ptr",
    "summary": "Member of pointer operation",
    "description": "With the `emitc.member_of_ptr` operation the member access operator `->`\n    can be applied.\n\n    Example:\n\n    ```mlir\n    %0 = \"emitc.member_of_ptr\" (%arg0) {member = \"a\"}\n        : (!emitc.lvalue<!emitc.ptr<!emitc.opaque<\"mystruct\">>>)\n        -> !emitc.lvalue<i32>\n    %1 = \"emitc.member_of_ptr\" (%arg0) {member = \"b\"}\n        : (!emitc.lvalue<!emitc.ptr<!emitc.opaque<\"mystruct\">>>)\n        -> !emitc.array<2xi32>\n    ```",
    "inputs": [
      { "name": "member", "type": "Arg" },
      { "name": "operand", "type": "EmitC_LValueOf" }
    ]
  },
  {
    "name": "emitc.mul",
    "summary": "Multiplication operation",
    "description": "With the `emitc.mul` operation the arithmetic operator * (multiplication) can\n    be applied.\n\n    Example:\n\n    ```mlir\n    // Custom form of the multiplication operation.\n    %0 = emitc.mul %arg0, %arg1 : (i32, i32) -> i32\n    %1 = emitc.mul %arg2, %arg3 : (f32, f32) -> f32\n    ```\n    ```c++\n    // Code emitted for the operations above.\n    int32_t v5 = v1 * v2;\n    float v6 = v3 * v4;\n    ```",
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "emitc.rem",
    "summary": "Remainder operation",
    "description": "With the `emitc.rem` operation the arithmetic operator % (remainder) can\n    be applied.\n\n    Example:\n\n    ```mlir\n    // Custom form of the remainder operation.\n    %0 = emitc.rem %arg0, %arg1 : (i32, i32) -> i32\n    ```\n    ```c++\n    // Code emitted for the operation above.\n    int32_t v5 = v1 % v2;\n    ```",
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "emitc.return",
    "summary": "Function return operation",
    "description": "The `emitc.return` operation represents a return operation within a function.\n    The operation takes zero or exactly one operand and produces no results.\n    The operand number and type must match the signature of the function\n    that contains the operation.\n\n    Example:\n\n    ```mlir\n    emitc.func @foo() -> (i32) {\n      ...\n      emitc.return %0 : i32\n    }\n    ```",
    "inputs": [
      { "name": "operand", "type": "Optional" }
    ],
    "assemblyFormat": "attr-dict ($operand^ `:` type($operand))?"
  },
  {
    "name": "emitc.sub",
    "summary": "Subtraction operation",
    "description": "With the `emitc.sub` operation the arithmetic operator - (subtraction) can\n    be applied.\n\n    Example:\n\n    ```mlir\n    // Custom form of the substraction operation.\n    %0 = emitc.sub %arg0, %arg1 : (i32, i32) -> i32\n    %1 = emitc.sub %arg2, %arg3 : (!emitc.ptr<f32>, i32) -> !emitc.ptr<f32>\n    %2 = emitc.sub %arg4, %arg5 : (!emitc.ptr<i32>, !emitc.ptr<i32>)\n        -> !emitc.ptrdiff_t\n    ```\n    ```c++\n    // Code emitted for the operations above.\n    int32_t v7 = v1 - v2;\n    float* v8 = v3 - v4;\n    ptrdiff_t v9 = v5 - v6;\n    ```",
    "inputs": [
      { "name": "lhs", "type": "EmitCType" },
      { "name": "rhs", "type": "EmitCType" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "emitc.subscript",
    "summary": "Subscript operation",
    "description": "With the `emitc.subscript` operation the subscript operator `[]` can be applied\n    to variables or arguments of array, pointer and opaque type.\n\n    Example:\n\n    ```mlir\n    %i = index.constant 1\n    %j = index.constant 7\n    %0 = emitc.subscript %arg0[%i, %j] : (!emitc.array<4x8xf32>, index, index)\n           -> !emitc.lvalue<f32>\n    %1 = emitc.subscript %arg1[%i] : (!emitc.ptr<i32>, index)\n           -> !emitc.lvalue<i32>\n    ```",
    "inputs": [
      { "name": "value", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "EmitC_LValueType" }
    ],
    "assemblyFormat": "$value `[` $indices `]` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "emitc.switch",
    "summary": "Switch operation",
    "description": "The `emitc.switch` is a control-flow operation that branches to one of\n    the given regions based on the values of the argument and the cases.\n    The operand to a switch operation is a opaque, integral or pointer\n    wide types.\n\n    The operation always has a \"default\" region and any number of case regions\n    denoted by integer constants. Control-flow transfers to the case region\n    whose constant value equals the value of the argument. If the argument does\n    not equal any of the case values, control-flow transfer to the \"default\"\n    region.\n\n    The operation does not return any value. Moreover, case regions must be\n    explicitly terminated using the `emitc.yield` operation. Default region is\n    yielded implicitly.\n\n    Example:\n\n    ```mlir\n    // Example:\n    emitc.switch %0 : i32\n    case 2 {\n      %1 = emitc.call_opaque \"func_b\" () : () -> i32\n      emitc.yield\n    }\n    case 5 {\n      %2 = emitc.call_opaque \"func_a\" () : () -> i32\n      emitc.yield\n    }\n    default {\n      %3 = \"emitc.constant\"(){value = 42.0 : f32} : () -> f32\n      emitc.call_opaque \"func2\" (%3) : (f32) -> ()\n    }\n    ```\n    ```c++\n    // Code emitted for the operations above.\n    switch (v1) {\n    case 2: {\n      int32_t v2 = func_b();\n      break;\n    }\n    case 5: {\n      int32_t v3 = func_a();\n      break;\n    }\n    default: {\n      float v4 = 4.200000000e+01f;\n      func2(v4);\n      break;\n    }\n    }\n    ```",
    "inputs": [
      { "name": "arg", "type": "IntegerIndexOrOpaqueType" }
    ],
    "attributes": [
      { "name": "cases", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$arg `:` type($arg) attr-dict custom<SwitchCases>($cases, $caseRegions) `\\n`\n    `` `default` $defaultRegion"
  },
  {
    "name": "emitc.unary_minus",
    "summary": "Unary minus operation",
    "description": "With the `emitc.unary_minus` operation the unary operator - (minus) can be\n    applied.\n\n    Example:\n\n    ```mlir\n    %0 = emitc.unary_minus %arg0 : (i32) -> i32\n    ```\n    ```c++\n    // Code emitted for the operation above.\n    int32_t v2 = -v1;\n    ```",
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "emitc.unary_plus",
    "summary": "Unary plus operation",
    "description": "With the `emitc.unary_plus` operation the unary operator + (plus) can be\n    applied.\n\n    Example:\n\n    ```mlir\n    %0 = emitc.unary_plus %arg0 : (i32) -> i32\n    ```\n    ```c++\n    // Code emitted for the operation above.\n    int32_t v2 = +v1;\n    ```",
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "emitc.variable",
    "summary": "Variable operation",
    "description": "The `emitc.variable` operation produces an SSA value equal to some value\n    specified by an attribute. This can be used to form simple integer and\n    floating point variables, as well as more exotic things like tensor\n    variables. The `emitc.variable` operation also supports the EmitC opaque\n    attribute and the EmitC opaque type. If further supports the EmitC\n    pointer type, whereas folding is not supported.\n    The `emitc.variable` is emitted as a C/C++ local variable.\n\n    Example:\n\n    ```mlir\n    // Integer variable\n    %0 = \"emitc.variable\"(){value = 42 : i32} : () -> !emitc.lvalue<i32>\n\n    // Variable emitted as `int32_t* = NULL;`\n    %1 = \"emitc.variable\"() {value = #emitc.opaque<\"NULL\">} \n      : () -> !emitc.lvalue<!emitc.ptr<!emitc.opaque<\"int32_t\">>>\n    ```\n\n    Since folding is not supported, it can be used with pointers.\n    As an example, it is valid to create pointers to `variable` operations\n    by using `apply` operations and pass these to a `call` operation.\n    ```mlir\n    %0 = \"emitc.variable\"() {value = 0 : i32} : () -> !emitc.lvalue<i32>\n    %1 = \"emitc.variable\"() {value = 0 : i32} : () -> !emitc.lvalue<i32>\n    %2 = emitc.apply \"&\"(%0) : (!emitc.lvalue<i32>) -> !emitc.ptr<i32>\n    %3 = emitc.apply \"&\"(%1) : (!emitc.lvalue<i32>) -> !emitc.ptr<i32>\n    emitc.call_opaque \"write\"(%2, %3)\n      : (!emitc.ptr<i32>, !emitc.ptr<i32>) -> ()\n    ```",
    "attributes": [
      { "name": "value", "type": "EmitC_OpaqueOrTypedAttr" }
    ]
  },
  {
    "name": "emitc.verbatim",
    "summary": "Verbatim operation",
    "description": "The `emitc.verbatim` operation produces no results and the value is emitted as is\n    followed by a line break  ('\\n' character) during translation.\n\n    Note: Use with caution. This operation can have arbitrary effects on the\n    semantics of the emitted code. Use semantically more meaningful operations\n    whenever possible. Additionally this op is *NOT* intended to be used to\n    inject large snippets of code.\n\n    This operation can be used in situations where a more suitable operation is\n    not yet implemented in the dialect or where preprocessor directives\n    interfere with the structure of the code. One example of this is to declare\n    the linkage of external symbols to make the generated code usable in both C\n    and C++ contexts:\n\n    ```c++\n    #ifdef __cplusplus\n    extern \"C\" {\n    #endif\n\n    ...\n    \n    #ifdef __cplusplus\n    }\n    #endif\n    ```\n\n    If the `emitc.verbatim` op has operands, then the `value` is interpreted as\n    format string, where `{}` is a placeholder for an operand in their order.\n    For example, `emitc.verbatim \"#pragma my src={} dst={}\" %src, %dest : i32, i32`\n    would be emitted as `#pragma my src=a dst=b` if `%src` became `a` and\n    `%dest` became `b` in the C code.\n    `{{` in the format string is interpreted as a single `{` and doesn't introduce\n    a placeholder.\n\n    Example:\n\n    ```mlir\n    emitc.verbatim \"typedef float f32;\"\n    emitc.verbatim \"#pragma my var={} property\" args %arg : f32\n    ```\n    ```c++\n    // Code emitted for the operation above.\n    typedef float f32;\n    #pragma my var=v1 property\n    ```",
    "inputs": [
      { "name": "fmtArgs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "value", "type": "StrAttr" }
    ],
    "assemblyFormat": "$value (`args` $fmtArgs^ `:` type($fmtArgs))? attr-dict"
  },
  {
    "name": "emitc.yield",
    "summary": "Block termination operation",
    "description": "The `emitc.yield` terminates its parent EmitC op's region, optionally yielding\n    an SSA value. The semantics of how the values are yielded is defined by the\n    parent operation.\n    If `emitc.yield` has an operand, the operand must match the parent operation's\n    result. If the parent operation defines no values, then the `emitc.yield`\n    may be left out in the custom syntax and the builders will insert one\n    implicitly. Otherwise, it has to be present in the syntax to indicate which\n    value is yielded.",
    "inputs": [
      { "name": "result", "type": "Optional" }
    ],
    "assemblyFormat": "attr-dict ($result^ `:` type($result))?"
  },
  {
    "name": "flow.call",
    "summary": "Calls a streamable external host function.",
    "description": "Calls a function taking/returning tensor values with stream semantics.\n    Tensors have their shapes captured and may be tied to denote in-place\n    operations. Asynchronous calls must have no side-effects.\n\n    Note that returned tensors must have their shapes declared prior to the call\n    as this is what allows the call to be made on the stream. If external host\n    logic is required to compute the shape (avoid at all costs!) a separate\n    func.call can be used outside of the stream to do so. If shapes are\n    unknowable until the operation is performed it should be made as a normal\n    asynchronous host call with 'coarse-fences' instead.",
    "inputs": [
      { "name": "arguments", "type": "Variadic" },
      { "name": "argument_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "callee", "type": "FlatSymbolRefAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$callee\n    `(` $arguments `)` attr-dict `:`\n    custom<ShapedFunctionType>(ref($arguments),\n                               type($arguments), $argument_dims,\n                               type($results), $result_dims,\n                               $tied_operands)"
  },
  {
    "name": "flow.channel.count",
    "summary": "Returns the total number of participants in the group.",
    "description": "Returns the total participant count in the collective communicator group.",
    "inputs": [
      { "name": "channel", "type": "FLOW_Channel" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$channel `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.channel.default",
    "summary": "Returns a default collective communication channel.",
    "description": "Returns a channel initialized using the runtime environment.",
    "outputs": [
      { "name": "result", "type": "FLOW_Channel" }
    ],
    "attributes": [
      { "name": "group", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($group^)?\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.channel.rank",
    "summary": "Returns the rank of the local participant in the group.",
    "description": "Returns the rank the channel represents as a participant in a collective\n    group in `[0, count)`.",
    "inputs": [
      { "name": "channel", "type": "FLOW_Channel" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$channel `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.channel.split",
    "summary": "Splits a collective communication channel.",
    "description": "Partitions the group associated with the given channel into disjoint\n    subgroups for each unique value of color. Each new subgroup contains all\n    participants of the same color and within each subgroup the key argument\n    is used to define the rank order. When multiple participants in a group\n    use the same key the tie will be broken using their rank in the parent\n    group.",
    "inputs": [
      { "name": "channel", "type": "FLOW_Channel" },
      { "name": "color", "type": "Index" },
      { "name": "key", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Channel" }
    ],
    "assemblyFormat": "$channel `,` $color `,` $key\n    `:` type($channel) `->` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.collective.all_gather",
    "summary": "Performs all-gather operation.",
    "description": "Gathers data from all ranks and concatenates them on the 0-th dimension.",
    "inputs": [
      { "name": "target", "type": "FLOW_Tensor" },
      { "name": "target_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "channel", "type": "FLOW_Channel" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "attributes": [
      { "name": "element_type", "type": "FLOW_CollectiveElementTypeAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$element_type `,` $target `,` $source `,` $channel `:`\n    `(` type($target) `,` type($source) `,` type($channel) `)` `->`\n    custom<ShapedTiedResult>(type($result), $target_dims, $tied_operands)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.collective.all_reduce",
    "summary": "Performs all-reduce operation.",
    "description": "Reduces data across all the ranks in the channel.",
    "inputs": [
      { "name": "target", "type": "FLOW_Tensor" },
      { "name": "target_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "channel", "type": "FLOW_Channel" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "attributes": [
      { "name": "reduction_op", "type": "FLOW_CollectiveReductionOpAttr" },
      { "name": "element_type", "type": "FLOW_CollectiveElementTypeAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$reduction_op `,` $element_type `,` $target `,` $source `,` $channel `:`\n    `(` type($target) `,` type($source) `,` type($channel) `)` `->`\n    custom<ShapedTiedResult>(type($result), $target_dims, $tied_operands)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.collective.all_to_all",
    "summary": "Performs all-to-all operation.",
    "description": "This operation mutually exchanges data acrosss all of the ranks in the channel.",
    "inputs": [
      { "name": "target", "type": "FLOW_Tensor" },
      { "name": "target_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "channel", "type": "FLOW_Channel" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "attributes": [
      { "name": "element_type", "type": "FLOW_CollectiveElementTypeAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$element_type `,` $target `,` $source `,` $channel `:`\n    `(` type($target) `,` type($source) `,` type($channel) `)` `->`\n    custom<ShapedTiedResult>(type($result), $target_dims, $tied_operands)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.collective.reduce_scatter",
    "summary": "Performs reduce and scatter operations.",
    "description": "The operation reduces data across all the ranks in the channel and\n    scatters the result to each rank.",
    "inputs": [
      { "name": "target", "type": "FLOW_Tensor" },
      { "name": "target_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "channel", "type": "FLOW_Channel" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "attributes": [
      { "name": "reduction_op", "type": "FLOW_CollectiveReductionOpAttr" },
      { "name": "element_type", "type": "FLOW_CollectiveElementTypeAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$reduction_op `,` $element_type `,` $target `,` $source `,` $channel `:`\n    `(` type($target) `,` type($source) `,` type($channel) `)` `->`\n    custom<ShapedTiedResult>(type($result), $target_dims, $tied_operands)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.collective.send_recv",
    "summary": "Performs a grouped send and receive operation.",
    "description": "The operation sends data to the rank specificied by send\n    and receives data from the rank specified by recv. If send is -1, this rank\n    will not send any data. If recv is -1, this rank will not receive any data\n    and the output will be all zeros.",
    "inputs": [
      { "name": "target", "type": "FLOW_Tensor" },
      { "name": "target_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "channel", "type": "FLOW_Channel" },
      { "name": "send", "type": "Index" },
      { "name": "recv", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "attributes": [
      { "name": "element_type", "type": "FLOW_CollectiveElementTypeAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$element_type `,` $target `,` $source `,` $channel `,` $send `,` $recv `:`\n    `(` type($target) `,` type($source) `,` type($channel) `,` type($send) `,` type($recv) `)` `->`\n    custom<ShapedTiedResult>(type($result), $target_dims, $tied_operands)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.dispatch",
    "summary": "A dispatch of workgroups across a grid.",
    "description": "Dispatches workgroups across an grid defined by the captured workload\n    parameters carrying the information required to compute the workgroup count\n    at runtime. The function for converting the workload into a 3D workgroup\n    count is attached to the dispatch entry point and may contain\n    arbitrary host logic.",
    "inputs": [
      { "name": "workload", "type": "Variadic" },
      { "name": "arguments", "type": "Variadic" },
      { "name": "argument_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "entry_points", "type": "SymbolRefArrayAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<DispatchEntryPoints>($entry_points)\n    (`[` $workload^ `]`)? ``\n    `(` $arguments `)` attr-dict `:`\n    custom<ShapedFunctionType>(ref($arguments),\n                               type($arguments), $argument_dims,\n                               type($results), $result_dims,\n                               $tied_operands)"
  },
  {
    "name": "flow.dispatch.region",
    "summary": "A grouping of ops with implicit capture.",
    "description": "This op is a container/grouping of ops. It represents a fusion group before\n    being lowered to a dispatch region. Ops are collected inside of the region\n    body of the op. Values from parent regions can be captured. Results are\n    yielded with a `return` terminator and returned from this op.\n\n    `dispatch.region` ops are lowered to `dispatch.workgroups` ops. Workgroups\n    isolated from above. `dispatch.region` ops are a more lightweight\n    abstraction for implementing fusion heuristics, i.e., the process of\n    deciding which ops should form a dispatch region.\n\n    This op also has a second region: `workload_count`. The arguments to the\n    region represent the workload for the dispatch, and returns the number of\n    workgroups for the dispatch. The region is lowered directly to\n    `workload_count` region of `dispatch.workgroups`.",
    "inputs": [
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "workload", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Variadic" }
    ]
  },
  {
    "name": "flow.dispatch.tie_shape",
    "summary": "Ties a runtime shape to a dispatch I/O argument.",
    "description": "Metadata op used to tie a runtime-computed shape with dynamic dimensions to\n    a dispatch input/output argument. All uses of the argument should use the\n    pass-through result of this op to allow for SSA-based shape resolution.",
    "inputs": [
      { "name": "operand", "type": "IREETensorExt_DispatchTensor" },
      { "name": "dynamic_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "IREETensorExt_DispatchTensor" }
    ],
    "assemblyFormat": "$operand attr-dict\n    `:` type($result) (`{` $dynamic_dims^ `}`)?"
  },
  {
    "name": "flow.dispatch.workgroup.count",
    "summary": "Returns the total workgroup count of the grid.",
    "description": "The total number of workgroups along each dimension in the dispatch grid.\n\n    Represented as a 3D grid classically written as XYZ.\n    Corresponds to the `NumWorkgroups` SPIR-V built-in and the `gridDim` CUDA\n    built-in variable.\n\n    ```mlir\n    %x = flow.dispatch.workgroup.count[0] : index\n    %y = flow.dispatch.workgroup.count[1] : index\n    %z = flow.dispatch.workgroup.count[2] : index\n    ```",
    "outputs": [
      { "name": "result", "type": "FLOW_Dim" }
    ],
    "attributes": [
      { "name": "dimension", "type": "IndexAttr" }
    ],
    "assemblyFormat": "`[` $dimension `]` attr-dict `:` type($result)"
  },
  {
    "name": "flow.dispatch.workgroup.id",
    "summary": "Returns the index of the current workgroup in the grid.",
    "description": "The global workgroup ID of the current workgroup in the range of\n    `[0, flow.dispatch.workgroup.count)` along each dimension.\n\n    Represented as a 3D grid classically written as XYZ.\n    Corresponds to the `WorkgroupId` SPIR-V built-in and the `blockIdx` CUDA\n    built-in variable.\n\n    ```mlir\n    %x = flow.dispatch.workgroup.id[0] : index\n    %y = flow.dispatch.workgroup.id[1] : index\n    %z = flow.dispatch.workgroup.id[2] : index\n    ```",
    "outputs": [
      { "name": "result", "type": "FLOW_Dim" }
    ],
    "attributes": [
      { "name": "dimension", "type": "IndexAttr" }
    ],
    "assemblyFormat": "`[` $dimension `]` attr-dict `:` type($result)"
  },
  {
    "name": "flow.dispatch.workgroup.size",
    "summary": "Returns the size of each workgroup in invocations.",
    "description": "The number of local invocations within the current workgroup along each\n    dimension. Depending on backend this may map to the SIMT thread count or\n    inner loop nest parameters.\n\n    Workgroup sizes are not determined at the flow dialect level as they are\n    dependent on the target backend determined when lowering into the HAL. It's\n    still possible to use the symbolic workgroup size inside of dispatch\n    executables as a placeholder for the resolved value once in the HAL.\n\n    Represented as a 3D grid classically written as XYZ.\n    Corresponds to the `WorkgroupSize` SPIR-V built-in and the `blockDim` CUDA\n    built-in variable.\n\n    ```mlir\n    %x = flow.dispatch.workgroup.size[0] : index\n    %y = flow.dispatch.workgroup.size[1] : index\n    %z = flow.dispatch.workgroup.size[2] : index\n    ```",
    "outputs": [
      { "name": "result", "type": "FLOW_Dim" }
    ],
    "attributes": [
      { "name": "dimension", "type": "IndexAttr" }
    ],
    "assemblyFormat": "`[` $dimension `]` attr-dict `:` type($result)"
  },
  {
    "name": "flow.dispatch.workgroups",
    "summary": "A dispatch of workgroups across a 3-dimensional grid.",
    "description": "Dispatches some number of workgroups across a 3-dimensional grid. The\n    body region will be invoked for each workgroup with a unique\n    `flow.dispatch.workgroup.id` in the range of\n    `[0, flow.dispatch.workgroup.count)` (along each dimension XYZ).\n\n    From the outside the dispatch operation has value semantics: some tensors\n    (and optionally other primitive types) are consumed and one or more new\n    result tensors are produced. Inside each workgroup, however, the input and\n    output tensors are available for arbitrary loads and stores. In many cases\n    each workgroup will load some particular tile(s) from the input tensors and\n    store some particular tile(s) to the output tensors unique to that\n    workgroup. Though it's possible for multiple workgroups to load the same\n    regions of the input tensors behavior is undefined if multiple workgroups\n    store to the same regions of the output tensors.\n\n    Though the representation is similar to the GPU-style grid dispatch model\n    here we still have not yet allocated buffers, determined the target device\n    for execution, or even completed fully resolving shapes/types/etc. Because\n    of this it's important that the workgroup body use the\n    `flow.dispatch.workgroup.*` ops to query the workgroup ID/count/size instead\n    of hardcoding them to a particular set of values. Assume that any workgroup\n    dispatch may end up being specialized for several different target devices\n    and even several different variants for a particular target device\n    (differing workgroup sizes, etc).\n\n    Because at this point in the layering devices have not yet been selected the\n    workgroup count cannot be fully evaluated. Instead workload parameters are\n    captured that are then passed to a function that when later evaluated\n    computes the actual workgroup count based on target information. The\n    workload is not limited to the 3D XYZ grid dispatch of the workgroup count\n    and can contain any number of parameters used to compute it.\n\n    ```mlir\n    %r = flow.dispatch.workgroups[%c5, %c5](%0, %1)\n        : (tensor<5x5xf32>, tensor<5xf32>) -> tensor<5x5xf32> =\n              (%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<5x5xf32>>,\n               %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<5xf32>>,\n               %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<5x5xf32>>) {\n      ...\n    }\n    ```\n\n    The number of results of the operation is equal to the number of results\n    in the type signature (`(tensor<5x5xf32>, tensor<5xf32>) -> tensor<5x5xf32>`).\n    Each tensor argument and result in the type signature has a corresponding\n    block argument of type `!iree_tensor_ext.dispatch.tensor`. Furthermore, each argument\n    has a corresponding `arguments` operand.\n\n    There are no `arguments` operands for results, but a result can be tied an\n    argument by writing the argument operand's SSA value instead of its type:\n    E.g., in the above example, `-> %0` would tie the first argument to the\n    result. In that case, there would be no separate block argument for the\n    result.",
    "inputs": [
      { "name": "workload", "type": "Variadic" },
      { "name": "arguments", "type": "Variadic" },
      { "name": "argument_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "tied_operands", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`[` $workload^ `]`)? ``\n    `(` $arguments `)` `:`\n    custom<ShapedFunctionType>(ref($arguments),\n                               type($arguments), $argument_dims,\n                               type($results), $result_dims,\n                               $tied_operands)\n    attr-dict-with-keyword\n    `=` `\\n` ` ` ` ` ` `\n    custom<DispatchWorkgroupBody>(ref(type($arguments)),\n                                  ref(type($results)),\n                                  $workgroup_body)\n    `` custom<DispatchWorkgroupsCountRegion>($workgroup_count)"
  },
  {
    "name": "flow.executable",
    "summary": "Generic executable module.",
    "description": "An executable module containing one or more public functions. The contents\n    of the functions are safe to dispatch and can be lowered further to\n    target-specific backend IR representations.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    $sym_name\n    attr-dict-with-keyword\n    regions"
  },
  {
    "name": "flow.executable_end",
    "summary": "Terminator pseudo-op for the executable op.",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "flow.executable.export",
    "summary": "Defines an executable entry point for dispatch operations.",
    "description": "Specifies an exported function with an externally-visible alias. Multiple\n    exports can reference the same internal function.\n\n    Each entry point can have a unique workgroup count calculation region.\n    This region takes the workload parameters passed to each flow.dispatch and\n    produces an XYZ workgroup count for the 3D grid dispatch.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_ref", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    custom<SymbolAlias>($sym_name, $function_ref)\n    custom<WorkgroupCountRegion>($workgroup_count)\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.func",
    "summary": "Streamable function declaration.",
    "description": "Declares a function that can be called as an asynchronous streaming\n    operation via `flow.call`. Today only external functions are allowed.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "tied_operands", "type": "OptionalAttr" },
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    $sym_name\n    ``\n    custom<ShapedFunctionSignature>($function_type,\n                                    $tied_operands,\n                                    $arg_attrs,\n                                    $res_attrs)\n    attr-dict-with-keyword\n    ($body^)?"
  },
  {
    "name": "flow.return",
    "summary": "Return from a flow.dispatch_region.",
    "description": "Returns the given values from the region and back to the host code.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "flow.tensor.alloca",
    "summary": "An empty tensor allocation with undefined contents.",
    "description": "Returns a new transient tensor allocation with undefined contents.\n    Subsequent writes must populate any ranges of the tensor that are later\n    read. The resulting tensor may be long-lived and allocated as part of a\n    dedicated allocation. Prefer using `flow.tensor.empty` whenever possible as\n    this op disables nearly all allocation-related optimizations performed by\n    the compiler. The presence of this op is often an indication of an improper\n    lowering.",
    "inputs": [
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "`:` type($result) (`{` $result_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.barrier",
    "summary": "Indicates a value that must have a specific affinity.",
    "description": "Prevents fusion and scheduling of a value across an affinity boundary.\n    May introduce copy-on-write behavior if the operand value is used as well as\n    the result and users should try to keep the operand to a single use by this\n    op.",
    "inputs": [
      { "name": "operand", "type": "FLOW_Tensor" },
      { "name": "operand_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "attributes": [
      { "name": "target", "type": "AnyAttr" }
    ],
    "assemblyFormat": "$operand `:` type($result) (`{` $operand_dims^ `}`)?\n    `on` $target\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.bitcast",
    "summary": "Bitcasts a tensor without modifying the contents.",
    "description": "Bitcasts a tensor |source| to the shape implied by this operations result\n    type interleaved with |result_dims|, potentially with a different element\n    type. For example,\n\n    ```\n    result_dims = {%0, %1}\n    result_type = tensor<1x?x2x?x3 x!eltype>\n    ```\n\n    produces a tensor of shape [1, %0, 2, %1, 3] and element type `!eltype`.\n    Note that the source and result tensors must serialized to the same size.",
    "inputs": [
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "source_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$source `:`\n    type($source) (`{` $source_dims^ `}`)? `->`\n    type($result) (`{` $result_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.clone",
    "summary": "Performs a full tensor clone operation.",
    "description": "Clones the input tensor into an identical output tensor.",
    "inputs": [
      { "name": "operand", "type": "FLOW_Tensor" },
      { "name": "operand_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$operand `:` type($result) (`{` $operand_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.constant",
    "summary": "Tensor constant that can have dynamic dimensions.",
    "description": "Allows specifying a tensor constant of IREE-specific types/attributes.\n\n    ```mlir\n    %cst = flow.tensor.constant #something_tensor_like : tensor<2x2xf32>\n    %res = math.absf %cst : tensor<2x2xf32>\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "TypedAttrInterface" }
    ],
    "assemblyFormat": "attr-dict $value"
  },
  {
    "name": "flow.tensor.dynamic_constant",
    "summary": "Tensor constant that can have dynamic dimensions.",
    "description": "Allows specifying a tensor constant of IREE-specific types/attributes with\n    a dynamic shape that approximates a value as passed from the user. This\n    disables many optimizations and should only be used when testing or\n    benchmarking and wanting to ensure that dynamic dimension behavior is\n    preserved.\n\n    ```mlir\n    %cst = flow.tensor.dynamic_constant #something_tensor_like : tensor<2x2xf32> -> tensor<?x2xf32>\n    %res = math.absf %cst : tensor<?x2xf32>\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "TypedAttrInterface" }
    ],
    "assemblyFormat": "attr-dict $value `->` type($result)"
  },
  {
    "name": "flow.tensor.empty",
    "summary": "An empty tensor carrying metadata but no contents.",
    "description": "Returns a tensor with undefined contents. Subsequent writes must populate\n    any ranges of the tensor that are later read.",
    "inputs": [
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "`:` type($result) (`{` $result_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.encode",
    "summary": "Performs a full tensor encode operation.",
    "description": "Encode the input tensor into an encoded output tensor.",
    "inputs": [
      { "name": "operand", "type": "FLOW_Tensor" },
      { "name": "operand_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$operand `:`\n    type($operand) (`{` $operand_dims^ `}`)? `->`\n    type($result) (`{` $result_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.load",
    "summary": "Loads a value from a tensor element.",
    "description": "Returns the element at the given location from within the tensor.",
    "inputs": [
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "source_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$source (`[` $indices^ `]`)? `:`\n    type($source) (`{` $source_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.reshape",
    "summary": "Reshapes a tensor without modifying the contents.",
    "description": "Reshapes a tensor |source| to the shape implied by this operations result\n    type interleaved with |result_dims|. For example,\n\n    ```\n    result_dims = {%0, %1}\n    result_type = tensor<1x?x2x?x3>\n    ```\n\n    produces a tensor of shape [1, %0, 2, %1, 3] and the same element type.",
    "inputs": [
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "source_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$source `:`\n    type($source) (`{` $source_dims^ `}`)? `->`\n    type($result) (`{` $result_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.slice",
    "summary": "Clones a subregion of a tensor.",
    "inputs": [
      { "name": "source", "type": "FLOW_Tensor" },
      { "name": "source_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "start_indices", "type": "Variadic" },
      { "name": "lengths", "type": "Variadic" },
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$source `[` $start_indices `for` $lengths `]` `:`\n    type($source) (`{` $source_dims^ `}`)? `->`\n    type($result) (`{` $result_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.splat",
    "summary": "Splats a value into a shaped tensor.",
    "description": "Returns a tensor initialized to the given primitive value.",
    "inputs": [
      { "name": "value", "type": "FLOW_PrimitiveType" },
      { "name": "result_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$value `:` type($result) (`{` $result_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.store",
    "summary": "Stores a value into a tensor element.",
    "description": "Returns a tensor with the element at the given index set to the given value.",
    "inputs": [
      { "name": "value", "type": "AnyTypeOf" },
      { "name": "target", "type": "FLOW_Tensor" },
      { "name": "target_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$value `,` $target (`[` $indices^ `]`)? `:`\n    type($target) (`{` $target_dims^ `}`)?\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.tie_shape",
    "summary": "Ties a runtime shape to a tensor value.",
    "description": "Metadata op used to tie tensors with their runtime-computed dynamic\n    dimensions. This only exists transiently in the IR as a witness to shape\n    calculations and is removed during lowering.",
    "inputs": [
      { "name": "operand", "type": "FLOW_Tensor" },
      { "name": "dynamic_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$operand attr-dict\n    `:` type($result) (`{` $dynamic_dims^ `}`)?"
  },
  {
    "name": "flow.tensor.trace",
    "summary": "Traces one or more tensor values at runtime.",
    "description": "Traces out to a runtime trace sink (console, log file, etc) the given\n    tensors. The key is arbitrary and can be used for identifying the set of\n    values being traced.",
    "inputs": [
      { "name": "values", "type": "Variadic" },
      { "name": "value_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" }
    ],
    "assemblyFormat": "$key `=` `[`\n    custom<ShapedOperandList>($values, type($values), $value_dims)\n    `]` attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.transfer",
    "summary": "Transfers a tensor to a target by copying if needed.",
    "description": "Transfers the tensor from whichever context it may be in to the specified\n    target context. If the contexts are compatible and can access each others\n    memory the operation may be elided and otherwise will become one or more\n    copies to transfer the tensor in cases where staging through an intermediate\n    context is required.",
    "inputs": [
      { "name": "operand", "type": "FLOW_Tensor" },
      { "name": "operand_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "attributes": [
      { "name": "target", "type": "AnyAttr" }
    ],
    "assemblyFormat": "$operand `:` type($result) (`{` $operand_dims^ `}`)?\n    `to` $target\n    attr-dict-with-keyword"
  },
  {
    "name": "flow.tensor.update",
    "summary": "Updates a tensor with the contents of another tensor.",
    "description": "Updates the target tensor with the contents of the update tensor at the\n    given offset indices.",
    "inputs": [
      { "name": "target", "type": "FLOW_Tensor" },
      { "name": "target_dims", "type": "FLOW_ShapeDynamicDims" },
      { "name": "start_indices", "type": "Variadic" },
      { "name": "update", "type": "FLOW_Tensor" },
      { "name": "update_dims", "type": "FLOW_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "FLOW_Tensor" }
    ],
    "assemblyFormat": "$update `,` $target `[` $start_indices `]` `:`\n    type($update) (`{` $update_dims^ `}`)? `->`\n    custom<ShapedTiedResult>(type($result), $target_dims)\n    attr-dict-with-keyword"
  },
  {
    "name": "func.call",
    "summary": "call operation",
    "description": "The `func.call` operation represents a direct call to a function that is\n    within the same symbol scope as the call. The operands and result types of\n    the call must match the specified function type. The callee is encoded as a\n    symbol reference attribute named \"callee\".\n\n    Example:\n\n    ```mlir\n    %2 = func.call @my_add(%0, %1) : (f32, f32) -> f32\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "callee", "type": "FlatSymbolRefAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "no_inline", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$callee `(` $operands `)` attr-dict `:` functional-type($operands, results)"
  },
  {
    "name": "func.call_indirect",
    "summary": "indirect call operation",
    "description": "The `func.call_indirect` operation represents an indirect call to a value\n    of function type. The operands and result types of the call must match the\n    specified function type.\n\n    Function values can be created with the\n    [`func.constant` operation](#funcconstant-constantop).\n\n    Example:\n\n    ```mlir\n    %func = func.constant @my_func : (tensor<16xf32>, tensor<16xf32>) -> tensor<16xf32>\n    %result = func.call_indirect %func(%0, %1) : (tensor<16xf32>, tensor<16xf32>) -> tensor<16xf32>\n    ```",
    "inputs": [
      { "name": "callee", "type": "FunctionType" },
      { "name": "callee_operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$callee `(` $callee_operands `)` attr-dict `:` type($callee)"
  },
  {
    "name": "func.constant",
    "summary": "constant",
    "description": "The `func.constant` operation produces an SSA value from a symbol reference\n    to a `func.func` operation\n\n    Example:\n\n    ```mlir\n    // Reference to function @myfn.\n    %2 = func.constant @myfn : (tensor<16xf32>, f32) -> tensor<16xf32>\n\n    // Equivalent generic forms\n    %2 = \"func.constant\"() { value = @myfn } : () -> ((tensor<16xf32>, f32) -> tensor<16xf32>)\n    ```\n\n    MLIR does not allow direct references to functions in SSA operands because\n    the compiler is multithreaded, and disallowing SSA values to directly\n    reference a function simplifies this\n    ([rationale](../Rationale/Rationale.md#multithreading-the-compiler)).",
    "attributes": [
      { "name": "value", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "attr-dict $value `:` type(results)"
  },
  {
    "name": "func.func",
    "summary": "An operation with a name containing a single `SSACFG` region",
    "description": "Operations within the function cannot implicitly capture values defined\n    outside of the function, i.e. Functions are `IsolatedFromAbove`. All\n    external references must use function arguments or attributes that establish\n    a symbolic connection (e.g. symbols referenced by name via a string\n    attribute like SymbolRefAttr). An external function declaration (used when\n    referring to a function declared in some other module) has no body. While\n    the MLIR textual form provides a nice inline syntax for function arguments,\n    they are internally represented as “block arguments” to the first block in\n    the region.\n\n    Only dialect attribute names may be specified in the attribute dictionaries\n    for function arguments, results, or the function itself.\n\n    Example:\n\n    ```mlir\n    // External function definitions.\n    func.func private @abort()\n    func.func private @scribble(i32, i64, memref<? x 128 x f32, #layout_map0>) -> f64\n\n    // A function that returns its argument twice:\n    func.func @count(%x: i64) -> (i64, i64)\n      attributes {fruit = \"banana\"} {\n      return %x, %x: i64, i64\n    }\n\n    // A function with an argument attribute\n    func.func private @example_fn_arg(%x: i32 {swift.self = unit})\n\n    // A function with a result attribute\n    func.func private @example_fn_result() -> (f64 {dialectName.attrName = 0 : i64})\n\n    // A function with an attribute\n    func.func private @example_fn_attr() attributes {dialectName.attrName = false}\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "no_inline", "type": "UnitAttr" }
    ]
  },
  {
    "name": "func.return",
    "summary": "Function return operation",
    "description": "The `func.return` operation represents a return operation within a function.\n    The operation takes variable number of operands and produces no results.\n    The operand number and types must match the signature of the function\n    that contains the operation.\n\n    Example:\n\n    ```mlir\n    func.func @foo() -> (i32, f8) {\n      ...\n      return %0, %1 : i32, f8\n    }\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "gpu.all_reduce",
    "summary": "Reduce values among workgroup.",
    "description": "The `all_reduce` op reduces the value of every work item across a local\n    workgroup. The result is equal for all work items of a workgroup.\n\n    For example, both\n\n    ```mlir\n    %1 = gpu.all_reduce add %0 {} : (f32) -> (f32)\n    %2 = gpu.all_reduce %0 {\n    ^bb(%lhs : f32, %rhs : f32):\n      %sum = arith.addf %lhs, %rhs : f32\n      \"gpu.yield\"(%sum) : (f32) -> ()\n    } : (f32) -> (f32)\n    ```\n\n    compute the sum of each work item's %0 value. The first version specifies\n    the accumulation as operation, whereas the second version specifies the\n    accumulation as code region. The reduction operation must be one of:\n    *  Integer types: `add`, `mul`, `minui`, `minsi`, `maxui`, `maxsi`, `and`,\n       `or`, `xor`\n    *  Floating point types: `add`, `mul`, `minnumf`, `maxnumf`, `minimumf`,\n       `maximumf`\n\n    If `uniform` flag is set either none or all work items of a workgroup\n    need to execute this op in convergence.",
    "inputs": [
      { "name": "value", "type": "AnyIntegerOrFloat" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyIntegerOrFloat" }
    ],
    "attributes": [
      { "name": "op", "type": "OptionalAttr" },
      { "name": "uniform", "type": "UnitAttr" }
    ],
    "assemblyFormat": "custom<AllReduceOperation>($op) $value\n                          (`uniform` $uniform^)? $body attr-dict\n                          `:` functional-type(operands, results)"
  },
  {
    "name": "gpu.alloc",
    "summary": "GPU memory allocation operation.",
    "description": "The `gpu.alloc` operation allocates a region of memory on the GPU. It is\n    similar to the `memref.alloc` op, but supports asynchronous GPU execution.\n\n    The op does not execute before all async dependencies have finished\n    executing.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it also returns a !gpu.async.token.\n\n    If the `host_shared` keyword is present, the memory will be allocated in a\n    memory accessible both on host and on device.\n\n    Example:\n\n    ```mlir\n    %memref, %token = gpu.alloc async [%dep] host_shared (%width) : memref<64x?xf32, 1>\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "dynamicSizes", "type": "Variadic" },
      { "name": "symbolOperands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "memref", "type": "Res" },
      { "name": "asyncToken", "type": "Optional" }
    ],
    "attributes": [
      { "name": "hostShared", "type": "UnitAttr" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies) (` ` `host_shared` $hostShared^)? ` `\n    `(` $dynamicSizes `)` (`` `[` $symbolOperands^ `]`)? attr-dict `:` type($memref)"
  },
  {
    "name": "gpu.barrier",
    "summary": "Synchronizes all work items of a workgroup.",
    "description": "The `barrier` op synchronizes all work items of a workgroup. It is used\n    to coordinate communication between the work items of the workgroup.\n\n    ```mlir\n    gpu.barrier\n    ```\n\n    waits until all work items in the workgroup have reached this point\n    and all memory accesses made by these work items prior to the op are\n    visible to all work items in the workgroup. Data hazards between work items\n    accessing the same memory can be avoided by synchronizing work items\n    in-between these accesses.\n\n    Either none or all work items of a workgroup need to execute this op\n    in convergence.",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "gpu.binary",
    "summary": "An Op for storing serialized GPU binary objects.",
    "description": "GPU binaries provide a semantic mechanism for storing GPU objects,\n    e.g. the result of compiling a GPU module to an object file.\n\n    This operation has 3 arguments:\n     - The name of the binary.\n     - An optional attribute implementing the offloading LLVM translation interface.\n     - An array of GPU object attributes.\n\n    During translation, the offloading attribute will be called for translating\n    GPU `binary` and `launch_func` operations. The default offloading handler is:\n    `#gpu.select_object`, this handler selects the first object from the array\n    and embeds it as a string.\n\n    Examples:\n    ```\n      // Selects the first object.\n      gpu.binary @myobject [#gpu.object<...>, #gpu.object<...>]\n      // Uses the `#foo.my_handler` for handling the binary during translation.\n      gpu.binary @myobject <#foo.my_handler> [#gpu.object<...>, #gpu.object<...>]\n      // Selects the object with the `#rocdl.target` target attribute.\n      gpu.binary @myobject <#gpu.select_object<#rocdl.target>> [#gpu.object<...>, #gpu.object<#rocdl.target, ...>]\n    ```",
    "assemblyFormat": "$sym_name custom<OffloadingHandler>($offloadingHandler) attr-dict $objects"
  },
  {
    "name": "gpu.block_dim",
    "description": "Returns the number of threads in the thread block (aka the block size) along\n    the x, y, or z `dimension`.\n\n    Example:\n\n    ```mlir\n    %bDimX = gpu.block_dim x\n    ```\n\n    If `known_block_size` is set on an this operation's enclosing `gpu.func`,\n    or `gpu.known_block_size` is set on an enclosing `FunctionOpInterface`\n    implementor, or if the enclosing `gpu.launch` specifies a constant size for\n    `dimension`'s blocks, these contextual facts may be used to infer that this\n    operation has a constant value, though such a transformation will not be\n    performed by canonicalization or the default constant folder. Executions which\n    cause that constant-value assumption to be false incur undefined behavior.\n\n    If `upper_bound` is set, executions where the bblock size along `dimension`\n    exceeds `upper_bound` cause undefined behavior.\n\n    There is an implicit upper bound of `kMaxDim` (currently uint32_t::max).",
    "assemblyFormat": "$dimension (`upper_bound` $upper_bound^)? attr-dict"
  },
  {
    "name": "gpu.block_id",
    "description": "Returns the block id, i.e. the index of the current block within the grid\n    along the x, y, or z `dimension`.\n\n    Example:\n\n    ```mlir\n    %bIdY = gpu.block_id y\n    ```\n\n    If `upper_bound` is set, or if one can be inferred from `known_grid_size`-type\n    annotations in context, executions where the block index in `dimension` would\n    be greater than or equal to that bound cause undefined behavior. `upper_bound`\n    takes priority over bounds inferrable from context.\n\n    There is an implicit upper bound of `kMaxDim` (currently uint32_t::max).",
    "assemblyFormat": "$dimension (`upper_bound` $upper_bound^)? attr-dict"
  },
  {
    "name": "gpu.cluster_block_id",
    "description": "Returns the block id within the cluster along the x, y, or z `dimension`.\n\n    Example:\n\n    ```mlir\n    %cBlockIdY = gpu.cluster_block_id y\n    ```\n\n    If `upper_bound` is set, then executing (a lowering of) this operation in an\n    environment where the number of thread blocks per cluster along `dimension`\n    is greater than `upper_bound` causes undefined behavior.\n\n    There is an implicit upper bound of `kMaxClusterDim` (currently 8).",
    "assemblyFormat": "$dimension (`upper_bound` $upper_bound^)? attr-dict"
  },
  {
    "name": "gpu.cluster_dim",
    "description": "Returns the number of cluster identifiers per grid along\n    the x, y, or z `dimension`.\n\n    Example:\n\n    ```mlir\n    %cDimX = gpu.cluster_dim x\n    ```\n\n    If `upper_bound` is set, then executing (a lowering of) this operation in an\n    environment where the clusters per grid is greater than `upper_bound` causes\n    undefined behavior.\n\n    There is an implicit upper bound of `kMaxDim` (currently uint32_t::max).",
    "assemblyFormat": "$dimension (`upper_bound` $upper_bound^)? attr-dict"
  },
  {
    "name": "gpu.cluster_dim_blocks",
    "description": "Returns the number of thread blocks in the cluster along\n    the x, y, or z `dimension`.\n\n    Example:\n\n    ```mlir\n    %cDimBlocksX = gpu.cluster_dim_blocks x\n    ```\n\n    If `upper_bound` is set, then executing (a lowering of) this operation in an\n    environment where the thread blocks per cluster  is greater than `upper_bound`\n    causes undefined behavior.\n\n    There is an implicit upper bound of `kMaxClusterDim` (currently 8).",
    "assemblyFormat": "$dimension (`upper_bound` $upper_bound^)? attr-dict"
  },
  {
    "name": "gpu.cluster_id",
    "description": "Returns the cluster id, i.e. the index of the current cluster within the\n    grid along the x, y, or z `dimension`.\n\n    Example:\n\n    ```mlir\n    %cIdY = gpu.cluster_id y\n    ```\n\n    If `upper_bound` is set, then executing (a lowering of) this operation in an\n    environment where the number of clusters in the grid along `dimension` is\n    greater than `upper_bound` causes undefined behavior.\n\n    There is an implicit upper bound of `kMaxDim` (currently uint32_t::max).",
    "assemblyFormat": "$dimension (`upper_bound` $upper_bound^)? attr-dict"
  },
  {
    "name": "gpu.create_2to4_spmat",
    "summary": "Create sparse matrix with 2:4 sparsity operation",
    "description": "The `gpu.create_2to4_spmat` operation initializes a sparse matrix in dense\n    format with 2:4 sparsity.\n    The buffers must already be copied from the host to the device prior to\n    using this operation. The operation returns a handle to the sparse\n    matrix descriptor.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token in addition to the environment.\n\n    Example:\n\n    ```mlir\n    %spmat, %token = gpu.create_2to4_spmat async [%dep] {PRUNE_AND_CHECK} %rows, %cols, %mem: memref<?xf64>\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "rows", "type": "Index" },
      { "name": "cols", "type": "Index" },
      { "name": "memref", "type": "AnyMemRef" }
    ],
    "outputs": [
      { "name": "spMat", "type": "Res" },
      { "name": "asyncToken", "type": "Optional" }
    ],
    "attributes": [
      { "name": "pruneFlag", "type": "GPU_Prune2To4SpMatFlagAttr" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n     `{` $pruneFlag `}` $rows `,` $cols `,` $memref attr-dict `:` type($memref)"
  },
  {
    "name": "gpu.create_bsr",
    "summary": "Create sparse matrix in BSR format operation",
    "description": "The `gpu.create_bsr` operation initializes a sparse matrix in BSR format\n    with the given sizes for the matrix and blocks from the given position,\n    index, and values buffers. The buffers must already be copied from the\n    host to the device prior to using this operation. The operation returns\n    a handle to the sparse matrix descriptor.\n\n    The BSR format is similar to CSR, where the column indices represent\n    two-dimensional blocks instead of a single matrix entry. Note that this\n    operation (currently) only supports storage with **square** blocks,\n    i.e., `rBlockSize == cBlockSize`.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token in addition to the environment.\n\n    Example:\n\n    ```mlir\n    %spmat, %token = gpu.create_bsr async [%dep]\n       %brows, %bcols, %bnnz, %rBlockSize, %cBlockSize,\n       %bRowPos, %bColIdxs, %values : memref<?xindex>, memref<?xindex>, memref<?xf64>\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "brows", "type": "Index" },
      { "name": "bcols", "type": "Index" },
      { "name": "bnnz", "type": "Index" },
      { "name": "rBlockSize", "type": "Index" },
      { "name": "cBlockSize", "type": "Index" },
      { "name": "bRowPos", "type": "AnyMemRef" },
      { "name": "bColIdxs", "type": "AnyMemRef" },
      { "name": "values", "type": "AnyMemRef" }
    ],
    "outputs": [
      { "name": "spmat", "type": "Res" },
      { "name": "asyncToken", "type": "Optional" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $brows `,` $bcols `,` $bnnz `,` $rBlockSize `,` $cBlockSize `,`\n    $bRowPos `,` $bColIdxs `,` $values attr-dict\n    `:` type($bRowPos) `,` type($bColIdxs) `,` type($values)"
  },
  {
    "name": "gpu.create_coo",
    "summary": "Create sparse matrix in COO format operation",
    "description": "The `gpu.create_coo` operation initializes a sparse matrix in COO format\n    with the given sizes from the given index and values buffers. The buffers\n    must already be copied from the host to the device prior to using this\n    operation. The operation returns a handle to the sparse matrix descriptor.\n    Note that this operation builds the COO in SoA format.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token in addition to the environment.\n\n    Example:\n\n    ```mlir\n    %spmat, %token = gpu.create_coo async [%dep] %rows, %cols, %nnz, %rowIdx,\n        %colIdx, %values : memref<?xindex>, memref<?xindex>, memref<?xf64>\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "rows", "type": "Index" },
      { "name": "cols", "type": "Index" },
      { "name": "nnz", "type": "Index" },
      { "name": "rowIdxs", "type": "AnyMemRef" },
      { "name": "colIdxs", "type": "AnyMemRef" },
      { "name": "values", "type": "AnyMemRef" }
    ],
    "outputs": [
      { "name": "spmat", "type": "Res" },
      { "name": "asyncToken", "type": "Optional" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $rows `,` $cols `,` $nnz `,` $rowIdxs `,` $colIdxs `,` $values attr-dict\n    `:` type($rowIdxs) `,` type($colIdxs) `,` type($values)"
  },
  {
    "name": "gpu.create_coo_aos",
    "summary": "Create sparse matrix in COO format operation (AoS)",
    "description": "The `gpu.create_coo_aos` operation initializes a sparse matrix in COO format\n    with the given sizes from the given index and values buffers. The buffers\n    must already be copied from the host to the device prior to using this\n    operation. The operation returns a handle to the sparse matrix descriptor.\n    Unlike the default `gpu.create_coo` operation, this operation builds the\n    COO format from a single index buffer in AoS format (note that this\n    feature has been deprecated in cuSparse 11.2).\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token in addition to the environment.\n\n    Example:\n\n    ```mlir\n    %spmat, %token = gpu.create_coo_aos async [%dep] %rows, %cols, %nnz, %idxs,\n        %values : memref<?xindex>, memref<?xf64>\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "rows", "type": "Index" },
      { "name": "cols", "type": "Index" },
      { "name": "nnz", "type": "Index" },
      { "name": "idxs", "type": "AnyMemRef" },
      { "name": "values", "type": "AnyMemRef" }
    ],
    "outputs": [
      { "name": "spmat", "type": "Res" },
      { "name": "asyncToken", "type": "Optional" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $rows `,` $cols `,` $nnz `,` $idxs `,` $values attr-dict\n    `:` type($idxs) `,` type($values)"
  },
  {
    "name": "gpu.create_csc",
    "summary": "Create sparse matrix in CSC format operation",
    "description": "The `gpu.create_csc` operation initializes a sparse matrix in CSC format\n    with the given sizes from the given position, index, and values buffers.\n    The buffers must already be copied from the host to the device prior to\n    using this operation. The operation returns a handle to the sparse\n    matrix descriptor.\n\n    The CSC format has exactly the same memory layout as its transpose\n    in CSR format (and vice versa).\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token in addition to the environment.\n\n    Example:\n\n    ```mlir\n    %spmat, %token = gpu.create_csc async [%dep] %rows, %cols, %nnz, %colPos,\n        %rowIdx, %values : memref<?xindex>, memref<?xindex>, memref<?xf64>\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "rows", "type": "Index" },
      { "name": "cols", "type": "Index" },
      { "name": "nnz", "type": "Index" },
      { "name": "colPos", "type": "AnyMemRef" },
      { "name": "rowIdxs", "type": "AnyMemRef" },
      { "name": "values", "type": "AnyMemRef" }
    ],
    "outputs": [
      { "name": "spmat", "type": "Res" },
      { "name": "asyncToken", "type": "Optional" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $rows `,` $cols `,` $nnz `,` $colPos `,` $rowIdxs `,` $values attr-dict\n    `:` type($colPos) `,` type($rowIdxs) `,` type($values)"
  },
  {
    "name": "gpu.create_csr",
    "summary": "Create sparse matrix in CSR format operation",
    "description": "The `gpu.create_csr` operation initializes a sparse matrix in CSR format\n    with the given sizes from the given position, index, and values buffers.\n    The buffers must already be copied from the host to the device prior to\n    using this operation. The operation returns a handle to the sparse\n    matrix descriptor.\n\n    The CSR format has exactly the same memory layout as its transpose\n    in CSC format (and vice versa).\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token in addition to the environment.\n\n    Example:\n\n    ```mlir\n    %spmat, %token = gpu.create_csr async [%dep] %rows, %cols, %nnz, %rowPos,\n        %colIdx, %values : memref<?xindex>, memref<?xindex>, memref<?xf64>\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "rows", "type": "Index" },
      { "name": "cols", "type": "Index" },
      { "name": "nnz", "type": "Index" },
      { "name": "rowPos", "type": "AnyMemRef" },
      { "name": "colIdxs", "type": "AnyMemRef" },
      { "name": "values", "type": "AnyMemRef" }
    ],
    "outputs": [
      { "name": "spmat", "type": "Res" },
      { "name": "asyncToken", "type": "Optional" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $rows `,` $cols `,` $nnz `,` $rowPos `,` $colIdxs `,` $values attr-dict\n    `:` type($rowPos) `,` type($colIdxs) `,` type($values)"
  },
  {
    "name": "gpu.create_dn_tensor",
    "summary": "Create dense tensor operation",
    "description": "The `gpu.create_dn_tensor` operation initializes a dense tensor from\n    the given values buffer and sizes. The buffer must already be copied\n    from the host to the device prior to using this operation. The\n    operation returns a handle to the dense tensor descriptor.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token in addition to the environment.\n\n    Example:\n\n    ```mlir\n    %dmat, %token = gpu.create_dn_tensor async [%dep] %mem, %dims : index, index into memref<?xf64>\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "memref", "type": "AnyMemRef" },
      { "name": "dims", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "dnTensor", "type": "Res" },
      { "name": "asyncToken", "type": "Optional" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $memref `,` $dims attr-dict `:` type($dims) `into` type($memref)"
  },
  {
    "name": "gpu.dealloc",
    "summary": "GPU memory deallocation operation",
    "description": "The `gpu.dealloc` operation frees the region of memory referenced by a\n    memref which was originally created by the `gpu.alloc` operation. It is\n    similar to the `memref.dealloc` op, but supports asynchronous GPU execution.\n\n    The op does not execute before all async dependencies have finished\n    executing.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token.\n\n    Example:\n\n    ```mlir\n    %token = gpu.dealloc async [%dep] %memref : memref<8x64xf32, 1>\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "memref", "type": "Arg" }
    ],
    "outputs": [
      { "name": "asyncToken", "type": "Optional" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $memref attr-dict `:` type($memref)"
  },
  {
    "name": "gpu.destroy_dn_tensor",
    "summary": "Destroy dense tensor operation",
    "description": "The `gpu.destroy_dn_tensor` operation releases all resources of a dense\n    tensor represented by a handle that was previously created by a\n    `gpu.create_dn_tensor` operation.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token in addition to the environment.\n\n    Example:\n\n    ```mlir\n    %token = gpu.destroy_dn_tensor async [%dep] %dnTensor\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "dnTensor", "type": "Arg" }
    ],
    "outputs": [
      { "name": "asyncToken", "type": "Optional" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $dnTensor attr-dict"
  },
  {
    "name": "gpu.destroy_sp_mat",
    "summary": "Destroy sparse matrix operation",
    "description": "The `gpu.destroy_sp_mat` operation releases all resources of a sparse\n    matrix represented by a handle that was previously created by a\n    one of the sparse matrix creation operations.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token in addition to the environment.\n\n    Example:\n\n    ```mlir\n    %token = gpu.destroy_sp_mat async [%dep] %spmat\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "spmat", "type": "Arg" }
    ],
    "outputs": [
      { "name": "asyncToken", "type": "Optional" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies) $spmat attr-dict"
  },
  {
    "name": "gpu.dynamic_shared_memory",
    "summary": "Get the memref for dynamic shared memory",
    "description": "This operation provides a memref pointer to the start of dynamic shared\n    memory, often referred to as workgroup memory. It's important to note that\n    this dynamic shared memory needs to be allocated at kernel launch. One can\n    conveniently utilize the `dynamic_shared_memory_size` parameter of\n    `gpu.launch` for this purpose.\n\n    Examples:\n    ```mlir\n    %0 = gpu.dynamic.shared.memory : memref<?xi8, #gpu.address_space<workgroup>>\n    %1 = memref.view %0[%c8192][] : memref<?xi8, #gpu.address_space<workgroup>>\n                            to memref<32x64xf32, #gpu.address_space<workgroup>>\n    %2 = memref.view %0[%c16384][] : memref<?xi8, #gpu.address_space<workgroup>>\n                            to memref<32x64xf32, #gpu.address_space<workgroup>>\n    ```",
    "outputs": [
      { "name": "resultMemref", "type": "Arg" }
    ],
    "assemblyFormat": "attr-dict `:` type($resultMemref)"
  },
  {
    "name": "gpu.func",
    "summary": "Function executable on a GPU",
    "description": "Defines a function that can be executed on a GPU. This supports memory\n    attribution and its body has a particular execution model.\n\n    GPU functions are either kernels (as indicated by the `kernel` attribute) or\n    regular functions. The former can be launched from the host side, while the\n    latter are device side only.\n\n    The memory attribution defines SSA values that correspond to memory buffers\n    allocated in the memory hierarchy of the GPU (see below).\n\n    The operation has one attached region that corresponds to the body of the\n    function. The region arguments consist of the function arguments without\n    modification, followed by buffers defined in memory annotations. The body of\n    a GPU function, when launched, is executed by multiple work items. There are\n    no guarantees on the order in which work items execute, or on the connection\n    between them. In particular, work items are not necessarily executed in\n    lock-step. Synchronization ops such as \"gpu.barrier\" should be used to\n    coordinate work items. Declarations of GPU functions, i.e. not having the\n    body region, are not supported.\n\n    A function may optionally be annotated with the block and/or grid sizes\n    that will be used when it is launched using the `known_block_size` and\n    `known_grid_size` attributes, respectively. If set, these attributes must\n    be arrays of three 32-bit integers giving the x, y, and z launch dimensions.\n    Launching a kernel that has these annotations, or that calls a function with\n    these annotations, using a block size or grid size other than what is specified\n    is undefined behavior. These attributes may be set on non-`gpu.func` functions\n    by using `gpu.known_block_size` or `gpu.known_grid_size`, but this carries\n    the risk that they will de discarded.\n\n    Syntax:\n\n    ```\n    op ::= `gpu.func` symbol-ref-id `(` argument-list `)` (`->`\n    function-result-list)?\n           memory-attribution `kernel`? function-attributes? region\n\n    memory-attribution ::= (`workgroup` `(` ssa-id-and-type-list `)`)?\n                           (`private` `(` ssa-id-and-type-list `)`)?\n    ```\n\n    Example:\n\n    ```mlir\n    gpu.func @foo(%arg0: index)\n        workgroup(%workgroup: memref<32xf32, 3>)\n        private(%private: memref<1xf32, 5>)\n        kernel\n        attributes {qux: \"quux\"} {\n      gpu.return\n    }\n    ```\n\n    The generic form illustrates the concept\n\n    ```mlir\n    \"gpu.func\"(%arg: index) {sym_name: \"foo\", kernel, qux: \"quux\"} ({\n    ^bb0(%arg0: index, %workgroup: memref<32xf32, 3>,\n         %private: memref<1xf32, 5>):\n      \"gpu.return\"() : () -> ()\n    }) : (index) -> ()\n    ```\n\n    Note the non-default memory spaces used in memref types in memory\n    attribution.",
    "attributes": [
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "workgroup_attrib_attrs", "type": "OptionalAttr" },
      { "name": "private_attrib_attrs", "type": "OptionalAttr" },
      { "name": "known_block_size", "type": "GPU_OptionalDimSizeHintAttr" },
      { "name": "known_grid_size", "type": "GPU_OptionalDimSizeHintAttr" }
    ]
  },
  {
    "name": "gpu.global_id",
    "description": "Returns the unique global workitem/thread id, i.e., the unique index of the\n    current workitem/thread within all workgroups / grid along the x, y, or z\n    `dimension`.\n\n    Example:\n\n    ```mlir\n    %gidX = gpu.global_id x\n    %gidX = gpu.global_id x upper_bound 65536\n    ```\n\n    The `upper_bound` attribute defines an upper bound analogously to the ones on\n    `thread_id` and `block_id`. If one is not set, the bound may be inferred from\n    a combination of `known_block_size` and `known_grid_size`-type annotations.",
    "assemblyFormat": "$dimension (`upper_bound` $upper_bound^)? attr-dict"
  },
  {
    "name": "gpu.grid_dim",
    "description": "Returns the number of thread blocks in the grid along the x, y, or z\n    `dimension`.\n\n    Example:\n\n    ```mlir\n    %gDimZ = gpu.grid_dim z\n    ```\n\n\n    If `known_grid_size` is set on an this operation's enclosing `gpu.func`,\n    or `gpu.known_grid_size` is set on an enclosing `FunctionOpInterface`\n    implementor, or if the enclosing `gpu.launch` specifies a constant size for\n    `dimension`'s grid length, these contextual facts may be used to infer that this\n    operation has a constant value, though such a transformation will not be\n    performed by canonicalization or the default constant folder. Executions which\n    cause that constant-value assumption to be false incur undefined behavior.\n\n    If `upper_bound` is set, executions where the grid size in `dimension` would\n    exceed `upper_bound` cause undefined behavior.\n\n    There is an implicit upper bound of `kMaxDim` (currently uint32_t::max).",
    "assemblyFormat": "$dimension (`upper_bound` $upper_bound^)? attr-dict"
  },
  {
    "name": "gpu.host_register",
    "summary": "Registers a memref for access from device.",
    "description": "This op maps the provided host buffer into the device address space.\n\n    This operation may not be supported in every environment, there is not yet a\n    way to check at runtime whether this feature is supported.\n\n    Writes from the host are guaranteed to be visible to device kernels that are\n    launched afterwards. Writes from the device are guaranteed to be visible on\n    the host after synchronizing with the device kernel completion.",
    "assemblyFormat": "$value attr-dict `:` type($value)"
  },
  {
    "name": "gpu.host_unregister",
    "summary": "Unregisters a memref for access from device.",
    "description": "This op unmaps the provided host buffer from the device address space.\n\n      This operation may not be supported in every environment, there is not yet a\n          way to check at runtime whether this feature is supported.",
    "assemblyFormat": "$value attr-dict `:` type($value)"
  },
  {
    "name": "gpu.lane_id",
    "description": "Returns the lane id within the subgroup (warp/wave).\n\n    Example:\n    ```mlir\n    %laneId = gpu.lane_id\n    ```\n\n    If `upper_bound` is set, executions with more than `upper_bound` lanes per\n    subgroup cause undefined behavior. In the abscence of `upper_bound`,\n    the lane id is still assumed to be non-negative and less than the\n    target-independent `kMaxSubgroupSize` (currently 128).",
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "attributes": [
      { "name": "upper_bound", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`upper_bound` $upper_bound^)? attr-dict"
  },
  {
    "name": "gpu.launch",
    "summary": "GPU kernel launch operation",
    "description": "Launch a kernel on the specified grid of thread blocks. The body of the\n    kernel is defined by the single region that this operation contains. The\n    operation takes an optional list of async dependencies followed by six\n    operands and an optional operand.\n\n    The `async` keyword indicates the kernel should be launched asynchronously;\n    the operation returns a new !gpu.async.token when the keyword is specified.\n    The kernel launched does not start executing until the ops producing its\n    async dependencies (optional operands) have completed.\n\n    The first three operands (following any async dependencies) are grid sizes\n    along the x,y,z dimensions and the following three are block sizes along the\n    x,y,z dimensions. When a lower-dimensional kernel is required, unused sizes\n    must be explicitly set to `1`.  The last operand is optional and corresponds\n    to the amount of dynamic shared memory a kernel's workgroup should be\n    allocated; when this operand is not present, a zero size is assumed.\n\n    The body region has at least _twelve_ arguments, or _eighteen_ if cluster\n    dimensions are present, grouped as follows:\n\n    -   three optional arguments that contain cluster identifiers along x,y,z\n        dimensions;\n    -   three arguments that contain block identifiers along x,y,z dimensions;\n    -   three arguments that contain thread identifiers along x,y,z dimensions;\n    -   operands of the `gpu.launch` operation as is (i.e. the operands for\n        grid and block sizes).\n    -   a variadic number of Workgroup memory attributions.\n    -   a variadic number of Private memory attributions.\n\n    The `function` and `module` attributes are optional and specifies\n    the kernel name and a module in which the kernel should be outlined.\n\n    Syntax:\n\n    ```\n    operation ::= `gpu.launch` (`async` (`[` ssa-id-list `]`)? )?\n                             ( `clusters` `(` ssa-id-list `)` `in` ssa-reassignment )?\n                             `blocks` `(` ssa-id-list `)` `in` ssa-reassignment\n                             `threads` `(` ssa-id-list `)` `in` ssa-reassignment\n                             (dynamic_shared_memory_size ssa-use)?\n                             (`module(` symbol-ref-id `)`)?\n                             (`function(` symbol-ref-id `)`)?\n                             memory-attribution\n                             region attr-dict?\n    ssa-reassignment ::= `(` ssa-id `=` ssa-use (`,` ssa-id `=` ssa-use)* `)`\n    memory-attribution ::= (`workgroup` `(` ssa-id-and-type-list `)`)?\n                           (`private` `(` ssa-id-and-type-list `)`)?\n    ```\n\n    Example:\n\n    ```mlir\n    gpu.launch blocks(%bx, %by, %bz) in (%sz_bx = %0, %sz_by = %1, %sz_bz = %2)\n               threads(%tx, %ty, %tz) in (%sz_tx = %3, %sz_ty = %4, %sz_tz = %5) {\n      // Block and thread identifiers, as well as block/grid sizes are\n      // immediately usable inside body region.\n      \"some_op\"(%bx, %tx) : (index, index) -> ()\n      // Assuming %val1 is defined outside the gpu.launch region.\n      %42 = load %val1[%bx] : memref<?xf32, 1>\n    }\n\n    // Generic syntax explains how the pretty syntax maps to the IR structure.\n    \"gpu.launch\"(%cst, %cst, %c1,  // Grid sizes.\n                 %cst, %c1, %c1)   // Block sizes.\n\n        {/*attributes*/}\n        // All sizes and identifiers have \"index\" size.\n        : (index, index, index, index, index, index) -> () {\n    // The operation passes block and thread identifiers, followed by grid and\n    // block sizes.\n    ^bb0(%bx : index, %by : index, %bz : index,\n         %tx : index, %ty : index, %tz : index,\n         %num_bx : index, %num_by : index, %num_bz : index,\n         %num_tx : index, %num_ty : index, %num_tz : index)\n      \"some_op\"(%bx, %tx) : (index, index) -> ()\n      %3 = \"memref.load\"(%val1, %bx) : (memref<?xf32, 1>, index) -> f32\n    }\n\n    // Launch with memory attributions.\n    gpu.launch blocks(%bx, %by, %bz) in (%sz_bx = %0, %sz_by = %1, %sz_bz = %2)\n               threads(%tx, %ty, %tz) in (%sz_tx = %3, %sz_ty = %4, %sz_tz = %5)\n               workgroup(%workgroup: memref<32xf32, 3>)\n               private(%private: memref<1xf32, 5>) {\n      // Block and thread identifiers, as well as block/grid sizes are\n      // immediately usable inside body region.\n      \"some_op\"(%bx, %tx) : (index, index) -> ()\n      // Assuming %val1 is defined outside the gpu.launch region.\n      %42 = load %workgroup[%bx] : memref<32xf32, 3>\n    }\n\n    // Launch with clusters.\n    gpu.launch clusters(%cx, %cy, %cz) in (%sz_cx = %0, %sz_cy = %1, %sz_cz = %2)\n               blocks(%bx, %by, %bz) in (%sz_bx = %3, %sz_by = %4, %sz_bz = %5)\n               threads(%tx, %ty, %tz) in (%sz_tx = %6, %sz_ty = %7, %sz_tz = %8)\n    {\n      // Cluster, block and thread identifiers, as well as cluster/block/grid\n      // sizes are immediately usable inside body region.\n      \"some_op\"(%cx, %bx, %tx) : (index, index, index) -> ()\n    }\n\n    // Launch with module and function attributes.\n    gpu.launch blocks(%bx, %by, %bz) in (%sz_bx = %0, %sz_by = %1, %sz_bz = %2)\n               threads(%tx, %ty, %tz) in (%sz_tx = %3, %sz_ty = %4, %sz_tz = %5)\n               module(@kernel_module) function(@kernel_func) {\n      \"some_op\"(%bx, %tx) : (index, index) -> ()\n      %42 = load %val1[%bx] : memref<?xf32, 1>\n    }\n    ```\n\n    Rationale: using operation/block arguments gives analyses a clear way of\n    understanding that a value has additional semantics (e.g., we will need to\n    know what value corresponds to threadIdx.x for coalescing). We can recover\n    these properties by analyzing the operations producing values, but it is\n    easier just to have that information by construction."
  },
  {
    "name": "gpu.launch_func",
    "summary": "Launches a function as a GPU kernel",
    "description": "Launch a kernel function on the specified grid of thread blocks.\n    `gpu.launch` operations are lowered to `gpu.launch_func` operations by\n    outlining the kernel body into a function in a dedicated module, which\n    reflects the separate compilation process. The kernel function is required\n    to have the `gpu.kernel` attribute. The module containing the kernel\n    function is required to be a gpu.module. And finally, the module containing\n    the kernel module (which thus cannot be the top-level module) is required\n    to have the `gpu.container_module` attribute. The `gpu.launch_func`\n    operation has a symbol attribute named `kernel` to identify the fully\n    specified kernel function to launch (both the gpu.module and func).\n\n    The `gpu.launch_func` supports async dependencies: the kernel does not start\n    executing until the ops producing those async dependencies have completed.\n\n    By the default, the host implicitly blocks until kernel execution has\n    completed. If the `async` keyword is present, the host does not block but\n    instead a `!gpu.async.token` is returned. Other async GPU ops can take this\n    token as dependency.\n\n    The operation requires at least the grid and block sizes along the x,y,z\n    dimensions as arguments. When a lower-dimensional kernel is required,\n    unused sizes must be explicitly set to `1`.\n\n    The remaining operands are optional. The first optional operand corresponds\n    to the amount of dynamic shared memory a kernel's workgroup should be\n    allocated; when this operand is not present, a zero size is assumed.\n\n    The remaining operands if present are passed as arguments to the kernel\n    function.\n\n    The `gpu.launch_func` also supports kernel launching with clusters if\n    supported by the target architecture. The cluster size can be set by\n    `clusterSizeX`, `clusterSizeY`, and `clusterSizeZ` arguments. When these\n    arguments are present, the Op launches a kernel that clusters the given\n    thread blocks. This feature is exclusive to certain architectures.\n\n    Example:\n\n    ```mlir\n    module attributes {gpu.container_module} {\n\n      // This module creates a separate compilation unit for the GPU compiler.\n      gpu.module @kernels {\n        func.func @kernel_1(%arg0 : f32, %arg1 : memref<?xf32, 1>)\n            attributes { nvvm.kernel = true } {\n\n          // Operations that produce block/thread IDs and dimensions are\n          // injected when outlining the `gpu.launch` body to a function called\n          // by `gpu.launch_func`.\n          %tIdX = gpu.thread_id x\n          %tIdY = gpu.thread_id y\n          %tIdZ = gpu.thread_id z\n\n          %bDimX = gpu.block_dim x\n          %bDimY = gpu.block_dim y\n          %bDimZ = gpu.block_dim z\n\n          %bIdX = gpu.block_id x\n          %bIdY = gpu.block_id y\n          %bIdZ = gpu.block_id z\n\n          %gDimX = gpu.grid_dim x\n          %gDimY = gpu.grid_dim y\n          %gDimZ = gpu.grid_dim z\n\n          // (Optional)  Cluster size only for support architectures\n          %cIdX = gpu.cluster_id x\n          %cIdY = gpu.cluster_id y\n          %cIdZ = gpu.cluster_id z\n\n          %cDimX = gpu.cluster_dim x\n          %cDimY = gpu.cluster_dim y\n          %cDimZ = gpu.cluster_dim z\n\n          \"some_op\"(%bx, %tx) : (index, index) -> ()\n          %42 = load %arg1[%bx] : memref<?xf32, 1>\n        }\n      }\n\n      %t0 = gpu.wait async\n      gpu.launch_func\n          async                           // (Optional) Don't block host, return token.\n          [%t0]                           // (Optional) Execute only after %t0 has completed.\n          @kernels::@kernel_1             // Kernel function.\n          clusters in (%cst, %cst, %cst)  // (Optional) Cluster size only for support architectures.\n          blocks in (%cst, %cst, %cst)    // Grid size.\n          threads in (%cst, %cst, %cst)   // Block size.\n          dynamic_shared_memory_size %s   // (Optional) Amount of dynamic shared\n                                          // memory to allocate for a workgroup.\n          args(%arg0 : f32,               // (Optional) Kernel arguments.\n               %arg1 : memref<?xf32, 1>)\n    }\n    ```",
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n      (`<` $asyncObject^ `:` type($asyncObject) `>`)?\n      $kernel\n      ( `clusters` `in` ` ` `(` $clusterSizeX^ `,` $clusterSizeY `,` $clusterSizeZ `)` )?\n      `blocks` `in` ` ` `(` $gridSizeX `,` $gridSizeY `,` $gridSizeZ `)`\n      `threads` `in` ` ` `(` $blockSizeX `,` $blockSizeY `,` $blockSizeZ `)`\n      custom<LaunchDimType>(type($gridSizeX), ref($clusterSizeX), type($clusterSizeX), type($clusterSizeY), type($clusterSizeZ))\n      (`dynamic_shared_memory_size` $dynamicSharedMemorySize^)?\n      custom<LaunchFuncOperands>($kernelOperands, type($kernelOperands)) attr-dict"
  },
  {
    "name": "gpu.memcpy",
    "summary": "GPU memcpy operation",
    "description": "The `gpu.memcpy` operation copies the content of one memref to another.\n\n    The op does not execute before all async dependencies have finished\n    executing.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token.\n\n    Example:\n\n    ```mlir\n    %token = gpu.memcpy async [%dep] %dst, %src : memref<?xf32, 1>, memref<?xf32>\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "dst", "type": "Arg" },
      { "name": "src", "type": "Arg" }
    ],
    "outputs": [
      { "name": "asyncToken", "type": "Optional" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $dst`,` $src `:` type($dst)`,` type($src) attr-dict"
  },
  {
    "name": "gpu.memset",
    "summary": "GPU memset operation",
    "description": "The `gpu.memset` operation sets the content of memref to a scalar value.\n\n    The op does not execute before all async dependencies have finished\n    executing.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token.\n\n    Example:\n\n    ```mlir\n    %token = gpu.memset async [%dep] %dst, %value : memref<?xf32, 1>, f32\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "dst", "type": "Arg" },
      { "name": "value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "asyncToken", "type": "Optional" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $dst`,` $value `:` type($dst)`,` type($value) attr-dict"
  },
  {
    "name": "gpu.module",
    "summary": "A top level compilation unit containing code to be run on a GPU.",
    "description": "GPU module contains code that is intended to be run on a GPU. A host device\n    can launch this code through a gpu.launc_func that creates a fully\n    qualified symbol through the gpu.module's symbol and a gpu.func symbol\n    contained in the gpu.module.\n\n    The module's top-level scope is modeled by a single region with a single\n    block. GPU modules are required to have a name that is used for symbol\n    resolution by the gpu.launch_func operation.\n\n    Using an op with a region to define a GPU module enables \"embedding\" GPU\n    modules with SIMT execution models in other dialects in a clean manner and\n    allows filtering of code regions to execute passes on only code intended to\n    or not intended to be run on the separate device.\n\n    Modules can contain zero or more target attributes. These attributes encode\n    how to transform modules into binary strings and are used by the\n    `gpu-module-to-binary` pass to transform modules into GPU binaries.\n\n    Modules can contain an optional `OffloadingTranslationAttr` attribute. This\n    attribute will be used during the `gpu-module-to-binary` pass to specify the\n    `OffloadingTranslationAttr` used when creating the `gpu.binary` operation.\n\n    ```\n    gpu.module @symbol_name {\n      gpu.func {}\n        ...\n    }\n    // Module with offloading handler and target attributes.\n    gpu.module @symbol_name2 <#gpu.select_object<1>> [\n        #nvvm.target,\n        #rocdl.target<chip = \"gfx90a\">] {\n      gpu.func {}\n        ...\n    }\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "targets", "type": "OptionalAttr" },
      { "name": "offloadingHandler", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$sym_name\n    (`<` $offloadingHandler^ `>`)?\n    ($targets^)?\n    attr-dict-with-keyword $bodyRegion"
  },
  {
    "name": "gpu.num_subgroups",
    "description": "Returns the number of subgroups within a workgroup.\n\n    Example:\n\n    ```mlir\n    %numSg = gpu.num_subgroups : index\n    ```\n\n    If `upper_bound` is set, executions with more than `upper_bound` subgroups\n    per workgroup cause undefined behavior. There is a default upper bound of\n    `kMaxDim` (currently uint32_t::max).",
    "assemblyFormat": "(`upper_bound` $upper_bound^)? attr-dict `:` type($result)"
  },
  {
    "name": "gpu.printf",
    "summary": "Device-side printf, as in CUDA or OpenCL, for debugging",
    "description": "`gpu.printf` takes a literal format string `format` and an arbitrary number of\n    scalar arguments that should be printed.\n\n    The format string is a C-style printf string, subject to any restrictions\n    imposed by one's target platform.",
    "assemblyFormat": "$format attr-dict (`,` $args^ `:` type($args))?"
  },
  {
    "name": "gpu.return",
    "summary": "Terminator for GPU functions.",
    "description": "A terminator operation for regions that appear in the body of `gpu.func`\n    functions. The operands to the `gpu.return` are the result values returned\n    by an invocation of the `gpu.func`.",
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "gpu.rotate",
    "summary": "Rotate values within a subgroup.",
    "description": "The \"rotate\" op moves values across lanes in a subgroup (a.k.a., local\n    invocations) within the same subgroup. The `width` attribute specifies the\n    number of lanes that participate in the rotation, and must be uniform across\n    all participating lanes. Further, the first `width` lanes of the subgroup\n    must be active.\n\n    `width` must be a power of two, and `offset` must be in the range\n    `[0, width)`.\n\n    Return the `rotateResult` of the invocation whose id within the group is\n    calculated as follows:\n\n    ```mlir\n    Invocation ID = ((LaneId + offset) & (width - 1)) + (LaneId & ~(width - 1))\n    ```\n\n    Returns the `rotateResult` and `true` if the current lane id is smaller than\n    `width`, and poison value and `false` otherwise.\n\n    example:\n\n    ```mlir\n    %1, %2 = gpu.rotate %0, 1, 16 : f32\n    ```\n\n    For lane `k`, returns the value from lane `(k + cst1) % width`.",
    "assemblyFormat": "$value `,` $offset `,` $width attr-dict `:` type($value)"
  },
  {
    "name": "gpu.sddmm",
    "summary": "SDDMM operation",
    "description": "The `gpu.sddmm` operation performs the SDDMM operation on the given sparse and\n    dense matrices, and buffer.  The operation expects handles returned by previous\n    sparse operations to construct an environment and the operands for SDDMM. The\n    buffer must have been allocated on the device.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token in addition to the environment.\n\n    Example:\n\n    ```mlir\n    %token = gpu.sddmm async [%dep] %dnmatA{TRANSPOSE}, %dnmatB{TRANSPOSE}, %spmatC, %buffer into f32\n    ```\n\n    The matrix arguments can also be associated with one of the following\n    operators: NON_TRANSPOSE, TRANSPOSE, CONJUGATE_TRANSPOSE. The default value\n    is NON_TRANSPOSE.",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "dnmatA", "type": "GPU_SparseDnTensorHandle" },
      { "name": "dnmatB", "type": "GPU_SparseDnTensorHandle" },
      { "name": "spmatC", "type": "GPU_SparseSpMatHandle" },
      { "name": "buffer", "type": "AnyMemRef" }
    ],
    "outputs": [
      { "name": "asyncToken", "type": "Optional" }
    ],
    "attributes": [
      { "name": "modeA", "type": "GPU_TransposeModeAttr" },
      { "name": "modeB", "type": "GPU_TransposeModeAttr" },
      { "name": "computeType", "type": "TypeAttr" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $dnmatA (`{` $modeA^ `}`)? `,` $dnmatB (`{` $modeB^ `}`)? `,` $spmatC `,` $buffer attr-dict `:` type($buffer) `into` $computeType"
  },
  {
    "name": "gpu.sddmm_buffer_size",
    "summary": "Precompute buffersize for SDDMM operation",
    "description": "The `gpu.sddmm_buffer_size` operation returns the buffer size required\n    to perform the SDDMM operation on the given sparse and dense matrices.\n    The operation expects handles returned by previous sparse operations\n    to construct an environment and the operands for SDDMM.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token in addition to the environment.\n\n    Example:\n\n    ```mlir\n    %buffersz, %token = gpu.sddmm_buffer_size async [%dep] %dnmatA{TRANSPOSE}, %dnmatB{TRANSPOSE}, %spmatC into f32\n    ```\n\n    The matrix arguments can also be associated with one of the following\n    operators: NON_TRANSPOSE, TRANSPOSE, CONJUGATE_TRANSPOSE. The default value\n    is NON_TRANSPOSE.",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "dnmatA", "type": "GPU_SparseDnTensorHandle" },
      { "name": "dnmatB", "type": "GPU_SparseDnTensorHandle" },
      { "name": "spmatC", "type": "GPU_SparseSpMatHandle" }
    ],
    "outputs": [
      { "name": "bufferSz", "type": "Res" },
      { "name": "asyncToken", "type": "Optional" }
    ],
    "attributes": [
      { "name": "modeA", "type": "GPU_TransposeModeAttr" },
      { "name": "modeB", "type": "GPU_TransposeModeAttr" },
      { "name": "computeType", "type": "TypeAttr" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $dnmatA (`{` $modeA^ `}`)? `,` $dnmatB (`{` $modeB^ `}`)? `,` $spmatC attr-dict `into` $computeType"
  },
  {
    "name": "gpu.set_csr_pointers",
    "summary": "SpGEMM get size operation",
    "description": "The `gpu.set_csr_pointers` assigns the given positions, coordinates,\n    and values buffer that reside on the device directly to the given sparse\n    matrix descriptor in csr format.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a `!gpu.async.token` in addition to the environment.\n\n    Example:\n\n    ```mlir\n    %token = gpu.set_csr_pointers async [%dep] %positions, %coordinates, %values\n          : memref<?xf32>, memref<?xindex>, memref<?xindex>\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "spmat", "type": "Arg" },
      { "name": "positions", "type": "AnyMemRef" },
      { "name": "coordinates", "type": "AnyMemRef" },
      { "name": "values", "type": "AnyMemRef" }
    ],
    "outputs": [
      { "name": "asyncToken", "type": "Optional" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n      $spmat `,` $positions `,` $coordinates `,` $values attr-dict\n        `:` type($positions) `,` type($coordinates) `,` type($values)"
  },
  {
    "name": "gpu.set_default_device",
    "summary": "Set default GPU for operations after this by index",
    "description": "Operation that sets the current default GPU, using a zero-based index\n    into the set of GPUs on the system. The default GPU setting may be\n    thread-local.",
    "assemblyFormat": "attr-dict $devIndex"
  },
  {
    "name": "gpu.shuffle",
    "summary": "Shuffles values within a subgroup.",
    "description": "The \"shuffle\" op moves values across lanes in a subgroup (a.k.a., local\n    invocation) within the same subgroup. The `width` argument specifies the\n    number of lanes that participate in the shuffle, and must be uniform\n    across all lanes. Further, the first `width` lanes of the subgroup must\n    be active.\n\n    The intepretation of the `offset` arguments depends on the selected\n    `mode`.\n\n    Returns the `shuffleResult` and `true` if the current lane id is smaller\n    than `width`, and an unspecified value and `false` otherwise.\n\n    `xor` example:\n\n    ```mlir\n    %1, %2 = gpu.shuffle xor %0, %offset, %width : f32\n    ```\n\n    For lane `k`, returns the value `%0` from lane `k ^ offset`. Every lane\n    trades value with exactly one other lane.\n\n    `down` example:\n\n    ```mlir\n    %cst1 = arith.constant 1 : i32\n    %3, %4 = gpu.shuffle down %0, %cst1, %width : f32\n    ```\n\n    For lane `k`, returns the value from lane `(k + cst1)`. If `(k + cst1)` is\n    bigger than or equal to `width`, the value is poison and `valid` is `false`.\n\n    `up` example:\n\n    ```mlir\n    %cst1 = arith.constant 1 : i32\n    %5, %6 = gpu.shuffle up %0, %cst1, %width : f32\n    ```\n\n    For lane `k`, returns the value from lane `(k - cst1)`. If `(k - cst1)` is\n    smaller than `0`, the value is poison and `valid` is `false`.\n\n    `idx` example:\n\n    ```mlir\n    %cst0 = arith.constant 0 : i32\n    %7, %8 = gpu.shuffle idx %0, %cst0, %width : f32\n    ```\n\n    Broadcasts the value from lane 0 to all lanes.",
    "assemblyFormat": "$mode $value `,` $offset `,` $width attr-dict `:` type($value)"
  },
  {
    "name": "gpu.spgemm_copy",
    "summary": "SpGEMM copy operation",
    "description": "The `gpu.spgemm_copy` operation copies the sparse matrix result of\n    a SpGEMM computation.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a `!gpu.async.token` in addition to the environment.\n\n    Example:\n\n    ```mlir\n    gpu.spgemm_copy %spmatA, %spmatB, %spmatC, %spgemmDesc: f32\n    ```\n\n    The matrix arguments can also be associated with one of the following\n    operators: NON_TRANSPOSE, TRANSPOSE, CONJUGATE_TRANSPOSE. The default value\n    is NON_TRANSPOSE.",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "desc", "type": "GPU_SparseSpGEMMOpHandle" },
      { "name": "spmatA", "type": "GPU_SparseSpMatHandle" },
      { "name": "spmatB", "type": "GPU_SparseSpMatHandle" },
      { "name": "spmatC", "type": "GPU_SparseSpMatHandle" }
    ],
    "outputs": [
      { "name": "asyncToken", "type": "Optional" }
    ],
    "attributes": [
      { "name": "modeA", "type": "GPU_TransposeModeAttr" },
      { "name": "modeB", "type": "GPU_TransposeModeAttr" },
      { "name": "computeType", "type": "TypeAttr" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $spmatA (`{` $modeA^ `}`)? `,` $spmatB (`{` $modeB^ `}`)? `,` $spmatC `,` $desc attr-dict `:` $computeType"
  },
  {
    "name": "gpu.spgemm_create_descr",
    "summary": "SpGEMM Create Descr operation",
    "description": "The `gpu.spgemm_create_descr` creates a descriptor for the SpGEMM operation.\n    The descriptor describes the SpGEMM operation and stores the internal data\n    throughout the computation. It needs to be passed as an argument to\n    spgemm_* operations.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a `!gpu.async.token` in addition to the environment.\n\n    Example:\n\n    ```mlir\n    %desc, %token = gpu.spgemm_create_descr async [%dep]\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "desc", "type": "GPU_SparseSpGEMMOpHandle" },
      { "name": "asyncToken", "type": "Optional" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    attr-dict"
  },
  {
    "name": "gpu.spgemm_destroy_descr",
    "summary": "SpGEMM Destroy Descr operation",
    "description": "The `gpu.spgemm_destroy_descr` destroys the SpGEMM operation descriptor.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a `!gpu.async.token` in addition to the environment.\n\n    Example:\n\n    ```mlir\n    %token = gpu.spgemm_destroy_descr async [%dep] %desc\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "desc", "type": "GPU_SparseSpGEMMOpHandle" }
    ],
    "outputs": [
      { "name": "asyncToken", "type": "Optional" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $desc attr-dict"
  },
  {
    "name": "gpu.spgemm_work_estimation_or_compute",
    "summary": "SpGEMM work estimation operation",
    "description": "The `gpu.spgemm_work_estimation_or_compute` is used to call\n    cusparseSpGEMM_workEstimation or cusparseSpGEMM_compute. Both of them are\n    for both determining the buffer size and performing the actual computation.\n    The operation expects handles returned by previous sparse operations to\n    construct an environment and the operands for SpGEMM.\n    The buffer must have been allocated on the device.\n\n    C' = alpha * op(A) * op(B) + beta * C\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a `!gpu.async.token` in addition to the environment.\n\n    Example:\n\n    ```mlir\n    %bufferSz, %token = gpu.spgemm_work_estimation_or_compute async [%dep] {COMPUTE}\n                          %desc, %spmatA{NON_TRANSPOSE}, %spmatB{NON_TRANSPOSE},\n                          %spmatC, %spgemmDesc, %c0, %alloc: f32 into\n                          memref<0xi8>\n    ```\n\n    The matrix arguments can also be associated with one of the following\n    operators: NON_TRANSPOSE, TRANSPOSE, CONJUGATE_TRANSPOSE. The default value\n    is NON_TRANSPOSE.",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "desc", "type": "GPU_SparseSpGEMMOpHandle" },
      { "name": "spmatA", "type": "GPU_SparseSpMatHandle" },
      { "name": "spmatB", "type": "GPU_SparseSpMatHandle" },
      { "name": "spmatC", "type": "GPU_SparseSpMatHandle" },
      { "name": "bufferSz", "type": "Index" },
      { "name": "buffer", "type": "AnyMemRef" }
    ],
    "outputs": [
      { "name": "bufferSzNew", "type": "Res" },
      { "name": "asyncToken", "type": "Optional" }
    ],
    "attributes": [
      { "name": "modeA", "type": "GPU_TransposeModeAttr" },
      { "name": "modeB", "type": "GPU_TransposeModeAttr" },
      { "name": "computeType", "type": "TypeAttr" },
      { "name": "kind", "type": "GPU_SpGEMMWorkEstimationOrComputeKindAttr" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    `{` $kind `}` $spmatA (`{` $modeA^ `}`)? `,` $spmatB (`{` $modeB^ `}`)? `,` $spmatC `,` $desc `,` $bufferSz `,` $buffer  attr-dict `:` $computeType `into` type($buffer)"
  },
  {
    "name": "gpu.spmat_get_size",
    "summary": "SpMat get size operation",
    "description": "The `gpu.spmat_get_size` operation retrieves the number of rows, number of\n    columns, and number of non-zero elements of a sparse matrix.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a `!gpu.async.token` in addition to the environment.\n\n    Example:\n\n    ```mlir\n    %rows, %cols, %nnz, %token = gpu.spmat_get_size async [%dep] %spmatC\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "spmat", "type": "GPU_SparseSpMatHandle" }
    ],
    "outputs": [
      { "name": "rows", "type": "Index" },
      { "name": "cols", "type": "Index" },
      { "name": "nnz", "type": "Index" },
      { "name": "asyncToken", "type": "Optional" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $spmat attr-dict"
  },
  {
    "name": "gpu.spmm",
    "summary": "SpMM operation",
    "description": "The `gpu.spmm` operation performs the SpMM operation on the given sparse and\n    dense matrix, and buffer.  The operation expects handles returned by previous\n    sparse operations to construct an environment and the operands for SpMM. The\n    buffer must have been allocated on the device.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token in addition to the environment.\n\n    The matrix arguments can also be associated with one of the following\n    operators: NON_TRANSPOSE, TRANSPOSE, CONJUGATE_TRANSPOSE. The default value\n    is NON_TRANSPOSE.\n\n    Example:\n\n    ```mlir\n    %token = gpu.spmm async [%dep] %spmatA{TRANSPOSE}, %dnmatB{TRANSPOSE}, %dnmatC, %buffers : type($buffers) into f32\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "spmatA", "type": "GPU_SparseSpMatHandle" },
      { "name": "dnmatB", "type": "GPU_SparseDnTensorHandle" },
      { "name": "dnmatC", "type": "GPU_SparseDnTensorHandle" },
      { "name": "buffers", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "asyncToken", "type": "Optional" }
    ],
    "attributes": [
      { "name": "modeA", "type": "GPU_TransposeModeAttr" },
      { "name": "modeB", "type": "GPU_TransposeModeAttr" },
      { "name": "computeType", "type": "TypeAttr" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $spmatA (`{` $modeA^ `}`)? `,` $dnmatB (`{` $modeB^ `}`)? `,` $dnmatC `,` $buffers attr-dict `:` type($buffers) `into` $computeType"
  },
  {
    "name": "gpu.spmm_buffer_size",
    "summary": "Precompute buffersize for SpMM operation",
    "description": "The `gpu.spmm_buffer_size` operation returns the buffer size required\n    to perform the SpMM operation on the given sparse and dense matrix.\n    The operation expects handles returned by previous sparse operations\n    to construct an environment and the operands for SpMM.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token in addition to the environment.\n\n    The matrix arguments can also be associated with one of the following\n    operators: NON_TRANSPOSE, TRANSPOSE, CONJUGATE_TRANSPOSE. The default value\n    is NON_TRANSPOSE.\n\n    Example:\n\n    ```mlir\n    %bufferszs, %token = gpu.spmm_buffer_size async [%dep] %spmatA{TRANSPOSE}, %dnmatB{TRANSPOSE}, %dnmatC : i64 into f32\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "spmatA", "type": "GPU_SparseSpMatHandle" },
      { "name": "dnmatB", "type": "GPU_SparseDnTensorHandle" },
      { "name": "dnmatC", "type": "GPU_SparseDnTensorHandle" }
    ],
    "outputs": [
      { "name": "bufferSzs", "type": "Variadic" },
      { "name": "asyncToken", "type": "Optional" }
    ],
    "attributes": [
      { "name": "modeA", "type": "GPU_TransposeModeAttr" },
      { "name": "modeB", "type": "GPU_TransposeModeAttr" },
      { "name": "computeType", "type": "TypeAttr" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $spmatA (`{` $modeA^ `}`)? `,` $dnmatB (`{` $modeB^ `}`)? `,` $dnmatC attr-dict `:` type($bufferSzs) `into` $computeType"
  },
  {
    "name": "gpu.spmv",
    "summary": "SpMV operation",
    "description": "The `gpu.spmv` operation performs the SpMV operation on the given sparse matrix,\n    dense vectors, and buffer.  The operation expects handles returned by previous\n    sparse operations to construct an environment and the operands for SpMV. The\n    buffer must have been allocated on the device.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token in addition to the environment.\n\n    The matrix arguments can also be associated with one of the following\n    operators: NON_TRANSPOSE, TRANSPOSE, CONJUGATE_TRANSPOSE. The default value\n    is NON_TRANSPOSE.\n\n    Example:\n\n    ```mlir\n    %token = gpu.spmv async [%dep] %spmatA{TRANSPOSE}, %dnX, %dnY : memref<?xf64> into bf16\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "spmatA", "type": "GPU_SparseSpMatHandle" },
      { "name": "dnX", "type": "GPU_SparseDnTensorHandle" },
      { "name": "dnY", "type": "GPU_SparseDnTensorHandle" },
      { "name": "buffer", "type": "AnyMemRef" }
    ],
    "outputs": [
      { "name": "asyncToken", "type": "Optional" }
    ],
    "attributes": [
      { "name": "modeA", "type": "GPU_TransposeModeAttr" },
      { "name": "computeType", "type": "TypeAttr" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $spmatA (`{` $modeA^ `}`)? `,` $dnX `,` $dnY `,` $buffer attr-dict `:` type($buffer) `into` $computeType"
  },
  {
    "name": "gpu.spmv_buffer_size",
    "summary": "Precompute buffersize for SpMV operation",
    "description": "The `gpu.spmv_buffer_size` operation returns the buffer size required\n    to perform the SpMV operation on the given sparse matrix and dense vectors.\n    The operation expects handles returned by previous sparse operations\n    to construct an environment and the operands for SpMV.\n\n    If the `async` keyword is present, the op is executed asynchronously (i.e.\n    it does not block until the execution has finished on the device). In\n    that case, it returns a !gpu.async.token in addition to the environment.\n\n    The matrix arguments can also be associated with one of the following\n    operators: NON_TRANSPOSE, TRANSPOSE, CONJUGATE_TRANSPOSE. The default value\n    is NON_TRANSPOSE.\n\n    Example:\n\n    ```mlir\n    %buffersz, %token = gpu.spmv_buffer_size async [%dep] %spmatA{TRANSPOSE}, %dnX, %dnY into f32\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" },
      { "name": "spmatA", "type": "GPU_SparseSpMatHandle" },
      { "name": "dnX", "type": "GPU_SparseDnTensorHandle" },
      { "name": "dnY", "type": "GPU_SparseDnTensorHandle" }
    ],
    "outputs": [
      { "name": "bufferSz", "type": "Res" },
      { "name": "asyncToken", "type": "Optional" }
    ],
    "attributes": [
      { "name": "modeA", "type": "GPU_TransposeModeAttr" },
      { "name": "computeType", "type": "TypeAttr" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies)\n    $spmatA (`{` $modeA^ `}`)? `,` $dnX `,` $dnY attr-dict  `into` $computeType"
  },
  {
    "name": "gpu.subgroup_broadcast",
    "summary": "Broadcasts a value from the specific lane across subgroup",
    "description": "Broadcasts a value from one lane to all active lanes in a subgroup. The\n      result is guaranteed to be uniform across the active lanes in subgroup.\n\n      The possible broadcast types are:\n\n      * `first_active_lane` - broadcasts the value from the first active lane\n      in the subgroup.\n      * `specific_lane` - broadcasts from the specified lane. The lane index\n      must be uniform and within the subgroup size. The result is poison if the\n      lane index is invalid, non subgroup-uniform, or if the source lane is not\n      active.",
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "$src `,` $broadcast_type ($lane^)? attr-dict `:` type($result)"
  },
  {
    "name": "gpu.subgroup_id",
    "description": "Returns the subgroup id, i.e., the index of the current subgroup within the\n    workgroup.\n\n    Example:\n\n    ```mlir\n    %sgId = gpu.subgroup_id : index\n    ```\n\n    Executions where there are more than `upper_bound` subgroups per workgroup\n    cause undefined behavior. There is an implicit upper bound of `kMaxDim`\n    (currently uint32_t::max).",
    "assemblyFormat": "(`upper_bound` $upper_bound^)? attr-dict `:` type($result)"
  },
  {
    "name": "gpu.subgroup_mma_compute",
    "summary": "GPU warp synchronous matrix multiply accumulate",
    "description": "The `gpu.subgroup_mma_compute` operation performs a matrix-multiply accumulate (mma)\n    operation using all the threads in a subgroup.\n\n    This operation takes three `!gpu.mma_matrix`s as arguments: these hold `A`,\n    `B` and `C`operands for the mma operation. The operation performed is represented\n    as `C += A * B`. The op returns a `!gpu.mma_matrix` which contains the result of\n    the operation held by all threads in a subgroup. `a_transpose` or\n    `b_transpose` if present, signify that the respective operand was loaded in a\n    transposed manner. The transpose operands are required to map to correct\n    underlying intrisics but they currently do not seem to affect correctness\n    even if they are absent given that the operands were loaded correctly using\n    the `transpose` attribute in `gpu.subgroup_mma_load_matrix` op.\n\n    For integer types, the `A` and `B` matrices carry their signedness with their\n    types. The accumulator type is expected to be signless and imply a signed integer\n    with a greater width than the other two operands.\n\n    This op is meant to be used along with `gpu.subgroup_mma_store_matrix` and\n    `gpu.subgroup_mma_load_matrix` ops.\n\n    Example:\n\n    ```mlir\n    %D = gpu.subgroup_mma_compute_matrix %A, %B, %C :\n      !gpu.mma_matrix<16x16xf16, \"AOp\">, !gpu.mma_matrix<16x16xf16, \"BOp\">>\n      -> !gpu.mma_matrix<16x16xf16, \"COp\">\n    ```",
    "inputs": [
      { "name": "opA", "type": "Arg" },
      { "name": "opB", "type": "Arg" },
      { "name": "opC", "type": "Arg" }
    ],
    "outputs": [
      { "name": "res", "type": "GPU_MMAMatrix" }
    ],
    "attributes": [
      { "name": "a_transpose", "type": "OptionalAttr" },
      { "name": "b_transpose", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$opA`,` $opB`,` $opC attr-dict `:` type($opA)`,` type($opB) `->` type($res)"
  },
  {
    "name": "gpu.subgroup_mma_constant_matrix",
    "summary": "GPU warp synchronous constant matrix",
    "description": "The `gpu.subgroup_mma_constant_matrix` creates a `!gpu.mma_matrix` with\n    constant elements.\n\n    The operation takes a scalar input and return a `!gpu.mma_matrix` where\n    each element of is equal to the operand constant. The destination\n    mma_matrix type must have elememt type equal to the constant type. Since\n    the layout of `!gpu.mma_matrix` is opaque this only support setting all the\n    elements to the same value.\n\n    This op is meant to be used along with `gpu.subgroup_mma_compute`.\n\n    Example:\n\n    ```mlir\n     %0 = gpu.subgroup_mma_constant_matrix %a :\n       !gpu.mma_matrix<16x16xf16, \"AOp\">\n     %1 = gpu.subgroup_mma_constant_matrix %b :\n       !gpu.mma_matrix<16x16xf32, \"COp\">\n    ```",
    "inputs": [
      { "name": "value", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "res", "type": "GPU_MMAMatrix" }
    ],
    "assemblyFormat": "$value attr-dict `:` type($res)"
  },
  {
    "name": "gpu.subgroup_mma_elementwise",
    "summary": "GPU warp elementwise operation on a matrix",
    "description": "The `gpu.subgroup_mma_elementwise` takes `!gpu.mma_matrix` inputs and\n    compute a new `!gpu.mma_matrix` by applying an elementwise operation to each\n    element.\n\n    Since the operation is elementwise and the matrix type must match, the\n    matrix elements are processed independently of the matrix layout.\n\n    This op is meant to be used along with `gpu.subgroup_mma_compute`.\n\n    Example:\n\n    ```mlir\n     %0 =  %A, %B { opType = \"ADD\" } :\n      (!gpu.mma_matrix<16x16xf16, \"COp\">, !gpu.mma_matrix<16x16xf16, \"COp\">)\n      -> !gpu.mma_matrix<16x16xf16, \"COp\">\n    ```",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "res", "type": "GPU_MMAMatrix" }
    ],
    "attributes": [
      { "name": "opType", "type": "MMAElementWiseAttr" }
    ],
    "assemblyFormat": "$opType $args attr-dict `:` functional-type($args, $res)"
  },
  {
    "name": "gpu.subgroup_mma_extract_thread_local",
    "summary": "Extract a value from GPU warp by invocation and indices",
    "description": "The `gpu.subgroup_mma_extract_thread_local` operation extracts a value from `!gpu.mma_matrix`\n    that is stored at subgroup level.\n\n    This operation takes `!gpu.mma_matrix` as its first operand. It is the source\n    matrix across a subgroup. The op returns a scalar value stored in the invocation\n    in the subgroup.\n\n    Since `matrix` is packed into the the threads within a subgroup, `indices` are\n    the indices into the values stored by each thread. That is, an index of 0 (or [0, 0])\n    does not necessarily refer to the first element of the matrix, but the first element\n    that a particular thread holds.\n\n    The mapping of matrix elements to threads is not defined by this operation and may\n    not be defined by some lowerings (such as the lowering to SPIR-V). However, if the\n    size of the subgroup is S, then `subgroup_mma_extract_thread_local` at each index in\n    `[0, (M * N) / S)` will have the entire matrix extracted across the subgroup.\n\n    Example:\n\n    ```mlir\n    %c0 = arith.constant 0 : index\n    %val = gpu.subgroup_mma_extract_thread_local %m[%c0] : !gpu.mma_matrix<16x16xf32, \"AOp\"> -> f32\n    ```",
    "inputs": [
      { "name": "matrix", "type": "GPU_MMAMatrix" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "res", "type": "AnyIntegerOrFloat" }
    ],
    "assemblyFormat": "$matrix`[`$indices`]` attr-dict `:` type($matrix) `->` type($res)"
  },
  {
    "name": "gpu.subgroup_mma_insert_thread_local",
    "summary": "Insert a value into GPU warp by invocation and indices",
    "description": "The `gpu.subgroup_mma_insert_thread_local` operation inserts a value to `!gpu.mma_matrix`\n    that is stored at subgroup level.\n\n    This operation takes scalar value as its first operand and `!gpu.mma_matrix`\n    as its second operand. The op inserts the scalar value to the matrix.\n\n    Since `matrix` is packed into the the threads within a subgroup, `indices` are\n    the indices into the values stored by each thread. That is, an index of 0 (or [0, 0])\n    does not necessarily refer to the first element of the matrix, but the first element\n    that a particular thread holds.\n\n    The mapping of matrix elements to threads is not defined by this operation and may\n    not be defined by some lowerings (such as the lowering to SPIR-V). However, if the\n    size of the subgroup is S, then `subgroup_mma_insert_thread_local` at each index in\n    `[0, (M * N) / S)` will have the entire matrix inserted across the subgroup.\n\n    The op returns `!gpu.mma_matrix` with the updated value.\n\n    Example:\n\n    ```mlir\n    %c0 = arith.constant 0 : index\n    %s0 = gpu.subgroup_mma_insert_thread_local %val, %m[%c0] : f16, !gpu.mma_matrix<16x16xf16, \"COp\">\n            -> !gpu.mma_matrix<16x16xf16, \"COp\">\n    ```",
    "inputs": [
      { "name": "value", "type": "AnyIntegerOrFloat" },
      { "name": "matrix", "type": "GPU_MMAMatrix" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "res", "type": "GPU_MMAMatrix" }
    ],
    "assemblyFormat": "$value`,` $matrix`[`$indices`]` attr-dict `:` type($value)`,` type($matrix) `->` type($res)"
  },
  {
    "name": "gpu.subgroup_mma_load_matrix",
    "summary": "GPU warp synchronous matrix load",
    "description": "The `gpu.subgroup_mma_load_matrix` operation loads a matrix collectively\n    using all the threads in a subgroup.\n\n    This operation takes a memref as its first operand: it is the source matrix\n    from which data is to be loaded. The op returns a `!gpu.mma_matrix`. The\n    source memref can be in global memory or shared memory. The load address is\n    determined using `indices`. The matrix being loaded into is the result.  The\n    `leadDimension` attribute specifies the leading dimension size of the source\n    matrix which eventually allows the lowering to determine the size of each\n    row.  If the `transpose` attribute is present then the op does a transposed load.\n\n    For integer types, the resulting `!gpu.mma_matrix` type needs to specify the\n    signedness of the data if the matrix type is an `A` or `B` operand for\n    `gpu.subgroup_mma_compute`.\n\n    This op is often meant to be used along with `gpu.subgroup_mma_store_matrix` and\n    `gpu.subgroup_mma_compute`.\n\n    Example:\n\n    ```mlir\n     %0 = gpu.subgroup_mma_load_matrix src[%i,%j] : {leadDimension = 32 : i32}\n          : memref<32x32xf16, 3>, !gpu.mma_matrix<16x16xf16, \"AOp\">\n    ```",
    "inputs": [
      { "name": "srcMemref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "res", "type": "GPU_MMAMatrix" }
    ],
    "attributes": [
      { "name": "leadDimension", "type": "IndexAttr" },
      { "name": "transpose", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$srcMemref`[`$indices`]` attr-dict `:` type($srcMemref) `->` type($res)"
  },
  {
    "name": "gpu.subgroup_mma_store_matrix",
    "summary": "GPU warp synchronous matrix store",
    "description": "The `gpu.subgroup_mma_store_matrix` operation stores a matrix collectively\n    using all the threads in a subgroup.\n\n    This operation takes a `!gpu.mma_matrix` and a memref as operands.\n    `!gpu.mma_matrix` is the source value containing the data to be stored into the\n    destination memref which can be in global or shared memory.  The store address\n    is determined using the indices provided. The `leadDimension` attribute\n    specifies the leading dimension of the destination matrix. If the\n    `transpose` attribute is present then the op does a transposed store.\n\n    This op is often meant to be used along with `gpu.subgroup_mma_load_matrix` and\n    `gpu.subgroup_mma_compute`.\n\n    Example:\n\n    ```mlir\n    gpu.subgroup_mma_store_matrix %D, %sg[%i,%j] : { leadDimension = 32 : i32}\n                    : !gpu.mma_matrix<16x16xf16, \"COp\">, memref<32x32xf16, 3>\n    ```",
    "inputs": [
      { "name": "src", "type": "Arg" },
      { "name": "dstMemref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "leadDimension", "type": "IndexAttr" },
      { "name": "transpose", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$src`,` $dstMemref`[`$indices`]` attr-dict `:` type($src)`,` type($dstMemref)"
  },
  {
    "name": "gpu.subgroup_reduce",
    "summary": "Reduce values among subgroup.",
    "description": "The `subgroup_reduce` op reduces the values of lanes (work items) across a\n    subgroup.\n\n    The subgroup is divided into clusters starting at lane index 0. Within each\n    cluster, there are `size` lanes, and the lane index advances by `stride`.\n    A reduction is done for each cluster in parallel: every lane in the cluster\n    is reduced, and the result is equal for all lanes in the cluster. If `size`\n    is omitted, there is a single cluster covering the entire subgroup. If\n    `stride` is omitted, the stride is 1 (the cluster's lanes are contiguous).\n\n    When the reduced value is of a vector type, each vector element is reduced\n    independently. Only 1-d vector types are allowed.\n\n    Example:\n\n    ```mlir\n    %1 = gpu.subgroup_reduce add %a : (f32) -> f32\n    %2 = gpu.subgroup_reduce add %b : (vector<4xf16>) -> vector<4xf16>\n    %3 = gpu.subgroup_reduce add %c cluster(size = 4) : (f32) -> f32\n    %3 = gpu.subgroup_reduce add %c cluster(size = 4, stride = 2) : (f32) -> f32\n    ```\n\n    If `uniform` flag is set either none or all lanes of a subgroup need to execute\n    this op in convergence.\n\n    The reduction operation must be one of:\n    *  Integer types: `add`, `mul`, `minui`, `minsi`, `maxui`, `maxsi`, `and`,\n       `or`, `xor`\n    *  Floating point types: `add`, `mul`, `minnumf`, `maxnumf`, `minimumf`,\n       `maximumf`",
    "inputs": [
      { "name": "value", "type": "AnyIntegerOrFloatOr1DVector" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyIntegerOrFloatOr1DVector" }
    ],
    "attributes": [
      { "name": "op", "type": "GPU_AllReduceOperationAttr" },
      { "name": "uniform", "type": "UnitAttr" },
      { "name": "cluster_size", "type": "OptionalAttr" },
      { "name": "cluster_stride", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "custom<AllReduceOperation>($op) $value\n                          (`uniform` $uniform^)?\n                          (`cluster` `(` `size` `=` $cluster_size^ (`,` `stride` `=` $cluster_stride^)? `)`)?\n                          attr-dict\n                          `:` functional-type(operands, results)"
  },
  {
    "name": "gpu.subgroup_size",
    "description": "Returns the number of threads within a subgroup.\n\n    Example:\n\n    ```mlir\n    %sgSz = gpu.subgroup_size : index\n    ```\n\n    Executions where the number of threads per subgroup exceed `upper_bound` cause\n    undefined behavior. When no `upper_bound` is specified, range analyses and\n    similar machinery assume the default bound of `kMaxSubgroupSize`, currently\n    128.",
    "assemblyFormat": "(`upper_bound` $upper_bound^)? attr-dict `:` type($result)"
  },
  {
    "name": "gpu.terminator",
    "summary": "Terminator for GPU launch regions.",
    "description": "A terminator operation for regions that appear in the body of `gpu.launch`\n    operation.  These regions are not expected to return any value so the\n    terminator takes no operands.",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "gpu.thread_id",
    "description": "Returns the thread id, i.e. the index of the current thread within the block\n    along the x, y, or z `dimension`.\n\n    Example:\n\n    ```mlir\n    %tIdX = gpu.thread_id x\n    ```\n\n    If `upper_bound` is set, or if one can be inferred from `known_block_size`-type\n    annotations in context, executions where the thread index would be greater\n    than or equal to that bound cause undefined behavior.\n\n    There is an implicit upper bound of `kMaxDim` (currently uint32_t::max).",
    "assemblyFormat": "$dimension (`upper_bound` $upper_bound^)? attr-dict"
  },
  {
    "name": "gpu.wait",
    "summary": "Wait for async gpu ops to complete.",
    "description": "This op synchronizes the host or the device with a list of dependent ops.\n\n    If the op contains the `async` keyword, it returns a new async token which\n    is synchronized with the op arguments. This new token is merely a shortcut\n    to the argument list, and one could replace the uses of the result with the\n    arguments for the same effect. The async version of this op is primarily\n    used to make each async token have a single use during lowering and\n    thereby make forks in async execution explicit. Example usage:\n\n    ```mlir\n    %t0 = gpu.foo async : !gpu.async.token\n    %t1 = gpu.bar async : !gpu.async.token\n    %t2 = gpu.wait async [%t0, %t1]\n    // gpu.baz doesn't run until gpu.foo and gpu.bar have both completed, just\n    // as if the async dependencies were [%t0, %t1].\n    %t3 = gpu.baz async [%t2]\n    ```\n\n    If the op does not contain the `async` keyword, it does not return a new\n    async token but blocks until all ops producing the async dependency tokens\n    finished execution. All dependent memory operations are visible to the host\n    once this op completes. Example usage:\n\n    ```mlir\n    %t0 = gpu.foo async : !gpu.async.token\n    %t1 = gpu.bar async : !gpu.async.token\n    // The gpu.wait op blocks until gpu.foo and gpu.bar have completed.\n    gpu.wait [%t0, %t1]\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "asyncToken", "type": "Optional" }
    ],
    "assemblyFormat": "custom<AsyncDependencies>(type($asyncToken), $asyncDependencies) attr-dict"
  },
  {
    "name": "gpu.warp_execute_on_lane_0",
    "summary": "Executes operations in the associated region on thread #0 of aSPMD program",
    "description": "`warp_execute_on_lane_0` is an operation used to bridge the gap between\n    vector programming and SPMD programming model like GPU SIMT. It allows to\n    trivially convert a region of vector code meant to run on a multiple threads\n    into a valid SPMD region and then allows incremental transformation to\n    distribute vector operations on the threads.\n\n    Any code present in the region would only be executed on first thread/lane\n    based on the `laneid` operand. The `laneid` operand is an integer ID between\n    [0, `warp_size`). The `warp_size` attribute indicates the number of lanes in\n    a warp.\n\n    Operands are vector values distributed on all lanes that may be used by\n    the single lane execution. The matching region argument is a vector of all\n    the values of those lanes available to the single active lane. The\n    distributed dimension is implicit based on the shape of the operand and\n    argument. the properties of the distribution may be described by extra\n    attributes (e.g. affine map).\n\n    Return values are distributed on all lanes using laneId as index. The\n    vector is distributed based on the shape ratio between the vector type of\n    the yield and the result type.\n    If the shapes are the same this means the value is broadcasted to all lanes.\n    In the future the distribution can be made more explicit using affine_maps\n    and will support having multiple Ids.\n\n    Therefore the `warp_execute_on_lane_0` operations allow to implicitly copy\n    between lane0 and the lanes of the warp. When distributing a vector\n    from lane0 to all the lanes, the data are distributed in a block cyclic way.\n    For example `vector<64xf32>` gets distributed on 32 threads and map to\n    `vector<2xf32>` where thread 0 contains vector[0] and vector[1].\n\n    During lowering values passed as operands and return value need to be\n    visible to different lanes within the warp. This would usually be done by\n    going through memory.\n\n    The region is *not* isolated from above. For values coming from the parent\n    region not going through operands only the lane 0 value will be accesible so\n    it generally only make sense for uniform values.\n\n    Example:\n    ```\n    // Execute in parallel on all threads/lanes.\n    gpu.warp_execute_on_lane_0 (%laneid)[32] {\n      // Serial code running only on thread/lane 0.\n      ...\n    }\n    // Execute in parallel on all threads/lanes.\n    ```\n\n    This may be lowered to an scf.if region as below:\n    ```\n      // Execute in parallel on all threads/lanes.\n      %cnd = arith.cmpi eq, %laneid, %c0 : index\n      scf.if %cnd {\n        // Serial code running only on thread/lane 0.\n        ...\n      }\n      // Execute in parallel on all threads/lanes.\n    ```\n\n    When the region has operands and/or return values:\n    ```\n    // Execute in parallel on all threads/lanes.\n    %0 = gpu.warp_execute_on_lane_0(%laneid)[32]\n    args(%v0 : vector<4xi32>) -> (vector<1xf32>) {\n    ^bb0(%arg0 : vector<128xi32>) :\n      // Serial code running only on thread/lane 0.\n      ...\n      gpu.yield %1 : vector<32xf32>\n    }\n    // Execute in parallel on all threads/lanes.\n    ```\n\n    values at the region boundary would go through memory:\n    ```\n    // Execute in parallel on all threads/lanes.\n    ...\n    // Store the data from each thread into memory and Synchronization.\n    %tmp0 = memreg.alloc() : memref<128xf32>\n    %tmp1 = memreg.alloc() : memref<32xf32>\n    %cnd = arith.cmpi eq, %laneid, %c0 : index\n    vector.store %v0, %tmp0[%laneid] : memref<128xf32>, vector<4xf32>\n    some_synchronization_primitive\n    scf.if %cnd {\n      // Serialized code running only on thread 0.\n      // Load the data from all the threads into a register from thread 0. This\n      // allow threads 0 to access data from all the threads.\n      %arg0 = vector.load %tmp0[%c0] : memref<128xf32>, vector<128xf32>\n      ...\n      // Store the data from thread 0 into memory.\n      vector.store %1, %tmp1[%c0] : memref<32xf32>, vector<32xf32>\n    }\n    // Synchronization and load the data in a block cyclic way so that the\n    // vector is distributed on all threads.\n    some_synchronization_primitive\n    %0 = vector.load %tmp1[%laneid] : memref<32xf32>, vector<32xf32>\n    // Execute in parallel on all threads/lanes.\n    ```",
    "inputs": [
      { "name": "laneid", "type": "Index" },
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "warp_size", "type": "I64Attr" }
    ]
  },
  {
    "name": "gpu.yield",
    "summary": "GPU yield operation",
    "description": "`gpu.yield` is a special terminator operation for blocks inside regions\n    in gpu ops. It returns values to the immediately enclosing gpu op.\n\n    Example:\n\n    ```mlir\n    gpu.yield %f0, %f1 : f32, f32\n    ```",
    "assemblyFormat": "attr-dict ($values^ `:` type($values))?"
  },
  {
    "name": "hal.allocator.allocate",
    "summary": "Empty buffer allocation operation.",
    "description": "Allocates a buffer of the given size from the allocator.\n    The size of the buffer returned may be larger than the requested size if the\n    allocator has specific alignment requirements or minimum allocation sizes.",
    "inputs": [
      { "name": "allocator", "type": "HAL_Allocator" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "memory_types", "type": "HAL_MemoryType" },
      { "name": "buffer_usage", "type": "HAL_BufferUsage" },
      { "name": "result_size", "type": "HAL_DeviceSize" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Buffer" }
    ],
    "assemblyFormat": "`<` $allocator `:` type($allocator) `>`\n    `affinity` `(` $queue_affinity `)`\n    `type` `(` $memory_types `)`\n    `usage` `(` $buffer_usage `)`\n    `:` custom<SizeAwareType>(type($result), $result_size)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.allocator.import",
    "summary": "Allocator-supported host buffer import operation.",
    "description": "Tries importing host memory backed by the given byte buffer into a\n    device accessible `!hal.buffer`. The returned buffer may be host-only and\n    not directly usable on devices. If the mapping cannot be completed (such as\n    trying to map the host memory as device-local on devices with discrete\n    memory) then `did_import` will indicate that the returned buffer is null.",
    "inputs": [
      { "name": "allocator", "type": "HAL_Allocator" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "memory_types", "type": "HAL_MemoryType" },
      { "name": "buffer_usage", "type": "HAL_BufferUsage" },
      { "name": "source", "type": "Util_BufferType" },
      { "name": "offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" }
    ],
    "outputs": [
      { "name": "did_import", "type": "I1" },
      { "name": "result", "type": "HAL_Buffer" }
    ],
    "assemblyFormat": "`<` $allocator `:` type($allocator) `>`\n    `source` `(` $source `:` type($source) `)` `` `[` $offset `,` $length `]`\n    `affinity` `(` $queue_affinity `)`\n    `type` `(` $memory_types `)`\n    `usage` `(` $buffer_usage `)`\n    `:` type($did_import) `,` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.allocator.resolve_memory_properties",
    "summary": "Resolves memory properties from resource lifetime and affinity.",
    "description": "Resolves the required memory types and buffer usage for a resource with\n    the given lifetime and affinity. This operation encapsulates the logic\n    for deriving buffer properties based on stream resource semantics.\n\n    This operation can be resolved later in compilation either by canonicalization\n    for single device affinties or using the topology attribute for multiple devices.\n\n    Example:\n    ```mlir\n    %memory_types, %buffer_usage = hal.allocator.resolve_memory_properties\n        for(#hal.device.affinity<@device_a>)\n        lifetime(transient) : i32, i32\n    ```",
    "outputs": [
      { "name": "memory_types", "type": "HAL_MemoryType" },
      { "name": "buffer_usage", "type": "HAL_BufferUsage" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" },
      { "name": "lifetime", "type": "HAL_LifetimeAttr" }
    ],
    "assemblyFormat": "(`for` `(` $affinity^ `)`)?\n    `lifetime` `(` $lifetime `)`\n    `:` type($memory_types) `,` type($buffer_usage)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.allocator.select",
    "summary": "Selects a device/queue from the given set for allocations.",
    "description": "Chooses one device and queue affinity pair from the given optimal set that\n    can service the requested memory type and usage for all devices in the set.\n    Returns a null device if no pair is able to satisfy the conditions.",
    "inputs": [
      { "name": "devices", "type": "Variadic" },
      { "name": "queue_affinities", "type": "Variadic" },
      { "name": "memory_types", "type": "HAL_MemoryType" },
      { "name": "buffer_usage", "type": "HAL_BufferUsage" }
    ],
    "outputs": [
      { "name": "selected_device", "type": "HAL_Device" },
      { "name": "selected_queue_affinity", "type": "HAL_DeviceQueueAffinity" }
    ],
    "assemblyFormat": "`from` `(` custom<DeviceQueueAffinityList>($devices, type($devices), $queue_affinities) `)`\n    `type` `(` $memory_types `)`\n    `usage` `(` $buffer_usage `)`\n    `:`\n    type($selected_device) `,` type($selected_queue_affinity)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer_usage",
    "summary": "An iree_hal_buffer_usage_t for the given usage bits.",
    "description": "Maps buffer usage bits to a runtime `iree_hal_buffer_usage_t` value.",
    "outputs": [
      { "name": "result", "type": "HAL_BufferUsage" }
    ],
    "attributes": [
      { "name": "usage", "type": "HAL_BufferUsageBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $usage `>`\n    attr-dict\n    `:` type($result)"
  },
  {
    "name": "hal.buffer_view.assert",
    "summary": "Buffer view contents assertion.",
    "description": "Asserts that the buffer view contains a data compatible tensor with the\n    given encoding. Program execution will abort as if `std.assert` had been\n    used.",
    "inputs": [
      { "name": "buffer_view", "type": "HAL_BufferView" },
      { "name": "element_type", "type": "HAL_ElementType" },
      { "name": "encoding_type", "type": "HAL_EncodingType" },
      { "name": "shape", "type": "HAL_Shape" }
    ],
    "attributes": [
      { "name": "message", "type": "StrAttr" }
    ],
    "assemblyFormat": "`<` $buffer_view `:` type($buffer_view) `>`\n    `message` `(` $message `)`\n    `shape` `(` `[` $shape `]` `)`\n    `type` `(` $element_type `)`\n    `encoding` `(` $encoding_type `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer_view.buffer",
    "summary": "Buffer view buffer accessor.",
    "description": "Returns the buffer backing this view's contents.",
    "inputs": [
      { "name": "buffer_view", "type": "HAL_BufferView" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_BufferType" }
    ],
    "assemblyFormat": "`<` $buffer_view `:` type($buffer_view) `>`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer_view.create",
    "summary": "Buffer view reference initializer.",
    "description": "Creates a reference to a buffer with a particular shape and element type.\n    The buffer is not copied and both the original and view references must be\n    synchronized. This makes it easier to associate commonly-carried metadata\n    along with the contents.",
    "inputs": [
      { "name": "source_buffer", "type": "HAL_BufferType" },
      { "name": "source_offset", "type": "HAL_DeviceSize" },
      { "name": "source_length", "type": "HAL_DeviceSize" },
      { "name": "element_type", "type": "HAL_ElementType" },
      { "name": "encoding_type", "type": "HAL_EncodingType" },
      { "name": "shape", "type": "HAL_Shape" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_BufferView" }
    ],
    "assemblyFormat": "`buffer` `(` $source_buffer `:` type($source_buffer) `)`\n    `` `[` $source_offset `,` $source_length `]`\n    `shape` `(` `[` $shape `]` `)`\n    `type` `(` $element_type `)`\n    `encoding` `(` $encoding_type `)`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer_view.dim",
    "summary": "Buffer view dimension value query.",
    "description": "Returns the value of the given dimension.",
    "inputs": [
      { "name": "buffer_view", "type": "HAL_BufferView" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Dim" }
    ],
    "attributes": [
      { "name": "index", "type": "IndexAttr" }
    ],
    "assemblyFormat": "`<` $buffer_view `:` type($buffer_view) `>`\n    `` `[` $index `]`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer_view.element_type",
    "summary": "Buffer view element type query.",
    "description": "Returns the element type of the buffer view.",
    "inputs": [
      { "name": "buffer_view", "type": "HAL_BufferView" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_ElementType" }
    ],
    "assemblyFormat": "`<` $buffer_view `:` type($buffer_view) `>`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer_view.encoding_type",
    "summary": "Buffer view encoding type query.",
    "description": "Returns the encoding type of the buffer view.",
    "inputs": [
      { "name": "buffer_view", "type": "HAL_BufferView" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_EncodingType" }
    ],
    "assemblyFormat": "`<` $buffer_view `:` type($buffer_view) `>`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer_view.rank",
    "summary": "Buffer view rank query.",
    "description": "Returns the rank of the buffer view.",
    "inputs": [
      { "name": "buffer_view", "type": "HAL_BufferView" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Dim" }
    ],
    "assemblyFormat": "`<` $buffer_view `:` type($buffer_view) `>`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer_view.trace",
    "summary": "Trace value(s) operation.",
    "description": "Traces out to a runtime trace sink (console, log file, etc) the given buffer\n    views and titles them with the given key. The key is informational only and\n    useful for titling/marking specific sets of buffers for easier searching.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" }
    ],
    "assemblyFormat": "$key `=`\n    $operands `:` type($operands)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer.allocation.discard",
    "summary": "Discards ownership of the underlying buffer allocation by the caller.",
    "description": "Decrementing the preserve count indicates that the owner is releasing its\n    ownership. When the last owner discards their ownership it is safe to\n    deallocate the buffer allocation even if there are still references\n    remaining to the buffer object.\n\n    Any code that _may_ receive asynchronously allocated buffers must properly\n    balance their preserves and discards. Code that will never receive\n    asynchronously allocated buffers - such as those using the inline HAL - can\n    ignore tracking as there's no asynchronous deallocation and allocation\n    lifetime is tied to buffer object lifetime. Note that unbalanced discards\n    will result in either correctness issues (buffer is deallocated too early)\n    or extended lifetime (buffer cannot be deallocated until all buffer object\n    references have been released).\n\n    Returns true if the caller was the last owner of the allocation and it can\n    now be deallocated.",
    "inputs": [
      { "name": "buffer", "type": "Arg" }
    ],
    "outputs": [
      { "name": "result", "type": "I1" }
    ],
    "assemblyFormat": "`<` $buffer `:` type($buffer) `>`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer.allocation.is_terminal",
    "summary": "Returns true if the underlying buffer allocation has a single owner.",
    "description": "This can be used to reuse a buffer that has no other owners.\n\n    Note that an allocated buffer may have multiple suballocations referencing\n    it and this query is only for the entire allocation. When reusing a buffer\n    one should ensure the allocation size matches (or is within threshold) so\n    that a reuse of 16MB doesn't keep an underlying allocation of 16GB wired.\n\n    Since device allocators are expected to reuse memory if in doubt prefer to\n    dealloca and alloca. This method should only be used in situations where the\n    buffer types are known to the application (such as fixed input and output\n    buffers).",
    "inputs": [
      { "name": "buffer", "type": "Arg" }
    ],
    "outputs": [
      { "name": "result", "type": "I1" }
    ],
    "assemblyFormat": "`<` $buffer `:` type($buffer) `>`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer.allocation.preserve",
    "summary": "Preserves the underlying buffer allocation for the caller.",
    "description": "Preservation is a way to track lifetime of an asynchronously-allocated\n    buffer on multiple device timelines. Incrementing the preserve count\n    indicates that there is a new co-owner of the buffer lifetime and that owner\n    must make a corresponding `hal.buffer.allocation.discard` call to release\n    their ownership and possibly deallocate the buffer.\n\n    Though intended for asynchronously-allocated buffers it is fine to preserve\n    synchronously-allocated ones. Any code that _may_ receive asynchronously\n    allocated buffers must properly balance their preserves and discards. Code\n    that will never receive asynchronously allocated buffers - such as those\n    using the inline HAL - can ignore tracking.\n\n    This preservation roughly translates to retaining logical ownership of the\n    allocation and may differ from the buffer object reference count. As an\n    example if the Python GC hasn't run there may still be several references to\n    the buffer object even after the application has stopped using the buffer.\n    Tracking the preserve count independently allows the application to eagerly\n    deallocate the buffer without relying on the lifetime of the object to do\n    so.\n\n    A preserved buffer will still be deallocated if there are no longer any\n    references to the buffer object. Preserving the buffer only prevents any\n    other owner from deallocating it while there are references outstanding.\n    See `hal.buffer.allocation.discard` for more information about releasing\n    ownership.",
    "inputs": [
      { "name": "buffer", "type": "Arg" }
    ],
    "assemblyFormat": "`<` $buffer `:` type($buffer) `>`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer.assert",
    "summary": "Buffer compatibility assertion.",
    "description": "Asserts that the buffer is compatible with the given allocator and usage.\n    Program execution will abort as if `std.assert` had been used.\n\n    This only checks that the buffer can be used and not that it matches the\n    given parameters exactly. Buffers may be from other allocators so long as\n    the allocators are compatible (devices can address each other's memory),\n    the type and usage contain all the requested bits (having more bits is ok),\n    and the length is at least the requested minimum (as padding may be\n    ignored).",
    "inputs": [
      { "name": "buffer", "type": "HAL_Buffer" },
      { "name": "allocator", "type": "HAL_Allocator" },
      { "name": "minimum_length", "type": "HAL_DeviceSize" }
    ],
    "attributes": [
      { "name": "message", "type": "StrAttr" },
      { "name": "memory_types", "type": "HAL_MemoryTypeBitfieldAttr" },
      { "name": "buffer_usage", "type": "HAL_BufferUsageBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $buffer `:` type($buffer) `>`\n    `message` `(` $message `)`\n    `allocator` `(` $allocator `:` type($allocator) `)`\n    `minimum_length` `(` $minimum_length `)`\n    `type` `(` $memory_types `)`\n    `usage` `(` $buffer_usage `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer.length",
    "summary": "Buffer byte length accessor.",
    "description": "Returns the allocated size of a buffer in bytes.\n    May be less than the underlying buffer allocation if this is a subspan or\n    view into another buffer.",
    "inputs": [
      { "name": "buffer", "type": "HAL_BufferType" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_DeviceSize" }
    ],
    "assemblyFormat": "`<` $buffer `:` type($buffer) `>`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer.load",
    "summary": "Buffer element load operation.",
    "description": "Loads a value from a buffer by mapping it.",
    "inputs": [
      { "name": "source_buffer", "type": "Arg" },
      { "name": "source_offset", "type": "HAL_DeviceSize" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "`<` $source_buffer `:` type($source_buffer) `>`\n    `` `[` $source_offset `]`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer.store",
    "summary": "Buffer element store operation.",
    "description": "Stores a value into a buffer by mapping it.",
    "inputs": [
      { "name": "value", "type": "AnyTypeOf" },
      { "name": "target_buffer", "type": "Arg" },
      { "name": "target_offset", "type": "HAL_DeviceSize" }
    ],
    "assemblyFormat": "`<` $target_buffer `:` type($target_buffer) `>`\n    `` `[` $target_offset `]`\n    `value` `(` $value `:` type($value) `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.buffer.subspan",
    "summary": "Buffer subspan operation.",
    "description": "Returns a reference to a subspan of the buffer.",
    "inputs": [
      { "name": "source_buffer", "type": "HAL_BufferType" },
      { "name": "source_offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_BufferType" }
    ],
    "assemblyFormat": "`<` $source_buffer `:` type($source_buffer) `>`\n    `` `[` $source_offset `,` $length `]`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.channel.create",
    "summary": "Creates a new channel for collective communication.",
    "description": "Returns a new channel with the given rank associated with the given device\n    queue. Collective operations using this channel must only be submitted on\n    compatible queues.\n\n    The group and ID are optional and may be null. A rank or count of -1 can be\n    used to indicate a default inherited from the environment or device\n    configuration.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "id", "type": "Util_BufferType" },
      { "name": "group", "type": "Util_BufferType" },
      { "name": "rank", "type": "I32" },
      { "name": "count", "type": "I32" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Channel" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_ChannelFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`device` `(` $device `:` type($device) `)`\n    `affinity` `(` $queue_affinity `)`\n    `flags` `(` $flags `)`\n    `id` `(` $id `)`\n    `group` `(` $group `)`\n    `rank` `(` $rank `)`\n    `count` `(` $count `)`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.channel.rank_and_count",
    "summary": "Returns the rank of the local participant in the group.",
    "description": "Returns the rank the channel represents as a participant in a collective\n    group in `[0, count)` and the total participant count.",
    "inputs": [
      { "name": "channel", "type": "HAL_Channel" }
    ],
    "outputs": [
      { "name": "rank", "type": "I32" },
      { "name": "count", "type": "I32" }
    ],
    "assemblyFormat": "`<` $channel `:` type($channel) `>`\n    `:` type($rank) `,` type($count)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.channel.split",
    "summary": "Splits a collective communication channel.",
    "description": "Partitions the group associated with the given channel into disjoint\n    subgroups for each unique value of color. Each new subgroup contains all\n    participants of the same color and within each subgroup the key argument\n    is used to define the rank order. When multiple participants in a group\n    use the same key the tie will be broken using their rank in the parent\n    group. A color of -1 indicates that the rank does not participate in any\n    subgroup and will return a null channel.",
    "inputs": [
      { "name": "channel", "type": "HAL_Channel" },
      { "name": "color", "type": "I32" },
      { "name": "key", "type": "I32" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Channel" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_ChannelFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $channel `:` type($channel) `>`\n    `color` `(` $color `)`\n    `key` `(` $key `)`\n    `flags` `(` $flags `)`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.begin_debug_group",
    "summary": "Pushes a command buffer debug group label.",
    "description": "Pushes a new debug group with the given label.\n    All commands between this and a mandatory matching call to\n    `hal.command_buffer.end_debug_group` will be grouped together with the\n    given label.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" }
    ],
    "attributes": [
      { "name": "label", "type": "StrAttr" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `label` `(` $label `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.collective",
    "summary": "Command buffer collective dispatch recording operation.",
    "description": "Dispatches a collective operation defined by op using the given buffers.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" },
      { "name": "channel", "type": "HAL_Channel" },
      { "name": "element_count", "type": "HAL_DeviceSize" },
      { "name": "param", "type": "Optional" },
      { "name": "send_buffer", "type": "Optional" },
      { "name": "send_offset", "type": "Optional" },
      { "name": "send_length", "type": "Optional" },
      { "name": "recv_buffer", "type": "Optional" },
      { "name": "recv_offset", "type": "Optional" },
      { "name": "recv_length", "type": "Optional" }
    ],
    "attributes": [
      { "name": "op", "type": "HAL_CollectiveAttr" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `channel` `(` $channel `:` type($channel) `)`\n    `op` `(` $op `)`\n    (`param` `(` $param^ `:` type($param) `)`)?\n    (`send` `(` $send_buffer^ `:` type($send_buffer) `)`\n     `` `[` $send_offset `,` $send_length `]`)?\n    (`recv` `(` $recv_buffer^ `:` type($recv_buffer) `)`\n     `` `[` $recv_offset `,` $recv_length `]`)?\n    `count` `(` $element_count `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.copy_buffer",
    "summary": "Command buffer buffer copy recording operation.",
    "description": "Copies a range of one buffer to another.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" },
      { "name": "source_buffer", "type": "AnyTypeOf" },
      { "name": "source_offset", "type": "HAL_DeviceSize" },
      { "name": "target_buffer", "type": "AnyTypeOf" },
      { "name": "target_offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_CopyFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `source` `(` $source_buffer `:` type($source_buffer) `)`\n    `` `[` $source_offset `]`\n    `target` `(` $target_buffer `:` type($target_buffer) `)`\n    `` `[` $target_offset `]`\n    `length` `(` $length `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.create",
    "summary": "Command buffer allocation operation.",
    "description": "Returns a command buffer from the device pool ready to begin recording.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "binding_capacity", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_CommandBuffer" }
    ],
    "attributes": [
      { "name": "modes", "type": "HAL_CommandBufferModeBitfieldAttr" },
      { "name": "command_categories", "type": "HAL_CommandCategoryBitfieldAttr" }
    ],
    "assemblyFormat": "`device` `(` $device `:` type($device) `)`\n    `mode` `(` $modes `)`\n    `categories` `(` $command_categories `)`\n    `affinity` `(` $queue_affinity `)`\n    (`bindings` `(` $binding_capacity^ `)`)?\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.device",
    "summary": "Command buffer device query operation.",
    "description": "Used during conversion to access the device used to create a command buffer.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" }
    ],
    "outputs": [
      { "name": "device", "type": "HAL_Device" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `:` type($device)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.dispatch",
    "summary": "Command buffer dispatch recording operation.",
    "description": "Dispatches an execution request.\n    The request may execute overlapped with any other transfer operation or\n    dispatch made within the same barrier-defined sequence.\n\n    The provided constant data and binding list will be recorded into the\n    command buffer and need not remain live beyond the call. Push constants are\n    always 4-byte values and treated as opaque, meaning that they may be\n    bit-casted floats, bit-packed booleans, etc. The provided buffers may either\n    be HAL buffers or indirect references into the command buffer binding table.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" },
      { "name": "executable", "type": "HAL_Executable" },
      { "name": "entry_point", "type": "HAL_Ordinal" },
      { "name": "workgroup_x", "type": "HAL_Dim" },
      { "name": "workgroup_y", "type": "HAL_Dim" },
      { "name": "workgroup_z", "type": "HAL_Dim" },
      { "name": "constants", "type": "Variadic" },
      { "name": "binding_buffers", "type": "Variadic" },
      { "name": "binding_offsets", "type": "Variadic" },
      { "name": "binding_lengths", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_DispatchFlagsAttr" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `target` `(` $executable `:` type($executable) `)`\n    `` `[` $entry_point `]`\n    `workgroups` `(` `[`\n        $workgroup_x `,`\n        $workgroup_y `,`\n        $workgroup_z\n    `]` `)`\n    (`constants` `(` `[` $constants^ `]` `)`)?\n    `bindings` `(` `[`\n    custom<Bindings>($binding_buffers,\n                     type($binding_buffers),\n                     $binding_offsets,\n                     $binding_lengths)\n    `]` `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.dispatch.indirect",
    "summary": "Command buffer indirect dispatch recording operation.",
    "description": "Dispatches an execution request with a deferred workgroup count.\n    This is the same as iree_hal_command_buffer_dispatch but the workgroup count\n    is read from the given |workgroups_ref| buffer at the specified offset as\n    3 uint32_t XYZ values immediately before performing the dispatch. This\n    allows prior dispatches within the command sequence to populate the\n    workgroup count or the workgroup count to change across submissions of the\n    same reusable command buffer.\n\n    The provided constant data and binding list will be recorded into the\n    command buffer and need not remain live beyond the call. Push constants are\n    always 4-byte values and treated as opaque, meaning that they may be\n    bit-casted floats, bit-packed booleans, etc. The provided buffers may either\n    be HAL buffers or indirect references into the command buffer binding table.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" },
      { "name": "executable", "type": "HAL_Executable" },
      { "name": "entry_point", "type": "HAL_Ordinal" },
      { "name": "workgroups_buffer", "type": "AnyTypeOf" },
      { "name": "workgroups_offset", "type": "HAL_DeviceSize" },
      { "name": "constants", "type": "Variadic" },
      { "name": "binding_buffers", "type": "Variadic" },
      { "name": "binding_offsets", "type": "Variadic" },
      { "name": "binding_lengths", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_DispatchFlagsAttr" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `target` `(` $executable `:` type($executable) `)`\n    `` `[` $entry_point `]`\n    `workgroups` `(` $workgroups_buffer `:` type($workgroups_buffer) `)`\n    `` `[` $workgroups_offset `]`\n    (`constants` `(` `[` $constants^ `]` `)`)?\n    `bindings` `(` `[`\n    custom<Bindings>($binding_buffers,\n                     type($binding_buffers),\n                     $binding_offsets,\n                     $binding_lengths)\n    `]` `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.end_debug_group",
    "summary": "Pops a command buffer debug group label.",
    "description": "Pops a debug group from the stack.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.execution_barrier",
    "summary": "Command buffer execution barrier recording operation.",
    "description": "Defines an execution dependency between all commands recorded before the\n    barrier and all commands recorded after the barrier. Only the stages\n    provided will be affected.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" }
    ],
    "attributes": [
      { "name": "source_stage_mask", "type": "HAL_ExecutionStageBitfieldAttr" },
      { "name": "target_stage_mask", "type": "HAL_ExecutionStageBitfieldAttr" },
      { "name": "flags", "type": "HAL_ExecutionBarrierFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `source` `(` $source_stage_mask `)`\n    `target` `(` $target_stage_mask `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.fill_buffer",
    "summary": "Command buffer buffer fill recording operation.",
    "description": "Fills the target buffer with the given repeating value.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" },
      { "name": "target_buffer", "type": "AnyTypeOf" },
      { "name": "target_offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" },
      { "name": "pattern", "type": "HAL_FillPatternType" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_FillFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `target` `(` $target_buffer `:` type($target_buffer) `)`\n    `` `[` $target_offset `,` $length `]`\n    `pattern` `(` $pattern `:` type($pattern) `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.finalize",
    "summary": "Finalizes command buffer recording.",
    "description": "Ends recording into the command buffer and prepares it for submission.\n    No more commands may be recorded into the command buffer.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.command_buffer.update_buffer",
    "summary": "Command buffer buffer update recording operation.",
    "description": "Copies a range of a host buffer into a device buffer. The host buffer\n    contents will be captured at the time of the call and embedded in the\n    command buffer.",
    "inputs": [
      { "name": "command_buffer", "type": "HAL_CommandBuffer" },
      { "name": "source_buffer", "type": "Util_BufferType" },
      { "name": "source_size", "type": "Util_Size" },
      { "name": "source_offset", "type": "Util_Size" },
      { "name": "target_buffer", "type": "AnyTypeOf" },
      { "name": "target_offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_UpdateFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $command_buffer `:` type($command_buffer) `>`\n    `source` `(` $source_buffer `:` type($source_buffer) `{` $source_size `}` `)`\n    `` `[` $source_offset `]`\n    `target` `(` $target_buffer `:` type($target_buffer) `)`\n    `` `[` $target_offset `]`\n    `length` `(` $length `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.allocator",
    "summary": "Device allocator accessor operation.",
    "description": "Returns the allocator that can be used to allocate buffers compatible with\n    the device.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Allocator" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>` `:` type($result) attr-dict-with-keyword"
  },
  {
    "name": "hal.device.memoize",
    "summary": "Memoizes resources for a particular device and queue affinity.",
    "description": "Executes the nested region once per device and affinity mask and memoizes\n    the results such that future references return the previously memoized\n    values. The initial execution may happen on demand or be hoisted to module\n    initialization time.\n\n    Any uses of the device or affinity specified within the nested region will\n    be substituted with the appropriate device and affinity during memoization.\n    All other implicitly captured values must be either constant or global\n    values available at the time the memoization occurs.\n\n    It is valid for the nested region contents to be inlined in place and never\n    memoized. This can be useful when diagnosing memoization issues and can be\n    forced with the `--iree-hal-memoization=false` flag.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `->` type($results)\n    attr-dict-with-keyword\n    $body"
  },
  {
    "name": "hal.device.query",
    "summary": "Returns a runtime configuration parameter from the device.",
    "description": "Queries a device configuration parameter with the given key.\n    Returns a status indicating whether the pair was recognized/available and if\n    it was the value converted to the specified type. Queries must return the\n    same value for the lifetime of the module though may vary from run to run.\n\n    This is roughly equivalent to the `sysconf` linux syscall\n    (https://man7.org/linux/man-pages/man3/sysconf.3.html) in that the exact\n    set of keys available and their interpretation is target-dependent.\n\n    Users of the op must check the `ok` result before using the value as what\n    set of keys is available may change over time. If in doubt: don't use this.\n    Each key used adds additional versioning and testing complexity as runtime\n    code path changes will explode combinatorially and should be treated with as\n    much care as a binary file format change. Keys should be prefixed with `ex.`\n    when experimental indicating that they are not expected to be present\n    forever; all non-experimental keys should be vetted.\n\n    Well-known keys:\n\n    * hal.device.id :: {some id pattern}\n      Returns 1 if the device identifier matches the given pattern string.\n\n    * hal.executable.format :: {some format pattern}\n      Returns 1 if the given format is supported by the device loader.\n\n    * hal.device :: concurrency\n      The maximum concurrently executable submissions, mapping roughly to the\n      queue count. The actual concurrency available may be less than this based\n      on dynamic runtime parameters such as power/thermal modes, quota limits,\n      or user choice.\n\n    * hal.dispatch :: concurrency\n      The maximum concurrently executable workgroups for a particular dispatch.\n      The actual concurrency available may be less depending on device state.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" }
    ],
    "outputs": [
      { "name": "ok", "type": "I1" },
      { "name": "value", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "category", "type": "StrAttr" },
      { "name": "key", "type": "StrAttr" },
      { "name": "default_value", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `key` `(` $category `:` `` `:` $key `)`\n    `:` type($ok) `,` type($value)\n    (`=` $default_value^)?\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.alloca",
    "summary": "Allocates a queue-ordered transient buffer.",
    "description": "Returns a queue-ordered transient buffer that will be available for use when\n    the signal fence is reached. The allocation will not be made until the\n    wait fence has been reached.\n\n    The size of the buffer returned may be larger than the requested size if the\n    allocator has specific alignment requirements or minimum allocation sizes.\n\n    The buffer handle will remain live so long as there are retainers but the\n    contents are undefined before the allocation signal fence has been signaled\n    and after the deallocation wait fence has been reached.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "pool", "type": "HAL_DeviceQueuePool" },
      { "name": "memory_types", "type": "HAL_MemoryType" },
      { "name": "buffer_usage", "type": "HAL_BufferUsage" },
      { "name": "result_size", "type": "HAL_DeviceSize" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Buffer" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_AllocaFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `pool` `(` $pool `)`\n    `type` `(` $memory_types `)`\n    `usage` `(` $buffer_usage `)`\n    `flags` `(` $flags `)`\n    `:` custom<SizeAwareType>(type($result), $result_size)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.barrier",
    "summary": "Enqueues an execution barrier.",
    "description": "Signals the provided fence once the wait fence is reached.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_ExecuteFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.copy",
    "summary": "Copies one device-visible buffer to another.",
    "description": "The source buffer and target buffer must both be visible to the device\n    queue performing the copy. In most cases the queue affinity should be set to\n    where the target buffer will be consumed so that it has a chance of being\n    cached. The source buffer must have transfer-source usage and the target\n    buffer must have transfer-target usage.\n\n    Note that individual queue transfer operations have a high overhead and they\n    should be batched with other operations in command buffers.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "source_buffer", "type": "HAL_Buffer" },
      { "name": "source_offset", "type": "HAL_DeviceSize" },
      { "name": "target_buffer", "type": "HAL_Buffer" },
      { "name": "target_offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_CopyFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `source` `(` $source_buffer `:` type($source_buffer) `)`\n    `` `[` $source_offset `]`\n    `target` `(` $target_buffer `:` type($target_buffer) `)`\n    `` `[` $target_offset `]`\n    `length` `(` $length `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.dealloca",
    "summary": "Deallocates a queue-ordered transient buffer.",
    "description": "Deallocates a queue-ordered transient buffer.\n    The deallocation will not be made until the wait fence has been reached and\n    once the storage is available for reuse the signal fence will be signaled.\n\n    After deallocation the contents of the buffer may still be accessible but\n    will have undefined contents as other operations reuse the memory.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "buffer", "type": "HAL_Buffer" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_DeallocaFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `buffer` `(` $buffer `:` type($buffer) `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.execute",
    "summary": "Enqueues command buffer execution.",
    "description": "Executes a command buffer on a device queue.\n    No commands will execute until the wait fence has been reached and the\n    signal fence will be signaled when all commands have completed.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "command_buffer", "type": "HAL_CommandBuffer" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_ExecuteFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `commands` `(` $command_buffer `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.execute.indirect",
    "summary": "Enqueues command buffer execution.",
    "description": "Executes a command buffer on a device queue with the given binding table.\n    No commands will execute until the wait fence has been reached and the\n    signal fence will be signaled when all commands have completed.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "command_buffer", "type": "HAL_CommandBuffer" },
      { "name": "binding_buffers", "type": "Variadic" },
      { "name": "binding_offsets", "type": "Variadic" },
      { "name": "binding_lengths", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_ExecuteFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `commands` `(` $command_buffer `)`\n    `bindings` `(` `[`\n    custom<BindingTable>($binding_buffers,\n                         type($binding_buffers),\n                         $binding_offsets,\n                         $binding_lengths)\n    `]` `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.fill",
    "summary": "Fills a buffer with a repeating pattern.",
    "description": "The target buffer must be visible to the device queue performing the update.\n    In most cases the queue affinity should be set to where the target buffer\n    will be consumed so that it has a chance of being cached.\n\n    Note that individual queue transfer operations have a high overhead and they\n    should be batched with other operations in command buffers.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "target_buffer", "type": "HAL_Buffer" },
      { "name": "target_offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" },
      { "name": "pattern", "type": "HAL_FillPatternType" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_FillFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `target` `(` $target_buffer `:` type($target_buffer) `)`\n    `` `[` $target_offset `]`\n    `length` `(` $length `)`\n    `pattern` `(` $pattern `:` type($pattern) `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.flush",
    "summary": "Flushes locally-pending submissions to the queue.",
    "description": "Flushes any locally-pending submissions in the queue.\n    When submitting many queue operations this can be used to eagerly flush\n    earlier submissions while later ones are still being constructed.\n    This may be a no-op.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.read",
    "summary": "Reads a segment from a file into a device buffer.",
    "description": "Enqueues a file read operation that streams a segment of the source file\n    defined by the source offset and length into the target HAL buffer at the\n    specified target offset. The queue affinity should be set to where the\n    target buffer will be consumed. The source file must have read permission\n    and the target buffer must have transfer-target usage. Read failure will\n    result in propagated semaphore failure or device loss.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "source_file", "type": "HAL_File" },
      { "name": "source_offset", "type": "I64" },
      { "name": "target_buffer", "type": "HAL_Buffer" },
      { "name": "target_offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_ReadFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `source` `(` $source_file `:` type($source_file) `)`\n    `` `[` $source_offset `]`\n    `target` `(` $target_buffer `:` type($target_buffer) `)`\n    `` `[` $target_offset `]`\n    `length` `(` $length `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.update",
    "summary": "Updates a buffer with the contents of a host buffer.",
    "description": "The provided host source buffer will be captured and need not remain live or\n    unchanged while the operation is queued. The target buffer must be visible\n    to the device queue performing the update. In most cases the queue affinity\n    should be set to where the target buffer will be consumed so that it has a\n    chance of being cached.\n\n    Some implementations may have limits on the size of the update or may\n    perform poorly if the size is larger than an implementation-defined limit.\n    Updates should be kept as small and infrequent as possible.\n\n    Note that individual queue transfer operations have a high overhead and they\n    should be batched with other operations in command buffers.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "source_buffer", "type": "Util_BufferType" },
      { "name": "source_offset", "type": "HAL_DeviceSize" },
      { "name": "target_buffer", "type": "HAL_Buffer" },
      { "name": "target_offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_UpdateFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `source` `(` $source_buffer `:` type($source_buffer) `)`\n    `` `[` $source_offset `]`\n    `target` `(` $target_buffer `:` type($target_buffer) `)`\n    `` `[` $target_offset `]`\n    `length` `(` $length `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.queue.write",
    "summary": "Writes a segment from a device buffer into a file.",
    "description": "Enqueues a file write operation that streams a segment of the source HAL\n    buffer defined by the source offset and length into the target file at the\n    specified target offset. The queue affinity should be set to where the\n    source buffer was produced. The source buffer must have transfer-source\n    usage and the target file must have write permission. Write failure will\n    result in propagated semaphore failure or device loss.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "wait_fence", "type": "HAL_Fence" },
      { "name": "signal_fence", "type": "HAL_Fence" },
      { "name": "source_buffer", "type": "HAL_Buffer" },
      { "name": "source_offset", "type": "HAL_DeviceSize" },
      { "name": "target_file", "type": "HAL_File" },
      { "name": "target_offset", "type": "I64" },
      { "name": "length", "type": "HAL_DeviceSize" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_WriteFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $device `:` type($device) `>`\n    `affinity` `(` $queue_affinity `)`\n    `wait` `(` $wait_fence `)`\n    `signal` `(` $signal_fence `)`\n    `source` `(` $source_buffer `:` type($source_buffer) `)`\n    `` `[` $source_offset `]`\n    `target` `(` $target_file `:` type($target_file) `)`\n    `` `[` $target_offset `]`\n    `length` `(` $length `)`\n    `flags` `(` $flags `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.device.resolve",
    "summary": "Resolves device handles based on affinity.",
    "description": "Examples:\n    ```\n    // Returns a HAL device.\n    = hal.device.resolve on(#something) : !hal.device\n    // Returns a HAL device, allocator, and (optional) queue affinity.\n    = hal.device.resolve on(#something) : !hal.device, !hal.allocator, i64\n    // Returns a HAL allocator and (optional) queue affinity.\n    = hal.device.resolve on(#something) : !hal.allocator, i64\n    // Returns \"any\" device. Should only be used as a fallback.\n    = hal.device.resolve : !hal.device\n    ```",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` qualified($affinity)^ `)`)?\n    attr-dict `:` type($results)"
  },
  {
    "name": "hal.devices.count",
    "summary": "Returns the number of available devices.",
    "description": "Returns the total number of available devices registered at runtime.",
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "attr-dict `:` type($result)"
  },
  {
    "name": "hal.devices.get",
    "summary": "Returns the device with the given index.",
    "description": "Returns the device with the given index in the [0, hal.devices.count) range.\n    Devices may be lazily initialized upon first use.",
    "inputs": [
      { "name": "index", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Device" }
    ],
    "assemblyFormat": "$index attr-dict `:` type($result)"
  },
  {
    "name": "hal.dispatch.extern",
    "summary": "A dispatch of workgroups across a 3-dimensional grid.",
    "description": "Dispatches some number of workgroups across a 3-dimensional grid using a\n    function defined externally in one or more referenced objects. Objects are\n    declared per executable target and selected automatically during linking\n    based on where the dispatch is used. Semantically this is equivalent to\n    a `flow.dispatch.workgroups` but with the workgroup region invisible to the\n    compiler. See `hal.executable` for more information about object linkage.\n\n    Note that since this happens at tensor level the dispatch operation has\n    value semantics: some tensors (and optionally other primitive types) are\n    consumed and one or more new result tensors are produced. Inside each\n    workgroup, however, the input and output tensors are available for arbitrary\n    loads and stores. In many cases each workgroup will load some particular\n    tile(s) from the input tensors and store some particular tile(s) to the\n    output tensors unique to that workgroup. Though it's possible for multiple\n    workgroups to load the same regions of the input tensors behavior is\n    undefined if multiple workgroups store to the same regions of the output\n    tensors. Codegen guarantees this behavior but when sourcing externally\n    authored dispatch functions it's critical that this behavior is observed.\n\n    Though the representation is similar to the GPU-style grid dispatch model\n    here we still have not yet allocated buffers, determined the target device\n    for execution, or even completed fully resolving shapes/types/etc. Because\n    of this it's important that the workgroup body use the platform-dependent\n    primitives for accessing workgroup ID, size, and count intrinsics instead\n    of hardcoding them to a particular set of values. Assume that any workgroup\n    dispatch may end up being specialized for several different target devices\n    and even several different variants for a particular target device\n    (differing workgroup sizes, etc). To aid deduplication code producing these\n    external dispatches should try not to specialize early for particular shapes\n    and instead emit the most generic code possible as having 500 slightly\n    different `hal.dispatch.extern` ops pointing at the same object file is\n    likely to require 500 copies of the object instead of 500 calls to the same\n    object.\n\n    Because at this point in the layering devices have not yet been selected the\n    workgroup count cannot be fully evaluated. Instead workload parameters are\n    captured that are then passed to a function that when later evaluated\n    computes the actual workgroup count based on target information. The\n    workload is not limited to the 3D XYZ grid dispatch of the workgroup count\n    and can contain any number of parameters used to compute it. If workgroup\n    size or distribution varies based on the target device a `!hal.device`\n    argument can be used by the workgroup count calculation region to factor in\n    device parameters. See `hal.device.query` for more information on how to\n    query information.\n\n    ```mlir\n    %r = hal.dispatch.extern \"some_function\"[%c5, %c5](%0, %1)\n        : (tensor<5x5xf32>, tensor<5xf32>) -> tensor<5x5xf32>\n      ...\n    ```\n\n    The number of results of the operation is equal to the number of results\n    in the type signature (`(tensor<5x5xf32>, tensor<5xf32>) -> tensor<5x5xf32>`).\n    Each tensor argument and result in the type signature has a corresponding\n    pipeline layout slot and must be declared. If multiple arguments or results\n    share the same layout slot they can be aliased using the `bindings`\n    attribute and otherwise each is assumed unique.\n\n    There are no `arguments` operands for results, but a result can be tied an\n    argument by writing the argument operand's SSA value instead of its type:\n    E.g., in the above example, `-> %0` would tie the first argument to the\n    result. In that case, there would be no separate block argument for the\n    result.\n\n    Objects for multiple targets can be specified and the ones used are selected\n    based on their target and an optional condition region that returns true if\n    the variant is valid for use on the provided runtime `!hal.device`. If no\n    variants within an executable are valid then loading will fail at runtime.\n    If multiple variants are valid the first valid one found will be loaded and\n    used for execution.",
    "inputs": [
      { "name": "workload", "type": "Variadic" },
      { "name": "arguments", "type": "Variadic" },
      { "name": "argument_dims", "type": "HAL_ShapeDynamicDims" },
      { "name": "result_dims", "type": "HAL_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "export_name", "type": "StrAttr" },
      { "name": "layout", "type": "HAL_PipelineLayoutAttr" },
      { "name": "targets", "type": "ArrayAttr" },
      { "name": "target_ordinals", "type": "HAL_OrdinalArrayAttr" },
      { "name": "target_objects", "type": "ArrayAttr" },
      { "name": "workgroup_size", "type": "OptionalAttr" },
      { "name": "subgroup_size", "type": "OptionalAttr" },
      { "name": "workgroup_local_memory", "type": "OptionalAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$export_name\n    (`[` $workload^ `]`)? ``\n    `(` $arguments `)` `:`\n    custom<ShapedFunctionType>(ref($arguments),\n                               type($arguments), $argument_dims,\n                               type($results), $result_dims,\n                               $tied_operands)\n    `count` `` custom<WorkgroupCountRegion>($workgroup_count)\n    `layout` `(` $layout `)`\n    `objects` `(` `{` custom<TargetConditionObjects>($targets,\n                                                     $target_ordinals,\n                                                     $target_objects,\n                                                     $target_regions) `}` `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.element_type",
    "summary": "An iree_hal_element_type_t for the given MLIR type.",
    "description": "Maps an MLIR type to a runtime `iree_hal_element_type_t` value for all types\n    that are convertable.",
    "outputs": [
      { "name": "result", "type": "HAL_ElementType" }
    ],
    "attributes": [
      { "name": "type", "type": "TypeAttr" }
    ],
    "assemblyFormat": "`<` $type `>`\n    attr-dict\n    `:` type($result)"
  },
  {
    "name": "hal.encoding_type",
    "summary": "An iree_hal_encoding_type_t for the given MLIR encoding.",
    "description": "Maps an MLIR encoding to a runtime `iree_hal_encoding_type_t` value for all\n    encodings that are convertable.",
    "outputs": [
      { "name": "result", "type": "HAL_EncodingType" }
    ],
    "attributes": [
      { "name": "encoding", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`<` ($encoding^):( `` `dense_row_major`)? `>`\n    attr-dict\n    `:` type($result)"
  },
  {
    "name": "hal.ex.file.from_memory",
    "summary": "Creates a file mapped into a byte range of a host buffer.",
    "description": "Returns a file handle that is backed by the given `buffer` contents.\n    Behavior is undefined if the buffer contents change while the accesses are\n    in-flight.\n\n    Experimental as the exact interface for getting files from module contents\n    still needs iteration. Most hardware APIs require a file descriptor or\n    native platform handle but here we only have host pointers. When\n    memory-mapped some systems allow for retrieval of the platform handle from\n    a virtual address (GetMappedFileNameA/posix_mem_offset) but the APIs are\n    sketchy and likely slow. Instead we should probably have a way to query for\n    a file handle derived from the calling module by stack-walking and asking\n    the VM module for its handle. Until we can figure this out this method will\n    be marked epxerimental.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "buffer", "type": "Util_BufferType" },
      { "name": "offset", "type": "HAL_DeviceSize" },
      { "name": "length", "type": "HAL_DeviceSize" },
      { "name": "flags", "type": "I32" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_File" }
    ],
    "attributes": [
      { "name": "access", "type": "HAL_MemoryAccessBitfieldAttr" }
    ],
    "assemblyFormat": "`device` `(` $device `:` type($device) `)`\n    `affinity` `(` $queue_affinity `)`\n    `access` `(` $access `)`\n    `buffer` `(` $buffer `:` type($buffer) `)`\n    `` `[` $offset `for` $length `]`\n    `flags` `(` $flags `)`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.ex.shared_device",
    "summary": "Returns the shared device instance.",
    "description": "Deprecated operation that returns the shared device instance.",
    "outputs": [
      { "name": "result", "type": "HAL_Device" }
    ],
    "assemblyFormat": "attr-dict `:` type($result)"
  },
  {
    "name": "hal.ex.submit_and_wait",
    "summary": "Submits a command buffer and waits for completion.",
    "description": "Deprecated operation that submits a command buffer to the device and waits for its completion.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "command_buffer", "type": "HAL_CommandBuffer" }
    ],
    "assemblyFormat": "$device `,` $command_buffer attr-dict"
  },
  {
    "name": "hal.executable",
    "summary": "Target-specific executable module.",
    "description": "An executable module representing a target-specific compiled\n    kernel/shader/etc. Executables are treated as independent compilation units\n    and may contain multiple exported entry points that are able to share code\n    internally. To support multi-targeting each executable may have one or more\n    target-specific variants that are lowered independently during compilation\n    while still appearing as one executable at runtime (ala fat binaries).\n\n    At runtime executables are loaded during module initialization and cached\n    for the lifetime of the module. If the `lazy` attribute is set the\n    executable _may_ have its loading deferred until first use.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "lazy", "type": "UnitAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    (`lazy` $lazy^)?\n    $sym_name\n    attr-dict-with-keyword\n    regions"
  },
  {
    "name": "hal.executable_end",
    "summary": "Terminator pseudo-op for the executable op.",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "hal.executable.binary",
    "summary": "Compiled executable binary data.",
    "description": "A compiled executable binary with an optional nested module containing the\n    IR prior to serialization (for debugging).",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "format", "type": "StrAttr" },
      { "name": "data", "type": "Util_AnySerializableAttr" },
      { "name": "mime_type", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    $sym_name\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.executable.calculate_workgroups",
    "summary": "Calculates workgroup count from workload for an exported function.",
    "description": "Calculates the workgroup count (grid XYZ) based on the given workload using\n    the workgroup count calculation region of the target\n    `hal.executable.export` op.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "workload", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "workgroup_x", "type": "HAL_Dim" },
      { "name": "workgroup_y", "type": "HAL_Dim" },
      { "name": "workgroup_z", "type": "HAL_Dim" }
    ],
    "attributes": [
      { "name": "entry_point", "type": "SymbolRefAttr" }
    ],
    "assemblyFormat": "`device` `(` $device `:` type($device) `)`\n    `target` `(` $entry_point `)`\n    (`workload` `(` `[` $workload^ `]` `)`)?\n    `:` type($workgroup_x) `,` type($workgroup_y) `,` type($workgroup_z)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.executable.condition",
    "summary": "Host code to determine if the executable is enabled.",
    "description": "Variants are selected based on their target and this optional condition\n    op that returns true if the variant is valid for use on the provided\n    runtime `!hal.device`. If no variants within an executable are valid then\n    loading will fail at runtime. If multiple variants are valid the first valid\n    one found will be loaded and used for execution.",
    "attributes": [
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "hal.executable.constant.block",
    "summary": "Executable constant block initializer.",
    "description": "Initializes one or more constants in the executable constant block by\n    returning one value per identified constant. Each constant block is\n    evaluated on the host prior to instantiating the executable for a given\n    device and allows for the executable to be specialized based on device\n    capabilities and limits.\n\n    The keys specified are unique per variant and will be deduplicated across\n    multiple constant blocks when present. They are only used during lowering\n    and will not survive to runtime so they need only have descriptive enough\n    names to avoid collisions and represent the semantics of the value.\n\n    Constant values can be loaded in the device code with the\n    `hal.executable.constant.load` op:\n\n    ```mlir\n    hal.executable.variant public @target {\n      hal.executable.constant.block(%device: !hal.device) -> (i32, i32) as (\"foo\", \"bar\") {\n        %0 = hal.device.query<%device> key(\"some.device.prop\")...\n        %1 = hal.device.query<%device> key(\"another.device.prop\")...\n        hal.return %0, %1 : i32, i32\n      }\n      builtin.module {\n        func @dispatch0() {\n          %0 = hal.executable.constant.load \"foo\" : i32\n          %1 = hal.executable.constant.load \"bar\" : i32\n          return\n        }\n      }\n    }\n    ```\n\n    Each target backend will implement the constant initialization and access in\n    a way compatible with its execution model. Examples:\n    - CPU: read-only buffer initialized on load and passed to each dispatch\n    - CUDA: read-only buffer initialized on load and passed to each dispatch\n    - SPIR-V: specialization constants\n    - Metal: function constants\n    - WebGPU: pipeline-overridable constants",
    "attributes": [
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "keys", "type": "ArrayAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "hal.executable.constant.load",
    "summary": "Loads a constant value from the executable constant block.",
    "description": "Loads a scalar constant value from the static executable constant block.\n    The value provided by a constant block with the given key will be loaded and\n    bitcast (possibly with truncation or zero-extension) to the result type.\n\n    Note that backends are allowed to implement their own mechanisms for\n    referencing constant block values and this is provided only as a default for\n    those not needing special behavior.",
    "outputs": [
      { "name": "result", "type": "HAL_PrimitiveType" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" }
    ],
    "assemblyFormat": "$key attr-dict `:` type($result)"
  },
  {
    "name": "hal.executable.create",
    "summary": "Creates an executable.",
    "description": "Creates a target-dependent executable cached on the provided device. Entry\n    points contained within the executable can be dispatched using the resulting\n    executable handle.\n\n    Depending on the driver creation may take a non-trivial amount of time\n    (such as when JITing/etc). As the cache is internally synchronized callers\n    can issue preparation requests from multiple threads - even for the same\n    executables - and calls will block until preparation completes.\n\n    Optional constants provide for specialization of the executable based on\n    runtime-derived parameters.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" },
      { "name": "queue_affinity", "type": "HAL_DeviceQueueAffinity" },
      { "name": "constants", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Executable" }
    ],
    "attributes": [
      { "name": "executable_target", "type": "SymbolRefAttr" }
    ],
    "assemblyFormat": "`device` `(` $device `:` type($device) `)`\n    `affinity` `(` $queue_affinity `)`\n    `target` `(` $executable_target `)`\n    (`constants` `(` `[` $constants^ `]` `)`)?\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.executable.export",
    "summary": "Executable entry point declaration.",
    "description": "An entry point exported by the executable with statically-available\n    information describing the IO interface it uses and other dispatch metadata.\n\n    The `workgroup_count` region represents the computation that\n    returns the number of workgroups to use in the 3D grid dispatch.\n    The arguments to the region represents the workload as captured by each\n    dispatch. It returns the number of workgroups along x, y, and z.\n\n    The optional `condition` region provides boolean logic determining whether\n    the export should be dispatched given the device and workload or if a\n    specified fallback export in the same executable should be dispatched\n    instead. Multiple exports can be chained together as fallbacks to allow for\n    arbitrarily complex decisions trees. Fallbacks for an export must match the\n    layout and workload exactly but may vary any other attribute (such as\n    workgroup size or translation configuration).\n\n    Workgroup count and condition regions that have dependencies on dynamic\n    workload information will be executed using indirect dispatch. If the\n    information is available on the host at the time a command buffer containing\n    the dispatch is available the indirect dispatch _may_ have lower overhead\n    by using `IREE_HAL_DISPATCH_FLAG_STATIC_INDIRECT_PARAMETERS`. If the\n    information required is data-dependent on work within the same command\n    buffer some backends will suffer a performance penalty. Condition regions\n    consuming dynamic workloads in particular may result in long chains of\n    indirect dispatches sent to the device or even host round-trips.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "ordinal", "type": "OptionalAttr" },
      { "name": "layout", "type": "HAL_PipelineLayoutAttr" },
      { "name": "condition_fallback", "type": "OptionalAttr" },
      { "name": "workgroup_size", "type": "OptionalAttr" },
      { "name": "subgroup_size", "type": "OptionalAttr" },
      { "name": "workgroup_local_memory", "type": "OptionalAttr" },
      { "name": "source_locs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    $sym_name\n    (`ordinal` `(` $ordinal^ `)`)?\n    `layout` `(` qualified($layout) `)`\n    (`condition` `` custom<ExportConditionRegion>($condition)^)?\n    (`fallback` `(` $condition_fallback^ `)`)?\n    (`count` `` custom<WorkgroupCountRegion>($workgroup_count)^)?\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.executable.export.ordinal",
    "summary": "Executable export ordinal lookup pseudo-op.",
    "description": "Resolves an executable export ordinal to a value once ordinals have been\n    assigned.",
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "attributes": [
      { "name": "entry_point", "type": "SymbolRefAttr" }
    ],
    "assemblyFormat": "`target` `(` $entry_point `)`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.executable.lookup",
    "summary": "Executable cache lookup pseudo-op.",
    "description": "Used during conversion to provide a placeholder for a globally cached and\n    possibly lazy-initialized executable.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Executable" }
    ],
    "attributes": [
      { "name": "executable", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "`device` `(` $device `:` type($device) `)`\n    `executable` `(` $executable `)`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.executable.source",
    "summary": "Generic source contents of an executable op.",
    "description": "This is an unspecialized source representation of an executable\n    module without an assigned target. This is useful for hand-authoring\n    executables prior to device specification.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "objects", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    $sym_name\n    attr-dict-with-keyword\n    $body"
  },
  {
    "name": "hal.executable.source_end",
    "summary": "Terminator pseudo-op for the executable source op.",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "hal.executable.variant",
    "summary": "Target-specific variant of an executable op.",
    "description": "The target IR for the executable. This can be preserved for debugging but\n    is usually removed during transformation.\n\n    Variants are selected based on their target and an optional condition\n    op that returns true if the variant is valid for use on the provided\n    runtime `!hal.device`. If no variants within an executable are valid then\n    loading will fail at runtime. If multiple variants are valid the first valid\n    one found will be loaded and used for execution.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "target", "type": "HAL_ExecutableTargetAttr" },
      { "name": "objects", "type": "OptionalAttr" },
      { "name": "sources", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    $sym_name\n    `target` `(` $target `)`\n    (`objects` `(` $objects^ `)` )?\n    (`sources` `(` $sources^ `)` )?\n    attr-dict-with-keyword\n    $body"
  },
  {
    "name": "hal.executable.variant_end",
    "summary": "Terminator pseudo-op for the executable variant op.",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "hal.fence.await",
    "summary": "Asynchronous fence wait operation.",
    "description": "Yields the caller until all fences is reached. Returns the `status` of the\n    fence after the wait, with a non-zero value indicating failure.",
    "inputs": [
      { "name": "timeout_millis", "type": "I32" },
      { "name": "fences", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "status", "type": "Util_Status" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_WaitFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`until` `(` `[` $fences `]` `)`\n    `timeout_millis` `(` $timeout_millis `)`\n    `flags` `(` $flags `)`\n    `:` type($status)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.fence.create",
    "summary": "Creates an unsignaled fence.",
    "description": "Returns a fence that defines a point in time. By default fences will remain\n    unsignaled unless they are explicitly signaled with `hal.fence.signal` or\n    asynchronously signaled by the device by passing them as an operand to\n    queue submission ops.",
    "inputs": [
      { "name": "device", "type": "HAL_Device" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Fence" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_FenceFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`device` `(` $device `:` type($device) `)`\n    `flags` `(` $flags `)`\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.fence.fail",
    "summary": "Fence failure operation.",
    "description": "Signals the fence with a failure. The `status` will be returned from\n    each timepoint semaphores `hal.semaphore.query` and `hal.semaphore.signal`\n    for the lifetime of each semaphore.",
    "inputs": [
      { "name": "fence", "type": "HAL_Fence" },
      { "name": "status", "type": "Util_Status" }
    ],
    "assemblyFormat": "`<` $fence `:` type($fence) `>`\n    `status` `(` $status `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.fence.join",
    "summary": "Creates a fence from the given timepoints.",
    "description": "Returns a fence that joins the input fences as a wait-all operation.",
    "inputs": [
      { "name": "fences", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "HAL_Fence" }
    ],
    "attributes": [
      { "name": "flags", "type": "HAL_FenceFlagBitfieldAttr" }
    ],
    "assemblyFormat": "`at` `(` `[` $fences `]` `)`\n    `flags` `(` $flags `)`\n    `->` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.fence.query",
    "summary": "Fence query operation.",
    "description": "Queries whether the fence has been reached and its status.\n    Returns OK if the fence has been signaled successfully, DEFERRED if it is\n    unsignaled, and otherwise an error indicating the failure.",
    "inputs": [
      { "name": "fence", "type": "HAL_Fence" }
    ],
    "outputs": [
      { "name": "status", "type": "Util_Status" }
    ],
    "assemblyFormat": "`<` $fence `:` type($fence) `>`\n    `:` type($status)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.fence.signal",
    "summary": "Fence signal operation.",
    "description": "Signals the fence to indicate that the timepoints contained have been\n    reached. Waiting work may begin immediately.",
    "inputs": [
      { "name": "fence", "type": "HAL_Fence" }
    ],
    "assemblyFormat": "`<` $fence `:` type($fence) `>`\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.instrument.memory.load",
    "summary": "Emits a memory load instrumentation event.",
    "description": "Emits a workgroup-specific memory load event indicating that a number of\n    bytes from the given resolved pointer have been loaded by the workgroup.",
    "inputs": [
      { "name": "buffer", "type": "AnyMemRef" },
      { "name": "workgroupKey", "type": "Index" },
      { "name": "loadValue", "type": "AnyType" },
      { "name": "base", "type": "AnyMemRef" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "`` `[` $buffer `:` type($buffer) `for` $workgroupKey `]`\n    $base `[` $indices `]` `,` $loadValue\n    attr-dict `:` type($base) `,` type($result)"
  },
  {
    "name": "hal.instrument.memory.store",
    "summary": "Emits a memory store instrumentation event.",
    "description": "Emits a workgroup-specific memory store event indicating that a number of\n    bytes have been stored to the given resolved pointer by the workgroup.",
    "inputs": [
      { "name": "buffer", "type": "AnyMemRef" },
      { "name": "workgroupKey", "type": "Index" },
      { "name": "storeValue", "type": "AnyType" },
      { "name": "base", "type": "AnyMemRef" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "`` `[` $buffer `:` type($buffer) `for` $workgroupKey `]`\n    $base `[` $indices `]` `,` $storeValue\n    attr-dict `:` type($base) `,` type($result)"
  },
  {
    "name": "hal.instrument.print",
    "summary": "Emits a human-readable printf-style string event.",
    "description": "Formats a string using a limited subset of printf format specifiers and the\n    provided values and then emits an `iree_instrument_dispatch_print_t` event. Final\n    formatted string lengths may be limited to as much as 1024 characters and\n    should be kept as small as possible to avoid easily exceeding the\n    instrumentation storage buffers with redundant strings.",
    "inputs": [
      { "name": "buffer", "type": "AnyMemRef" },
      { "name": "workgroupKey", "type": "Index" },
      { "name": "values", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "format", "type": "StrAttr" }
    ],
    "assemblyFormat": "`` `[` $buffer `:` type($buffer) `for` $workgroupKey `]`\n    $format (`*` `(` $values^ `:` type($values) `)`)?\n    attr-dict"
  },
  {
    "name": "hal.instrument.value",
    "summary": "Emits a scalar value instrumentation event.",
    "description": "Emits a workgroup-specific typed value with the given workgroup-relative\n    ordinal.\n\n    This op will be preserved even if the output is not used as it is only for\n    debugging purposes.",
    "inputs": [
      { "name": "buffer", "type": "AnyMemRef" },
      { "name": "workgroupKey", "type": "Index" },
      { "name": "operand", "type": "AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "ordinal", "type": "AnyI8Attr" }
    ],
    "assemblyFormat": "`` `[` $buffer `:` type($buffer) `for` $workgroupKey `]`\n    $ordinal `=` $operand attr-dict `:` type($operand)"
  },
  {
    "name": "hal.instrument.workgroup",
    "summary": "Emits a dispatch workgroup instrumentation event.",
    "description": "Emits an `iree_instrument_dispatch_workgroup_t` event into the\n    instrumentation stream. The workgroup event identifies the unique dispatch,\n    its workgroup count, and the ID of the emitting workgroup within the\n    dispatch. Optionally targets that support querying the processor ID\n    executing the workgroup can attach that information for tracking purposes.\n\n    On targets such as CPUs where entire workgroups execute as atomic units\n    only one workgroup event should be emitted. On targets such as GPUs where\n    there may be multiple invocations executing as part of a single workgroup\n    only the first invocation within the workgroup should emit the workgroup\n    event (by checking if the LocalInvocationIndex or threadIdx == 0, etc).\n\n    The resulting workgroup key is used by subsequent workgroup-specific\n    instrumentation events.",
    "inputs": [
      { "name": "buffer", "type": "AnyMemRef" },
      { "name": "dispatchId", "type": "I32" }
    ],
    "outputs": [
      { "name": "workgroupKey", "type": "Index" }
    ],
    "assemblyFormat": "`` `[` $buffer `:` type($buffer) `]`\n    `dispatch` `(` $dispatchId `)`\n    attr-dict `:` type($workgroupKey)"
  },
  {
    "name": "hal.interface.binding.subspan",
    "summary": "Returns an alias to a subspan of interface binding data.",
    "description": "Returns a subspan of an interface binding storage buffer in a generic type.\n    The exact shape, type, and alignment of the returned type are defined by\n    the result type (tensor, memref, etc).\n\n    An optional alignment indicates the byte alignment of the base binding\n    resource. Note that the byte offset is added to the base and the alignment\n    will be the minimum of the two.",
    "inputs": [
      { "name": "byte_offset", "type": "Optional" },
      { "name": "dynamic_dims", "type": "HAL_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "layout", "type": "HAL_PipelineLayoutAttr" },
      { "name": "binding", "type": "IndexAttr" },
      { "name": "alignment", "type": "OptionalAttr" },
      { "name": "descriptor_flags", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`layout` `(` $layout `)`\n    `binding` `(` $binding `)`\n    (`alignment` `(` $alignment^ `)`)?\n    (`offset` `(` $byte_offset^ `)`)?\n    (`flags` `(` $descriptor_flags^ `)`)?\n    attr-dict `:` type($result) (`{` $dynamic_dims^ `}`)?"
  },
  {
    "name": "hal.interface.constant.load",
    "summary": "Loads a constant value from the interface constant block.",
    "description": "Loads a scalar constant value from an executable IO push constant block.\n    The value will be loaded from the given constant offset and will be\n    bitcast (possibly with truncation or zero-extension) to the result type.\n\n    An optional alignment indicates the byte alignment of potential values for\n    the constant when it could be determined from analysis. If omitted the value\n    may be anything and its interpretation is up to the usage. This is intended\n    to provide pointer alignment-like semantics to constants that are used to\n    index into binding resources.\n\n    An optional set of values indicates all possible values that can be passed\n    to the constant from all dispatch sites in the program. If omitted the value\n    may be from an unanalyzable source (outside of the program, indirect, etc)\n    and must be assumed to have any value.",
    "outputs": [
      { "name": "result", "type": "HAL_PrimitiveType" }
    ],
    "attributes": [
      { "name": "layout", "type": "HAL_PipelineLayoutAttr" },
      { "name": "ordinal", "type": "HAL_HostSizeAttr" },
      { "name": "alignment", "type": "OptionalAttr" },
      { "name": "values", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`layout` `(` $layout `)`\n    `ordinal` `(` $ordinal `)`\n    (`alignment` `(` $alignment^ `)`)?\n    (`values` `(` $values^ `)`)?\n    attr-dict `:` type($result)"
  },
  {
    "name": "hal.interface.workgroup.count",
    "summary": "Returns the total workgroup count of the grid.",
    "description": "The total number of workgroups along each dimension in the dispatch grid.\n    Matches what was passed to the `hal.command_buffer.dispatch` command (or\n    what was indirectly specified).\n\n    Corresponds to the `NumWorkgroups` SPIR-V built-in and the `gridDim` CUDA\n    built-in variable.\n\n    ```mlir\n    %x = hal.interface.workgroup.count[0] : index\n    %y = hal.interface.workgroup.count[1] : index\n    %z = hal.interface.workgroup.count[2] : index\n    ```",
    "outputs": [
      { "name": "result", "type": "HAL_Dim" }
    ],
    "attributes": [
      { "name": "dimension", "type": "IndexAttr" },
      { "name": "upper_bound", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`[` $dimension `]` (`upper_bound` $upper_bound^)? attr-dict `:` type($result)"
  },
  {
    "name": "hal.interface.workgroup.id",
    "summary": "Returns the index of the current workgroup in the grid.",
    "description": "The global workgroup ID of the current tile in the range of\n    `[0, hal.interface.workgroup.count)` along each XYZ dimension.\n\n    Corresponds to the `WorkgroupId` SPIR-V built-in and the `blockIdx` CUDA\n    built-in variable.\n\n    ```mlir\n    %x = hal.interface.workgroup.id[0] : index\n    %y = hal.interface.workgroup.id[1] : index\n    %z = hal.interface.workgroup.id[2] : index\n    ```",
    "outputs": [
      { "name": "result", "type": "HAL_Dim" }
    ],
    "attributes": [
      { "name": "dimension", "type": "IndexAttr" },
      { "name": "upper_bound", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`[` $dimension `]` (`upper_bound` $upper_bound^)? attr-dict `:` type($result)"
  },
  {
    "name": "hal.interface.workgroup.size",
    "summary": "Returns the size of each workgroup in invocations.",
    "description": "The number of local invocations within the current workgroup along each\n    dimension. Depending on backend this may map to the SIMT thread count or\n    inner loop nest parameters.\n\n    Corresponds to the `WorkgroupSize` SPIR-V built-in and the `blockDim` CUDA\n    built-in variable.\n\n    ```mlir\n    %x = hal.interface.workgroup.size[0] : index\n    %y = hal.interface.workgroup.size[1] : index\n    %z = hal.interface.workgroup.size[2] : index\n    ```",
    "outputs": [
      { "name": "result", "type": "HAL_Dim" }
    ],
    "attributes": [
      { "name": "dimension", "type": "IndexAttr" },
      { "name": "upper_bound", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`[` $dimension `]` (`upper_bound` $upper_bound^)? attr-dict `:` type($result)"
  },
  {
    "name": "hal.memory_type",
    "summary": "An iree_hal_memory_type_t for the given memory type bits.",
    "description": "Maps memory type bits to a runtime `iree_hal_memory_type_t` value.",
    "outputs": [
      { "name": "result", "type": "HAL_MemoryType" }
    ],
    "attributes": [
      { "name": "type", "type": "HAL_MemoryTypeBitfieldAttr" }
    ],
    "assemblyFormat": "`<` $type `>`\n    attr-dict\n    `:` type($result)"
  },
  {
    "name": "hal.return",
    "summary": "Return from a hal.* region.",
    "description": "Returns the given values from the region and back to the host code.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "($operands^ `:` type($operands))? attr-dict"
  },
  {
    "name": "hal.tensor.alias",
    "summary": "Hints that tensor storage should alias a HAL buffer view.",
    "description": "Hints that the backing storage of an entire tensor aliases the given storage\n    buffer. There's no guarantee that the storage will alias and instead only\n    that the tensor contents will be written to the storage as if a copy had\n    occurred. This allows the compiler to avoid copies in the ideal case of a\n    producer that is able to produce directly into the target storage but still\n    handle cases where the producer is not able to be in-place.\n\n    The storage buffer provided must have sufficient space for the tensor once\n    encoded. Dynamically shaped tensors may not consume the entire provided\n    storage. If a buffer view is provided the metadata is ignored and only the\n    backing buffer is used.\n\n    An optional wait fence can be provided in cases where the storage is not\n    immediately available. Producers that may alias the storage will wait until\n    the storage is available before updating the contents.\n\n    Explicit aliasing side-steps any analysis that may be performed by the\n    compiler and requires users to guarantee that the safety of the aliasing.\n    Copy-on-write, alias analysis for overlap detection, and ordering via\n    use-def chains are all ignorant of the aliased buffer memory and only ensure\n    the compiler consumes or produces the aliased memory consistent with itself.\n\n    Example:\n    ```mlir\n    %init = tensor.empty\n    %value = linalg.generic ... outs(%init)\n    %aliased = hal.tensor.alias %value : tensor<...> to %buffer : !hal.buffer\n    ... linalg.generic ins(%aliased) ...\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyTensor" },
      { "name": "source_dims", "type": "HAL_ShapeDynamicDims" },
      { "name": "storage", "type": "AnyTypeOf" },
      { "name": "wait_fence", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    (`wait` `(` $wait_fence^ `)` `=` `` `>`)?\n    $source `:` type($source) (`{` $source_dims^ `}`)?\n    `to`\n    $storage `:` type($storage)\n    attr-dict"
  },
  {
    "name": "hal.tensor.barrier",
    "summary": "Signals a fence when all tensors are available.",
    "description": "Defines a barrier that is used to indicate availability of an entire set of\n    tensors by signaling a fence. The source tensors are returned for chaining.",
    "inputs": [
      { "name": "sources", "type": "Variadic" },
      { "name": "signal_fence", "type": "HAL_Fence" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "`join` `` `(` $sources `:` type($sources) `)`\n    `=` `` `>`\n    $signal_fence `:` type($signal_fence)\n    attr-dict-with-keyword"
  },
  {
    "name": "hal.tensor.export",
    "summary": "Exports a tensor to a HAL buffer view.",
    "description": "Defines an export of an SSA-form tensor to an external HAL buffer view.\n\n    The provided `source_encoding`, if different from the `source` type,\n    indicates that the ABI-facing type may differ from the internal\n    representation. The types must be bitcastable (same storage size) and\n    dynamically shaped values must have the same number of dynamic dimensions.\n    This allows for casting between rank-0 and rank-N types, different element\n    types, etc.",
    "inputs": [
      { "name": "source", "type": "AnyTensor" },
      { "name": "source_dims", "type": "HAL_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "target", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "source_encoding", "type": "TypeAttr" },
      { "name": "name", "type": "OptionalAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $source\n    ($name^)?\n    `:`\n    custom<TypeAlias>($source_encoding, type($source)) (`{` $source_dims^ `}`)?\n    `->`\n    type($target)\n    attr-dict"
  },
  {
    "name": "hal.tensor.import",
    "summary": "Imports a tensor from a HAL buffer view.",
    "description": "Defines an import of an external HAL buffer view into a SSA-form tensor.\n    An optional fence can be specified indicating when the buffer view is\n    available for use. If no fence is provided it is assumed the buffer view is\n    immediately available.\n\n    The provided `target_encoding`, if different from the `target` type,\n    indicates that the ABI-facing type may differ from the internal\n    representation. The types must be bitcastable (same storage size) and\n    dynamically shaped values must have the same number of dynamic dimensions.\n    This allows for casting between rank-0 and rank-N types, different element\n    types, etc.\n\n    `consume` can be used to indicate a transfer of ownership. Though the\n    imported value may still have external references when consumed a resource\n    will be conceptually released from its existing owner and retained by the\n    importer atomically.",
    "inputs": [
      { "name": "source", "type": "AnyTypeOf" },
      { "name": "target_dims", "type": "HAL_ShapeDynamicDims" },
      { "name": "wait_fence", "type": "Optional" }
    ],
    "outputs": [
      { "name": "target", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "target_encoding", "type": "TypeAttr" },
      { "name": "consume", "type": "UnitAttr" },
      { "name": "name", "type": "OptionalAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    (`wait` `(` $wait_fence^ `)` `=` `` `>`)?\n    (`consume` $consume^)?\n    $source\n    ($name^)?\n    `:` type($source) `->`\n    custom<TypeAlias>($target_encoding, type($target)) (`{` $target_dims^ `}`)?\n    attr-dict"
  },
  {
    "name": "index.add",
    "summary": "index addition",
    "description": "The `index.add` operation takes two index values and computes their sum.\n\n    Example:\n\n    ```mlir\n    // c = a + b\n    %c = index.add %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.and",
    "summary": "index bitwise and",
    "description": "The `index.and` operation takes two index values and computes their bitwise\n    and.\n\n    Example:\n\n    ```mlir\n    // c = a & b\n    %c = index.and %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.bool.constant",
    "summary": "boolean constant",
    "description": "The `index.bool.constant` operation produces an bool-typed SSA value equal\n    to either `true` or `false`.\n\n    This operation is used to materialize bool constants that arise when folding\n    `index.cmp`.\n\n    Example:\n\n    ```mlir\n    %0 = index.bool.constant true\n    ```",
    "outputs": [
      { "name": "result", "type": "I1" }
    ],
    "attributes": [
      { "name": "value", "type": "BoolAttr" }
    ],
    "assemblyFormat": "attr-dict $value"
  },
  {
    "name": "index.casts",
    "summary": "index signed cast",
    "description": "The `index.casts` operation enables conversions between values of index type\n    and concrete fixed-width integer types. If casting to a wider integer, the\n    value is sign-extended. If casting to a narrower integer, the value is\n    truncated.\n\n    Example:\n\n    ```mlir\n    // Cast to i32\n    %0 = index.casts %a : index to i32\n\n    // Cast from i64\n    %1 = index.casts %b : i64 to index\n    ```",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$input attr-dict `:` type($input) `to` type($output)"
  },
  {
    "name": "index.castu",
    "summary": "index unsigned cast",
    "description": "The `index.castu` operation enables conversions between values of index type\n    and concrete fixed-width integer types. If casting to a wider integer, the\n    value is zero-extended. If casting to a narrower integer, the value is\n    truncated.\n\n    Example:\n\n    ```mlir\n    // Cast to i32\n    %0 = index.castu %a : index to i32\n\n    // Cast from i64\n    %1 = index.castu %b : i64 to index\n    ```",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$input attr-dict `:` type($input) `to` type($output)"
  },
  {
    "name": "index.ceildivs",
    "summary": "index signed ceil division",
    "description": "The `index.ceildivs` operation takes two index values and computes their\n    signed quotient. Treats the leading bit as the sign and rounds towards\n    positive infinity, i.e. `7 / -2 = -3`.\n\n    Note: division by zero and signed division overflow are undefined behaviour.\n\n    Example:\n\n    ```mlir\n    // c = ceil(a / b)\n    %c = index.ceildivs %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.ceildivu",
    "summary": "index unsigned ceil division",
    "description": "The `index.ceildivu` operation takes two index values and computes their\n    unsigned quotient. Treats the leading bit as the most significant and rounds\n    towards positive infinity, i.e. `6 / -2 = 1`.\n\n    Note: division by zero is undefined behaviour.\n\n    Example:\n\n    ```mlir\n    // c = ceil(a / b)\n    %c = index.ceildivu %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.cmp",
    "summary": "index compare",
    "description": "The `index.cmp` operation takes two index values and compares them according\n    to the comparison predicate and returns an `i1`. The following comparisons\n    are supported:\n\n    -   `eq`:  equal\n    -   `ne`:  not equal\n    -   `slt`: signed less than\n    -   `sle`: signed less than or equal\n    -   `sgt`: signed greater than\n    -   `sge`: signed greater than or equal\n    -   `ult`: unsigned less than\n    -   `ule`: unsigned less than or equal\n    -   `ugt`: unsigned greater than\n    -   `uge`: unsigned greater than or equal\n\n    The result is `1` if the comparison is true and `0` otherwise.\n\n    Example:\n\n    ```mlir\n    // Signed less than comparison.\n    %0 = index.cmp slt(%a, %b)\n\n    // Unsigned greater than or equal comparison.\n    %1 = index.cmp uge(%a, %b)\n\n    // Not equal comparison.\n    %2 = index.cmp ne(%a, %b)\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "I1" }
    ],
    "attributes": [
      { "name": "pred", "type": "IndexCmpPredicateAttr" }
    ],
    "assemblyFormat": "`` $pred `(` $lhs `,` $rhs `)` attr-dict"
  },
  {
    "name": "index.constant",
    "summary": "index constant",
    "description": "The `index.constant` operation produces an index-typed SSA value equal to\n    some index-typed integer constant.\n\n    Example:\n\n    ```mlir\n    %0 = index.constant 42\n    ```",
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "attributes": [
      { "name": "value", "type": "IndexAttr" }
    ],
    "assemblyFormat": "attr-dict $value"
  },
  {
    "name": "index.divs",
    "summary": "index signed division",
    "description": "The `index.divs` operation takes two index values and computes their signed\n    quotient. Treats the leading bit as the sign and rounds towards zero, i.e.\n    `6 / -2 = -3`.\n\n    Note: division by zero and signed division overflow are undefined behaviour.\n\n    Example:\n\n    ```mlir\n    // c = a / b\n    %c = index.divs %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.divu",
    "summary": "index unsigned division",
    "description": "The `index.divu` operation takes two index values and computes their\n    unsigned quotient. Treats the leading bit as the most significant and rounds\n    towards zero, i.e. `6 / -2 = 0`.\n\n    Note: division by zero is undefined behaviour.\n\n    Example:\n\n    ```mlir\n    // c = a / b\n    %c = index.divu %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.floordivs",
    "summary": "index signed floor division",
    "description": "The `index.floordivs` operation takes two index values and computes their\n    signed quotient. Treats the leading bit as the sign and rounds towards\n    negative infinity, i.e. `5 / -2 = -3`.\n\n    Note: division by zero and signed division overflow are undefined behaviour.\n\n    Example:\n\n    ```mlir\n    // c = floor(a / b)\n    %c = index.floordivs %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.maxs",
    "summary": "index signed maximum",
    "description": "The `index.maxs` operation takes two index values and computes their signed\n    maximum value. Treats the leading bit as the sign, i.e. `max(-2, 6) = 6`.\n\n    Example:\n\n    ```mlir\n    // c = max(a, b)\n    %c = index.maxs %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.maxu",
    "summary": "index unsigned maximum",
    "description": "The `index.maxu` operation takes two index values and computes their\n    unsigned maximum value. Treats the leading bit as the most significant, i.e.\n    `max(15, 6) = 15` or `max(-2, 6) = -2`.\n\n    Example:\n\n    ```mlir\n    // c = max(a, b)\n    %c = index.maxu %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.mins",
    "summary": "index signed minimum",
    "description": "The `index.mins` operation takes two index values and computes their signed\n    minimum value. Treats the leading bit as the sign, i.e. `min(-2, 6) = -2`.\n\n    Example:\n\n    ```mlir\n    // c = min(a, b)\n    %c = index.mins %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.minu",
    "summary": "index unsigned minimum",
    "description": "The `index.minu` operation takes two index values and computes their\n    unsigned minimum value. Treats the leading bit as the most significant, i.e.\n    `min(15, 6) = 6` or `min(-2, 6) = 6`.\n\n    Example:\n\n    ```mlir\n    // c = min(a, b)\n    %c = index.minu %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.mul",
    "summary": "index multiplication",
    "description": "The `index.mul` operation takes two index values and computes their product.\n\n    Example:\n\n    ```mlir\n    // c = a * b\n    %c = index.mul %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.or",
    "summary": "index bitwise or",
    "description": "The `index.or` operation takes two index values and computes their bitwise\n    or.\n\n    Example:\n\n    ```mlir\n    // c = a | b\n    %c = index.or %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.rems",
    "summary": "index signed remainder",
    "description": "The `index.rems` operation takes two index values and computes their signed\n    remainder. Treats the leading bit as the sign, i.e. `6 % -2 = 0`.\n\n    Example:\n\n    ```mlir\n    // c = a % b\n    %c = index.rems %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.remu",
    "summary": "index unsigned remainder",
    "description": "The `index.remu` operation takes two index values and computes their\n    unsigned remainder. Treats the leading bit as the most significant, i.e.\n    `6 % -2 = 6`.\n\n    Example:\n\n    ```mlir\n    // c = a % b\n    %c = index.remu %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.shl",
    "summary": "index shift left",
    "description": "The `index.shl` operation shifts an index value to the left by a variable\n    amount. The low order bits are filled with zeroes. The RHS operand is always\n    treated as unsigned. If the RHS operand is equal to or greater than the\n    index bitwidth, the result is a poison value.\n\n    Example:\n\n    ```mlir\n    // c = a << b\n    %c = index.shl %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.shrs",
    "summary": "signed index shift right",
    "description": "The `index.shrs` operation shifts an index value to the right by a variable\n    amount. The LHS operand is treated as signed. The high order bits are filled\n    with copies of the most significant bit. If the RHS operand is equal to or\n    greater than the index bitwidth, the result is a poison value.\n\n    Example:\n\n    ```mlir\n    // c = a >> b\n    %c = index.shrs %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.shru",
    "summary": "unsigned index shift right",
    "description": "The `index.shru` operation shifts an index value to the right by a variable\n    amount. The LHS operand is treated as unsigned. The high order bits are\n    filled with zeroes. If the RHS operand is equal to or greater than the index\n    bitwidth, the result is a poison value.\n\n    Example:\n\n    ```mlir\n    // c = a >> b\n    %c = index.shru %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.sizeof",
    "summary": "size in bits of the index type",
    "description": "The `index.sizeof` operation produces an index-typed SSA value equal to the\n    size in bits of the `index` type. For example, on 32-bit systems, the result\n    is `32 : index`, and on 64-bit systems, the result is `64 : index`.\n\n    Example:\n\n    ```mlir\n    %0 = index.sizeof\n    ```",
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "index.sub",
    "summary": "index subtraction",
    "description": "The `index.sub` operation takes two index values and computes the difference\n    of the first from the second operand.\n\n    Example:\n\n    ```mlir\n    // c = a - b\n    %c = index.sub %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "index.xor",
    "summary": "index bitwise xor",
    "description": "The `index.xor` operation takes two index values and computes their bitwise\n    xor.\n\n    Example:\n\n    ```mlir\n    // c = a ^ b\n    %c = index.xor %a, %b\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Index" },
      { "name": "rhs", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "interpreter.print",
    "summary": "Print operation",
    "description": "Print the value to stdout.\n\n    This is useful to print intermediate states of the tensors while debugging.\n    This should only be used to debug small tensors since every instance of this\n    op and its contents are printed to stdout. To gather information in bulk for\n    larger tensors, prefer using ProbeOp.\n\n    Example:\n    ```mlir\n    interpreter.print %operand : tensor<i1>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand)"
  },
  {
    "name": "interpreter.probe",
    "description": "Probe and store the values of the input tensor at runtime, using the NumPy\n    file format. Writes tensor input value to\n    `<output-dir>/<probe_id>_<iteration>.npy` (where output-dir is specified by\n    the `--probe_output_dir` flag). Additionally, adds an entry to\n    <output-dir>/index.csv metadata file which maps probe IDs, types and\n    filenames with their tensor values.\n\n    The `probe` operation will not modify its input in any way. Probe\n    instrumentation may however slow down the interpretation of a module as\n    there will be increased file I/O.\n\n    Note that `probe_id` should be unique for each `probe` instruction in a\n    StableHLO module. A `probe` may run more than once, in which case it will\n    produce separate serialized data for each iteration in the form\n    `probe_id_#` where # is a 1-based counter.\n\n    Example:\n    ```mlir\n    %result = interpreter.probe %operand, probe_id = \"probe0\" : tensor<3xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "probe_id", "type": "StrAttr" }
    ],
    "assemblyFormat": "$operand `,` `probe_id` `=` $probe_id attr-dict `:` type($result)"
  },
  {
    "name": "interpreter.run_parallel",
    "summary": "RunParallel operation",
    "description": "Runs a two-dimensional grid of `num_replicas` by `num_partitions` StableHLO\n    processes based on programs specified by two-dimensional grid of `programs`.\n\n    Given that each program in the grid can have a different number of inputs\n    and outputs, we'd want a ragged three-dimensional tensor to model `inputs`\n    and `results`, which cannot be easily achieved via conventional means.\n\n    Instead, `inputs` and `results` are represented in a flattened way, obtained\n    by reshaping the said three-dimensional tensors into a one-dimensional form\n    (with the same semantics as StableHLO's reshape). This is a reversible\n    transformation because StableHLO programs have a static number of inputs and\n    outputs.\n\n    For example, for a 2x2 StableHLO process grid, `inputs` first has inputs\n    from the process `(0, 0)` (i.e. replica_id = 0 and partition_id = 0),\n    then `(0, 1)` (i.e. replica_id = 0 and partition_id = 1), etc.\n\n    Example:\n    ```mlir\n    %results:2 = \"interpreter.run_parallel\"() {\n      infeed=[@infeed_queue0, @infeed_queue1]\n      programs=[[@foo], [@bar]]\n    } : () -> (tensor<ui32>, tensor<ui32>)\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "infeed", "type": "OptionalAttr" },
      { "name": "programs", "type": "Interpreter_ArrayOfFlatSymbolRefArrayAttr" }
    ]
  },
  {
    "name": "irdl.all_of",
    "summary": "Constraints to the intersection of the provided constraints",
    "description": "`irdl.all_of` defines a constraint that accepts any type or attribute that\n    satisfies all of its provided constraints.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n      irdl.type @complex_f32 {\n        %0 = irdl.is i32\n        %1 = irdl.is f32\n        %2 = irdl.any_of(%0, %1) // is 32-bit\n\n        %3 = irdl.is f32\n        %4 = irdl.is f64\n        %5 = irdl.any_of(%3, %4) // is a float\n\n        %6 = irdl.all_of(%2, %5) // is a 32-bit float\n        irdl.parameters(%6)\n      }\n    }\n    ```\n\n    The above program defines a type `complex` inside the dialect `cmath` that\n    has one parameter that must be 32-bit long and a float (in other\n    words, that must be `f32`).",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "IRDL_AttributeType" }
    ],
    "assemblyFormat": "`(` $args `)` ` ` attr-dict"
  },
  {
    "name": "irdl.any",
    "summary": "Accept any type or attribute",
    "description": "`irdl.any` defines a constraint that accepts any type or attribute.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n      irdl.type @complex_flexible {\n        %0 = irdl.any\n        irdl.parameters(%0)\n      }\n    }\n    ```\n\n    The above program defines a type `complex_flexible` inside the dialect\n    `cmath` that has a single parameter that can be any attribute.",
    "outputs": [
      { "name": "output", "type": "IRDL_AttributeType" }
    ],
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "irdl.any_of",
    "summary": "Constraints to the union of the provided constraints",
    "description": "`irdl.any_of` defines a constraint that accepts any type or attribute that\n    satisfies at least one of its provided type constraints.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n      irdl.type @complex {\n        %0 = irdl.is i32\n        %1 = irdl.is i64\n        %2 = irdl.is f32\n        %3 = irdl.is f64\n        %4 = irdl.any_of(%0, %1, %2, %3)\n        irdl.parameters(%4)\n      }\n    }\n    ```\n\n    The above program defines a type `complex` inside the dialect `cmath` that\n    has a single type parameter that can be either `i32`, `i64`, `f32` or\n    `f64`.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "IRDL_AttributeType" }
    ],
    "assemblyFormat": "`(` $args `)` ` ` attr-dict"
  },
  {
    "name": "irdl.attribute",
    "summary": "Define a new attribute",
    "description": "`irdl.attribute` defines a new attribute belonging to the `irdl.dialect`\n    parent.\n\n    The attribute parameters can be defined with an `irdl.parameters` operation\n    in the optional region.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @testd {\n      irdl.attribute @enum_attr {\n        %0 = irdl.is \"foo\"\n        %1 = irdl.is \"bar\"\n        %2 = irdl.any_of(%0, %1)\n        irdl.parameters(%2)\n      }\n    }\n    ```\n\n    The above program defines an `enum_attr` attribute inside the `testd`\n    dialect. The attribute has one `StringAttr` parameter that should be\n    either a `\"foo\"` or a `\"bar\"`.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "$sym_name attr-dict-with-keyword custom<SingleBlockRegion>($body)"
  },
  {
    "name": "irdl.attributes",
    "summary": "Define the attributes of an operation",
    "description": "`irdl.attributes` defines the attributes of the `irdl.operation` parent\n    operation definition.\n\n    In the following example, `irdl.attributes` defines the attributes of the\n    `attr_op` operation:\n\n    ```mlir\n    irdl.dialect @example {\n\n      irdl.operation @attr_op {\n        %0 = irdl.any\n        %1 = irdl.is i64\n        irdl.attibutes {\n          \"attr1\" = %0,\n          \"attr2\" = %1\n        }\n      }\n    }\n    ```\n\n    The operation will expect an arbitrary attribute \"attr1\" and an\n    attribute \"attr2\" with value `i64`.",
    "inputs": [
      { "name": "attributeValues", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "attributeValueNames", "type": "StrArrayAttr" }
    ],
    "assemblyFormat": "custom<AttributesOp>($attributeValues, $attributeValueNames) attr-dict"
  },
  {
    "name": "irdl.base",
    "summary": "Constraints an attribute/type base",
    "description": "`irdl.base` defines a constraint that only accepts a single type\n    or attribute base, e.g. an `IntegerType`. The attribute base is defined\n    either by a symbolic reference to the corresponding IRDL definition,\n    or by the name of the base. Named bases are prefixed with `!` or `#`\n    respectively for types and attributes.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n      irdl.type @complex {\n        %0 = irdl.base \"!builtin.integer\"\n        irdl.parameters(%0)\n      }\n\n      irdl.type @complex_wrapper {\n        %0 = irdl.base @cmath::@complex\n        irdl.parameters(%0)\n      }\n    }\n    ```\n\n    The above program defines a `cmath.complex` type that expects a single\n    parameter, which is a type with base name `builtin.integer`, which is the\n    name of an `IntegerType` type.\n    It also defines a `cmath.complex_wrapper` type that expects a single\n    parameter, which is a type of base type `cmath.complex`.",
    "outputs": [
      { "name": "output", "type": "IRDL_AttributeType" }
    ],
    "attributes": [
      { "name": "base_ref", "type": "OptionalAttr" },
      { "name": "base_name", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($base_ref^)? ($base_name^)? ` ` attr-dict"
  },
  {
    "name": "irdl.c_pred",
    "summary": "Constraints an attribute using a C++ predicate",
    "description": "`irdl.c_pred` defines a constraint that is written in C++.\n\n    Dialects using this operation cannot be registered at runtime, as it relies\n    on C++ code.\n\n    Special placeholders can be used to refer to entities in the context where\n    this predicate is used. They serve as \"hooks\" to the enclosing environment.\n    The following special placeholders are supported in constraints for an op:\n\n    * `$_builder` will be replaced by a mlir::Builder instance.\n    * `$_op` will be replaced by the current operation.\n    * `$_self` will be replaced with the entity this predicate is attached to.\n       Compared to ODS, `$_self` is always of type `mlir::Attribute`, and types\n       are manipulated as `TypeAttr` attributes.\n\n    Example:\n    ```mlir\n    irdl.type @op_with_attr {\n      %0 = irdl.c_pred \"::llvm::isa<::mlir::IntegerAttr>($_self)\"\n      irdl.parameters(%0)\n    }\n    ```\n\n    In this example, @op_with_attr is defined as a type with a single\n    parameter, which is an `IntegerAttr`, as constrained by the C++ predicate.",
    "outputs": [
      { "name": "output", "type": "IRDL_AttributeType" }
    ],
    "attributes": [
      { "name": "pred", "type": "StrAttr" }
    ],
    "assemblyFormat": "$pred ` ` attr-dict"
  },
  {
    "name": "irdl.IRDL_Dialect",
    "summary": "Define a new dialect",
    "description": "The `irdl.dialect` operation defines a dialect. All operations, attributes,\n    and types defined inside its region will be part of the dialect.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n      ...\n    }\n    ```\n\n    The above program defines a `cmath` dialect.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "$sym_name attr-dict-with-keyword custom<SingleBlockRegion>($body)"
  },
  {
    "name": "irdl.is",
    "summary": "Constraints an attribute/type to be a specific attribute instance",
    "description": "`irdl.is` defines a constraint that only accepts a specific instance of a\n    type or attribute.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n      irdl.type @complex_i32 {\n        %0 = irdl.is i32\n        irdl.parameters(%0)\n      }\n    }\n    ```\n\n    The above program defines a `complex_i32` type inside the dialect `cmath`\n    that can only have a `i32` as its parameter.",
    "outputs": [
      { "name": "output", "type": "IRDL_AttributeType" }
    ],
    "attributes": [
      { "name": "expected", "type": "AnyAttr" }
    ],
    "assemblyFormat": "$expected ` ` attr-dict"
  },
  {
    "name": "irdl.operands",
    "summary": "Define the operands of an operation",
    "description": "`irdl.operands` define the operands of the `irdl.operation` parent operation\n    definition. Each operand is named after an identifier.\n\n    In the following example, `irdl.operands` defines the operands of the\n    `mul` operation:\n\n    ```mlir\n    irdl.dialect @cmath {\n\n      irdl.type @complex { /* ... */ }\n\n      irdl.operation @mul {\n        %0 = irdl.any\n        %1 = irdl.parametric @cmath::@complex<%0>\n        irdl.results(res: %1)\n        irdl.operands(lhs: %1, rhs: %1)\n      }\n    }\n    ```\n\n    The `mul` operation will expect two operands of type `cmath.complex`, that\n    have the same type, and return a result of the same type.\n\n    The operands can also be marked as variadic or optional:\n    ```mlir\n    irdl.operands(foo: %0, bar: single %1, baz: optional %2, qux: variadic %3)\n    ```\n\n    Here, foo and bar are required single operands, baz is an optional operand,\n    and qux is a variadic operand.\n\n    When more than one operand is marked as optional or variadic, the operation\n    will expect a 'operandSegmentSizes' attribute that defines the number of\n    operands in each segment.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "names", "type": "StrArrayAttr" },
      { "name": "variadicity", "type": "VariadicityArrayAttr" }
    ],
    "assemblyFormat": "`` custom<NamedValueListWithVariadicity>($args, $names, $variadicity) attr-dict"
  },
  {
    "name": "irdl.operation",
    "summary": "Define a new operation",
    "description": "`irdl.operation` defines a new operation belonging to the `irdl.dialect`\n    parent.\n\n    Operations can define constraints on their operands and results with the\n    `irdl.results` and `irdl.operands` operations. If these operations are not\n    present in the region, the results or operands are expected to be empty.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n\n      irdl.type @complex { /* ... */ }\n\n      irdl.operation @norm {\n        %0 = irdl.any\n        %1 = irdl.parametric @cmath::@complex<%0>\n        irdl.results(%0)\n        irdl.operands(%1)\n      }\n    }\n    ```\n\n    The above program defines an operation `norm` inside the dialect `cmath`.\n    The operation expects a single operand of base type `cmath.complex`, and\n    returns a single result of the element type of the operand.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "$sym_name attr-dict-with-keyword custom<SingleBlockRegion>($body)"
  },
  {
    "name": "irdl.parameters",
    "summary": "Define the constraints on parameters of a type/attribute definition",
    "description": "`irdl.parameters` defines the constraints on parameters of a type or\n    attribute definition. Each parameter is named after an identifier.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n      irdl.type @complex {\n        %0 = irdl.is i32\n        %1 = irdl.is i64\n        %2 = irdl.any_of(%0, %1)\n        irdl.parameters(elem: %2)\n      }\n    }\n    ```\n\n    The above program defines a type `complex` inside the dialect `cmath`. The\n    type has a single parameter `elem` that should be either `i32` or `i64`.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "names", "type": "StrArrayAttr" }
    ],
    "assemblyFormat": "`` custom<NamedValueList>($args, $names) attr-dict"
  },
  {
    "name": "irdl.parametric",
    "summary": "Constraints an attribute/type base and its parameters",
    "description": "`irdl.parametric` defines a constraint that accepts only a single type\n    or attribute base. The attribute base is defined by a symbolic reference\n    to the corresponding definition. It will additionally constraint the\n    parameters of the type/attribute.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n\n      irdl.type @complex { /* ... */ }\n\n      irdl.operation @norm {\n        %0 = irdl.any\n        %1 = irdl.parametric @cmath::@complex<%0>\n        irdl.operands(%1)\n        irdl.results(%0)\n      }\n    }\n    ```\n\n    The above program defines an operation `norm` inside the dialect `cmath` that\n    for any `T` takes a `cmath.complex` with parameter `T` and returns a `T`.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "IRDL_AttributeType" }
    ],
    "attributes": [
      { "name": "base_type", "type": "SymbolRefAttr" }
    ],
    "assemblyFormat": "$base_type `<` $args `>` ` ` attr-dict"
  },
  {
    "name": "irdl.region",
    "summary": "Define a region of an operation",
    "description": "The irdl.region construct defines a set of characteristics\n    that a region of an operation should satify. Each region is named after\n    an identifier.\n\n    These characteristics include constraints for the entry block arguments\n    of the region and the total number of blocks it contains.\n    The number of blocks must be a non-zero and non-negative integer,\n    and it is optional by default.\n    The set of constraints for the entry block arguments may be optional or\n    empty. If no parentheses are provided, the set is assumed to be optional,\n    and the arguments are not constrained in any way. If parentheses are\n    provided with no arguments, it means that the region must have\n    no entry block arguments\n\n\n    Example:\n\n    ```mlir\n    irdl.dialect @example {\n      irdl.operation @op_with_regions {\n          %r0 = irdl.region\n          %r1 = irdl.region()\n          %v0 = irdl.is i32\n          %v1 = irdl.is i64\n          %r2 = irdl.region(%v0, %v1)\n          %r3 = irdl.region with size 3\n\n          irdl.regions(foo: %r0, bar: %r1, baz: %r2, qux: %r3)\n      }\n    }\n    ```\n\n    The above snippet demonstrates an operation named `@op_with_regions`,\n    which is constrained to have four regions.\n\n    * Region `foo` doesn't have any constraints on the arguments\n      or the number of blocks.\n    * Region `bar` should have an empty set of arguments.\n    * Region `baz` should have two arguments of types `i32` and `i64`.\n    * Region `qux` should contain exactly three blocks.",
    "inputs": [
      { "name": "entryBlockArgs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "IRDL_RegionType" }
    ],
    "attributes": [
      { "name": "numberOfBlocks", "type": "OptionalAttr" },
      { "name": "constrainedArguments", "type": "UnitAttr" }
    ],
    "assemblyFormat": "``(`(` $entryBlockArgs $constrainedArguments^ `)`)?\n    ``(` ` `with` `size` $numberOfBlocks^)? attr-dict"
  },
  {
    "name": "irdl.regions",
    "summary": "Define the regions of an operation",
    "description": "`irdl.regions` defines the regions of an operation by accepting\n    values produced by `irdl.region` operation as arguments. Each\n    region has an identifier as name.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @example {\n      irdl.operation @op_with_regions {\n        %r1 = irdl.region with size 3\n        %0 = irdl.any\n        %r2 = irdl.region(%0)\n        irdl.regions(foo: %r1, bar: %r2)\n      }\n    }\n    ```\n\n    In the snippet above the operation is constrained to have two regions.\n    The first region (`foo`) should contain three blocks.\n    The second region (`bar`) should have one region with one argument.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "names", "type": "StrArrayAttr" }
    ],
    "assemblyFormat": "`` custom<NamedValueList>($args, $names) attr-dict"
  },
  {
    "name": "irdl.results",
    "summary": "Define the results of an operation",
    "description": "`irdl.results` define the results of the `irdl.operation` parent operation\n    definition. Each result is named after an identifier.\n\n    In the following example, `irdl.results` defines the results of the\n    `get_values` operation:\n\n    ```mlir\n    irdl.dialect @cmath {\n\n      irdl.type @complex { /* ... */ }\n\n      /// Returns the real and imaginary parts of a complex number.\n      irdl.operation @get_values {\n        %0 = irdl.any\n        %1 = irdl.parametric @cmath::@complex<%0>\n        irdl.results(re: %0, im: %0)\n        irdl.operands(complex: %1)\n      }\n    }\n    ```\n\n    The operation will expect one operand of the `cmath.complex` type, and two\n    results that have the underlying type of the `cmath.complex`.\n\n    The results can also be marked as variadic or optional:\n    ```mlir\n    irdl.results(foo: %0, bar: single %1, baz: optional %2, qux: variadic %3)\n    ```\n\n    Here, foo and bar are required single results, baz is an optional result,\n    and qux is a variadic result.\n\n    When more than one result is marked as optional or variadic, the operation\n    will expect a 'resultSegmentSizes' attribute that defines the number of\n    results in each segment.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "names", "type": "StrArrayAttr" },
      { "name": "variadicity", "type": "VariadicityArrayAttr" }
    ],
    "assemblyFormat": "`` custom<NamedValueListWithVariadicity>($args, $names, $variadicity) attr-dict"
  },
  {
    "name": "irdl.type",
    "summary": "Define a new type",
    "description": "`irdl.type` defines a new type belonging to the `irdl.dialect` parent.\n\n    The type parameters can be defined with an `irdl.parameters` operation in\n    the optional region.\n\n    Example:\n\n    ```mlir\n    irdl.dialect @cmath {\n      irdl.type @complex {\n        %0 = irdl.is i32\n        %1 = irdl.is i64\n        %2 = irdl.any_of(%0, %1)\n        irdl.parameters(%2)\n      }\n    }\n    ```\n\n    The above program defines a type `complex` inside the dialect `cmath`. The\n    type has a single parameter that should be either `i32` or `i64`.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "$sym_name attr-dict-with-keyword custom<SingleBlockRegion>($body)"
  },
  {
    "name": "iree_codegen.extract_strided_metadata",
    "summary": "Extracts a buffer base with offset and strides.",
    "description": "This op is implemented similarly to the upstream MemRef::ExtractStridedMetadataOp\n    with the following differences.\n\n    1. It does not fold away static offset/stride information.\n    Hence unlike the upstream Op the link between the memref and consumers of the\n    metadata is not broken when later passes change this information. A common\n    example in IREE of this is buffer binding optimizations.\n\n    2. Helper functions getConstifiedMixed{Offset|Strides|Sizes} are not implemented\n    as the expectation is you should lower to the upstream op before using those\n    functions if you need them.\n\n    Copy of MemRef::ExtractStridedMetadataOp description for reference below.\n    Extracts a base buffer, offset and strides. This op allows additional layers\n    of transformations and foldings to be added as lowering progresses from\n    higher-level dialect to lower-level dialects such as the LLVM dialect.\n\n    The op requires a strided memref source operand. If the source operand is not\n    a strided memref, then verification fails.\n\n    This operation is also useful for completeness to the existing memref.dim op.\n    While accessing strides, offsets and the base pointer independently is not\n    available, this is useful for composing with its natural complement op:\n    `memref.reinterpret_cast`.\n\n    Intended Use Cases:\n\n    The main use case is to expose the logic for manipulate memref metadata at a\n    higher level than the LLVM dialect.\n    This makes lowering more progressive and brings the following benefits:\n      - not all users of MLIR want to lower to LLVM and the information to e.g.\n        lower to library calls---like libxsmm---or to SPIR-V was not available.\n      - foldings and canonicalizations can happen at a higher level in MLIR:\n        before this op existed, lowering to LLVM would create large amounts of\n        LLVMIR. Even when LLVM does a good job at folding the low-level IR from\n        a performance perspective, it is unnecessarily opaque and inefficient to\n        send unkempt IR to LLVM.",
    "inputs": [
      { "name": "source", "type": "AnyStridedMemRef" }
    ],
    "outputs": [
      { "name": "base_buffer", "type": "AnyStridedMemRefOfRank" },
      { "name": "offset", "type": "Index" },
      { "name": "sizes", "type": "Variadic" },
      { "name": "strides", "type": "Variadic" }
    ],
    "assemblyFormat": "$source `:` type($source) `->` type(results) attr-dict"
  },
  {
    "name": "iree_codegen.inner_tiled",
    "summary": "Models an inner-tiled operation that may perform contractions.",
    "description": "Represents an operation that updates \"inner tiles\" (vector slices) of its accumulator\n    outputs using inner tiles of its input operands. The way the inner tiles are\n    used is determined by the semantics of the `kind` attribute. These inner tiles\n    are the trailing dimensions of the tensor/vector operands that are not present\n    in the indexing maps.\n\n    In the case of a matrix-multiply-accumulate (MMA) inner tiled operation,\n    the semantics logically match `vector.contraction`. However, instead of a\n    combiner type, it has a intrinsic description that specifies how the inner tiles\n    are combined.\n\n    Similar to `vector.contract`, an iterator type attribute list must be\n    specified, where each element of the list represents an iterator over one\n    of the outer dimensions. Iteration of inner dimensions is defined solely by\n    the intrinsic and may be opaque.\n\n    An indexing map attribute list must be specified with an entry for input and\n    output arguments. An indexing map attribute specifies a mapping from each\n    outer loop iterator in the iterator type list, to each dimension of each\n    operand.\n\n    Example:\n\n    ```mlir\n    #contraction_accesses = [\n     affine_map<(i, j, k) -> (i, k)>,\n     affine_map<(i, j, k) -> (k, j)>,\n     affine_map<(i, j, k) -> (i, j)>\n    ]\n    #contraction_trait = {\n      indexing_maps = #contraction_accesses,\n      iterator_types = [\"parallel\", \"parallel\", \"reduction\"],\n      kind = #iree_gpu.mma_layout<MFMA_F32_16x16x16_F16>,\n      distributed = true,\n      opaque = false\n    }\n    %3 = iree_codegen.inner_tiled ins(%0, %1) outs(%2) #contraction_trait\n      : vector<2x3x4xf16>, vector<3x5x4xf16> into vector<2x5x4xf32>\n\n    // Takes tensors as well, however the inner dimensions must always be\n    // static.\n    %7 = iree_codegen.inner_tiled ins(%4, %5) outs(%6) #contraction_trait\n      : tensor<?x?x4xf16>, tensor<?x?x4xf16> into tensor<?x?x4xf32>\n    ```\n\n    The example above can be logically lowered directly to loops like this\n    (ignoring type conversions from tensor to vector needed for the mfma).\n    ```\n    %outer_m = tensor.dim %6, %c0 : index\n    %outer_n = tensor.dim %6, %c1 : index\n    %outer_k = tensor.dim %4, %c1 : index\n    %7 = scf.for %i = %c0 to %outer_m iter_args(%arg0 = %6) {\n      %8 = scf.for %j = %c0 to %outer_n iter_args(%arg1 = %arg0) {\n        %9 = scf.for %k = %c0 to %outer_k iter_args(%arg2 = %arg1) {\n          %lhs = tensor.extract_slice %4 [%i, %k, 0] [1, 1, 4] [1, 1, 1] : tensor<4xf16>\n          %rhs = tensor.extract_slice %5 [%k, %j, 0] [1, 1, 4] [1, 1, 1] : tensor<4xf16>\n          %acc = tensor.extract_slice %arg2 [%i, %j, 0] [1, 1, 4] [1, 1, 1] : tensor<4xf32>\n          %res = amdgpu.mfma %lhs, %rhs, %acc : tensor<4xf32>\n          %ret = tensor.insert_slice %acc into %arg2 [%i, %j, 0] [1, 1, 4] [1, 1, 1] : tensor<?x?x4xf32>\n          scf.yield %ret : tensor<?x?x4xf32>\n        }\n        scf.yield %9 : tensor<?x?x4xf32>\n      }\n      scf.yield %8 : tensor<?x?x4xf32>\n    }\n    ```\n\n    Or alternatively unrolled to a single intrinsic when operating on vectors.\n    ```mlir\n    #contraction_accesses = [\n     affine_map<() -> ()>,\n     affine_map<() -> ()>,\n     affine_map<() -> ()>\n    ]\n    #contraction_trait = {\n      indexing_maps = #contraction_accesses,\n      iterator_types = [],\n      kind = #iree_gpu.mma_layout<MFMA_F32_16x16x16_F16>,\n      distributed = true,\n      opaque = false\n    }\n    %3 = iree_codegen.inner_tiled ins(%0, %1) outs(%2) #contraction_trait\n      : vector<4xf16>, vector<4xf16> into vector<4xf32>\n    ```\n\n    This operation can represent an intrinsic both in undistributed\n    (workgroup/subgroup/warp) and distributed (thread) level. This is indicated\n    by the `distributed` boolean attribute. The same `kind` attribute may be\n    used in both cases.\n\n    The inner-tile shapes of tensor operands (defined as above as the trailing\n    dimensions not present in the indexing maps) must agree with the shapes of\n    the vector tiles implied by the intrinsic specified in `kind`. The exact\n    criterion depends on the `opaque` boolean attribute:\n    * When `opaque = true`, the only requirement is that they have the same\n      number of elements.\n    * When `opaque = false`, the shapes must match exactly after omitting any\n      unit-dimensions.\n\n    Note that different `kind` attributes may adopt different conventions as to\n    the intrinsic vector tile formats. Some may use only flattened 1-D vectors,\n    and it may be necessary to use `opaque = true` to accomodate them. Others\n    may use higher-rank vectors with shapes reflecting semantics, allowing to\n    use `opaque = false`, which is preferable when possible at all thanks to the\n    stricter semantics. However, that is not only a function of the `kind`\n    attribute: using `opaque = false` also requires the tensor operands to have\n    appropriate shapes.\n\n    In some cases, variations on the inner tiled operations can be expressed\n    with the permutations attribute. This attribute represents the permutation\n    from that intrinsic's \"canonical\" layout (in the case of matrix multiplication,\n    this is row-major storage of the inner tile) to the format of the inner tile\n    in the arguments, with a permutation specified for each argument.\n\n    For example, an MMT product of inner dimensions with warp semantics can be\n    represented with the following.\n\n    ```mlir\n    #contraction_accesses = [\n     affine_map<(i, j, k) -> (i, k)>,\n     affine_map<(i, j, k) -> (k, j)>,\n     affine_map<(i, j, k) -> (i, j)>\n    ]\n    #contraction_trait = {\n      indexing_maps = #contraction_accesses,\n      iterator_types = [\"parallel\", \"parallel\", \"reduction\"],\n      kind = #iree_gpu.mma_layout<MFMA_F32_16x16x16_F16>,\n      permutations = [[0, 1], [1, 0], [0, 1]]\n    }\n    %7 = iree_codegen.inner_tiled ins(%4, %5) outs(%6) #contraction_trait\n      : tensor<?x?x16x16xf16>, tensor<?x?x16x16xf16> into tensor<?x?x16x16xf32>\n    ```\n\n    #### Motivation, Design Choices, and Pitfalls\n\n    This operation grew out of a general representation for matrix multiplication\n    intrinsics on GPUs, where the inner tiles would be the tiles of the A, B,\n    and C matrices that were computed by an entire GPU workgroup or subgroup. It\n    is now used for generalizations of such multiplications. Currently,\n    the only usage is for scaled matrix-multiply-accumulate, where block scales\n    must be passed in as additional inputs, but it's possible more uses\n    will be possible in the future.\n\n    The idea behind this operation is to decouple the layout setting/tiling\n    required to target certain intrinsics from the lowering to them. Because\n    typically tiling of this sort happens on tensor operands, however the target\n    intrinsics operate on vectors, we use this operation to bridge the gap. The\n    choice for a shared operation is intended to ease the lowering process and\n    allow for different transformations at different stages of the pipeline\n    without needing to essentially clone this op.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "indexing_maps", "type": "TypedArrayAttrBase" },
      { "name": "iterator_types", "type": "IteratorTypeArrayAttr" },
      { "name": "kind", "type": "IREECodegen_AnyInnerTileDescAttr" },
      { "name": "semantics", "type": "IREECodegen_AnyInnerTiledSemanticscAttr" },
      { "name": "permutations", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`ins` `(` $inputs `)` `outs` `(` $outputs `)` attr-dict\n    `:` type($inputs) `into` type($outputs)"
  },
  {
    "name": "iree_codegen.load_from_buffer",
    "summary": "Loads a tensor from a memref.",
    "description": "Loads a tensor from a memref with a compatible shape and the same element\n    type.",
    "inputs": [
      { "name": "buffer", "type": "AnyStridedMemRef" }
    ],
    "outputs": [
      { "name": "tensor", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "$buffer attr-dict `:` type($buffer) `->` type($tensor)"
  },
  {
    "name": "iree_codegen.null_pointer",
    "summary": "Returns a null_pointer value.",
    "description": "This is meant to be used only as arguments to microkernels.",
    "outputs": [
      { "name": "result", "type": "NullPointer" }
    ],
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "iree_codegen.query_tile_sizes",
    "summary": "Yields tile sizes for the specified tensor type.",
    "description": "For targets where tile sizes can't be resolved at compile time, this\n    operation allows querying the sizes at runtime. Today this only applies\n    to VMVX.",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "tensor_type", "type": "TensorTypeAttr" }
    ],
    "assemblyFormat": "attr-dict $tensor_type `->` type($results)"
  },
  {
    "name": "iree_codegen.store_to_buffer",
    "summary": "Stores a tensor into a memref.",
    "description": "Stores a tensor into a memref with a compatible shape and the same element\n    type.",
    "inputs": [
      { "name": "tensor", "type": "AnyRankedTensor" },
      { "name": "buffer", "type": "AnyStridedMemRef" }
    ],
    "assemblyFormat": "$tensor `,` $buffer\n    attr-dict `:` type($tensor) `into` type($buffer)"
  },
  {
    "name": "iree_codegen.swizzle_hint",
    "summary": "Hint to swizzle accesses according to an access pattern.",
    "description": "Optimization hint to swizzle all accesses to the memref this takes a view\n    of. This only affects reads/writes immediately consuming this operation and\n    is best effort. If the desired swizzling is not apparently possible, this\n    op will no-op. As a result, it should not be relied on for correctness.\n\n    Any subviews on this operation will cause swizzling to fail. The expectation\n    is for all view like operations to fold into the accessing ops\n    (loads/stores) before this op takes effect.\n\n    Note that this only rewrites direct users. If there are any aliased loads\n    or stores of the data from/to the |src| memref of a hintOp, those accesses\n    will not be swizzled. This allows reusing an allocation with different\n    swizzled access patterns as long as there is no data dependency between\n    memory with different layouts. For example:\n\n    ```\n    %0 = alloc()\n    %1 = iree_codegen.swizzle_hint %0, #layout_0\n    %2 = iree_codegen.swizzle_hint %0, #layout_1\n    {\n       vector.store %1\n       vector.load %1\n         ^\n         |\n        unrelated\n         |\n         v\n       vector.store %2\n       vector.load %2\n    }\n    ```\n\n    If there is a data dependency between the accesses of %1 and %2, for example\n    a value stored to %1 is loaded from %2, this is undefined behavior. Aliasing\n    is otherwise perfectly legal.",
    "inputs": [
      { "name": "operand", "type": "MemRefRankOf" }
    ],
    "outputs": [
      { "name": "result", "type": "MemRefRankOf" }
    ],
    "attributes": [
      { "name": "swizzle", "type": "IREECodegen_AnySwizzleAttr" }
    ],
    "assemblyFormat": "$operand `[` $swizzle attr-dict `]` `:` type($result)"
  },
  {
    "name": "iree_encoding.set_encoding",
    "summary": "Perform pack and pad operation on source.",
    "description": "Operation to assign an encoding to a tensor. The operation does not change\n    the rank or extent of a tensor. Instead it adds a LayoutResolverAttr\n    attribute to the tensor type to represent a change in layout.",
    "inputs": [
      { "name": "source", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "attr-dict $source `:` type($source) `->` type($result)"
  },
  {
    "name": "iree_encoding.unset_encoding",
    "summary": "Perform unpack and extract operation on source.",
    "description": "Operation to convert a tensor with LayoutResolverAttr encoding that\n    represents its data layout into a tensor with default layout\n    (i.e. no encoding). For now in IREE the default layout is row-major.",
    "inputs": [
      { "name": "source", "type": "AnyRankedTensor" },
      { "name": "result_dims", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "attr-dict $source `:` type($source) `->` type($result) (`` `{` $result_dims^ `}`)?"
  },
  {
    "name": "iree_gpu.barrier_region",
    "summary": "Synchronizes workers on a region of shared code.",
    "description": "This op is designed to represent synchronization of workers on the operands\n    and results of the given region. This operation naturally arises when combining\n    the regions of producer-consumer `scf.forall` operations that share a\n    mapping type.\n\n    For example, consider the following pair of parallel loops.\n    ```mlir\n      %0 = scf.forall (%idy, %idx) in (2, 32) shared_outs(%init = %empty) -> (tensor<4x128xf32>) {\n        %in = ...\n        %2 = affine.apply #affine_map<(d0) -> (d0 * 2)> (%idy)\n        %3 = affine.apply #affine_map<(d0) -> (d0 * 4)> (%idx)\n        scf.forall.in_parallel {\n          tensor.parallel_insert_slice %in into %init[%2, %3] [2, 4] [1, 1]\n            : tensor<2x4xf32> into tensor<4x128xf32>\n        }\n      } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}\n      %1 = scf.forall (%idy, %idx) in (8, 8) -> (tensor<128x128xf32>) {\n        %4 = affine.apply #affine_map<(d0) -> (d0 * 16)> (%idx)\n        %extracted_slice = tensor.extract_slice %0[0, %4] [4, 16] [1, 1]\n          : tensor<4x128xf32> to tensor<4x16xf32>\n        ...\n      } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}\n    ```\n\n    Because these loops share the same worker type and total count, the bodies\n    of these two loops can be merged with a barrier an insert_slice and a\n    shuffle where the boundary of the loops currently is.\n\n    ```mlir\n      %0 = scf.forall (%idy, %idx) in (8, 8) -> (tensor<4x128xf32>) {\n        %alloc = bufferization.alloc_tensor {memory_space = #gpu.address_space<workgroup>}\n          : tensor<4x128xf32>\n        %barrier = iree_gpu.barrier_region %alloc {\n        ^bb0(%shared: tensor<4x128xf32>):\n          %ids = affine.delinearize_index %idy * 8 + %idx to (2, 32) : index\n          %in = ...\n          %2 = affine.apply #affine_map<(d0) -> (d0 * 2)> (%ids#0)\n          %3 = affine.apply #affine_map<(d0) -> (d0 * 4)> (%ids#1)\n          %inserted_slice = tensor.insert_slice %in into %shared[%2, %3] [2, 4] [1, 1]\n            : tensor<2x4xf32> to tensor<4x128xf32>\n          iree_gpu.yield %slice : tensor<4x16xf32>\n        } : tensor<4x128xf32> -> tensor<4x16xf32>\n        %4 = affine.apply #affine_map<(d0) -> (d0 * 16)> (%idx)\n        %slice = tensor.extract_slice %barrier[0, %4] [4, 16] [1, 1] : tensor<4x128xf32> to tensor<4x16xf32>\n        ...\n      } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}\n    ```\n\n    A barrier_region can be lowered to two barriers, one on the input operands\n    and a second one on the results.\n\n    Movtivation and Intended Use Cases:\n\n    The primary way this op is generated is when fusing parallel loops with\n    tensor results. This operation helps to make lowerings more progressive\n    and flexible.\n      - Lowering directly to an alloc + reads and writes breaks the dependency\n        chain making transformations like barrier placement and pipelining\n        potentially more difficult.\n      - Allows the option of non-vector based lowering paths.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "(`ins` `(` $inputs^ `:` type($inputs) `)` )?\n    $region attr-dict `:` type($results)"
  },
  {
    "name": "iree_gpu.buffer_resource_cast",
    "summary": "Represents a cast to addr_space<7> (buffer resource) before bufferization.",
    "description": "Nominal cast of a tensor to AMDGPU buffer resource memory space before\n    bufferization. This op takes the parameters with which to perform the cast\n    if |input| bufferizes to `storage_buffer` memory space. If |input| resolves\n    to any other memory space this op is silently dropped and has no effect.\n\n    If |cache_swizzle_stride| is present, there is verification before\n    bufferization that all producers of |input| are view-like and single source\n    and user (i.e. trivially no alias). In all other cases this op is best\n    effort and has no verification or failure modes.\n\n    // TODO: Add other parameters for casting as needed.",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "cache_swizzle_stride", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "$input oilist (`cacheSwizzleStride` `(` $cache_swizzle_stride `)` )\n    attr-dict `:` type($result)"
  },
  {
    "name": "iree_gpu.coalesced_gather_dma",
    "summary": "Coalesced gather DMA operation for efficient GPU memory access",
    "description": "Performs a coalesced gather operation.\n    This operation can exist in two forms: a tensor-based (value-semantic) form\n    and a buffer-based (memref-semantic) form.\n\n    In both forms, it reads elements from a source operand based on indices.\n    * When the source is a tensor, the operation produces a new result tensor\n      containing the gathered data.\n    * When the source is a memref, the operation writes the gathered data into\n      a destination out memref operand.\n\n    The operation is specifically designed for subgroup-level parallelism, where\n    threads within a subgroup cooperatively gather data with coalesced memory\n    accesses. It implements ParallelCombiningOpInterface and must live inside an\n    op implementing `InParallelOpInterface`, such as `scf.forall.in_parallel`.\n\n    ## Lowering Paths\n\n    Two lowering strategies are supported:\n    1. Lowers to `amdgpu.gather_to_lds` operations when lowering requirements\n       are met.\n    2. Default lowering using `vector.gather` operations.\n\n    ## Operands and Results\n\n    * `$indices`: Tensor/memref of index type specifying gather locations in\n      the source tensor\n    * `$source`: Source tensor/memref containing the data to be gathered\n    * `$init`: Destination tensor/memref receiving the gathered data\n      (destination-passing style)\n    * `$result`: Output tensor/memref with gathered data (same type as `$init`)\n\n    ## Example\n\n    The following example shows how this op is designed to be used in a tiled\n    scenario, which sets up lowering path for efficient gathering.\n\n    1. Outer `scf.forall` represents workgroup-level parallelism.\n    2. Inner `scf.forall` represents subgroup-level parallelism.\n    3. Each thread in the subgroup coalesces its memory accesses when\n       gathering data and writes to its lane-offset in the destination tensor.\n\n    ```mlir\n    %result = scf.forall (%wg_i, %wg_j) in (16, 1) shared_outs(%wg_out = %dest) -> (tensor<128x16xf32>) {\n      %indices_wg_slice = tensor.extract_slice ...\n      %dest_wg_slice = tensor.extract_slice ...\n\n      %inner_result = scf.forall (%sg_i, %sg_j) in (32, 1) shared_outs(%sg_out = %dest_wg_slice) -> (tensor<8x16xf32>) {\n        scf.forall.in_parallel {\n          iree_gpu.coalesced_gather_dma %indices_wg_slice, %source into %sg_out : ...\n        }\n      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}\n\n      scf.forall.in_parallel {\n        tensor.parallel_insert_slice %inner_result into %wg_out[...] : ...\n      }\n    } {mapping = [#gpu.warp<linear_dim_1>, #gpu.warp<linear_dim_0>]}\n    ```",
    "inputs": [
      { "name": "indices", "type": "AnyTypeOf" },
      { "name": "source", "type": "AnyRankedTensorOrMemRef" },
      { "name": "init", "type": "AnyRankedTensorOrMemRef" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensorOrMemRef" }
    ],
    "assemblyFormat": "$indices `,` $source `into` $init attr-dict\n    `:` type($indices) `,` type($source) `,` type($init) `->` type($result)"
  },
  {
    "name": "iree_gpu.global_load_dma",
    "summary": "Does a global load DMA operation",
    "description": "This operation represents a subgroup-level global load DMA operation.\n    It is used to represent a direct gathering operation from global memory to workgroup.\n    To be specific, the thread gathers data from the global memoryspace at the designated\n    indices, and stores it to the thread's lane-offset of the workgroup memref at the\n    designated indices.\n\n    Specifically, if the thread's subgroup lane id is `lane_id`, the thread will load the data\n    from `$source[sourceIndices]` and store it to `$target[targetIndices] + lane_id`.\n    Collectively, all threads in the subgroup orchestrate the load DMA operation.\n\n    Note: each gather has a load width is 32bit.",
    "inputs": [
      { "name": "source", "type": "AnyMemRef" },
      { "name": "sourceIndices", "type": "Variadic" },
      { "name": "target", "type": "AnyMemRef" },
      { "name": "targetIndices", "type": "Variadic" }
    ],
    "assemblyFormat": "$source`[` $sourceIndices `]` `->` $target `[` $targetIndices `]` attr-dict\n      `:` type($source) `->` type($target)"
  },
  {
    "name": "iree_gpu.value_barrier",
    "summary": "Synchronizes workers on a value semantic tensor or vector.",
    "description": "This operation acts as a barrier on a value semantic SSA values (tensor or\n    vector). It takes multiple operands and produces a value equivalent to each\n    input. This does not have copy and/or data movement semantics and simply\n    represents a barrier on all writes in the tensor case, and a barrier until\n    all threads acquire the input vector in the vector case.\n\n    The inputs must be either all tensors, or all vectors.\n\n    This operation is a no-op when not present in a parallel context. This\n    operation is pure as it only requires synchronization for the value it\n    produces.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$inputs attr-dict `:` type($inputs)"
  },
  {
    "name": "iree_gpu.yield",
    "summary": "Yield values from a iree_gpu region.",
    "description": "This operation is used to yield values from a within a region.",
    "inputs": [
      { "name": "values", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($values^ `:` type($values))?"
  },
  {
    "name": "iree_linalg_ext.arg_compare",
    "summary": "Performs an arg-reduction using a user-defined comparator.",
    "description": "The `arg_compare` op performs a reduction over a given dimension of a tensor,\n    returning both the selected value and its corresponding index. The selection\n    logic is defined by a user-specified comparator region.\n\n    The comparator region receives two candidate values and returns a single `i1`\n    result indicating whether the first argument should be preferred over the second.\n\n    This region defines the sorting rule, e.g., \"greater than\" for argmax or\n    \"less than\" for argmin. It allows for generalization beyond simple argmax-style\n    behavior.\n\n    Example (argmax over dim 1):\n    ```mlir\n    %input = memref<2x10xf32>\n    %out_val = memref<2xf32>\n    %out_idx = memref<2xi32>\n    iree_linalg_ext.arg_compare\n      dimension(1)\n      ins(%input : memref<2x10xf32>)\n      outs(%out_val, %out_idx : memref<2xf32>, memref<2xi32>) {\n    ^bb0(%a: f32, %b: f32):\n      %cmp = arith.cmpf ogt, %a, %b : f32\n      iree_linalg_ext.yield %cmp : i1\n    }\n    ```\n\n    Example with index_base = 5 (i.e., indices start counting from 5):\n    ```mlir\n    %input = memref<2x10xf32>\n    %out_val = memref<2xf32>\n    %out_idx = memref<2xi32>\n    %base = arith.constant 5 : index\n    iree_linalg_ext.arg_compare\n      dimension(1)\n      ins(%input : memref<2x10xf32>)\n      outs(%out_val, %out_idx : memref<2xf32>, memref<2xi32>)\n      index_base(%base : index) {\n    ^bb0(%a: f32, %b: f32):\n      %cmp = arith.cmpf ogt, %a, %b : f32\n      iree_linalg_ext.yield %cmp : i1\n    }\n    ```\n\n    The `index_base` is optional. When specified, it is added to the selected index\n    in the result, which is useful when reducing over a tiled or sliced subregion.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" },
      { "name": "index_base", "type": "Optional" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "I64Attr" }
    ],
    "assemblyFormat": "attr-dict\n    `dimension` `(` $dimension `)`\n    (`ins` `(` $inputs^ `:` type($inputs) `)`)?\n    `outs` `(` $outputs `:` type($outputs) `)`\n    (`index_base` `(` $index_base^ `:` type($index_base) `)`)?\n    $region (`->` type($results)^)?"
  },
  {
    "name": "iree_linalg_ext.attention",
    "summary": "Attention operator.",
    "description": "Computes the scaled dot product attention function:\n\n    attention(Q, K, V, scale) = softmax(Q @ K.T * scale) @ V\n\n    Here Q, K, V are given tensors and scale is a scalar value specifying\n    the scale to use.\n\n    If an additional mask argument M is included, the result of the first matmul is modified according to:\n\n    Q @ K.T += M",
    "inputs": [
      { "name": "query", "type": "AnyShaped" },
      { "name": "key", "type": "AnyShaped" },
      { "name": "value", "type": "AnyShaped" },
      { "name": "scale", "type": "AnyFloat" },
      { "name": "mask", "type": "Optional" },
      { "name": "output", "type": "AnyShaped" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "indexing_maps", "type": "AffineMapArrayAttr" },
      { "name": "decomposition_config", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "attr-dict\n    `ins` `(` $query `,` $key `,` $value `,` $scale (`,` $mask^)?  `:` type($query) `,` type($key) `,` type($value) `,` type($scale) (`,` type($mask)^ )?`)`\n    `outs` `(` $output `:` type($output) `)`\n    $region\n    (`->` type($results)^)?"
  },
  {
    "name": "iree_linalg_ext.custom_op",
    "summary": "Custom operation for compiling with IREE.",
    "description": "This operation is meant to allow computation sequences that are fused at\n    tile level prescriptively. This is to account for cases where such fusion\n    cannot/is not yet discovered appropriately.\n\n    The operation implements all the interfaces needed to be able to\n    1. Compile e2e using IREE\n    2. Still be able to fuse with other operations that the compiler can\n       figure out automatically.\n\n    Similar to how `LinalgOp`s represent a perfectly nested loop computation\n    with\n    - `indexing_maps` representing how the `ins`/`outs` are accessed\n    - `region` representing the scalar computation performed\n    - `iterator_types` representing the dependence along each iteration space\n      dimension\n    this operation represent a tiled computation with perfectly nested\n    inter-tile loop nest.\n    - `indexing_maps` represent what slices slices of the `ins`/`outs` are\n      needed for each iteration of the tiled computation.\n    - `region` represents the tiled computation performed using these slices\n    - `iterator_types` represents the dependence between tiles along each\n      iteration space.\n\n    Some modifications required to handle the tile-level semantics are\n    - Some dimensions of operands might not be accessed by dimensions of the\n      inter-tile iteration space. This means that along these dimensions the\n      slice size matches the dimension size. This access pattern of operands\n      is captured in the respective indexing map using a `symbol` to represent\n      that the entire dimension needs to be sliced.\n    - The basic block arguments of the region represent the slice of the\n      operand. These are either scalar types (if the corresponding operand is a\n      scalar), or a `tensor` type with dynamic shapes (if the corresponding\n      operand is a `tensor` type).\n\n    For example, one could represent a prescriptively fused matmul computation\n    as follows\n\n    ```\n    %0:2 = iree_linalg_ext.custom_op {\n        indexing_maps = [affine_map<(d0, d1)[s0, s1] -> (d0, s0)>,\n                         affine_map<(d0, d1)[s0, s1] -> (s0, s1)>,\n                         affine_map<(d0, d1)[s0, s1] -> (s1, d1)>,\n                         affine_map<(d0, d1)[s0, s1] -> (d0, s1)>,\n                         affine_map<(d0, d1)[s0, s1] -> (d0, d1)],\n        iterator_types = [\"parallel\", \"parallel\"]}\n        ins(%lhs1, %rhs1, %rhs2\n            : tensor<1000000x?xf32>, tensor<?x?xf32>, tensor<?x?xf32>)\n        outs(%outs1, %outs2 : tensor<1000000x?xf32>, tensor<1000000x?xf32>) {\n      ^bb0(%t0 : tensor<?x?xf32>, %t1 : tensor<?x?xf32>, %t2 : tensor<?x?xf32>,\n           %t3 : tensor<?x?xf32>, %t4 : tensor<?x?xf32>) :\n        %0 = linalg.matmul ins(%t0, %t1 : tensor<?x?xf32>, tensor<?x?xf32>)\n            outs(%t3 : tensor<?x?xf32>) -> tensor<?x?xf32>\n        %1 = linalg.matmul ins(%0, %t2 : tensor<?x?xf32>, tensor<?x?xf32>)\n            outs(%t4 : tensor<?x?xf32>) -> tensor<?x?xf32>\n        iree_linalg_ext.yield %0, %1 : tensor<?x?xf32>, tensor<?x?xf32>\n    } -> tensor<1000000x?xf32>, tensor<x?xf32>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "indexing_maps", "type": "AffineMapArrayAttr" },
      { "name": "iterator_types", "type": "IREELinalgExt_IteratorTypeArrayAttr" }
    ],
    "assemblyFormat": "`{` `indexing_maps` `=` $indexing_maps `,`\n    `iterator_types` `=` $iterator_types `}`\n    attr-dict-with-keyword\n    (`ins` `(` $inputs^ `:` type($inputs) `)`)?\n    (`outs` `(` $outputs^ `:` type($outputs) `)`)?\n    $region (`->` type($results)^)?"
  },
  {
    "name": "iree_linalg_ext.exp_reduction",
    "summary": "A linalg.generic extension with support for exponential reduction.",
    "description": "This operation is a restricted form of a linalg.generic operation,\n    representing a reduction over `e^{x}` where `x` is an input, and it is known\n    that the function's result is independent of translation over x,\n      i.e. f(x) = f(x + c).\n\n    Since `e{x}` grows exponentially for `x > 0`, if the values of `x` are\n    unbounded, the intermediate computation for `e^{x}` in `f(x)`, may not fit\n    in the floating point range, even if the result does. To prevent such\n    numerical inaccuracies, we can instead compute `f(max(x) - x)`, which will\n    ensure that `e{max(x) - x}` always fits in the range (0, 1], because\n    0 < e{negative} <= 1.\n\n    These properties allow us to compute an online normalization of this\n    function, in which we don't have to compute the `max(x)` as a seperate\n    reduction, but as a running max.\n\n    Generally, these properties are hard to infer from a linalg.generic, because\n    two back-to-back reduction combiners in a linalg.generic can have hard to\n    infer properties. This restricted form ensures we don't have to do\n    complicated analysis to infer these properties.\n\n    The first input and output operands are intrinsically linked.\n    - The first output operand is always the maximum of the first input operand,\n      henceforth 'nmax'.\n    - The first input operand in the basic block is always `2^{nmax-x}`, where\n      `x` is the value\n\n    `norm` is calculated by `2^{nmax - pmax}`, where\n    - `pmax` is the value from the previous iteration.\n    - `nmax` is the new maximum, read from the current element from the input\n      tensor\n\n    The operands listed in `exp_reduced_operands` will be multiplied by `norm`\n    before executing the basic block.\n\n    For example,\n\n    ```mlir\n    // compute the exponentials and the sum\n    %M, %O = iree_linalg_ext.exp_reduction {\n      indexing_maps = [\n        affine_map<(i) -> (i)>,\n        affine_map<(i) -> ()>,\n        affine_map<(i) -> ()>\n      ],\n      iterator_types = [\n        #iree_linalg_ext.iterator_type<reduction>\n      ],\n      exp_reduced_operands = [ 1 ]\n    } ins(%V: tensor<256xf32>)\n      outs(%M_init, %S: tensor<f32>, tensor<f32>) {\n      ^bb0(%exp_v: f32, %m: f32, %s: f32):\n        // take note of the operand names when comparing with the subsequent\n        // code snippets\n        %add = arith.addf %exp_v, %s : f32\n        linalg.yield %m, %add        : f32, f32\n    } -> tensor<f32>, tensor<f32>\n  ```\n\n  Encodes the following computation\n\n  ```\n  %M, %O = linalg.generic {\n    indexing_maps = [\n      affine_map<(i) -> (i)>,\n      affine_map<(i) -> ()>,\n      affine_map<(i) -> ()>\n    ],\n    iterator_types = [\n      \"reduction\"\n    ],\n  } ins(%V: tensor<256xf32>)\n    outs(%M_init, %S: tensor<f32>, tensor<f32>) {\n    ^bb0(%v: f32, %m: f32, %s: f32):\n      %exp_v = math.exp2 %v       : f32\n      %add = arith.addf %norm, %s : f32\n      linalg.yield %m, %add       : f32, f32\n  } -> tensor<256xf32>, tensor<f32>\n  ```\n\n  And transforms it into operations equivalent to the following:\n\n  ```\n  %M, %O = linalg.generic {\n    indexing_maps = [\n      affine_map<(i) -> (i)>,\n      affine_map<(i) -> ()>,\n      affine_map<(i) -> ()>\n    ],\n    iterator_types = [\n      \"reduction\"\n    ],\n  } ins(%V: tensor<256xf32>)\n    outs(%M_init, %S: tensor<f32>, tensor<f32>) {\n    ^bb0(%v: f32, %prev_m: f32, %prev_s: f32):\n      %m      = arith.maximumf %v, %prev_m : f32\n      %m_diff = arith.subf %m, %prev_m     : f32\n      %norm   = math.exp2 %m_diff          : f32\n\n      %exp_v  = math.exp2 %v               : f32  // raise v to the exponent\n      %s      = arith.mulf %prev_s, %norm  : f32  // multiply s by the norm\n      %add    = arith.addf %exp_v, %s      : f32\n      linalg.yield %m, %add                : f32, f32\n  } -> tensor<256xf32>, tensor<f32>\n  ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "indexing_maps", "type": "AffineMapArrayAttr" },
      { "name": "iterator_types", "type": "IREELinalgExt_IteratorTypeArrayAttr" },
      { "name": "exp_reduced_operands", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "`{`\n      `indexing_maps` `=` $indexing_maps `,`\n      `iterator_types` `=` $iterator_types `,`\n      `exp_reduced_operands` `=` $exp_reduced_operands\n    `}`\n    attr-dict\n    `ins` `(` $inputs `:` type($inputs) `)`\n    `outs` `(` $outputs `:` type($outputs) `)`\n    $region (`->` type($results)^)?"
  },
  {
    "name": "iree_linalg_ext.fft",
    "summary": "Fft operator.",
    "description": "Apply 1D FFT to innermost dim. This is an iterative FFT, not recurrsive.\n    Thus, the bit reversal is assumed applied on the input. The op carries an\n    input -- stage, which indicates the level of reduction loop in the\n    algorithm. It represents the computation body. For more details, see\n    \"Data reordering, bit reversal, and in-place algorithms\" section in\n    https://en.wikipedia.org/wiki/Cooley%E2%80%93Tukey_FFT_algorithm\n\n    The size of innermost dim is expected to be a power of 2.\n\n    It is optional to carry coefficient tensors/buffers as inputs. In this\n    context, they will be the second and third inputs.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict (`ins` `(` $inputs^ `:` type($inputs) `)`)?\n    `outs` `(` $outputs `:` type($outputs) `)`\n    (`:` type($results)^)?"
  },
  {
    "name": "iree_linalg_ext.gather",
    "summary": "Gathers slices from a source based on a tensor of indices.",
    "description": "Takes two inputs (`source` and `indices`) and outputs value (`output`).\n    The operation returns the value at the slices specified by `indices`.\n\n    The size of the `dimension_map` attribute is used to determine how many\n    indices are used to index into `source`, i.e. `index_depth`. The\n    `dimension_map` attribute describes which index value maps to which dimension\n    in the destination.\n\n    This operation preforms the opposite operation of `iree_linalg_ext.scatter`.\n    Instead of scattering `updates` into `original`, it gathers the values from\n    `source` into `output` using the indices in `indices`. See the documentation\n    on `iree_linalg_ext.scatter` for more details regarding the indexing/shape\n    semantics.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension_map", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "attr-dict `dimension_map` `=` $dimension_map\n    (`ins` `(` $inputs^ `:` type($inputs) `)`)?\n    `outs` `(` $outputs `:` type($outputs) `)`\n    (`->` type($results)^)?"
  },
  {
    "name": "iree_linalg_ext.im2col",
    "summary": "Im2col operation for convolutions.",
    "description": "Im2col op for convolutions. The operation performs a transformation on the\n    input to convert it from a convolution input to an equivalent gemm input.\n    The op is defined by its input, output, some conv metadata, and some\n    indexing metadata. The `strides`, `dilations`, and `kernel_size` are taken\n    from the convolution from which this op is generated, and they define how\n    the input operand is indexed when the operation is decomposed. The shape of\n    the output should be `tensor<BxMxK>`, and the `m_pos`, `k_pos`, and\n    `batch_pos` indicate which input dimensions map to which output dimensions.\n\n    The `k_offset` is an offset within the output K dimension from which the\n    iteration space of the operation begins. This is used for tiling, since the\n    tiled implementation must leave the output K dimension untiled. Similarly,\n    `m_offset` is the offset within the output M dimension from which the\n    iteration space of the operation begins.\n    The iteration space is the full output shape of the im2col op, so if the\n    im2col op were tiled to loops with a scalar inner tile, it would look like\n    the following:\n    ```\n      %im2col = iree_linalg_ext.im2col\n          strides = [1, 1] dilations = [1, 1] kernel_size = [3, 3]\n          m_offset = [0] * [1] k_offset = [0] * [1]\n          batch_pos = [0] m_pos = [1, 2] k_pos = [3]\n          ins(%in : tensor<2x34x34x640xf32>)\n          outs(%out : tensor<2x1024x5760xf32>) -> tensor<2x1024x5760xf32>\n    ```\n    becomes:\n    ```\n      scf.for %arg0 = %c0 to %c2 step %c1\n        scf.for %arg1 = %c0 to %c1024 step %c1\n          scf.for %arg2 = %c0 to %c5760 step %c1\n            %im2col = iree_linalg_ext.im2col\n                strides = [1, 1] dilations = [1, 1] kernel_size = [3, 3]\n                m_offset = [%arg1] * [1] k_offset = [%arg2] * [1]\n                batch_pos = [0] m_pos = [1, 2] k_pos = [3]\n                ins(%in_tile : tensor<1x34x34x640xf32>)\n                outs(%out_tile : tensor<1x1x1xf32>) -> tensor<1x1x1xf32>\n    ```\n    Then, when the tiled op is decomposed, it becomes a loop over the iteration\n    space of the im2col op, whith an extract_slice from the `%in_tile` followed\n    by an insert_slice to the `%out_tile`. The indices for the extract slice are\n    computed using the `m_offset` and `k_offset` as:\n    (b, m, k) -> (b, M / 32 + K / (640*3), M % 32 + K % (640*3) / 640, K % 640)\n    Where `(b, m, k)` are the indices of the tiled op's iteration space, and\n    `M = m + m_offset` and `K = k + K_offset`.\n\n    The `m_strides` and `k_strides` fields are used as a basis for linearizing\n    the `m_offset` and `k_offset`. This is used when there are multiple M or K\n    output dimensions, and therefore multiple `m_offset` or `k_offset` values.\n    The strides fields are assembled in the IR as if they are multiplied as an\n    inner product with `m_offset` and `k_offset, indicating that the total\n    linear offset along the dimension is equal to this inner product. These\n    strides fields also determine the strides of the output dimensions along\n    M and K. For example, an op with `m_strides = [32, 1]`, `k_strides = [4, 1]`,\n    and output type `tensor<BxM0xM1xK0xK1>` (expanded from `tensor<BxMxK>`),\n    would have strides along the M dim of 32 for `M0`, meaning as `M0` increases\n    by 1, the index into the flat `M` increases by 32. Along the K dim, strides\n    would be 4 for `K0`, and 1 for `K1`, meaning as `K0` increases by 1, the\n    index into the flat `K` increases by 4. The strides in M from `m_strides`\n    are orthogonal to the strides in `K` from `k_strides`.\n\n    The `input_k_perm` attribute defines the permutation needed to align the\n    reduction dimensions of the input layout with those of the filter layout\n    when computing the K dimension of the im2col output. This is useful when the\n    layout of the filter (e.g., `CHW`) differs from that of the input (e.g., `HWC`).\n    For instance, an `input_k_perm = [2, 0, 1]` indicates the input indices needs\n    to be transposed from `HWC` to `CHW` layout before extracting slices during\n    decomposition. The identity permutation (e.g., input_k_perm = [0, 1, 2])\n    indicates that the input layout is already aligned with the filter layout\n    in terms of reduction dimensions, so no transposition of indices is necessary\n    before slice extraction.",
    "inputs": [
      { "name": "input", "type": "AnyShaped" },
      { "name": "output", "type": "AnyShaped" },
      { "name": "kernel_size", "type": "Variadic" },
      { "name": "m_offset", "type": "Variadic" },
      { "name": "m_strides", "type": "Variadic" },
      { "name": "k_offset", "type": "Variadic" },
      { "name": "k_strides", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "strides", "type": "DenseI64ArrayAttr" },
      { "name": "dilations", "type": "DenseI64ArrayAttr" },
      { "name": "static_kernel_size", "type": "DenseI64ArrayAttr" },
      { "name": "static_m_offset", "type": "DenseI64ArrayAttr" },
      { "name": "static_m_strides", "type": "DenseI64ArrayAttr" },
      { "name": "static_k_offset", "type": "DenseI64ArrayAttr" },
      { "name": "static_k_strides", "type": "DenseI64ArrayAttr" },
      { "name": "batch_pos", "type": "DenseI64ArrayAttr" },
      { "name": "m_pos", "type": "DenseI64ArrayAttr" },
      { "name": "k_pos", "type": "DenseI64ArrayAttr" },
      { "name": "input_k_perm", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "attr-dict\n    `strides` `=` $strides\n    `dilations` `=` $dilations\n    `kernel_size` `=`\n    custom<DynamicIndexList>($kernel_size, $static_kernel_size)\n    `m_offset` `=`\n    custom<DynamicIndexList>($m_offset, $static_m_offset)\n    `*` custom<DynamicIndexList>($m_strides, $static_m_strides)\n    `k_offset` `=`\n    custom<DynamicIndexList>($k_offset, $static_k_offset)\n    `*` custom<DynamicIndexList>($k_strides, $static_k_strides)\n    `batch_pos` `=` $batch_pos\n    `m_pos` `=` $m_pos\n    `k_pos` `=` $k_pos\n    `input_k_perm` `=` $input_k_perm\n    `ins` `(` $input `:` type($input) `)`\n    `outs` `(` $output `:` type($output) `)`\n    (`->` type($results)^)?"
  },
  {
    "name": "iree_linalg_ext.map_scatter",
    "summary": "Scatter with a mapping from source indices to result indices.",
    "description": "Takes two operands, `input` and `output`, and stores every element of\n    `input` to a unique location in `output`. If the operands are tensors, the\n    op will also return a Value for the result. For an element of the `input`,\n    the index to store into the `output` is determined by index computation\n    mapping the index in the `input` value to an index in the `output` value.\n    This computation is contained in the `transformation_region`, which contains\n    a single block, with one argument for each dimension in the `input` value.\n    The block arguments represent the index of a given element along the\n    corresponding dimension (i.e., block arg `i` represents the index along\n    dimension `i` of the `input` value). The block is terminated by an\n    iree_linalg_ext.yield op, which yields one index for each dimension in\n    the `output`, and an additional `i1` Value, which represents a mask on\n    whether or not to write to the `output` at the yielded set of indices. Iff\n    the mask value is true, the input value will be written.",
    "inputs": [
      { "name": "input", "type": "AnyShaped" },
      { "name": "output", "type": "AnyRankedTensorOrMemRef" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict $input `into` $output\n    $transformation_region\n    `:` type($input) `into` type($output) (`->` type($results)^)?"
  },
  {
    "name": "iree_linalg_ext.online_attention",
    "summary": "Online Attention operator.",
    "description": "Traditional scaled dot product attention computes:\n\n    attention(Q, K, V, scale) = softmax(Q @ K.T * scale) @ V\n\n    Online Attention on the other hand, uses an online normalizer instead of\n    softmax:\n\n    online_attention(Q, K, V, scale, running_max, running_sum)\n      = online_normalizer(Q @ K.T * scale, running_max, running_sum) @ V\n\n    If an additional mask argument M is included, the result of the first matmul is modified according to:\n\n    Q @ K.T += M\n\n    The advantage of this online_normalizer is that it can be tiled along\n    its reduction dimension, making the online_attention operator:\n      - Tilable along softmax reduction dimension\n      - Associative along softmax reduction dimension\n      - Commutative along softmax associative dimension\n\n    Note: The results of online_attention need to be combined after computing\n    it over the entire softmax reduction dimension by:\n      x, _, sum : results\n      x = (1 / sum) * x",
    "inputs": [
      { "name": "query", "type": "AnyShaped" },
      { "name": "key", "type": "AnyShaped" },
      { "name": "value", "type": "AnyShaped" },
      { "name": "scale", "type": "AnyFloat" },
      { "name": "mask", "type": "Optional" },
      { "name": "output", "type": "AnyShaped" },
      { "name": "max", "type": "AnyShaped" },
      { "name": "sum", "type": "AnyShaped" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "indexing_maps", "type": "AffineMapArrayAttr" },
      { "name": "decomposition_config", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "attr-dict\n    `ins` `(` $query `,` $key `,` $value `,` $scale (`,` $mask^)?  `:` type($query) `,` type($key) `,` type($value) `,` type($scale) (`,` type($mask)^ )?`)`\n    `outs` `(` $output `,` $max `,` $sum `:` type($output) `,` type($max) `,` type($sum) `)`\n    $region\n    (`->` type($results)^)?"
  },
  {
    "name": "iree_linalg_ext.pack",
    "summary": "LinalgExt pack operation for both tensors and buffers.",
    "description": "The pack operation converts an `input` into a tiled and packed layout. The\n    dimensions to be tiled are obtained from `inner_dims_pos` and the size of the\n    tile is obtained from `inner_tiles`. The dimensions listed in `inner_dims_pos`\n    do not need to be contiguous in which case the tile will get transposed.  We\n    handle only full tiles if `padding_value` is not set; it is UB if the tile does\n    not perfectly divide the dimension. If `padding_value` is set, it will pad\n    along high dimensions, i.e., it pads at the bottom and on the right if the\n    input has rank 2, and the result type shape, will be dynamic in any dimension\n    if and only if the input shape is. As optional input, the operation takes\n    `outer_dims_perm` that allows to permute the tiled loops.\n\n    Example KC_to_KCck:\n\n    ```mlir\n    iree_linalg_ext.pack %arg0 inner_dims_pos = [1, 0]\n      inner_tiles = [32, 8] into %arg1 : (memref<128x256xf32> memref<16x8x32x8xf32>)\n    ```\n\n    Example NC_to_NCnc:\n\n    ```mlir\n    iree_linalg_ext.pack %arg0 inner_dims_pos = [0, 1]\n      inner_tiles = [8, 32] into %arg1 : (memref<128x256xf32> memref<16x8x8x32xf32>)\n    ```\n    Example KC_to_CKkc\n\n    ```mlir\n    iree_linalg_ext.pack %arg0 outer_dims_perm = [1, 0] inner_dims_pos = [0, 1]\n      inner_tiles = [32, 8] into %arg1 : (memref<128x256xf32> memref<32x4x32x8xf32>)\n    ```\n\n    In all cases, dimension at position 0 in the input memref (128) is tiled\n    with a factor of 8, while dimension at position 1 (256) is tiled with a factor\n    of 32. In the KC_to_KCck example, the point loops are interchanged, while in the\n    KC_to_CKkc example the tiled loops.\n\n    Example NC_to_NCnc with padding:\n\n    ```mlir\n    iree_linalg_ext.pack %arg padding_value(%pad : f32) inner_dims_pos = [0, 1]\n      inner_tiles = [8, 2] into %arg1 : (memref<13x15xf32> memref<2x8x8x2xf32>)\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" },
      { "name": "inner_tiles", "type": "Variadic" },
      { "name": "padding_value", "type": "Optional" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "outer_dims_perm", "type": "DefaultValuedOptionalAttr" },
      { "name": "inner_dims_pos", "type": "DenseI64ArrayAttr" },
      { "name": "static_inner_tiles", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "attr-dict\n    $inputs\n    (`padding_value` `(` $padding_value^ `:` type($padding_value) `)`)?\n    (`outer_dims_perm` `=` $outer_dims_perm^)?\n    `inner_dims_pos` `=` $inner_dims_pos\n    `inner_tiles` `=`\n    custom<DynamicIndexList>($inner_tiles, $static_inner_tiles)\n    `into` $outputs `:` `(` type($inputs) type($outputs) `)`\n     (`->` type($results)^)?"
  },
  {
    "name": "iree_linalg_ext.scan",
    "summary": "Scan operator.",
    "description": "Computes the inclusive/exclusive scan along a given dimension.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "I64Attr" },
      { "name": "inclusive", "type": "BoolAttr" }
    ],
    "assemblyFormat": "attr-dict\n    `dimension` `(` $dimension `)`\n    `inclusive` `(` $inclusive `)`\n    `ins` `(` $inputs `:` type($inputs) `)`\n    `outs` `(` $outputs `:` type($outputs) `)`\n    $region (`->` type($results)^)?"
  },
  {
    "name": "iree_linalg_ext.scatter",
    "summary": "Scatters an input in slices based on a tensor of indices.",
    "description": "Takes two `inputs` (`update` and `indices`) and `outputs` value (`original`).\n    The operation updates the value at the slices specified by `indices` by\n    combining the current value with the value in `updates` using the computation\n    specified in `region`. The `region` specifies a binary operation\n    of signature `(T, T) -> T`, where `T` is the element-type of\n    `updates` (and `original`). The first argument is from `updates`,\n    and the second is from `original`.\n\n    The size of the `dimension_map` attribute is used to determine how many\n    indices are used to index into `original`, i.e. `index_depth`. The\n    `dimension_map` attribute describes which index value maps to which dimension\n    in the destination.\n\n    The operand `indices` is a N-D tensor/memref type that is composed\n    of two logical parts. The first `N-1` dimensions represent the batch of\n    updates. The last dim (at index `N-1`) is the `index_depth`, which can be\n    omitted if `index_depth` is 1.\n\n    The operand `update` is a M-D tensor/memref type and similarly\n    consists of two parts. The first `N-1` dimensions represent the batch of\n    updates. This must exactly match to the first `N-1` dimensions in `indices`.\n    Dimensions `N..M-1` represent the slice scattered into `original`,\n    `update_slice`, and must match the last dimensions in original. This\n    represents a contiguous slice to be inserted into `original`.\n\n    The operand `original` is a DPS init representing the destination that\n    `update` gets scattered to.\n    Where `rank(original) = rank(update_slice) + index_depth`\n\n    The unique_indices attribute carries the information whether all the\n    indices are unique. If `unique_indices` is `true` and two or more updates\n    scatter to the same location in `original` the final value in `original` is\n    not guaranteed. If `unique_indices` is set to false, the first\n    `batch_rank` iteration loops will be marked as reduction.\n\n    The shapes definition follows tensorflow operations. See more information in\n      https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension_map", "type": "DenseI64ArrayAttr" },
      { "name": "unique_indices", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "attr-dict `dimension_map` `=` $dimension_map\n              `unique_indices` `(` $unique_indices `)`\n    (`ins` `(` $inputs^ `:` type($inputs) `)`)?\n    `outs` `(` $outputs `:` type($outputs) `)`\n    $region (`->` type($results)^)?"
  },
  {
    "name": "iree_linalg_ext.sort",
    "summary": "Sorts a tensor a specified dimension.",
    "description": "Based on XLA operation semantics, sorts the given `operands` at the given\n    `dimension` with the given `comparator`.\n\n    See https://www.tensorflow.org/xla/operation_semantics#sort.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "I64Attr" }
    ],
    "assemblyFormat": "attr-dict\n    `dimension` `(` $dimension `)`\n    (`ins` `(` $inputs^ `:` type($inputs) `)`)?\n    `outs` `(` $outputs `:` type($outputs) `)`\n    $region (`->` type($results)^)?"
  },
  {
    "name": "iree_linalg_ext.topk",
    "summary": "Top-K operator.",
    "description": "A Top-K operation for N-D tensors. Reduces the target dimension from the input\n   size N down to K elements based on the supplied binary region.\n\n   Accepts an N-D tensor input consisting of values and an optioanl N-D tensor\n   for indices of those values (i32 type). If input indices aren't provided, the\n   index mapping is inferred based on the k dim.  Both input values/indices\n   tensors and output values/indicies tensors must have the same shape. Top-K is\n   computed along the target dimension (from dimension()). Returns two output\n   tensors of values and the indicies of Top-K results. The output dimensions\n   must match the input save for the dimension that is reduced to K results.\n\n   Region accepts lhs=[next N input] and rhs=[exiting K output] and yeilds an\n   i1. If true, the two values are swapped:\n     - For Top-K compoarision: >\n     - For Min-K comparision: <\n   Note: when the two values are equal, the first occurence is always selected.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "I64Attr" }
    ],
    "assemblyFormat": "attr-dict\n    `dimension` `(` $dimension `)`\n    `ins` `(` $inputs `:` type($inputs) `)`\n    `outs` `(` $outputs `:` type($outputs) `)`\n    $region (`->` type($results)^)?"
  },
  {
    "name": "iree_linalg_ext.unpack",
    "summary": "LinalgExt unpack operation for both tensors and buffers.",
    "description": "The unpack operation converts a tiled and packed input to an unpacked\n    output. See `pack` for more details on `inner_tiles` and `dims_pos`; it is UB\n    if the tile does not perfectly divide the dimension. Optionally, the operation\n    also supports permuting the tiled loops.\n\n    Example KCck_to_KC:\n\n    ```mlir\n    iree_linalg_ext.unpack %arg0 dims_pos = [1, 0]\n      inner_tiles = [32, 8] into %arg1 : (memref<16x8x32x8xf32> memref<128x256xf32>)\n    ```\n\n    Example NCnc_to_NC:\n\n    ```mlir\n    iree_linalg_ext.unpack %arg0 dims_pos = [0, 1]\n      inner_tiles = [8, 32] into %arg1 : (memref<16x8x8x32xf32> memref<128x256xf32>)\n    ```\n\n    Example CKkc_to_KC:\n\n    ```mlir\n    iree_linalg_ext.unpack %arg1 outer_dims_perm = [1, 0] inner_dims_pos = [0, 1]\n      inner_tiles = [32, 8] into %arg0 : (memref<32x4x32x8xf32> memref<128x256xf32>)\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" },
      { "name": "inner_tiles", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "outer_dims_perm", "type": "DefaultValuedOptionalAttr" },
      { "name": "inner_dims_pos", "type": "DefaultValuedAttr" },
      { "name": "static_inner_tiles", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "attr-dict\n    $inputs\n    (`outer_dims_perm` `=` $outer_dims_perm^)?\n    `inner_dims_pos` `=` $inner_dims_pos\n    `inner_tiles` `=`\n    custom<DynamicIndexList>($inner_tiles, $static_inner_tiles)\n    `into` $outputs `:` `(` type($inputs) type($outputs) `)`\n     (`->` type($results)^)?"
  },
  {
    "name": "iree_linalg_ext.winograd.filter_transform",
    "summary": "Winograd Filter Transform operator.",
    "description": "This operator is part of the first step in converting a convolution to\n    its Winograd equivalent. Given a tile of a convolution filter (F),\n    this operator computes matmul(G, matmul(F, transpose(B))).\n    The filter tile is assumed to be the full m x m convolutional kernel,\n    and the result of the transformation on this tile is a square with each\n    side of size m + r - 1, where the output tile size is r x r. G is a constant\n    2-d matrix of shape (m + r - 1) x m. The input to the operator is a filter\n    of shape (H, W, C, F) or (F, C, H, W) and the output is an operator of shape\n    (m + r - 1, m + r - 1, C, F). The result of this operator is first collapsed\n    and then fed to a batch matmul op.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "output_tile_size", "type": "I64Attr" },
      { "name": "kernel_size", "type": "I64Attr" },
      { "name": "kernel_dimensions", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "attr-dict\n    `output_tile_size` `(` $output_tile_size `)`\n    `kernel_size` `(` $kernel_size `)`\n    `kernel_dimensions` `(` $kernel_dimensions `)`\n    `ins` `(` $inputs `:` type($inputs) `)`\n    `outs` `(` $outputs `:` type($outputs) `)`\n    (`->` type($result)^)?"
  },
  {
    "name": "iree_linalg_ext.winograd.input_transform",
    "summary": "Winograd Input Transform operator.",
    "description": "This operator is part of the first step in converting a convolution to\n    its Winograd equivalent. Given a tile of an input image (I),\n    this operator computes matmul(transpose(B), matmul(I, B)).\n    The input tile is assumed to be square with each side of size m + r - 1,\n    where the convolutional kernel is m x m and the output tile size is r x r.\n    B is a constant 2-d square matrix of the same shape as the input tile I.\n    The input to the operator is an image of shape (N, H, W, C) or (N, C, H, W)\n    and the output is an operator of shape (m + r - 1, m + r - 1, N, H', W', C)\n    where H' = ceil((H - m + 1)/r) and W' = ceil((W - m + 1)/r). The result\n    of this operator is first collapsed and then fed to a batch matmul op.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "output_tile_size", "type": "I64Attr" },
      { "name": "kernel_size", "type": "I64Attr" },
      { "name": "image_dimensions", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "attr-dict\n    `output_tile_size` `(` $output_tile_size `)`\n    `kernel_size` `(` $kernel_size `)`\n    `image_dimensions` `(` $image_dimensions `)`\n    `ins` `(` $inputs `:` type($inputs) `)`\n    `outs` `(` $outputs `:` type($outputs) `)`\n    (`->` type($result)^)?"
  },
  {
    "name": "iree_linalg_ext.winograd.output_transform",
    "summary": "Winograd Output Transform operator.",
    "description": "This operator is the last transform in converting a convolution to\n    its Winograd equivalent. After convolution in the Winograd domain\n    (which turns into an elementwise product for a single channel and\n    batch matrix multiplication for many channels), this operator converts\n    the output back into the original domain. Given a tile of the\n    output (O) in the Winograd domain, this operator computes\n    matmul(transpose(A), matmul(O, A)). The output tile is square with\n    each side of size m + r - 1, where the convolutional kernel is m x m\n    and the output tile size is r x r. A is a constant 2-d matrix of\n    shape (m + r - 1) x r. The input to the operator is a tensor of\n    shape (m + r - 1, m + r - 1, N, H', W', C) and the output is a\n    tensor of shape (N, H, W, C) or (N, C, H, W) where H = r H' and W = r W'.\n    This operator is followed by a tensor.extract_slice which extracts\n    only the non-padded part of the output.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "output_tile_size", "type": "I64Attr" },
      { "name": "kernel_size", "type": "I64Attr" },
      { "name": "image_dimensions", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "attr-dict\n    `output_tile_size` `(` $output_tile_size `)`\n    `kernel_size` `(` $kernel_size `)`\n    `image_dimensions` `(` $image_dimensions `)`\n    `ins` `(` $inputs `:` type($inputs) `)`\n    `outs` `(` $outputs `:` type($outputs) `)`\n    (`->` type($result)^)?"
  },
  {
    "name": "iree_vector_ext.to_layout",
    "summary": "Layout conversion operator.",
    "description": "The layout conversion operator takes a shaped value and a layout and\n    transforms the value to have that layout.\n\n    If the \"shared_memory_conversion\" attribute is set, then this layout\n    change has to be materialized through shared memory.",
    "inputs": [
      { "name": "input", "type": "AnyShaped" },
      { "name": "layout", "type": "VectorLayoutInterface" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyShaped" }
    ],
    "attributes": [
      { "name": "shared_memory_conversion", "type": "DefaultValuedAttr" },
      { "name": "mma_kind", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$input `to` `layout` `(` $layout `)` attr-dict `:` type($input)"
  },
  {
    "name": "iree_vector_ext.to_simd",
    "summary": "SIMT to SIMD conversion operation.",
    "description": "This operation is a temporary operation useful for source/target\n    materializations when doing type conversions between distributed and not\n    distributed vectors.",
    "inputs": [
      { "name": "input", "type": "AnyVectorOfAnyRank" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyVectorOfAnyRank" }
    ],
    "assemblyFormat": "$input attr-dict `:` type($input) `->` type($output)"
  },
  {
    "name": "iree_vector_ext.to_simt",
    "summary": "SIMD to SIMT conversion operation.",
    "description": "This operation is a temporary operation useful for source/target\n    materializations when doing type conversions between distributed and not\n    distributed vectors.",
    "inputs": [
      { "name": "input", "type": "AnyVectorOfAnyRank" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyVectorOfAnyRank" }
    ],
    "assemblyFormat": "$input attr-dict `:` type($input) `->` type($output)"
  },
  {
    "name": "iree_vector_ext.transfer_gather",
    "summary": "Gathers a supervector from memory into an SSA vector value.",
    "description": "The iree_vector_ext.transfer_gather operation provides a structured\n    abstraction for gathers, by preserving the iteration space mapping between\n    the result vector and the memory dimensions being indexed.\n\n    The operation is a generalization of `vector.transfer_read` op, where the\n    slice from which the read is performed is not guranteed to be contiguous,\n    and instead how the slice is gathered is defined explicitly in the\n    operation.\n\n    The operation can be thought of as:\n      1. A contiguous slice gathered from the base as described by the operation\n      2. A `vector.transfer_read` on the contiguous slice\n\n    The operation defines `permutation_map`, `padding`, `mask`, `in_bounds` in\n    the same way as `vector.transfer_read` defines, but on the inferred\n    contiguous slice.\n\n    The other parameters of the operation define how the contiguous slice is\n    gathered from the source. `indices` define a base to offset the source by.\n    `indexed` defines for each dimension if the dimension is gathered or\n    contiguous.\n\n    The `indices` contains a base to offset the source by. The `indexed` array\n    defines if a dimension is gathered or not. For example, for the following\n    gather:\n\n    ```\n    slice[i, j, k] = base[i + i_offset][j][indices[i][j][k]]\n    ```\n\n    The operation would represent this as:\n\n    ```\n    indices = %i_offset, 0, 0\n    indexed = [False, False, True]\n    ```\n\n    For every dimension that is gathered, the operation defines how it is\n    gathered. For each gathered dimension, the operation expects a vector of\n    indices in `index_vecs` to act as a source of indices for that dimension\n    and an AffineMap in `index_maps` describing how this source of indices is\n    indexed. For example, for the following gather:\n\n    ```\n    slice[i, j, k] = base[i][indices0[i] + offset][indices1[j, k]]\n    ```\n\n    The indexing would be described by:\n\n    ```\n    indices      = 0, %offset, 0\n    indexed      = [False, True, True]\n    index_vecs   = %index_vec1, %index_vec2\n    index_maps = [\n      affine_map<(i, j, k) -> (i),\n      affine_map<(i, j, k) -> (j, k)\n    ]\n    ```\n\n    With these additional parameters, the operation can define a supervector\n    read from a non-contiguous slice. For example:\n\n    ```\n    base: memref<8192x8x16xf32>\n    indices0 : vector<2xindex>\n    indices1 : vector<4x8xindex>\n\n    slice[i, j, k] = base[indices0[k]][j][indices1[i, j]]\n    vector = read(slice) : memref<8192x8x16xf32> -> vector<16x8x2xf32>\n    ```\n\n    Can be represented by:\n\n    ```\n    %vector = vector.transfer_gather %base[0, 0, 0](%indices0, %indices1) {\n      gather_dims = [0, 2],\n      index_maps = [\n        affine_map<(i, j, k) -> (k)>,\n        affine_map<(i, j, k) -> (i, j)>\n      ],\n      in_bounds = [true, true, true],\n      permutation_map = affine_map<(i, j, k) -> (k, j, i)>\n    } : memref<8192x8x16xf32> -> vector<16x8x2xf32>\n    ```\n\n    The crucial structure of the operation relies on the index_vec and\n    the result vector's indexing being defined based on the dimensions of the\n    memory. This mapping can be exploited to simplify gathered dimensions\n    to contiguous dimensions.",
    "inputs": [
      { "name": "base", "type": "AnyShaped" },
      { "name": "indices", "type": "Variadic" },
      { "name": "index_vecs", "type": "Variadic" },
      { "name": "padding", "type": "AnyType" },
      { "name": "mask", "type": "Optional" }
    ],
    "outputs": [
      { "name": "vector", "type": "AnyVectorOfAnyRank" }
    ],
    "attributes": [
      { "name": "indexed", "type": "BoolArrayAttr" },
      { "name": "indexed_maps", "type": "AffineMapArrayAttr" },
      { "name": "permutation_map", "type": "AffineMapAttr" },
      { "name": "in_bounds", "type": "BoolArrayAttr" }
    ]
  },
  {
    "name": "krnl.acos",
    "summary": "Krnl acos scalar operation",
    "description": "Krnl acos scalar operation.",
    "inputs": [
      { "name": "in", "type": "AnyFloat" }
    ],
    "outputs": [
      { "name": "out", "type": "AnyFloat" }
    ]
  },
  {
    "name": "krnl.acosh",
    "summary": "Krnl acosh scalar operation",
    "description": "Krnl acosh scalar operation.",
    "inputs": [
      { "name": "in", "type": "AnyFloat" }
    ],
    "outputs": [
      { "name": "out", "type": "AnyFloat" }
    ]
  },
  {
    "name": "krnl.asin",
    "summary": "Krnl asin scalar operation",
    "description": "Krnl asin scalar operation.",
    "inputs": [
      { "name": "in", "type": "AnyFloat" }
    ],
    "outputs": [
      { "name": "out", "type": "AnyFloat" }
    ]
  },
  {
    "name": "krnl.asinh",
    "summary": "Krnl asinh scalar operation",
    "description": "Krnl asinh scalar operation.",
    "inputs": [
      { "name": "in", "type": "AnyFloat" }
    ],
    "outputs": [
      { "name": "out", "type": "AnyFloat" }
    ]
  },
  {
    "name": "krnl.atan",
    "summary": "Krnl atan scalar operation",
    "description": "Krnl atan scalar operation.",
    "inputs": [
      { "name": "in", "type": "AnyFloat" }
    ],
    "outputs": [
      { "name": "out", "type": "AnyFloat" }
    ]
  },
  {
    "name": "krnl.atanh",
    "summary": "Krnl atanh scalar operation",
    "description": "Krnl atanh scalar operation.",
    "inputs": [
      { "name": "in", "type": "AnyFloat" }
    ],
    "outputs": [
      { "name": "out", "type": "AnyFloat" }
    ]
  },
  {
    "name": "krnl.block",
    "summary": "Krnl block operation",
    "description": "Block a single for loop by a constant tile size. For instance,\n    ```\n    $ib, $il = krnl.block %i, 4\n    ```\n    means to block the for loop referred to by %i using a tile size of 4.",
    "inputs": [
      { "name": "loop", "type": "AnyType" }
    ],
    "outputs": [
      { "name": "loop_block", "type": "AnyType" },
      { "name": "loop_local", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "tile_size", "type": "I64Attr" }
    ],
    "assemblyFormat": "$loop $tile_size attr-dict `:` functional-type($loop, results)"
  },
  {
    "name": "krnl.call",
    "summary": "call operation",
    "description": "The call operation provides a generic way to replace an ONNX Op with a call\n    to an external function at Krnl level.\n    `funcName` attributes determines which function to call.\n    `parameters` is the inputs to Krnl.Call. It includes the outputs and inputs\n    of the ONNX Op. The outputs and inputs are already lowered to MemRefs.\n    The external function is assumed NOT to allocate or free any memory.\n    'numOfOutput` attribute to tell how manu outputs Memref in parameters.\n    mlir::OpTrait::AttrSizedOperandSegments is not used to put outputs and\n    inputs into separate variadic parameters because I am thinking of mixing\n    the inputs and outpus as required by external library.\n\n    The attributes of the ONNX Op will be copied to KrnlCallOp under the control\n    of the user.\n    In Krnl To llvm lowering, the parameters and attributes will be lowered to\n    parameters of the llvm function call.\n\n    Several builder is defined to help translating an ONNX Op to Krnl.Call.\n    User can provides the allocated MemRefs for outputs and the inputs\n    separately. The inputs are usually the operands of the ONNX Op.\n    The attributes of ONNX Op can be copied or not copied based on a bool\n    parameter in the builder. Builder also provide a mechanism for user\n    to selectively copy some attributes.\n\n    The krnl.call op will be lowered to llvm at krnl-to-llvm conversion in which\n    OMTensor is used as a container for MemRef arguments. Other representation\n    of parameters, such as data pointer only, will be supported in future.",
    "inputs": [
      { "name": "parameters", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "returnValue", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "funcName", "type": "StrAttr" },
      { "name": "numOfOutput", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "krnl.copy_from_tile_buffer",
    "summary": "Copy from buffer.",
    "description": "Operation that copy a destination memory from a buffer memory.\n    Starts indicate where the buffer data starts to go into the destination\n    memory. Start values must be at multiples of buffer size in all dimensions.\n    The buffer rank and dimensions are compile time constants.\n\n    If the buffer was oversized with respect of the actual data contained\n    in the tile, the actual tile size can be given using the tileSize\n    optional attribute. This attributes has the same rank as the buffer size,\n    and each dimension must be smaller or equal to the actual buffer size.",
    "inputs": [
      { "name": "buffer", "type": "Arg" },
      { "name": "dest", "type": "Arg" },
      { "name": "starts", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "tileSize", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$buffer `,` $dest `[` $starts `]`  attr-dict `:` type($buffer) `,` type($dest)"
  },
  {
    "name": "krnl.copy_to_tile_buffer",
    "summary": "Copy to buffer.",
    "description": "Operation that copy a source memory to a buffer memory.\n    Starts indicate where the source data starts to come from within\n    the source memory. Start values must be at multiples of buffer size\n    in all dimensions. The buffer rank and dimensions are compile time\n    constants.\n\n    The buffer will be entirely filled with the source data. By default,\n    the amount of data to copy is given by the size of the buffer.\n    In some cases, we may want to oversize a buffer for better cache,\n    simd, or loop unroll and jam reasons. If that is the case, the\n    actual tile size of the data to be copied over is given by an\n    optional tileSize attribute. This attributes has the same rank as\n    the buffer size, and each dimension must be smaller or equal to\n    the actual buffer size.\n\n    If there is not enough data in the source memory to fill the buffer,\n    because the operation reaches the upper bounds of the source memory,\n    several actions may happen.\n\n    * If`padToNext` attribute is given, the pad value will be copied from\n      the last source data of to the next index for which index modulo `padToNext`\n      is zero, i.e. to the end of a \"cache line\" of side `padToLine`. Pad\n      of 1 means no padding, pad of buffer size means fully pad the buffer.\n      Default is no padding (1). `PadValue` is used to initialized the padded\n      areas.\n\n    * If `overreadToNext` attribute is given, the copy may read source past\n      its upper bound value. This enable optimized code, e.g. using SIMD\n      read operations even if going past the last value of the source\n      memory, or unrolling and jamming copy loops to reduce memory latency.\n      `overreadToNext` is expressed like padToNext: value of 1 means no\n      reading past boundary; value of buffer size enables reading\n      as many additional source value as needed to fill the full\n      buffer. Default is buffer-size.\n\n    `padToNext` and `overreadToNex`t are of the same rank as source and memory\n    memrefs.",
    "inputs": [
      { "name": "buffer", "type": "Arg" },
      { "name": "source", "type": "Arg" },
      { "name": "starts", "type": "Variadic" },
      { "name": "padValue", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "tileSize", "type": "OptionalAttr" },
      { "name": "padToNext", "type": "OptionalAttr" },
      { "name": "transpose", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$buffer `,` $source `[` $starts `]` `,`  $padValue  attr-dict\n     `:` type($buffer) `,` type($source)"
  },
  {
    "name": "krnl.define_loops",
    "summary": "define_loops operation",
    "description": "The \"krnl.define_loops\" operation is used to define input loops,\n    those are the for loops appearing in the input program that we\n    intend to optimize."
  },
  {
    "name": "krnl.entry_point",
    "summary": "Indicate ONNX entry point",
    "description": "The \"krnl.entry_point\" function indicates the main entry\n                           point of ONNX model."
  },
  {
    "name": "krnl.erf",
    "summary": "Krnl erf scalar operation",
    "description": "Krnl erf scalar operation.",
    "inputs": [
      { "name": "in", "type": "AnyFloat" }
    ],
    "outputs": [
      { "name": "out", "type": "AnyFloat" }
    ]
  },
  {
    "name": "krnl.find_index",
    "summary": "Retrieve an index into a perfect hash table described by G and V.",
    "description": "This operation can be used to generate a call to a runtime function which,\n    given two arrays of int32_t values (G and V), which are used to represent a perfect\n    hash table for a dictionary, returns the index corresponding to the input value.\n    The index returned is valid only if 'input' is in the dictionary described by G and V.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "G", "type": "I32MemRef" },
      { "name": "V", "type": "I32MemRef" },
      { "name": "len", "type": "I32" }
    ],
    "outputs": [
      { "name": "index", "type": "Index" }
    ]
  },
  {
    "name": "krnl.get_induction_var_value",
    "summary": "Krnl",
    "description": "Krnl operation to convert loop references to corresponding induction\n     variable values. This is useful for accessing optimized loop induction\n     variables, as they are not otherwise accessible during Krnl Dialect.\n\n     For example, this operation can be applied to loop references corresponding to\n     inter-tile iterations. The return values will be the starting index of the\n     current tile being iterated over.",
    "inputs": [
      { "name": "loops", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "ind_var_vals", "type": "Variadic" }
    ],
    "assemblyFormat": "`(` $loops `)` attr-dict `:` functional-type($loops, results)"
  },
  {
    "name": "krnl.get_linear_offset_index",
    "summary": "A Krnl operation to compute a linear offset index from a N-D index.",
    "description": "Given a MemRef and an N-D index (id_1, id_2, ..., id_n), where n is\n    the rank of the MemRef, this operation computes a linear offset index.",
    "inputs": [
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "attributes": [
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "krnl.global",
    "summary": "Krnl global operation",
    "description": "Operation for holding global data values. A global constant can have a\n    meaningful name recorded as its `name` attribute. Its content is stored\n    in the `value` dense element attribute.",
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "shape", "type": "AnyAttr" },
      { "name": "name", "type": "StrAttr" },
      { "name": "value", "type": "OptionalAttr" },
      { "name": "offset", "type": "OptionalAttr" },
      { "name": "alignment", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "krnl.isinf",
    "summary": "Krnl isinf scalar operation",
    "description": "Krnl isinf scalar operation.",
    "inputs": [
      { "name": "in", "type": "AnyFloat" }
    ],
    "outputs": [
      { "name": "out", "type": "I1" }
    ]
  },
  {
    "name": "krnl.isnan",
    "summary": "Krnl isnan scalar operation",
    "description": "Krnl isnan scalar operation.",
    "inputs": [
      { "name": "in", "type": "AnyFloat" }
    ],
    "outputs": [
      { "name": "out", "type": "I1" }
    ]
  },
  {
    "name": "krnl.iterate",
    "summary": "iterate operation",
    "description": "The \"krnl.iterate\" operation is conceptually equivalent to a nested for loops.\n\n    For instance, say we have the following two\n    ```\n    %l0, %l1 = krnl.define_loops 2\n    %o0, %o1 = krnl.optimize_loops  {\n        // Identity schedule.\n        krnl.return_loops %l0, %l1\n    }\n    ```\n\n    Then, consider the following krnl.iterate operation:\n    ```\n    krnl.iterate (%o0, %o1) with (%l0 -> %i0 = 0 to 10, %l1 -> %i1 = 0 to 10) {\n      // Some operations.\n    }\n    ```\n\n    It is equivalent to:\n    ```\n    for (i0 = 0; i0 < 10; i0++)\n      for (i1 = 0; i1 < 10; i1++)\n        // Some operations.\n    ```",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "krnl.load",
    "summary": "A Krnl operation to load data from the memref.",
    "description": "The `krnl.load` op reads an element from a memref specified by an index\n    list. The output of load is a new value with the same type as the elements\n    of the memref. The arity of indices is the rank of the memref (i.e., if the\n    memref loaded from is of rank 3, then 3 indices are required for the load\n    following the memref identifier).",
    "inputs": [
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "$memref `[` $indices `]` attr-dict `:` type($memref)"
  },
  {
    "name": "krnl.matmul",
    "summary": "Matmul operation for a single pannel.",
    "description": "Perform a matrix multiplication AA * BB + CC with sizes `[IxK] * [KxJ] + [IxJ]`.\n    The original matrices AA, BB, and CC can be buffered in buffered arrays\n    which may be padded. The original matrices and the padded array might\n    have a higher rank than 2, but the actual matrix multiplication operation\n    only deal with the innermost 2 ranks of the matrices to perform its matrix\n    multiplication operations.\n\n    The computations may also compute only a sub-tile of the buffered arrays.\n    This region is depicted using stars '*' below.\n\n    All indices passed to this operation are the global indices in the original\n    computation, so as to better know if we have boundary conditions.\n\n    ORIGINAL ARRAY: denoted as AA, BB, CC with sizes AA: `*xIxK`; BB: `*xKxJ`; CC: `*xI*J`).\n\n    BUFFER ARRAYS: denoted as A, B, and C. Note that this operation does\n      not require the use of buffers arrays. If none are used, then A=AA,\n      B=BB, C=CC. If buffers are used, it is the responsibility of the caller\n      to properly fill the buffers with the appropriate data. Buffers are\n      typically used for cache tiling.\n\n     ORIGINAL ARRAY\n\n```\n     -------------------------------------------------\n     |                                               ]\n     |                                               ]\n     |             buffer array       buffer pad     ]\n     |            (3)---------------- ++++           ]\n     |             |                 |   +           ]\n     |             |     (1)****     |   +           ]\n     |             |      *    *     |   +           ]\n     |             |      *    *     |   +           ]\n     |             |      ****(5)    |   +           ]\n     |             |                 |   +           ]\n     |             |                 |   +           ]\n     |             ------------------|   +           ]\n     |             +                     +           ]\n     |             +++++++++++++++++++++(4)          ]\n     |                                               ]\n     -----------------------------------------------(2)\n```\n\n* (1) `iGlobalIndexComputeStart`/`jGlobalIndexComputeStart`/`kGlobalIndexComputeStart`,\n   required, each three are global 1D indices.\n* (2) `iGlobalUB`/`jGlobalUB`/`jGlobalUB`, required, each three are global 1D indices.\n* (3) `aGlobalIndexMemStart`/`bGlobalIndexMemStart`/`cGlobalIndexMemStart`,\n   required, global nD indices with the same rank as the buffers A, B, and C.\n* (4) `aTileSize`/`bTileSize`/`cTileSize`, required when padding, each 2D sizes.\n* (5) `computeTileSizes`, required when tiled computation within buffer, 3D sizes (I, J, K).\n\n    The `iGlobalIndexComputeStart`/`jGlobalIndexComputeStart`/\n    `kGlobalIndexComputeStart` (1) indicate the global indices of the\n    first element of a tile to be computed in the original computations.\n\n    The `iGlobalUB`/`jGlobalUB`/`kGlobalUB` (2) indicate the global upper bounds\n    in the original computations.\n\n    We provide 3 buffers for matrix multiply: A, B, and C. For each buffer,\n    we indicate the global indices pointing the beginning of the buffer:\n    `aGlobalIndexMemStart`, `bGlobalIndexMemStart`, and `cGlobalIndexMemStart` (3).\n    If no buffers are used, i.e. the computation starts directly in the\n    original memory, the global index is 0. If a buffer for AA is used to\n    put data into it starting at indices `[i1, k1]`, where `i1` & `k1` are the\n    global indices in the original computations, then `aGlobalIndexMemStart0`\n    and `aGlobalIndexMemStart1` are `i1` & `k1`, respectively.\n\n    If the A, B, or C buffers are larger than the actual data tile they\n    contain (see `copy_to_tile_buffer`), then the actual tile size must be\n    given using an optional attribute: `aTileSize`, `bTileSize`, or `cTileSize` (4).\n    These optional tile size have a rank of 2, and their values must be\n    equal or smaller than their corresponding buffer memrefs.\n\n    If the computation are further tiled with respect to the size of the\n    buffers A, B, or C, then the actual computation tile is given by\n    the optional tile attribute `computeTileSize` (5). Its rank is 3, for the\n    I, J, and K dimension. The actual A, B, and C buffer tile size\n    (possibly specified by the optional parameters) must be a multiple of\n    the I, J, and K `computeTileSizes`, in their respective\n    dimensions (A: `[IxK]`, B: `[KxJ]`, C: `[IxJ]`).\n\n    Note that the buffers A, B, and C can be of higher dimensionality than\n    the traditional 2D mentioned up to now, because of broadcasting rules.\n    At this time, we only support broadcast of arrays having ranks of 2 or\n    more. Because of the broadcast rules, the higher dimensions have a\n    constant index during one matrix multiply. These fixed indices are\n    given as prefix dimensions in the starting indices for AA, BB, and CC\n    as described above. E.g. if AA has a rank of 3, and BB has a rank of 2,\n    the starting indices for AA are `[d, i1, k1]` where `i1` and `k1` are as\n    above, and d is index pointing to the current instance of the `IxK`\n    AA matrix to be computed. B start indices would be unchanged at `[k1, j1]`.\n\n    Simdize is used to state if simdization is requested.\n    Unrolling is used to unroll and jam loops as warranted.\n\n    Below is an example calculating a matrix multiply with pre-zeroed\n    C matrix with the sizes below.\n\n```\n    %A: memref<40x60xf32>, %B: memref<60x80xf32>, %C: memref<40x80xf32>\n\n    // 3 tiled loops.\n    %ii, %jj, %kk = krnl.define_loops 3\n    %ib, %il = krnl.block %ii 10 : (!krnl.loop) -> (!krnl.loop, !krnl.loop)\n    %jb, %jl = krnl.block %jj 8 : (!krnl.loop) -> (!krnl.loop, !krnl.loop)\n    %kb, %kl = krnl.block %kk 10 : (!krnl.loop) -> (!krnl.loop, !krnl.loop)\n    // 3 subtiles.\n    %ilb, %ill = krnl.block %il 5 : (!krnl.loop) -> (!krnl.loop, !krnl.loop)\n    %jlb, %jll = krnl.block %jl 4 : (!krnl.loop) -> (!krnl.loop, !krnl.loop)\n    %klb, %kll = krnl.block %kl 5 : (!krnl.loop) -> (!krnl.loop, !krnl.loop)\n    // Permute.\n    krnl.permute(%ib, %ilb, %ill, %jb, %jlb, %jll, %kb, %klb, %kll)\n        [0, 3, 6, 1, 4, 7, 2, 5, 8] :\n        !krnl.loop, !krnl.loop, !krnl.loop, !krnl.loop, !krnl.loop,\n        !krnl.loop, !krnl.loop, !krnl.loop, !krnl.loop\n    // Outer 2 for i, j.\n    krnl.iterate(%ib, %jb) with (%ii -> %i = 0 to 40,\n                                 %jj -> %j = 0 to 80,\n                                 %kk -> %k = 0 to 60) {\n        %i1, %j1 = krnl.get_induction_var_value(%ib, %jb) :\n          (!krnl.loop,!krnl.loop) -> (index, index)\n        // Fill C buffer.\n        %Cbuff = alloca(): memref<10x8xf32>  // n x m_simd\n        krnl.copy_to_tile_buffer %Cbuff, %C[%i1, %j1], %f0 :\n          memref<10x8xf32>, memref<40x80xf32>\n        // Outer 1 for k.\n        krnl.iterate(%kb) with () {\n            %k1 = krnl.get_induction_var_value(%kb) : (!krnl.loop) -> (index)\n            // Fill A and B buffer\n            %Abuff = alloca(): memref<10x10xf32> // i x k\n            %Bbuff = alloca(): memref<10x8xf32>  // k x j_simd\n            krnl.copy_to_tile_buffer %Abuff, %A[%i1, %k1], %f0 :\n              memref<10x10xf32>, memref<40x60xf32>\n            krnl.copy_to_tile_buffer %Bbuff, %B[%k1, %j1], %f0 :\n              memref<10x8xf32>, memref<60x80xf32>\n\n            // Inner iterations for subtiles.\n            krnl.iterate(%ilb, %jlb, %klb) with () {\n                %i2, %j2, %k2 = krnl.get_induction_var_value(%ilb, %jlb, %klb) :\n                (!krnl.loop,!krnl.loop,!krnl.loop) -> (index,index,index)\n\n                krnl.matmul %Abuff[%i1, %k1], %Bbuff[%k1, %j1], %Cbuff[%i1, %j1],\n                    (%ill, %jll, %kll), (%i2, %j2, %k2), (%c40, %c80, %c60)\n                    { computeTileSize=[5,4,5], simdize=false, unroll=false } :\n                    memref<10x10xf32>, memref<10x8xf32>, memref<10x8xf32>,\n                    (!krnl.loop,!krnl.loop,!krnl.loop)\n            }\n        }\n        // Copy back the data into C.\n        krnl.copy_from_tile_buffer %Cbuff, %C[%i1, %j1] :\n          memref<10x8xf32>, memref<40x80xf32>\n    }\n```\n\n    Note that code is simdized along the J dim (last dim of B and C matrices).\n    For simd to be enabled, the simdized flag must be set to true, and the\n    following condition must be true:\n    1) The vector length is the second entry of (i, j, k) compute tile size.\n       The vector length must be a compile time constant.",
    "inputs": [
      { "name": "A", "type": "Arg" },
      { "name": "aGlobalIndexMemStart", "type": "Variadic" },
      { "name": "B", "type": "Arg" },
      { "name": "bGlobalIndexMemStart", "type": "Variadic" },
      { "name": "C", "type": "Arg" },
      { "name": "cGlobalIndexMemStart", "type": "Variadic" },
      { "name": "loops", "type": "Variadic" },
      { "name": "iGlobalIndexComputeStart", "type": "Index" },
      { "name": "jGlobalIndexComputeStart", "type": "Index" },
      { "name": "kGlobalIndexComputeStart", "type": "Index" },
      { "name": "iGlobalUB", "type": "Index" },
      { "name": "jGlobalUB", "type": "Index" },
      { "name": "kGlobalUB", "type": "Index" }
    ],
    "attributes": [
      { "name": "computeTileSize", "type": "OptionalAttr" },
      { "name": "aTileSize", "type": "OptionalAttr" },
      { "name": "bTileSize", "type": "OptionalAttr" },
      { "name": "cTileSize", "type": "OptionalAttr" },
      { "name": "simdize", "type": "DefaultValuedAttr" },
      { "name": "unroll", "type": "DefaultValuedAttr" },
      { "name": "overcompute", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$A `[` $aGlobalIndexMemStart `]` `,`\n    $B `[` $bGlobalIndexMemStart `]` `,`\n    $C `[` $cGlobalIndexMemStart `]` `,`\n    `(` $loops `)` `,`\n    `(` $iGlobalIndexComputeStart `,` $jGlobalIndexComputeStart `,`\n        $kGlobalIndexComputeStart `)` `,`\n    `(` $iGlobalUB `,` $jGlobalUB `,` $kGlobalUB `)`\n    attr-dict `:` type($A) `,` type($B)`,` type($C) `,` `(` type($loops) `)`"
  },
  {
    "name": "krnl.memcpy",
    "summary": "Krnl memcpy operation",
    "description": "Copy `num_elems` elements from `src` to `dest` MemRef.\n\n    Starting positions for `src` and `dest` are defined by `src_offset` and\n    `dest_offset`, respectively.\n\n    It is the users' responsibility to make sure there is no out-of-bound read/write.",
    "inputs": [
      { "name": "dest", "type": "AnyMemRef" },
      { "name": "src", "type": "AnyMemRef" },
      { "name": "num_elems", "type": "I64" },
      { "name": "dest_offset", "type": "Index" },
      { "name": "src_offset", "type": "Index" }
    ]
  },
  {
    "name": "krnl.memset",
    "summary": "Set buffer to a given value.",
    "description": "Krnl operation that sets a buffer to a given value.\n    In case that the buffer is a MemRef with affine_map, `delayed` indicates\n    whether we set values along original or extended iteration space.\n\n    For example, given\n    - an affine_map `#tile = affine_map < (i)->(i floordiv 4, i mod 4) >`, and\n    - a buffer of type `memref<5xf32, #tile>`\n\n    Original iteration space is along the first axis that has 5 elements.\n\n    If we do normalization, the memref becomes `memref<2x4xf32>`. Now we have\n    an extended iteration space along two axes of sizes 2 and 4, respectively.\n    This extended iteration space has 8 elements in total.\n\n    If `delayed = false`, the original iteration space is used to set values.\n    In the above example, only 5 out of 8 elementes will be set to the given value.\n\n    If `delayed = true`, the extended iteration space is used to set values.\n    In the above example, all 8 elements will be set to the given value.",
    "inputs": [
      { "name": "dest", "type": "AnyMemRef" },
      { "name": "value", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "delayed", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$dest `,` $value attr-dict `:` type($dest)"
  },
  {
    "name": "krnl.movable",
    "summary": "Krnl movable operation",
    "description": "Encapsulates a list of operations, which should be moved under a newly lowered\n     affine for operation eventually, but cannot presently because the destination\n     affine for operation is not materialized yet.\n\n     This operation is automatically generated by the lowering of Krnl to affine dialect\n     to assist with maintaining the relative positioning of loop and inner-loop statements.\n     This construct is particularly helpful, for example, for lowering statements that\n     are nested imperfectly between an \"eager\" and a \"lazy\" loop.",
    "assemblyFormat": "$region attr-dict"
  },
  {
    "name": "krnl.noValue",
    "summary": "An operation representing the absence of a value.",
    "description": "This operation can be used to represent the absence of a value. It is\n    typically used as an argument to operators that have optional parameters,\n    and converted into nullptr while krnl to llvm lowering.\n    Typically it is used for optional arguments used in KrnlCallop.",
    "outputs": [
      { "name": "none_val", "type": "NoneType" }
    ],
    "attributes": [
      { "name": "value", "type": "UnitAttr" }
    ]
  },
  {
    "name": "krnl.parallel",
    "summary": "Mark Krnl loops as parallel loops",
    "description": "Parallelize the specified loops. When multiple loop specifiers are passed\n    as parameters, there loops can be parallelized as a collapsed loop.\n    krnl.parallel should be placed as the last operator before krnl.iterate,\n    Since we do not want to parallelize the loop until we interpret krnl.block,\n    krnl.permute and krnl.unroll.\n\n    Optionally, a value may specifiy the number of threads requested for the\n    parallel loop. A proc_bind string may also be specified; valid values are\n    \"primary\", \"close\", or \"spread\". Default values are used when not specified.\n\n    ```\n    krnl.parallel (%i0, %i1) : !Krnl.loop, !Krnl.loop\n    ```",
    "inputs": [
      { "name": "loops", "type": "Variadic" },
      { "name": "num_threads", "type": "Optional" }
    ],
    "attributes": [
      { "name": "proc_bind", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`(` $loops `)` (`,` `num_threads` `(` $num_threads^ `)`)? attr-dict `:` type($loops)"
  },
  {
    "name": "krnl.parallel_clause",
    "summary": "Attach OpenMP clauses to an index varialbe",
    "description": "Attach OpenMP clauses to an index variable. That index variable\n    is used to uniquely associate a parallel loop with its clauses.",
    "inputs": [
      { "name": "parallel_loop_index", "type": "Index" },
      { "name": "num_threads", "type": "Optional" }
    ],
    "attributes": [
      { "name": "proc_bind", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`(` $parallel_loop_index `)` (`,` `num_threads` `(` $num_threads^ `)`)? \n      attr-dict `:` type($parallel_loop_index)"
  },
  {
    "name": "krnl.permute",
    "summary": "Krnl permute operation",
    "description": "Permute a set of affine for loops using a specified permutation map.\n    The permutation map `map` should be constructed in such way that the\n    for loop referred to by the i-th operand to permute operation is sent\n    to the `map[i]`-th position.\n\n    For example, the following krnl dialect IR:\n    ```\n    %ii, %jj, %kk = krnl.define_loops 3\n    krnl.permute(%ii, %jj, %kk) [1, 2, 0] : !krnl.loop, !krnl.loop, !krnl.loop\n    krnl.iterate (%ii, %jj, %kk) with (%ii -> %i = 0 to 10, %jj -> %j = 0 to 20, %kk -> %k = 0 to 30) {}\n    ```\n    will be lowered to:\n    ```\n    // Referenced by %kk\n    affine.for %arg0 = 0 to 30 {\n      // Referenced by %ii\n      affine.for %arg1 = 0 to 10 {\n        // Referenced by %jj\n        affine.for %arg2 = 0 to 20 {\n        }\n      }\n    }\n    ```\n\n    For a more complicated example, we demonstrate 3-D tiling using krnl.block in\n    conjunction with krnl.permute:\n    ```\n    %ii, %jj, %kk = krnl.define_loops 3\n    // Blocking each loop by a factor of 4.\n    %ib, %il = krnl.block %ii 4 : (!krnl.loop) -> (!krnl.loop, !krnl.loop)\n    %jb, %jl = krnl.block %jj 4 : (!krnl.loop) -> (!krnl.loop, !krnl.loop)\n    %kb, %kl = krnl.block %kk 4 : (!krnl.loop) -> (!krnl.loop, !krnl.loop)\n    // Move iteration over tile coordinates to be the outer loops and iterateion over\n    // the inter-tile elements to be the inner loops.\n    krnl.permute(%ib, %il, %jb, %jl, %kb, %kl) [0, 3, 1, 4, 2, 5] : !krnl.loop, !krnl.loop, !krnl.loop, !krnl.loop, !krnl.loop, !krnl.loop\n    krnl.iterate(%ib, %il, %jb, %jl, %kb, %kl) with (%ii -> %i = 0 to 1024, %jj -> %j = 0 to 2048, %kk -> %k = 0 to 4096)  {\n    }\n    ```\n\n    The above IR gets lowered to:\n    ```\n    affine.for %arg0 = 0 to 1024 step 4 {\n      affine.for %arg1 = 0 to 2048 step 4 {\n        affine.for %arg2 = 0 to 4096 step 4 {\n          affine.for %arg3 = #map0(%arg0) to #map1(%arg0) {\n            affine.for %arg4 = #map0(%arg1) to #map1(%arg1) {\n              affine.for %arg5 = #map0(%arg2) to #map1(%arg2) {\n              }\n            }\n          }\n        }\n      }\n    }\n    ```",
    "inputs": [
      { "name": "loops", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "map", "type": "I64ArrayAttr" }
    ],
    "assemblyFormat": "`(` $loops `)` $map attr-dict `:` type($loops)"
  },
  {
    "name": "krnl.prefetch",
    "summary": "A Krnl operation to compute a linear offset index from a N-D index.",
    "description": "Given a MemRef and an N-D index (id_1, id_2, ..., id_n), prefetch the memory\n    location pointed by this memory reference.",
    "inputs": [
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "isWrite", "type": "BoolAttr" },
      { "name": "localityHint", "type": "ConfinedAttr" },
      { "name": "isDataCache", "type": "BoolAttr" },
      { "name": "map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "krnl.print",
    "summary": "Print a value.",
    "description": "This operation can be used to print the input value. The user needs to provide a\n    format string (à la printf) to specify how to print the input value.\n    If the input value is not specified the operator will print the format string.",
    "inputs": [
      { "name": "input", "type": "Optional" }
    ],
    "attributes": [
      { "name": "format", "type": "StrAttr" }
    ]
  },
  {
    "name": "krnl.print_tensor",
    "summary": "Print a tensor.",
    "description": "This operation can be used to generate a call to a runtime function which prints a tensor.\n    At the beginning of the msg string, user can add formatting instructions. The flags are:\n\n    *  `%s`: detailed signature (including shape, type, offsets),\n    *  `%t`: compact type (ala MLIR: `32x16xfloat`),\n    *  `%d`: data values.\n\n    When no formatting is provided, `%s%d` is used (detailed signature and data) by default.\n    Print operation ends with a newline, except when only requesting a compact types (`%t`).",
    "inputs": [
      { "name": "input", "type": "AnyMemRef" }
    ],
    "attributes": [
      { "name": "msg", "type": "StrAttr" }
    ]
  },
  {
    "name": "krnl.random_normal",
    "summary": "Generate a random normal tensor.",
    "description": "Operation that generates a random normally distributed tensor.",
    "inputs": [
      { "name": "output", "type": "Arg" },
      { "name": "numberOfValues", "type": "Index" },
      { "name": "mean", "type": "AnyFloat" },
      { "name": "scale", "type": "AnyFloat" },
      { "name": "seed", "type": "AnyFloat" }
    ]
  },
  {
    "name": "krnl.region",
    "summary": "Affine boundary for krnl loops",
    "description": "This Op has a region with AffineScope trait and is used to limit the\n    scope of `affine.for`. The loop inside `krnl.region` can be affined if\n    its boundary is defined at the level of `krnl.region`. The `krnl.region` does\n    not guarantee or require the loops inside it to be affine.\n    With `krnl.region`, a krnl loop may not be  affine if its boundary symbol\n    is not defined inside a enclosing region without AffineScope trait.\n    In MLIR, FuncOp has the AffineScope trait.\n    The `krnl.region` will be removed after affine.for is lowered.\n    ToFix: current `krnl.region` does not have input and output. You cannot\n    create a new memref inside the region and use it outside of the region."
  },
  {
    "name": "krnl.round_even",
    "summary": "Krnl round to nearest even operation",
    "description": "Krnl round to nearest even operation.  Accept scalar or vector float values.\n    Vector must be 1D of a size that is a multiple of the hardware vector size.",
    "inputs": [
      { "name": "in", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "out", "type": "FloatLike" }
    ]
  },
  {
    "name": "krnl.runtime_instrument",
    "summary": "instrumentation point.",
    "description": "Operation that invokes the runtime instrument utility.\n    May be used for gdb.",
    "attributes": [
      { "name": "opName", "type": "StrAttr" },
      { "name": "tag", "type": "I64Attr" },
      { "name": "nodeName", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "krnl.seqalloc",
    "summary": "Krnl create a sequence",
    "description": "This op allocates a memref for a new sequence according to the input Type and length.\n    The output is tagged with Allocate side effect, and a deallocation is defined for\n    sequence. This deallocation will free all the elements in the sequence as well as\n    the sequence itself.",
    "inputs": [
      { "name": "length", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyMemRef" }
    ]
  },
  {
    "name": "krnl.seqdealloc",
    "summary": "Krnl dealloc a sequence",
    "description": "This op deallocate the elements in the sequence and the sequence itself\n    with memref::dealloc. This Op is a deep dealloc for sequence type.",
    "inputs": [
      { "name": "input_sequence", "type": "AnyMemRef" }
    ]
  },
  {
    "name": "krnl.seqextract",
    "summary": "Krnl load from a sequence",
    "description": "This op loads an element from the input sequence 'seq' at position 'index'.\n    The loaded element is copied and then return.\n    The position value is guaranteed to be positive. Negative position allowed\n    by ONNX Op definition should be handled before lowered to KrnlSeqExtract.\n\n    Attribute 'copy' provides an optimization for copying.\n    When the attribute 'copy' is 1 (default value): the extracted element is copied and then return.\n    When the attribute 'copy' is 0: the extracted element is directly returned\n    without copy.\n\n    The returned element is marked as allocated by this Op with the bufferation\n    interface so that deallocation can be generated correctly through the\n    Bufferization::Deallocation pass.",
    "inputs": [
      { "name": "seq", "type": "AnyMemRef" },
      { "name": "index", "type": "Index" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "copy", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "krnl.seqstore",
    "summary": "Krnl store into a seq",
    "description": "This op is similar to KrnSeqInsertOp but assumes that the input seq has\n    the space for the new element and\n    only need to copy the element and store it into the sequence.\n    There is no return of a new seq, different from KrnlSeqInsertOp.\n    This Op is introduced to accumulate a dynamic tensor in a LoopOp with\n    statically known iteration count.",
    "inputs": [
      { "name": "input", "type": "AnyType" },
      { "name": "seq", "type": "AnyMemRef" },
      { "name": "index", "type": "Index" }
    ]
  },
  {
    "name": "krnl.specialized_kernel",
    "summary": "Krnl specialized kernel op",
    "description": "Krnl operation to convert.",
    "inputs": [
      { "name": "loops", "type": "Variadic" }
    ],
    "assemblyFormat": "`(` $loops `)` attr-dict `:` type($loops)"
  },
  {
    "name": "krnl.store",
    "summary": "A Krnl operation to store data to the memref.",
    "description": "The `krnl.store` stores a value to a memref location given by indices. The\n    value stored should have the same type as the elemental type of the memref.\n    The number of arguments provided within brackets need to match the rank of\n    the memref.",
    "inputs": [
      { "name": "value", "type": "AnyType" },
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "assemblyFormat": "$value `,` $memref `[` $indices `]` attr-dict `:` type($memref)"
  },
  {
    "name": "krnl.strlen",
    "summary": "Compute the length of a string.",
    "description": "Krnl operation that computes the length of a string.",
    "inputs": [
      { "name": "str", "type": "StringType" }
    ],
    "outputs": [
      { "name": "res", "type": "I64" }
    ]
  },
  {
    "name": "krnl.strncmp",
    "summary": "Perform string comparison up to N bytes.",
    "description": "Krnl operation that performs a string comparison up to N bytes.",
    "inputs": [
      { "name": "str1", "type": "StringType" },
      { "name": "str2", "type": "StringType" },
      { "name": "len", "type": "I64" }
    ],
    "outputs": [
      { "name": "res", "type": "I32" }
    ]
  },
  {
    "name": "krnl.tan",
    "summary": "Krnl tan scalar operation",
    "description": "Krnl tan scalar operation.",
    "inputs": [
      { "name": "in", "type": "AnyFloat" }
    ],
    "outputs": [
      { "name": "out", "type": "AnyFloat" }
    ]
  },
  {
    "name": "krnl.terminate",
    "summary": "Krnl terminator operation",
    "description": "Krnl terminator is a special terminator operation for blocks inside krnl\n    iterate operations. It unconditionally transmits the control flow to the\n    successor of the operation enclosing the region.\n\n    This operation does _not_ have a custom syntax. However, krnl control\n    operations omit the terminator in their custom syntax for brevity."
  },
  {
    "name": "krnl.unroll",
    "summary": "Krnl unroll operation",
    "description": "Fully unroll the specified loops.\n    ```\n    krnl.unroll %i\n    ```\n    unrolls the loop referred to by %i fully.",
    "inputs": [
      { "name": "loop", "type": "AnyType" }
    ],
    "assemblyFormat": "$loop attr-dict `:` type($loop)"
  },
  {
    "name": "krnl.vector_type_cast",
    "summary": "vector type cast operation",
    "description": "The \"vector_type_cast\" operation converts a memref from an non-vector\n    element type to another memref of a vector elemental type while not changing\n    the source memref's element type. The last dimension size of the source\n    dimension is divided (floor division) by the vector size to obtain the\n    corresponding dimension for target memref type.\n\n    ```\n    %MV = vector_type_cast %M : memref<64x16xf32> to memref<64x2xvector<8xf32>>\n    %AV = vector_type_cast %A : memref<?x?xf32> to memref<?x?xvector<8xf32>>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyMemRef" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyMemRef" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($result)"
  },
  {
    "name": "krnl.yield",
    "summary": "Yield values to parent operation",
    "description": "The `krnl.yield` yields zero or more SSA values from an krnl.iterate op region and\n    terminates the region. The semantics of how the values yielded are used\n    is defined by the parent operation.\n    If `krnl.yield` has any operands, the operands must match the parent\n    operation's results.\n    If the parent operation defines no values, then the `krnl.yield` may be\n    left out in the custom syntax and the builders will insert one implicitly.\n    Otherwise, it has to be present in the syntax to indicate which values are\n    yielded.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "linalg.index",
    "summary": "linalg index operation",
    "description": "The `linalg.index` operation returns the iteration index of the immediately\n    enclosing linalg structured operation for the iteration dimension `dim`. The\n    `dim` attribute specifies the position of the accessed dimension in the\n    indexing map domain.\n\n    Example:\n\n    ```mlir\n    #map = affine_map<(i, j) -> (i, j)>\n    linalg.generic {indexing_maps = [#map, #map],\n                    iterator_types = [\"parallel\", \"parallel\"]}\n      outs(%I, %J : memref<?x?xindex>, memref<?x?xindex>) {\n      ^bb0(%arg0 : index, %arg1 : index):\n      // Access the outer iteration dimension i\n      %i = linalg.index 0 : index\n      // Access the inner iteration dimension j\n      %j = linalg.index 1 : index\n      linalg.yield %i, %j : index, index\n    }\n    ```\n\n    This may lower to IR resembling:\n\n    ```mlir\n    %0 = dim %I, %c0 : memref<?x?xindex>\n    %1 = dim %I, %c1 : memref<?x?xindex>\n    scf.for %i = %c0 to %0 step %c1 {\n      scf.for %j = %c0 to %1 step %c1 {\n        store %i, %I[%i, %j] : memref<?x?xindex>\n        store %j, %J[%i, %j] : memref<?x?xindex>\n      }\n    }\n    ```",
    "assemblyFormat": "$dim attr-dict `:` type($result)"
  },
  {
    "name": "linalg.softmax",
    "summary": "Softmax operator",
    "description": "linalg.softmax computes a numerically stable version of softmax.\n\n    For a given input tensor and a specified dimension `d`, compute:\n      1. the max `m` along that dimension `d`\n      2. f(x) = exp(x - m)\n      3. sum f(x) along dimension d to get l(x).\n      4. compute the final result f(x) / l(x).\n\n    This is an aggregate linalg operation that further reduces to a small DAG of\n    structured operations.\n\n    Warning: Regarding the tiling capabilities, the implementation doesn't\n    check that the provided dimensions make sense. This is the responsability\n    of the transformation calling the tiling to ensure that the provided\n    sizes for each dimension make sense with respect to the semantic of\n    softmax.",
    "inputs": [
      { "name": "input", "type": "AnyShaped" },
      { "name": "output", "type": "AnyShaped" }
    ],
    "outputs": [
      { "name": "result", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "I64Attr" }
    ],
    "assemblyFormat": "attr-dict\n    `dimension` `(` $dimension `)`\n    `ins` `(` $input `:` type($input) `)`\n    `outs` `(` $output `:` type($output) `)`\n    (`->` type($result)^)?",
    "category": "Activation"
  },
  {
    "name": "linalg.winograd_filter_transform",
    "summary": "Winograd filter transform operator",
    "description": "Winograd Conv2D algorithm will convert linalg Conv2D operator into batched\n    matrix multiply. Before the matrix multiply, it will convert filter and\n    input into a format suitable for batched matrix multiply. After the matrix\n    multiply, it will convert output to the final result tensor.\n\n    The algorithm F(m x m, r x r) is\n\n    Y = A^T x [(G x g x G^T) @ (B^T x d x B)] x A\n\n    The size of output Y is m x m. The size of filter g is r x r. The size of\n    input d is (m + r - 1) x (m + r - 1). A^T, A, G^T, G, B^T, and B are\n    transformation matrices.\n\n    This operator is defined to represent the high level concept of filter\n    transformation (G x g x G^T) in the Winograd Conv2D algorithm.",
    "inputs": [
      { "name": "filter", "type": "TensorRankOf" },
      { "name": "output", "type": "TensorRankOf" },
      { "name": "fmr", "type": "WinogradConv2DFmr" }
    ],
    "outputs": [
      { "name": "result", "type": "TensorRankOf" }
    ],
    "assemblyFormat": "attr-dict\n    `fmr` `(` $fmr `)`\n    `ins` `(` $filter `:` type($filter) `)`\n    `outs` `(` $output `:` type($output) `)`\n    `->` type($result)"
  },
  {
    "name": "linalg.winograd_input_transform",
    "summary": "Winograd input transform operator",
    "description": "Winograd Conv2D algorithm will convert linalg Conv2D operator into batched\n    matrix multiply. Before the matrix multiply, it will convert filter and\n    input into a format suitable for batched matrix multiply. After the matrix\n    multiply, it will convert output to the final result tensor.\n\n    The algorithm F(m x m, r x r) is\n\n    Y = A^T x [(G x g x G^T) @ (B^T x d x B)] x A\n\n    The size of output Y is m x m. The size of filter g is r x r. The size of\n    input d is (m + r - 1) x (m + r - 1). A^T, A, G^T, G, B^T, and B are\n    transformation matrices.\n\n    This operator is defined to represent the high level concept of input\n    transformation (B^T x d x B) in the Winograd Conv2D algorithm.",
    "inputs": [
      { "name": "input", "type": "TensorRankOf" },
      { "name": "output", "type": "TensorRankOf" },
      { "name": "fmr", "type": "WinogradConv2DFmr" }
    ],
    "outputs": [
      { "name": "result", "type": "TensorRankOf" }
    ],
    "assemblyFormat": "attr-dict\n    `fmr` `(` $fmr `)`\n    `ins` `(` $input `:` type($input) `)`\n    `outs` `(` $output `:` type($output) `)`\n    `->` type($result)"
  },
  {
    "name": "linalg.winograd_output_transform",
    "summary": "Winograd output transform operator",
    "description": "Winograd Conv2D algorithm will convert linalg Conv2D operator into batched\n    matrix multiply. Before the matrix multiply, it will convert filter and\n    input into a format suitable for batched matrix multiply. After the matrix\n    multiply, it will convert output to the final result tensor.\n\n    The algorithm F(m x m, r x r) is\n\n    Y = A^T x [(G x g x G^T) @ (B^T x d x B)] x A\n\n    The size of output Y is m x m. The size of filter g is r x r. The size of\n    input d is (m + r - 1) x (m + r - 1). A^T, A, G^T, G, B^T, and B are\n    transformation matrices.\n\n    This operator is defined to represent the high level concept of output\n    transformation (A^T x y x A) in the Winograd Conv2D algorithm.",
    "inputs": [
      { "name": "value", "type": "TensorRankOf" },
      { "name": "output", "type": "TensorRankOf" },
      { "name": "fmr", "type": "WinogradConv2DFmr" }
    ],
    "outputs": [
      { "name": "result", "type": "TensorRankOf" }
    ],
    "assemblyFormat": "attr-dict\n    `fmr` `(` $fmr `)`\n    `ins` `(` $value `:` type($value) `)`\n    `outs` `(` $output `:` type($output) `)`\n    `->` type($result)"
  },
  {
    "name": "linalg.yield",
    "summary": "Linalg yield operation",
    "description": "`linalg.yield` is a special terminator operation for blocks inside regions\n    in `linalg` generic ops. It returns values to the immediately enclosing\n    `linalg` generic op.\n\n    Example:\n\n    ```mlir\n    linalg.yield %f0, %f1 : f32, f32\n    ```"
  },
  {
    "name": "llvm.add",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$lhs `,` $rhs ($overflowFlags^)? attr-dict `:` type($res)"
  },
  {
    "name": "llvm.addrspacecast",
    "inputs": [
      { "name": "arg", "type": "type" }
    ],
    "outputs": [
      { "name": "res", "type": "resultType" }
    ],
    "assemblyFormat": "$arg attr-dict `:` type($arg) `to` type($res)"
  },
  {
    "name": "llvm.alloca",
    "inputs": [
      { "name": "arraySize", "type": "AnySignlessInteger" }
    ],
    "outputs": [
      { "name": "res", "type": "Res" }
    ],
    "attributes": [
      { "name": "alignment", "type": "OptionalAttr" },
      { "name": "elem_type", "type": "TypeAttr" },
      { "name": "inalloca", "type": "UnitAttr" }
    ]
  },
  {
    "name": "llvm.and",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($res)"
  },
  {
    "name": "llvm.ashr",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "(`exact` $isExact^)? $lhs `,` $rhs attr-dict `:` type($res)"
  },
  {
    "name": "llvm.atomicrmw",
    "outputs": [
      { "name": "res", "type": "LLVM_AtomicRMWType" }
    ],
    "assemblyFormat": "(`volatile` $volatile_^)? $bin_op $ptr `,` $val\n    (`syncscope` `(` $syncscope^ `)`)? $ordering attr-dict `:`\n    qualified(type($ptr)) `,` type($val)"
  },
  {
    "name": "llvm.bitcast",
    "inputs": [
      { "name": "arg", "type": "type" }
    ],
    "outputs": [
      { "name": "res", "type": "resultType" }
    ],
    "assemblyFormat": "$arg attr-dict `:` type($arg) `to` type($res)"
  },
  {
    "name": "llvm.blockaddress",
    "summary": "Creates a LLVM blockaddress ptr",
    "description": "Creates an SSA value containing a pointer to a basic block. The block\n    address information (function and block) is given by the `BlockAddressAttr`\n    attribute. This operation assumes an existing `llvm.blocktag` operation\n    identifying an existing MLIR block within a function. Example:\n\n    ```mlir\n    llvm.mlir.global private @g() : !llvm.ptr {\n      %0 = llvm.blockaddress <function = @fn, tag = <id = 0>> : !llvm.ptr\n      llvm.return %0 : !llvm.ptr\n    }\n\n    llvm.func @fn() {\n      llvm.br ^bb1\n    ^bb1:  // pred: ^bb0\n      llvm.blocktag <id = 0>\n      llvm.return\n    }\n    ```",
    "outputs": [
      { "name": "res", "type": "LLVM_AnyPointer" }
    ],
    "attributes": [
      { "name": "block_addr", "type": "LLVM_BlockAddressAttr" }
    ],
    "assemblyFormat": "$block_addr\n    attr-dict `:` qualified(type($res))"
  },
  {
    "name": "llvm.blocktag",
    "description": "This operation uses a `tag` to uniquely identify an MLIR block in a\n    function. The same tag is used by `llvm.blockaddress` in order to compute\n    the target address.\n\n    A given function should have at most one `llvm.blocktag` operation with a\n    given `tag`. This operation cannot be used as a terminator.\n\n    Example:\n\n    ```mlir\n    llvm.func @f() -> !llvm.ptr {\n      %addr = llvm.blockaddress <function = @f, tag = <id = 1>> : !llvm.ptr\n      llvm.br ^bb1\n    ^bb1:\n      llvm.blocktag <id = 1>\n      llvm.return %addr : !llvm.ptr\n    }\n    ```",
    "attributes": [
      { "name": "tag", "type": "LLVM_BlockTagAttr" }
    ],
    "assemblyFormat": "$tag attr-dict"
  },
  {
    "name": "llvm.br",
    "inputs": [
      { "name": "destOperands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "loop_annotation", "type": "OptionalAttr" }
    ],
    "successors": [
      {
        "name": "dest"
      }
    ],
    "assemblyFormat": "$dest (`(` $destOperands^ `:` type($destOperands) `)`)? attr-dict"
  },
  {
    "name": "llvm.call",
    "summary": "Call to an LLVM function.",
    "description": "In LLVM IR, functions may return either 0 or 1 value. LLVM IR dialect\n    implements this behavior by providing a variadic `call` operation for 0- and\n    1-result functions. Even though MLIR supports multi-result functions, LLVM\n    IR dialect disallows them.\n\n    The `call` instruction supports both direct and indirect calls. Direct calls\n    start with a function name (`@`-prefixed) and indirect calls start with an\n    SSA value (`%`-prefixed). The direct callee, if present, is stored as a\n    function attribute `callee`. For indirect calls, the callee is of `!llvm.ptr` type\n    and is stored as the first value in `callee_operands`. If and only if the\n    callee is a variadic function, the `var_callee_type` attribute must carry\n    the variadic LLVM function type. The trailing type list contains the\n    optional indirect callee type and the MLIR function type, which differs from\n    the LLVM function type that uses an explicit void type to model functions\n    that do not return a value.\n\n    If this operatin has the `no_inline` attribute, then this specific function call\n    will never be inlined. The opposite behavior will occur if the call has `always_inline`\n    attribute. The `inline_hint` attribute indicates that it is desirable to inline\n    this function call.\n\n    Examples:\n\n    ```mlir\n    // Direct call without arguments and with one result.\n    %0 = llvm.call @foo() : () -> (f32)\n\n    // Direct call with arguments and without a result.\n    llvm.call @bar(%0) : (f32) -> ()\n\n    // Indirect call with an argument and without a result.\n    %1 = llvm.mlir.addressof @foo : !llvm.ptr\n    llvm.call %1(%0) : !llvm.ptr, (f32) -> ()\n\n    // Direct variadic call.\n    llvm.call @printf(%0, %1) vararg(!llvm.func<i32 (ptr, ...)>) : (!llvm.ptr, i32) -> i32\n\n    // Indirect variadic call\n    llvm.call %1(%0) vararg(!llvm.func<void (...)>) : !llvm.ptr, (i32) -> ()\n    ```",
    "outputs": [
      { "name": "result", "type": "Optional" }
    ]
  },
  {
    "name": "llvm.call_intrinsic",
    "summary": "Call to an LLVM intrinsic function.",
    "description": "Call the specified llvm intrinsic. If the intrinsic is overloaded, use\n    the MLIR function type of this op to determine which intrinsic to call.",
    "inputs": [
      { "name": "args", "type": "Variadic" },
      { "name": "op_bundle_operands", "type": "VariadicOfVariadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Optional" }
    ],
    "attributes": [
      { "name": "intrin", "type": "StrAttr" },
      { "name": "fastmathFlags", "type": "DefaultValuedAttr" },
      { "name": "op_bundle_sizes", "type": "DenseI32ArrayAttr" },
      { "name": "op_bundle_tags", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "llvm.cmpxchg",
    "outputs": [
      { "name": "res", "type": "LLVM_AnyStruct" }
    ],
    "assemblyFormat": "(`weak` $weak^)? (`volatile` $volatile_^)? $ptr `,` $cmp `,` $val\n    (`syncscope` `(` $syncscope^ `)`)? $success_ordering $failure_ordering\n    attr-dict `:` qualified(type($ptr)) `,` type($val)"
  },
  {
    "name": "llvm.comdat",
    "summary": "LLVM dialect comdat region",
    "description": "Provides access to object file COMDAT section/group functionality.\n\n    Examples:\n    ```mlir\n    llvm.comdat @__llvm_comdat {\n      llvm.comdat_selector @any any\n    }\n    llvm.mlir.global internal constant @has_any_comdat(1 : i64) comdat(@__llvm_comdat::@any) : i64\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "$sym_name $body attr-dict"
  },
  {
    "name": "llvm.comdat_selector",
    "summary": "LLVM dialect comdat selector declaration",
    "description": "Provides access to object file COMDAT section/group functionality.\n\n    Examples:\n    ```mlir\n    llvm.comdat @__llvm_comdat {\n      llvm.comdat_selector @any any\n    }\n    llvm.mlir.global internal constant @has_any_comdat(1 : i64) comdat(@__llvm_comdat::@any) : i64\n    ```",
    "inputs": [
      { "name": "comdat", "type": "Comdat" }
    ],
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "$sym_name $comdat attr-dict"
  },
  {
    "name": "llvm.cond_br",
    "inputs": [
      { "name": "condition", "type": "I1" },
      { "name": "trueDestOperands", "type": "Variadic" },
      { "name": "falseDestOperands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "branch_weights", "type": "OptionalAttr" },
      { "name": "loop_annotation", "type": "OptionalAttr" }
    ],
    "successors": [
      {
        "name": "trueDest"
      },
      {
        "name": "falseDest"
      }
    ],
    "assemblyFormat": "$condition ( `weights` `(` $branch_weights^ `)` )? `,`\n    $trueDest (`(` $trueDestOperands^ `:` type($trueDestOperands) `)`)? `,`\n    $falseDest (`(` $falseDestOperands^ `:` type($falseDestOperands) `)`)?\n    attr-dict"
  },
  {
    "name": "llvm.dso_local_equivalent",
    "summary": "Creates a LLVM dso_local_equivalent ptr",
    "description": "Creates an SSA value containing a pointer to a global value (function or\n    alias to function). It represents a function which is functionally\n    equivalent to a given function, but is always defined in the current\n    linkage unit. The target function may not have `extern_weak` linkage.\n\n    Examples:\n\n    ```mlir\n    llvm.mlir.global external constant @const() : i64 {\n      %0 = llvm.mlir.addressof @const : !llvm.ptr\n      %1 = llvm.ptrtoint %0 : !llvm.ptr to i64\n      %2 = llvm.dso_local_equivalent @func : !llvm.ptr\n      %4 = llvm.ptrtoint %2 : !llvm.ptr to i64\n      llvm.return %4 : i64\n    }\n    ```",
    "outputs": [
      { "name": "res", "type": "LLVM_AnyPointer" }
    ],
    "attributes": [
      { "name": "function_name", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$function_name attr-dict `:` qualified(type($res))"
  },
  {
    "name": "llvm.extractelement",
    "summary": "Extract an element from an LLVM vector.",
    "inputs": [
      { "name": "vector", "type": "LLVM_AnyVector" },
      { "name": "position", "type": "AnySignlessInteger" }
    ],
    "outputs": [
      { "name": "res", "type": "LLVM_Type" }
    ],
    "assemblyFormat": "$vector `[` $position `:` type($position) `]` attr-dict `:` type($vector)"
  },
  {
    "name": "llvm.extractvalue",
    "summary": "Extract a value from an LLVM struct.",
    "inputs": [
      { "name": "container", "type": "LLVM_AnyAggregate" }
    ],
    "outputs": [
      { "name": "res", "type": "LLVM_Type" }
    ],
    "attributes": [
      { "name": "position", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$container `` $position attr-dict `:` type($container)\n    custom<InsertExtractValueElementType>(type($res), ref(type($container)),\n                                          ref($position))"
  },
  {
    "name": "llvm.fadd",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($res)"
  },
  {
    "name": "llvm.fcmp",
    "inputs": [
      { "name": "predicate", "type": "FCmpPredicate" },
      { "name": "lhs", "type": "LLVM_ScalarOrVectorOf" },
      { "name": "rhs", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "fastmathFlags", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "llvm.fdiv",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($res)"
  },
  {
    "name": "llvm.fence",
    "inputs": [
      { "name": "ordering", "type": "AtomicOrdering" }
    ],
    "attributes": [
      { "name": "syncscope", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`syncscope` `(` $syncscope^ `)`)? $ordering attr-dict"
  },
  {
    "name": "llvm.fmul",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($res)"
  },
  {
    "name": "llvm.fneg",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "res", "type": "type" }
    ],
    "attributes": [
      { "name": "fastmathFlags", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($res)"
  },
  {
    "name": "llvm.fpext",
    "inputs": [
      { "name": "arg", "type": "type" }
    ],
    "outputs": [
      { "name": "res", "type": "resultType" }
    ],
    "assemblyFormat": "$arg attr-dict `:` type($arg) `to` type($res)"
  },
  {
    "name": "llvm.fptosi",
    "inputs": [
      { "name": "arg", "type": "type" }
    ],
    "outputs": [
      { "name": "res", "type": "resultType" }
    ],
    "assemblyFormat": "$arg attr-dict `:` type($arg) `to` type($res)"
  },
  {
    "name": "llvm.fptoui",
    "inputs": [
      { "name": "arg", "type": "type" }
    ],
    "outputs": [
      { "name": "res", "type": "resultType" }
    ],
    "assemblyFormat": "$arg attr-dict `:` type($arg) `to` type($res)"
  },
  {
    "name": "llvm.fptrunc",
    "inputs": [
      { "name": "arg", "type": "type" }
    ],
    "outputs": [
      { "name": "res", "type": "resultType" }
    ],
    "assemblyFormat": "$arg attr-dict `:` type($arg) `to` type($res)"
  },
  {
    "name": "llvm.freeze",
    "inputs": [
      { "name": "val", "type": "LLVM_Type" }
    ],
    "outputs": [
      { "name": "res", "type": "LLVM_Type" }
    ],
    "assemblyFormat": "$val attr-dict `:` type($val)"
  },
  {
    "name": "llvm.frem",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($res)"
  },
  {
    "name": "llvm.fsub",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($res)"
  },
  {
    "name": "llvm.func",
    "summary": "LLVM dialect function.",
    "description": "MLIR functions are defined by an operation that is not built into the IR\n    itself. The LLVM dialect provides an `llvm.func` operation to define\n    functions compatible with LLVM IR. These functions have LLVM dialect\n    function type but use MLIR syntax to express it. They are required to have\n    exactly one result type. LLVM function operation is intended to capture\n    additional properties of LLVM functions, such as linkage and calling\n    convention, that may be modeled differently by the built-in MLIR function.\n\n    ```mlir\n    // The type of @bar is !llvm<\"i64 (i64)\">\n    llvm.func @bar(%arg0: i64) -> i64 {\n      llvm.return %arg0 : i64\n    }\n\n    // Type type of @foo is !llvm<\"void (i64)\">\n    // !llvm.void type is omitted\n    llvm.func @foo(%arg0: i64) {\n      llvm.return\n    }\n\n    // A function with `internal` linkage.\n    llvm.func internal @internal_func() {\n      llvm.return\n    }\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "StrAttr" },
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "linkage", "type": "DefaultValuedAttr" },
      { "name": "dso_local", "type": "UnitAttr" },
      { "name": "CConv", "type": "DefaultValuedAttr" },
      { "name": "comdat", "type": "OptionalAttr" },
      { "name": "convergent", "type": "OptionalAttr" },
      { "name": "personality", "type": "OptionalAttr" },
      { "name": "garbageCollector", "type": "OptionalAttr" },
      { "name": "passthrough", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "function_entry_count", "type": "OptionalAttr" },
      { "name": "memory_effects", "type": "OptionalAttr" },
      { "name": "visibility_", "type": "DefaultValuedAttr" },
      { "name": "arm_streaming", "type": "OptionalAttr" },
      { "name": "arm_locally_streaming", "type": "OptionalAttr" },
      { "name": "arm_streaming_compatible", "type": "OptionalAttr" },
      { "name": "arm_new_za", "type": "OptionalAttr" },
      { "name": "arm_in_za", "type": "OptionalAttr" },
      { "name": "arm_out_za", "type": "OptionalAttr" },
      { "name": "arm_inout_za", "type": "OptionalAttr" },
      { "name": "arm_preserves_za", "type": "OptionalAttr" },
      { "name": "section", "type": "OptionalAttr" },
      { "name": "unnamed_addr", "type": "OptionalAttr" },
      { "name": "alignment", "type": "OptionalAttr" },
      { "name": "vscale_range", "type": "OptionalAttr" },
      { "name": "frame_pointer", "type": "OptionalAttr" },
      { "name": "target_cpu", "type": "OptionalAttr" },
      { "name": "tune_cpu", "type": "OptionalAttr" },
      { "name": "reciprocal_estimates", "type": "OptionalAttr" },
      { "name": "prefer_vector_width", "type": "OptionalAttr" },
      { "name": "target_features", "type": "OptionalAttr" },
      { "name": "no_infs_fp_math", "type": "OptionalAttr" },
      { "name": "no_nans_fp_math", "type": "OptionalAttr" },
      { "name": "no_signed_zeros_fp_math", "type": "OptionalAttr" },
      { "name": "denormal_fp_math", "type": "OptionalAttr" },
      { "name": "denormal_fp_math_f32", "type": "OptionalAttr" },
      { "name": "fp_contract", "type": "OptionalAttr" },
      { "name": "instrument_function_entry", "type": "OptionalAttr" },
      { "name": "instrument_function_exit", "type": "OptionalAttr" },
      { "name": "no_inline", "type": "OptionalAttr" },
      { "name": "always_inline", "type": "OptionalAttr" },
      { "name": "inline_hint", "type": "OptionalAttr" },
      { "name": "no_unwind", "type": "OptionalAttr" },
      { "name": "will_return", "type": "OptionalAttr" },
      { "name": "optimize_none", "type": "OptionalAttr" },
      { "name": "vec_type_hint", "type": "OptionalAttr" },
      { "name": "work_group_size_hint", "type": "OptionalAttr" },
      { "name": "reqd_work_group_size", "type": "OptionalAttr" },
      { "name": "intel_reqd_sub_group_size", "type": "OptionalAttr" },
      { "name": "uwtable_kind", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "llvm.getelementptr",
    "description": "This operation mirrors LLVM IRs 'getelementptr' operation that is used to\n    perform pointer arithmetic.\n\n    Like in LLVM IR, it is possible to use both constants as well as SSA values\n    as indices. In the case of indexing within a structure, it is required to\n    either use constant indices directly, or supply a constant SSA value.\n\n    The no-wrap flags can be used to specify the low-level pointer arithmetic\n    overflow behavior that LLVM uses after lowering the operation to LLVM IR.\n    Valid options include 'inbounds' (pointer arithmetic must be within object\n    bounds), 'nusw' (no unsigned signed wrap), and 'nuw' (no unsigned wrap).\n    Note that 'inbounds' implies 'nusw' which is ensured by the enum\n    definition. The flags can be set individually or in combination.\n\n    Examples:\n\n    ```mlir\n    // GEP with an SSA value offset\n    %0 = llvm.getelementptr %1[%2] : (!llvm.ptr, i64) -> !llvm.ptr, f32\n\n    // GEP with a constant offset and the inbounds attribute set\n    %0 = llvm.getelementptr inbounds %1[3] : (!llvm.ptr) -> !llvm.ptr, f32\n\n    // GEP with constant offsets into a structure\n    %0 = llvm.getelementptr %1[0, 1]\n       : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i32, f32)>\n    ```",
    "inputs": [
      { "name": "base", "type": "LLVM_ScalarOrVectorOf" },
      { "name": "dynamicIndices", "type": "Variadic" },
      { "name": "noWrapFlags", "type": "GEPNoWrapFlagsProp" }
    ],
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "rawConstantIndices", "type": "DenseI32ArrayAttr" },
      { "name": "elem_type", "type": "TypeAttr" }
    ],
    "assemblyFormat": "($noWrapFlags^)?\n    $base `[` custom<GEPIndices>($dynamicIndices, $rawConstantIndices) `]` attr-dict\n    `:` functional-type(operands, results) `,` $elem_type"
  },
  {
    "name": "llvm.icmp",
    "inputs": [
      { "name": "predicate", "type": "ICmpPredicate" },
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ]
  },
  {
    "name": "llvm.indirectbr",
    "description": "Transfer control flow to address in `$addr`. A list of possible target\n    blocks in `$successors` can be provided and maybe used as a hint in LLVM:\n\n    ```mlir\n    ...\n    llvm.func @g(...\n      %dest = llvm.blockaddress <function = @g, tag = <id = 0>> : !llvm.ptr\n      llvm.indirectbr %dest : !llvm.ptr, [\n        ^head\n      ]\n    ^head:\n      llvm.blocktag <id = 0>\n      llvm.return %arg0 : i32\n      ...\n    ```\n\n    It also supports a list of operands that can be passed to a target block:\n\n    ```mlir\n      llvm.indirectbr %dest : !llvm.ptr, [\n        ^head(%arg0 : i32),\n        ^tail(%arg1, %arg0 : i32, i32)\n      ]\n    ^head(%r0 : i32):\n      llvm.return %r0 : i32\n    ^tail(%r1 : i32, %r2 : i32):\n      ...\n    ```",
    "inputs": [
      { "name": "addr", "type": "LLVM_AnyPointer" },
      { "name": "succOperands", "type": "VariadicOfVariadic" }
    ],
    "attributes": [
      { "name": "indbr_operand_segments", "type": "DenseI32ArrayAttr" }
    ],
    "successors": [
      {
        "name": "successors"
      }
    ],
    "assemblyFormat": "$addr `:` type($addr) `,`\n      custom<IndirectBrOpSucessors>(ref(type($addr)),\n                                    $successors,\n                                    $succOperands,\n                                    type($succOperands))\n    attr-dict"
  },
  {
    "name": "llvm.inline_asm",
    "description": "The InlineAsmOp mirrors the underlying LLVM semantics with a notable\n    exception: the embedded `asm_string` is not allowed to define or reference\n    any symbol or any global variable: only the operands of the op may be read,\n    written, or referenced.\n    Attempting to define or reference any symbol or any global behavior is\n    considered undefined behavior at this time.\n    If `tail_call_kind` is used, the operation behaves like the specified\n    tail call kind. The `musttail` kind it's not available for this operation,\n    since it isn't supported by LLVM's inline asm.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "res", "type": "Optional" }
    ],
    "attributes": [
      { "name": "asm_string", "type": "StrAttr" },
      { "name": "constraints", "type": "StrAttr" },
      { "name": "has_side_effects", "type": "UnitAttr" },
      { "name": "is_align_stack", "type": "UnitAttr" },
      { "name": "tail_call_kind", "type": "DefaultValuedAttr" },
      { "name": "asm_dialect", "type": "OptionalAttr" },
      { "name": "operand_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`has_side_effects` $has_side_effects^)?\n    (`is_align_stack` $is_align_stack^)?\n    (`tail_call_kind` `=` $tail_call_kind^)?\n    (`asm_dialect` `=` $asm_dialect^)?\n    (`operand_attrs` `=` $operand_attrs^)?\n    attr-dict\n    $asm_string `,` $constraints\n    operands `:` functional-type(operands, results)"
  },
  {
    "name": "llvm.insertelement",
    "summary": "Insert an element into an LLVM vector.",
    "inputs": [
      { "name": "vector", "type": "LLVM_AnyVector" },
      { "name": "value", "type": "LLVM_PrimitiveType" },
      { "name": "position", "type": "AnySignlessInteger" }
    ],
    "outputs": [
      { "name": "res", "type": "LLVM_AnyVector" }
    ],
    "assemblyFormat": "$value `,` $vector `[` $position `:` type($position) `]` attr-dict `:`\n    type($vector)"
  },
  {
    "name": "llvm.insertvalue",
    "summary": "Insert a value into an LLVM struct.",
    "inputs": [
      { "name": "container", "type": "LLVM_AnyAggregate" },
      { "name": "value", "type": "LLVM_PrimitiveType" }
    ],
    "outputs": [
      { "name": "res", "type": "LLVM_AnyAggregate" }
    ],
    "attributes": [
      { "name": "position", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$value `,` $container `` $position attr-dict `:` type($container)\n    custom<InsertExtractValueElementType>(type($value), ref(type($container)),\n                                          ref($position))"
  },
  {
    "name": "llvm.inttoptr",
    "inputs": [
      { "name": "arg", "type": "type" }
    ],
    "outputs": [
      { "name": "res", "type": "resultType" }
    ],
    "attributes": [
      { "name": "dereferenceable", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$arg (`dereferenceable` `` $dereferenceable^)? attr-dict `:` type($arg) `to` type($res)"
  },
  {
    "name": "llvm.invoke",
    "inputs": [
      { "name": "callee_operands", "type": "Variadic" },
      { "name": "normalDestOperands", "type": "Variadic" },
      { "name": "unwindDestOperands", "type": "Variadic" },
      { "name": "op_bundle_operands", "type": "VariadicOfVariadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Optional" }
    ],
    "attributes": [
      { "name": "var_callee_type", "type": "OptionalAttr" },
      { "name": "callee", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "branch_weights", "type": "OptionalAttr" },
      { "name": "CConv", "type": "DefaultValuedAttr" },
      { "name": "op_bundle_sizes", "type": "DenseI32ArrayAttr" },
      { "name": "op_bundle_tags", "type": "OptionalAttr" }
    ],
    "successors": [
      {
        "name": "normalDest"
      },
      {
        "name": "unwindDest"
      }
    ]
  },
  {
    "name": "llvm.landingpad",
    "outputs": [
      { "name": "res", "type": "LLVM_Type" }
    ],
    "attributes": [
      { "name": "cleanup", "type": "UnitAttr" }
    ]
  },
  {
    "name": "llvm.linker_options",
    "summary": "Options to pass to the linker when the object file is linked",
    "description": "Pass the given options to the linker when the resulting object file is linked.\n    This is used extensively on Windows to determine the C runtime that the object\n    files should link against.\n\n    Examples:\n    ```mlir\n    // Link against the MSVC static threaded CRT.\n    llvm.linker_options [\"/DEFAULTLIB:\", \"libcmt\"]\n\n    // Link against aarch64 compiler-rt builtins\n    llvm.linker_options [\"-l\", \"clang_rt.builtins-aarch64\"]\n    ```",
    "attributes": [
      { "name": "options", "type": "StrArrayAttr" }
    ],
    "assemblyFormat": "$options attr-dict"
  },
  {
    "name": "llvm.load",
    "description": "The `load` operation is used to read from memory. A load may be marked as\n    atomic, volatile, and/or nontemporal, and takes a number of optional\n    attributes that specify aliasing information.\n\n    An atomic load only supports a limited set of pointer, integer, and\n    floating point types, and requires an explicit alignment.\n\n    Examples:\n    ```mlir\n    // A volatile load of a float variable.\n    %0 = llvm.load volatile %ptr : !llvm.ptr -> f32\n\n    // A nontemporal load of a float variable.\n    %0 = llvm.load %ptr {nontemporal} : !llvm.ptr -> f32\n\n    // An atomic load of an integer variable.\n    %0 = llvm.load %ptr atomic monotonic {alignment = 8 : i64}\n        : !llvm.ptr -> i64\n    ```\n\n    See the following link for more details:\n    https://llvm.org/docs/LangRef.html#load-instruction",
    "outputs": [
      { "name": "res", "type": "LLVM_LoadableType" }
    ],
    "assemblyFormat": "(`volatile` $volatile_^)? $addr\n    (`atomic` (`syncscope` `(` $syncscope^ `)`)? $ordering^)?\n    (`invariant` $invariant^)?\n    (`invariant_group` $invariantGroup^)?\n    (`dereferenceable` `` $dereferenceable^)?\n    attr-dict `:` qualified(type($addr)) `->` type($res)"
  },
  {
    "name": "llvm.lshr",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "(`exact` $isExact^)? $lhs `,` $rhs attr-dict `:` type($res)"
  },
  {
    "name": "llvm.mlir.addressof",
    "summary": "Creates a pointer pointing to a global, alias or a function",
    "description": "Creates an SSA value containing a pointer to a global value (function,\n    variable or alias). The global value can be defined after its first\n    referenced. If the global value is a constant, storing into it is not\n    allowed.\n\n    Examples:\n\n    ```mlir\n    func @foo() {\n      // Get the address of a global variable.\n      %0 = llvm.mlir.addressof @const : !llvm.ptr\n\n      // Use it as a regular pointer.\n      %1 = llvm.load %0 : !llvm.ptr -> i32\n\n      // Get the address of a function.\n      %2 = llvm.mlir.addressof @foo : !llvm.ptr\n\n      // The function address can be used for indirect calls.\n      llvm.call %2() : !llvm.ptr, () -> ()\n\n      // Get the address of an aliased global.\n      %3 = llvm.mlir.addressof @const_alias : !llvm.ptr\n    }\n\n    // Define the global.\n    llvm.mlir.global @const(42 : i32) : i32\n\n    // Define an alias.\n    llvm.mlir.alias @const_alias : i32 {\n      %0 = llvm.mlir.addressof @const : !llvm.ptr\n      llvm.return %0 : !llvm.ptr\n    }\n    ```",
    "outputs": [
      { "name": "res", "type": "LLVM_AnyPointer" }
    ],
    "attributes": [
      { "name": "global_name", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$global_name attr-dict `:` qualified(type($res))"
  },
  {
    "name": "llvm.mlir.alias",
    "summary": "LLVM dialect alias.",
    "description": "`llvm.mlir.alias` is a top level operation that defines a global alias for\n    global variables and functions. The operation is always initialized by\n    using a initializer region which could be a direct map to another global\n    value or contain some address computation on top of it.\n\n    It uses a symbol for its value, which will be uniqued by the module\n    with respect to other symbols in it.\n\n    Similarly to functions and globals, they can also have a linkage attribute.\n    This attribute is placed between `llvm.mlir.alias` and the symbol name. If\n    the attribute is omitted, `external` linkage is assumed by default.\n\n    Examples:\n\n    ```mlir\n    // Global alias use @-identifiers.\n    llvm.mlir.alias external @foo_alias {addr_space = 0 : i32} : !llvm.ptr {\n      %0 = llvm.mlir.addressof @some_function : !llvm.ptr\n      llvm.return %0 : !llvm.ptr\n    }\n\n    // More complex initialization.\n    llvm.mlir.alias linkonce_odr hidden @glob\n    {addr_space = 0 : i32, dso_local} : !llvm.array<32 x i32> {\n      %0 = llvm.mlir.constant(1234 : i64) : i64\n      %1 = llvm.mlir.addressof @glob.private : !llvm.ptr\n      %2 = llvm.ptrtoint %1 : !llvm.ptr to i64\n      %3 = llvm.add %2, %0 : i64\n      %4 = llvm.inttoptr %3 : i64 to !llvm.ptr\n      llvm.return %4 : !llvm.ptr\n    }\n    ```",
    "inputs": [
      { "name": "linkage", "type": "Linkage" }
    ],
    "attributes": [
      { "name": "alias_type", "type": "TypeAttr" },
      { "name": "sym_name", "type": "StrAttr" },
      { "name": "dso_local", "type": "UnitAttr" },
      { "name": "thread_local_", "type": "UnitAttr" },
      { "name": "unnamed_addr", "type": "OptionalAttr" },
      { "name": "visibility_", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "llvm.mlir.constant",
    "summary": "Defines a constant of LLVM type.",
    "description": "Unlike LLVM IR, MLIR does not have first-class constant values. Therefore,\n    all constants must be created as SSA values before being used in other\n    operations. `llvm.mlir.constant` creates such values for scalars, vectors,\n    strings, structs, and array of structs. It has a mandatory `value` attribute\n    whose type depends on the type of the constant value. The type of the constant\n    value must correspond to the attribute type converted to LLVM IR type.\n\n    When creating constant scalars, the `value` attribute must be either an\n    integer attribute or a floating point attribute. The type of the attribute\n    may be omitted for `i64` and `f64` types that are implied.\n\n    When creating constant vectors, the `value` attribute must be either an\n    array attribute, a dense attribute, or a sparse attribute that contains\n    integers or floats. The number of elements in the result vector must match\n    the number of elements in the attribute.\n\n    When creating constant strings, the `value` attribute must be a string\n    attribute. The type of the constant must be an LLVM array of `i8`s, and the\n    length of the array must match the length of the attribute.\n\n    When creating constant structs, the `value` attribute must be an array\n    attribute that contains integers or floats. The type of the constant must be\n    an LLVM struct type. The number of fields in the struct must match the\n    number of elements in the attribute, and the type of each LLVM struct field\n    must correspond to the type of the corresponding attribute element converted\n    to LLVM IR.\n\n    When creating an array of structs, the `value` attribute must be an array\n    attribute, itself containing zero, or undef, or array attributes for each\n    potential nested array type, and the elements of the leaf array attributes\n    for must match the struct element types or be zero or undef attributes.\n\n    Examples:\n\n    ```mlir\n    // Integer constant, internal i32 is mandatory\n    %0 = llvm.mlir.constant(42 : i32) : i32\n\n    // It's okay to omit i64.\n    %1 = llvm.mlir.constant(42) : i64\n\n    // Floating point constant.\n    %2 = llvm.mlir.constant(42.0 : f32) : f32\n\n    // Splat dense vector constant.\n    %3 = llvm.mlir.constant(dense<1.0> : vector<4xf32>) : vector<4xf32>\n    ```",
    "outputs": [
      { "name": "res", "type": "LLVM_Type" }
    ],
    "attributes": [
      { "name": "value", "type": "AnyAttr" }
    ],
    "assemblyFormat": "`(` $value `)` attr-dict `:` type($res)"
  },
  {
    "name": "llvm.mlir.global",
    "summary": "LLVM dialect global.",
    "description": "Since MLIR allows for arbitrary operations to be present at the top level,\n    global variables are defined using the `llvm.mlir.global` operation. Both\n    global constants and variables can be defined, and the value may also be\n    initialized in both cases.\n\n    There are two forms of initialization syntax. Simple constants that can be\n    represented as MLIR attributes can be given in-line:\n\n    ```mlir\n    llvm.mlir.global @variable(32.0 : f32) : f32\n    ```\n\n    This initialization and type syntax is similar to `llvm.mlir.constant` and\n    may use two types: one for MLIR attribute and another for the LLVM value.\n    These types must be compatible.\n\n    More complex constants that cannot be represented as MLIR attributes can be\n    given in an initializer region:\n\n    ```mlir\n    // This global is initialized with the equivalent of:\n    //   i32* getelementptr (i32* @g2, i32 2)\n    llvm.mlir.global constant @int_gep() : !llvm.ptr {\n      %0 = llvm.mlir.addressof @g2 : !llvm.ptr\n      %1 = llvm.mlir.constant(2 : i32) : i32\n      %2 = llvm.getelementptr %0[%1]\n         : (!llvm.ptr, i32) -> !llvm.ptr, i32\n      // The initializer region must end with `llvm.return`.\n      llvm.return %2 : !llvm.ptr\n    }\n    ```\n\n    Only one of the initializer attribute or initializer region may be provided.\n\n    `llvm.mlir.global` must appear at top-level of the enclosing module. It uses\n    an @-identifier for its value, which will be uniqued by the module with\n    respect to other @-identifiers in it.\n\n    Examples:\n\n    ```mlir\n    // Global values use @-identifiers.\n    llvm.mlir.global constant @cst(42 : i32) : i32\n\n    // Non-constant values must also be initialized.\n    llvm.mlir.global @variable(32.0 : f32) : f32\n\n    // Strings are expected to be of wrapped LLVM i8 array type and do not\n    // automatically include the trailing zero.\n    llvm.mlir.global @string(\"abc\") : !llvm.array<3 x i8>\n\n    // For strings globals, the trailing type may be omitted.\n    llvm.mlir.global constant @no_trailing_type(\"foo bar\")\n\n    // A complex initializer is constructed with an initializer region.\n    llvm.mlir.global constant @int_gep() : !llvm.ptr {\n      %0 = llvm.mlir.addressof @g2 : !llvm.ptr\n      %1 = llvm.mlir.constant(2 : i32) : i32\n      %2 = llvm.getelementptr %0[%1]\n         : (!llvm.ptr, i32) -> !llvm.ptr, i32\n      llvm.return %2 : !llvm.ptr\n    }\n    ```\n\n    Similarly to functions, globals have a linkage attribute. In the custom\n    syntax, this attribute is placed between `llvm.mlir.global` and the optional\n    `constant` keyword. If the attribute is omitted, `external` linkage is\n    assumed by default.\n\n    Examples:\n\n    ```mlir\n    // A constant with internal linkage will not participate in linking.\n    llvm.mlir.global internal constant @cst(42 : i32) : i32\n\n    // By default, \"external\" linkage is assumed and the global participates in\n    // symbol resolution at link-time.\n    llvm.mlir.global @glob(0 : f32) : f32\n\n    // Alignment is optional\n    llvm.mlir.global private constant @y(dense<1.0> : tensor<8xf32>) : !llvm.array<8 x f32>\n    ```\n\n    Like global variables in LLVM IR, globals can have an (optional)\n    alignment attribute using keyword `alignment`. The integer value of the\n    alignment must be a positive integer that is a power of 2.\n\n    Examples:\n\n    ```mlir\n    // Alignment is optional\n    llvm.mlir.global private constant @y(dense<1.0> : tensor<8xf32>) { alignment = 32 : i64 } : !llvm.array<8 x f32>\n    ```\n\n    The `target_specific_attrs` attribute provides a mechanism to preserve\n    target-specific LLVM IR attributes that are not explicitly modeled in the\n    LLVM dialect.\n\n    The attribute is an array containing either string attributes or\n    two-element array attributes of strings. The value of a standalone string\n    attribute is interpreted as the name of an LLVM IR attribute on the global.\n    A two-element array is interpreted as a key-value pair.\n\n    Example:\n\n    ```mlir\n    llvm.mlir.global external @example() {\n      target_specific_attrs = [\"value-less-attr\", [\"int-attr\", \"4\"], [\"string-attr\", \"string\"]]} : f64\n    ```",
    "inputs": [
      { "name": "linkage", "type": "Linkage" }
    ],
    "attributes": [
      { "name": "global_type", "type": "TypeAttr" },
      { "name": "constant", "type": "UnitAttr" },
      { "name": "sym_name", "type": "StrAttr" },
      { "name": "dso_local", "type": "UnitAttr" },
      { "name": "thread_local_", "type": "UnitAttr" },
      { "name": "externally_initialized", "type": "UnitAttr" },
      { "name": "value", "type": "OptionalAttr" },
      { "name": "alignment", "type": "OptionalAttr" },
      { "name": "addr_space", "type": "DefaultValuedAttr" },
      { "name": "unnamed_addr", "type": "OptionalAttr" },
      { "name": "section", "type": "OptionalAttr" },
      { "name": "comdat", "type": "OptionalAttr" },
      { "name": "dbg_exprs", "type": "OptionalAttr" },
      { "name": "visibility_", "type": "DefaultValuedAttr" },
      { "name": "target_specific_attrs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "llvm.mlir.global_ctors",
    "summary": "LLVM dialect global_ctors.",
    "description": "Specifies a list of constructor functions, priorities, and associated data.\n    The functions referenced by this array will be called in ascending order\n    of priority (i.e. lowest first) when the module is loaded. The order of\n    functions with the same priority is not defined. This operation is\n    translated to LLVM's global_ctors global variable. The initializer\n    functions are run at load time. However, if the associated data is not\n    `#llvm.zero`, functions only run if the data is not discarded.\n\n    Examples:\n\n    ```mlir\n    llvm.func @ctor() {\n      ...\n      llvm.return\n    }\n    llvm.mlir.global_ctors ctors = [@ctor], priorities = [0],\n                                   data = [#llvm.zero]\n    ```",
    "attributes": [
      { "name": "ctors", "type": "FlatSymbolRefArrayAttr" },
      { "name": "priorities", "type": "I32ArrayAttr" },
      { "name": "data", "type": "ArrayAttr" }
    ],
    "assemblyFormat": "`ctors` `=` $ctors\n    `,` `priorities` `=` $priorities\n    `,` `data` `=` $data\n    attr-dict"
  },
  {
    "name": "llvm.mlir.global_dtors",
    "summary": "LLVM dialect global_dtors.",
    "description": "Specifies a list of destructor functions and priorities. The functions\n    referenced by this array will be called in descending order of priority\n    (i.e. highest first) when the module is unloaded. The order of functions\n    with the same priority is not defined. This operation is translated to\n    LLVM's global_dtors global variable. The destruction functions are run at\n    load time. However, if the associated data is not `#llvm.zero`, functions\n    only run if the data is not discarded.\n\n    Examples:\n\n    ```mlir\n    llvm.func @dtor() {\n      llvm.return\n    }\n    llvm.mlir.global_dtors dtors = [@dtor], priorities = [0],\n                                   data = [#llvm.zero]\n    ```",
    "attributes": [
      { "name": "dtors", "type": "FlatSymbolRefArrayAttr" },
      { "name": "priorities", "type": "I32ArrayAttr" },
      { "name": "data", "type": "ArrayAttr" }
    ],
    "assemblyFormat": "`dtors` `=` $dtors\n    `,` `priorities` `=` $priorities\n    `,` `data` `=` $data\n    attr-dict"
  },
  {
    "name": "llvm.mlir.ifunc",
    "summary": "LLVM dialect ifunc",
    "description": "`llvm.mlir.ifunc` is a top level operation that defines a global ifunc.\n    It defines a new symbol and takes a symbol refering to a resolver function.\n    IFuncs can be called as regular functions. The function type is the same\n    as the IFuncType. The symbol is resolved at runtime by calling a resolver\n    function.\n\n    Examples:\n\n    ```mlir\n    // IFuncs resolve a symbol at runtime using a resovler function.\n    llvm.mlir.ifunc external @foo: !llvm.func<f32 (i64)>, !llvm.ptr @resolver\n\n    llvm.func @foo_1(i64) -> f32\n    llvm.func @foo_2(i64) -> f32\n\n    llvm.func @resolve_foo() -> !llvm.ptr attributes {\n      %0 = llvm.mlir.addressof @foo_2 : !llvm.ptr\n      %1 = llvm.mlir.addressof @foo_1 : !llvm.ptr\n\n      // ... Logic selecting from foo_{1, 2}\n\n      // Return function pointer to the selected function\n      llvm.return %7 : !llvm.ptr\n    }\n\n    llvm.func @use_foo() {\n      // IFuncs are called as regular functions\n      %res = llvm.call @foo(%value) : i64 -> f32\n    }\n    ```",
    "inputs": [
      { "name": "linkage", "type": "Linkage" }
    ],
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "i_func_type", "type": "TypeAttr" },
      { "name": "resolver", "type": "FlatSymbolRefAttr" },
      { "name": "resolver_type", "type": "TypeAttr" },
      { "name": "dso_local", "type": "UnitAttr" },
      { "name": "address_space", "type": "DefaultValuedAttr" },
      { "name": "unnamed_addr", "type": "DefaultValuedAttr" },
      { "name": "visibility_", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "custom<LLVMLinkage>($linkage) ($visibility_^)? ($unnamed_addr^)?\n    $sym_name `:` $i_func_type `,` $resolver_type $resolver attr-dict"
  },
  {
    "name": "llvm.mlir.none",
    "summary": "Defines a value containing an empty token to LLVM type.",
    "description": "Unlike LLVM IR, MLIR does not have first-class token values. They must be\n    explicitly created as SSA values using `llvm.mlir.none`. This operation has\n    no operands or attributes, and returns a none token value of a wrapped LLVM IR\n    pointer type.\n\n    Examples:\n\n    ```mlir\n    %0 = llvm.mlir.none : !llvm.token\n    ```",
    "outputs": [
      { "name": "res", "type": "LLVM_TokenType" }
    ],
    "assemblyFormat": "attr-dict `:` type($res)"
  },
  {
    "name": "llvm.mlir.poison",
    "summary": "Creates a poison value of LLVM dialect type.",
    "description": "Unlike LLVM IR, MLIR does not have first-class poison values. Such values\n    must be created as SSA values using `llvm.mlir.poison`. This operation has\n    no operands or attributes. It creates a poison value of the specified LLVM\n    IR dialect type.\n\n    Example:\n\n    ```mlir\n    // Create a poison value for a structure with a 32-bit integer followed\n    // by a float.\n    %0 = llvm.mlir.poison : !llvm.struct<(i32, f32)>\n    ```",
    "outputs": [
      { "name": "res", "type": "LLVM_Type" }
    ],
    "assemblyFormat": "attr-dict `:` type($res)"
  },
  {
    "name": "llvm.mlir.undef",
    "summary": "Creates an undefined value of LLVM dialect type.",
    "description": "Unlike LLVM IR, MLIR does not have first-class undefined values. Such values\n    must be created as SSA values using `llvm.mlir.undef`. This operation has no\n    operands or attributes. It creates an undefined value of the specified LLVM\n    IR dialect type.\n\n    Example:\n\n    ```mlir\n    // Create a structure with a 32-bit integer followed by a float.\n    %0 = llvm.mlir.undef : !llvm.struct<(i32, f32)>\n    ```",
    "outputs": [
      { "name": "res", "type": "LLVM_Type" }
    ],
    "assemblyFormat": "attr-dict `:` type($res)"
  },
  {
    "name": "llvm.mlir.zero",
    "summary": "Creates a zero-initialized value of LLVM dialect type.",
    "description": "Unlike LLVM IR, MLIR does not have first-class zero-initialized values.\n    Such values must be created as SSA values using `llvm.mlir.zero`. This\n    operation has no operands or attributes. It creates a zero-initialized\n    value of the specified LLVM IR dialect type.\n\n    Example:\n\n    ```mlir\n    // Create a zero-initialized value for a structure with a 32-bit integer\n    // followed by a float.\n    %0 = llvm.mlir.zero : !llvm.struct<(i32, f32)>\n    ```",
    "outputs": [
      { "name": "res", "type": "LLVM_Type" }
    ],
    "assemblyFormat": "attr-dict `:` type($res)"
  },
  {
    "name": "llvm.module_flags",
    "summary": "Information about module properties",
    "description": "Represents the equivalent in MLIR for LLVM's `llvm.module.flags` metadata,\n    which requires a list of metadata triplets. Each triplet entry is described\n    by a `ModuleFlagAttr`.\n\n    Example:\n    ```mlir\n    llvm.module.flags [\n      #llvm.mlir.module_flag<error, \"wchar_size\", 4>,\n      #llvm.mlir.module_flag<max, \"PIC Level\", 2>\n    ]\n    ```",
    "attributes": [
      { "name": "flags", "type": "ArrayAttr" }
    ],
    "assemblyFormat": "$flags attr-dict"
  },
  {
    "name": "llvm.mul",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$lhs `,` $rhs ($overflowFlags^)? attr-dict `:` type($res)"
  },
  {
    "name": "llvm.or",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "(`disjoint` $isDisjoint^)? $lhs `,` $rhs attr-dict `:` type($res)"
  },
  {
    "name": "llvm.ptrtoint",
    "inputs": [
      { "name": "arg", "type": "type" }
    ],
    "outputs": [
      { "name": "res", "type": "resultType" }
    ],
    "assemblyFormat": "$arg attr-dict `:` type($arg) `to` type($res)"
  },
  {
    "name": "llvm.resume",
    "inputs": [
      { "name": "value", "type": "LLVM_Type" }
    ],
    "assemblyFormat": "$value attr-dict `:` type($value)"
  },
  {
    "name": "llvm.return",
    "inputs": [
      { "name": "arg", "type": "Optional" }
    ],
    "assemblyFormat": "attr-dict ($arg^ `:` type($arg))?"
  },
  {
    "name": "llvm.sdiv",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "(`exact` $isExact^)? $lhs `,` $rhs attr-dict `:` type($res)"
  },
  {
    "name": "llvm.select",
    "inputs": [
      { "name": "condition", "type": "LLVM_ScalarOrVectorOf" },
      { "name": "trueValue", "type": "LLVM_Type" },
      { "name": "falseValue", "type": "LLVM_Type" }
    ],
    "outputs": [
      { "name": "res", "type": "LLVM_Type" }
    ],
    "attributes": [
      { "name": "fastmathFlags", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($condition) `,` type($res)"
  },
  {
    "name": "llvm.sext",
    "inputs": [
      { "name": "arg", "type": "type" }
    ],
    "outputs": [
      { "name": "res", "type": "resultType" }
    ],
    "assemblyFormat": "$arg attr-dict `:` type($arg) `to` type($res)"
  },
  {
    "name": "llvm.shl",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$lhs `,` $rhs ($overflowFlags^)? attr-dict `:` type($res)"
  },
  {
    "name": "llvm.shufflevector",
    "summary": "Construct a permutation of two vectors.",
    "inputs": [
      { "name": "v1", "type": "LLVM_AnyVector" },
      { "name": "v2", "type": "LLVM_AnyVector" }
    ],
    "outputs": [
      { "name": "res", "type": "LLVM_AnyVector" }
    ],
    "attributes": [
      { "name": "mask", "type": "DenseI32ArrayAttr" }
    ],
    "assemblyFormat": "$v1 `,` $v2 $mask attr-dict `:` type($v1)\n    custom<ShuffleType>(ref(type($v1)), type($res), ref($mask))"
  },
  {
    "name": "llvm.sitofp",
    "inputs": [
      { "name": "arg", "type": "type" }
    ],
    "outputs": [
      { "name": "res", "type": "resultType" }
    ],
    "assemblyFormat": "$arg attr-dict `:` type($arg) `to` type($res)"
  },
  {
    "name": "llvm.srem",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($res)"
  },
  {
    "name": "llvm.store",
    "description": "The `store` operation is used to write to memory. A store may be marked as\n    atomic, volatile, and/or nontemporal, and takes a number of optional\n    attributes that specify aliasing information.\n\n    An atomic store only supports a limited set of pointer, integer, and\n    floating point types, and requires an explicit alignment.\n\n    Examples:\n    ```mlir\n    // A volatile store of a float variable.\n    llvm.store volatile %val, %ptr : f32, !llvm.ptr\n\n    // A nontemporal store of a float variable.\n    llvm.store %val, %ptr {nontemporal} : f32, !llvm.ptr\n\n    // An atomic store of an integer variable.\n    llvm.store %val, %ptr atomic monotonic {alignment = 8 : i64}\n        : i64, !llvm.ptr\n    ```\n\n    See the following link for more details:\n    https://llvm.org/docs/LangRef.html#store-instruction",
    "assemblyFormat": "(`volatile` $volatile_^)? $value `,` $addr\n    (`atomic` (`syncscope` `(` $syncscope^ `)`)? $ordering^)?\n    (`invariant_group` $invariantGroup^)?\n    attr-dict `:` type($value) `,` qualified(type($addr))"
  },
  {
    "name": "llvm.sub",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$lhs `,` $rhs ($overflowFlags^)? attr-dict `:` type($res)"
  },
  {
    "name": "llvm.switch",
    "inputs": [
      { "name": "value", "type": "AnySignlessInteger" },
      { "name": "defaultOperands", "type": "Variadic" },
      { "name": "caseOperands", "type": "VariadicOfVariadic" }
    ],
    "attributes": [
      { "name": "case_values", "type": "OptionalAttr" },
      { "name": "case_operand_segments", "type": "DenseI32ArrayAttr" },
      { "name": "branch_weights", "type": "OptionalAttr" }
    ],
    "successors": [
      {
        "name": "defaultDestination"
      },
      {
        "name": "caseDestinations"
      }
    ],
    "assemblyFormat": "$value `:` type($value) `,`\n    $defaultDestination (`(` $defaultOperands^ `:` type($defaultOperands) `)`)?\n    custom<SwitchOpCases>(ref(type($value)), $case_values, $caseDestinations,\n                                   $caseOperands, type($caseOperands))\n    attr-dict"
  },
  {
    "name": "llvm.trunc",
    "inputs": [
      { "name": "arg", "type": "type" },
      { "name": "overflowFlags", "type": "LLVM_IntegerOverflowFlagsProp" }
    ],
    "outputs": [
      { "name": "res", "type": "resultType" }
    ],
    "assemblyFormat": "$arg ($overflowFlags^)? attr-dict `:` type($arg) `to` type($res)"
  },
  {
    "name": "llvm.udiv",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "(`exact` $isExact^)? $lhs `,` $rhs attr-dict `:` type($res)"
  },
  {
    "name": "llvm.uitofp",
    "inputs": [
      { "name": "arg", "type": "type" }
    ],
    "outputs": [
      { "name": "res", "type": "resultType" }
    ],
    "attributes": [
      { "name": "nonNeg", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`nneg` $nonNeg^)? $arg attr-dict `:` type($arg) `to` type($res)"
  },
  {
    "name": "llvm.unreachable",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "llvm.urem",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($res)"
  },
  {
    "name": "llvm.va_arg",
    "inputs": [
      { "name": "arg", "type": "LLVM_AnyPointer" }
    ],
    "outputs": [
      { "name": "res", "type": "LLVM_Type" }
    ],
    "assemblyFormat": "$arg attr-dict `:` functional-type($arg, $res)"
  },
  {
    "name": "llvm.xor",
    "outputs": [
      { "name": "res", "type": "LLVM_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($res)"
  },
  {
    "name": "llvm.zext",
    "inputs": [
      { "name": "arg", "type": "type" }
    ],
    "outputs": [
      { "name": "res", "type": "resultType" }
    ],
    "attributes": [
      { "name": "nonNeg", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`nneg` $nonNeg^)? $arg attr-dict `:` type($arg) `to` type($res)"
  },
  {
    "name": "math.absf",
    "summary": "floating point absolute-value operation",
    "description": "The `absf` operation computes the absolute value. It takes one operand of\n    floating point type (i.e., scalar, tensor or vector) and returns one result\n    of the same type.\n\n    Example:\n\n    ```mlir\n    // Scalar absolute value.\n    %a = math.absf %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.absi",
    "summary": "integer absolute-value operation",
    "description": "The `absi` operation computes the absolute value. It takes one operand of\n    integer type (i.e., scalar, tensor or vector) and returns one result of the\n    same type.\n\n    Example:\n\n    ```mlir\n    // Scalar absolute value.\n    %a = math.absi %b : i64\n    ```",
    "inputs": [
      { "name": "operand", "type": "SignlessIntegerOrIndexLike" }
    ],
    "outputs": [
      { "name": "result", "type": "SignlessIntegerOrIndexLike" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "math.acos",
    "summary": "arcus cosine of the specified value",
    "description": "The `acos` operation computes the arcus cosine of a given value. It takes one\n    operand of floating point type (i.e., scalar, tensor or vector) and returns one\n    result of the same type.  It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar arcus cosine value.\n    %a = math.acos %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.acosh",
    "summary": "Hyperbolic arcus cosine of the given value",
    "description": "Syntax:\n\n    ```\n    operation ::= ssa-id `=` `math.acosh` ssa-use `:` type\n    ```\n\n    The `acosh` operation computes the arcus cosine of a given value.  It takes\n    one operand of floating point type (i.e., scalar, tensor or vector) and returns\n    one result of the same type. It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Hyperbolic arcus cosine of scalar value.\n    %a = math.acosh %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.asin",
    "summary": "arcus sine of the given value",
    "description": "Syntax:\n\n    ```\n    operation ::= ssa-id `=` `math.asin` ssa-use `:` type\n    ```\n\n    The `asin` operation computes the arcus sine of a given value.  It takes\n    one operand of floating point type (i.e., scalar, tensor or vector) and returns\n    one result of the same type. It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Arcus sine of scalar value.\n    %a = math.asin %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.asinh",
    "summary": "hyperbolic arcus sine of the given value",
    "description": "Syntax:\n\n    ```\n    operation ::= ssa-id `=` `math.asinh` ssa-use `:` type\n    ```\n\n    The `asinh` operation computes the hyperbolic arcus sine of a given value.  It takes\n    one operand of floating point type (i.e., scalar, tensor or vector) and returns\n    one result of the same type. It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Hyperbolic arcus sine of scalar value.\n    %a = math.asinh %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.atan",
    "summary": "arcus tangent of the given value",
    "description": "The `atan` operation computes the arcus tangent of a given value.  It takes\n    one operand of floating point type (i.e., scalar, tensor or vector) and returns\n    one result of the same type. It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Arcus tangent of scalar value.\n    %a = math.atan %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.atan2",
    "summary": "2-argument arcus tangent of the given values",
    "description": "The `atan2` operation takes two operands and returns one result, all of\n    which must be of the same type.  The operands must be of floating point type\n    (i.e., scalar, tensor or vector).\n\n    The 2-argument arcus tangent `atan2(y, x)` returns the angle in the\n    Euclidian plane between the positive x-axis and the ray through the point\n    (x, y).  It is a generalization of the 1-argument arcus tangent which\n    returns the angle on the basis of the ratio y/x.\n\n    See also https://en.wikipedia.org/wiki/Atan2\n\n    Example:\n\n    ```mlir\n    // Scalar variant.\n    %a = math.atan2 %b, %c : f32\n    ```",
    "inputs": [
      { "name": "lhs", "type": "FloatLike" },
      { "name": "rhs", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.atanh",
    "summary": "hyperbolic arcus tangent of the given value",
    "description": "Syntax:\n\n    ```\n    operation ::= ssa-id `=` `math.atanh` ssa-use `:` type\n    ```\n\n    The `atanh` operation computes the hyperbolic arcus tangent of a given value.  It takes\n    one operand of floating point type (i.e., scalar, tensor or vector) and returns\n    one result of the same type. It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Hyperbolic arcus tangent of scalar value.\n    %a = math.atanh %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.cbrt",
    "summary": "cube root of the specified value",
    "description": "The `cbrt` operation computes the cube root. It takes one operand of\n    floating point type (i.e., scalar, tensor or vector) and returns one result\n    of the same type. It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar cube root value.\n    %a = math.cbrt %b : f64\n    ```\n\n    Note: This op is not equivalent to powf(..., 1/3.0).",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.ceil",
    "summary": "ceiling of the specified value",
    "description": "The `ceil` operation computes the ceiling of a given value. It takes one\n    operand of floating point type (i.e., scalar, tensor or vector) and returns one\n    result of the same type.  It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar ceiling value.\n    %a = math.ceil %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.clampf",
    "summary": "floating point clamping operation",
    "description": "The `clampf` operation takes three operands and returns one result, each of\n    these is required to be the same type. Operands must be of floating point type\n    (i.e., scalar, tensor or vector).\n\n    The semantics of the operation are described by:\n    ```\n      clampf(value, min, max) = maxf(minf(value, min), max)\n    ```\n\n    Example:\n\n    ```mlir\n    %d = math.clampf %value to [%min, %max] : f64\n    ```",
    "inputs": [
      { "name": "value", "type": "FloatLike" },
      { "name": "min", "type": "FloatLike" },
      { "name": "max", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$value `to` ` ` `[` $min `,` $max `]` (`fastmath` `` $fastmath^)?\n    attr-dict `:` type($result)"
  },
  {
    "name": "math.copysign",
    "summary": "A copysign operation",
    "description": "The `copysign` returns a value with the magnitude of the first operand and\n    the sign of the second operand. It takes two operands and returns one result of\n    the same type. The operands must be of floating point type (i.e., scalar,\n    tensor or vector). It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar copysign value.\n    %a = math.copysign %b, %c : f64\n    ```",
    "inputs": [
      { "name": "lhs", "type": "FloatLike" },
      { "name": "rhs", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.cos",
    "summary": "cosine of the specified value",
    "description": "The `cos` operation computes the cosine of a given value. It takes one\n    operand of floating point type (i.e., scalar, tensor or vector) and returns one\n    result of the same type.  It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar cosine value.\n    %a = math.cos %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.cosh",
    "summary": "hyperbolic cosine of the specified value",
    "description": "The `cosh` operation computes the hyperbolic cosine. It takes one operand\n    of floating point type (i.e., scalar, tensor or vector) and returns one\n    result of the same type. It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar hyperbolic cosine value.\n    %a = math.cosh %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.ctlz",
    "summary": "counts the leading zeros an integer value",
    "description": "The `ctlz` operation computes the number of leading zeros of an integer value.\n    It operates on scalar, tensor or vector.\n\n    Example:\n\n    ```mlir\n    // Scalar ctlz function value.\n    %a = math.ctlz %b : i32\n    ```",
    "inputs": [
      { "name": "operand", "type": "SignlessIntegerOrIndexLike" }
    ],
    "outputs": [
      { "name": "result", "type": "SignlessIntegerOrIndexLike" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "math.ctpop",
    "summary": "counts the number of set bits of an integer value",
    "description": "The `ctpop` operation computes the number of set bits of an integer value.\n    It operates on scalar, tensor or vector.\n\n    Example:\n\n    ```mlir\n    // Scalar ctpop function value.\n    %a = math.ctpop %b : i32\n    ```",
    "inputs": [
      { "name": "operand", "type": "SignlessIntegerOrIndexLike" }
    ],
    "outputs": [
      { "name": "result", "type": "SignlessIntegerOrIndexLike" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "math.cttz",
    "summary": "counts the trailing zeros an integer value",
    "description": "The `cttz` operation computes the number of trailing zeros of an integer value.\n    It operates on scalar, tensor or vector.\n\n    Example:\n\n    ```mlir\n    // Scalar cttz function value.\n    %a = math.cttz %b : i32\n    ```",
    "inputs": [
      { "name": "operand", "type": "SignlessIntegerOrIndexLike" }
    ],
    "outputs": [
      { "name": "result", "type": "SignlessIntegerOrIndexLike" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "math.erf",
    "summary": "error function of the specified value",
    "description": "The `erf` operation computes the error function. It takes one operand of\n    floating point type (i.e., scalar, tensor or vector) and returns one result of\n    the same type. It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar error function value.\n    %a = math.erf %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.erfc",
    "summary": "complementary error function of the specified value",
    "description": "The `erfc` operation computes the complementary error function, defined as\n    1-erf(x). This function is part of libm and is needed for accuracy, since\n    simply calculating 1-erf(x) when x is close to 1 will give inaccurate results.\n    It takes one operand of floating point type (i.e., scalar,\n    tensor or vector) and returns one result of the same type. It has no\n    standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar error function value.\n    %a = math.erfc %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.exp",
    "summary": "base-e exponential of the specified value",
    "description": "The `exp` operation takes one operand of floating point type (i.e., scalar,\n    tensor or vector) and returns one result of the same type. It has no standard\n    attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar natural exponential.\n    %a = math.exp %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.exp2",
    "summary": "base-2 exponential of the specified value",
    "description": "The `exp` operation takes one operand of floating point type (i.e., scalar,\n    tensor or vector) and returns one result of the same type. It has no standard\n    attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar natural exponential.\n    %a = math.exp2 %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.expm1",
    "summary": "base-e exponential of the specified value minus 1",
    "description": "expm1(x) := exp(x) - 1\n\n    The `expm1` operation takes one operand of floating point type (i.e.,\n    scalar, tensor or vector) and returns one result of the same type. It has no\n    standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar natural exponential minus 1.\n    %a = math.expm1 %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.floor",
    "summary": "floor of the specified value",
    "description": "The `floor` operation computes the floor of a given value. It takes one\n    operand of floating point type (i.e., scalar, tensor or vector) and returns one\n    result of the same type.  It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar floor value.\n    %a = math.floor %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.fma",
    "summary": "floating point fused multipy-add operation",
    "description": "The `fma` operation takes three operands and returns one result, each of\n    these is required to be the same type. Operands must be of floating point type\n    (i.e., scalar, tensor or vector).\n\n    Example:\n\n    ```mlir\n    // Scalar fused multiply-add: d = a*b + c\n    %d = math.fma %a, %b, %c : f64\n    ```\n\n    The semantics of the operation correspond to those of the `llvm.fma`\n    [intrinsic](https://llvm.org/docs/LangRef.html#llvm-fma-intrinsic). In the\n    particular case of lowering to LLVM, this is guaranteed to lower\n    to the `llvm.fma.*` intrinsic.",
    "inputs": [
      { "name": "a", "type": "FloatLike" },
      { "name": "b", "type": "FloatLike" },
      { "name": "c", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$a `,` $b `,` $c (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.fpowi",
    "summary": "floating point raised to the signed integer power",
    "description": "The `fpowi` operation takes a `base` operand of floating point type\n    (i.e. scalar, tensor or vector) and a `power` operand of integer type\n    (also scalar, tensor or vector) and returns one result of the same type\n    as `base`. The result is `base` raised to the power of `power`.\n    The operation is elementwise for non-scalars, e.g.:\n\n    ```mlir\n    %v = math.fpowi %base, %power : vector<2xf32>, vector<2xi32\n    ```\n\n    The result is a vector of:\n\n    ```\n    [<math.fpowi %base[0], %power[0]>, <math.fpowi %base[1], %power[1]>]\n    ```\n\n    Example:\n\n    ```mlir\n    // Scalar exponentiation.\n    %a = math.fpowi %base, %power : f64, i32\n    ```",
    "inputs": [
      { "name": "lhs", "type": "FloatLike" },
      { "name": "rhs", "type": "SignlessIntegerOrIndexLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($lhs) `,` type($rhs)"
  },
  {
    "name": "math.ipowi",
    "summary": "signed integer raised to the power of operation",
    "description": "The `ipowi` operation takes two operands of integer type (i.e., scalar,\n    tensor or vector) and returns one result of the same type. Operands\n    must have the same type.\n\n    Example:\n\n    ```mlir\n    // Scalar signed integer exponentiation.\n    %a = math.ipowi %b, %c : i32\n    ```",
    "inputs": [
      { "name": "lhs", "type": "SignlessIntegerOrIndexLike" },
      { "name": "rhs", "type": "SignlessIntegerOrIndexLike" }
    ],
    "outputs": [
      { "name": "result", "type": "SignlessIntegerOrIndexLike" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($result)"
  },
  {
    "name": "math.isfinite",
    "summary": "returns true if the operand classifies as finite",
    "description": "Determines if the given floating-point number has finite value i.e. it\n    is normal, subnormal or zero, but not infinite or NaN.\n\n    Example:\n\n    ```mlir\n    %f = math.isfinite %a : f32\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "BoolLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand)"
  },
  {
    "name": "math.isinf",
    "summary": "returns true if the operand classifies as infinite",
    "description": "Determines if the given floating-point number is positive or negative\n    infinity.\n\n    Example:\n\n    ```mlir\n    %f = math.isinf %a : f32\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "BoolLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand)"
  },
  {
    "name": "math.isnan",
    "summary": "returns true if the operand classifies as NaN",
    "description": "Determines if the given floating-point number is a not-a-number (NaN)\n    value.\n\n    Example:\n\n    ```mlir\n    %f = math.isnan %a : f32\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "BoolLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand)"
  },
  {
    "name": "math.isnormal",
    "summary": "returns true if the operand classifies as normal",
    "description": "Determines if the given floating-point number is normal, i.e. is neither\n    zero, subnormal, infinite, nor NaN.\n\n    Example:\n\n    ```mlir\n    %f = math.isnormal %a : f32\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "BoolLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand)"
  },
  {
    "name": "math.log",
    "summary": "base-e logarithm of the specified value",
    "description": "Computes the base-e logarithm of the given value. It takes one operand of\n    floating point type (i.e., scalar, tensor or vector) and returns one result of\n    the same type.\n\n    Example:\n\n    ```mlir\n    // Scalar log operation.\n    %y = math.log %x : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.log10",
    "summary": "base-10 logarithm of the specified value",
    "description": "Computes the base-10 logarithm of the given value. It takes one operand of\n    floating point type (i.e., scalar, tensor or vector) and returns one result of\n    the same type.\n\n    Example:\n\n    ```mlir\n    // Scalar log10 operation.\n    %y = math.log10 %x : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.log1p",
    "summary": "Computes the natural logarithm of one plus the given value",
    "description": "Computes the base-e logarithm of one plus the given value. It takes one\n    operand of floating point type (i.e., scalar, tensor or vector) and returns one\n    result of the same type.\n\n    log1p(x) := log(1 + x)\n\n    Example:\n\n    ```mlir\n    // Scalar log1p operation.\n    %y = math.log1p %x : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.log2",
    "summary": "base-2 logarithm of the specified value",
    "description": "Computes the base-2 logarithm of the given value. It takes one operand of\n    floating point type (i.e., scalar, tensor or vector) and returns one result of\n    the same type.\n\n    Example:\n\n    ```mlir\n    // Scalar log2 operation.\n    %y = math.log2 %x : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.powf",
    "summary": "floating point raised to the power of operation",
    "description": "The `powf` operation takes two operands of floating point type (i.e.,\n    scalar, tensor or vector) and returns one result of the same type. Operands\n    must have the same type.\n\n    Example:\n\n    ```mlir\n    // Scalar exponentiation.\n    %a = math.powf %b, %c : f64\n    ```",
    "inputs": [
      { "name": "lhs", "type": "FloatLike" },
      { "name": "rhs", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.round",
    "summary": "round of the specified value",
    "description": "The `round` operation returns the operand rounded to the nearest integer\n    value in floating-point format. It takes one operand of floating point type\n    (i.e., scalar, tensor or vector) and produces one result of the same type.  The\n    operation rounds the argument to the nearest integer value in floating-point\n    format, rounding halfway cases away from zero, regardless of the current\n    rounding direction.\n\n    Example:\n\n    ```mlir\n    // Scalar round operation.\n    %a = math.round %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.roundeven",
    "summary": "round of the specified value with halfway cases to even",
    "description": "The `roundeven` operation returns the operand rounded to the nearest integer\n    value in floating-point format. It takes one operand of floating point type\n    (i.e., scalar, tensor or vector) and produces one result of the same type.  The\n    operation rounds the argument to the nearest integer value in floating-point\n    format, rounding halfway cases to even, regardless of the current\n    rounding direction.\n\n    Example:\n\n    ```mlir\n    // Scalar round operation.\n    %a = math.roundeven %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.rsqrt",
    "summary": "reciprocal of sqrt (1 / sqrt of the specified value)",
    "description": "The `rsqrt` operation computes the reciprocal of the square root. It takes\n    one operand of floating point type (i.e., scalar, tensor or vector) and returns\n    one result of the same type. It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar reciprocal square root value.\n    %a = math.rsqrt %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.sin",
    "summary": "sine of the specified value",
    "description": "The `sin` operation computes the sine of a given value. It takes one\n    operand of floating point type (i.e., scalar, tensor or vector) and returns one\n    result of the same type.  It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar sine value.\n    %a = math.sin %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.sincos",
    "summary": "sine and cosine of the specified value",
    "description": "The `sincos` operation computes both the sine and cosine of a given value\n    simultaneously. It takes one operand of floating point type (i.e., scalar,\n    tensor or vector) and returns two results of the same type. This operation\n    can be more efficient than computing sine and cosine separately when both\n    values are needed.\n\n    Example:\n\n    ```mlir\n    // Scalar sine and cosine values.\n    %sin, %cos = math.sincos %input : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "sin", "type": "FloatLike" },
      { "name": "cos", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($operand)"
  },
  {
    "name": "math.sinh",
    "summary": "hyperbolic sine of the specified value",
    "description": "The `sinh` operation computes the hyperbolic sine. It takes one operand\n    of floating point type (i.e., scalar, tensor or vector) and returns one\n    result of the same type. It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar hyperbolic sine value.\n    %a = math.sinh %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.sqrt",
    "summary": "sqrt of the specified value",
    "description": "The `sqrt` operation computes the square root. It takes one operand of\n    floating point type (i.e., scalar, tensor or vector) and returns one result of\n    the same type. It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar square root value.\n    %a = math.sqrt %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.tan",
    "summary": "tangent of the specified value",
    "description": "The `tan` operation computes the tangent. It takes one operand\n    of floating point type (i.e., scalar, tensor or vector) and returns one\n    result of the same type. It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar tangent value.\n    %a = math.tan %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.tanh",
    "summary": "hyperbolic tangent of the specified value",
    "description": "The `tanh` operation computes the hyperbolic tangent. It takes one operand\n    of floating point type (i.e., scalar, tensor or vector) and returns one\n    result of the same type. It has no standard attributes.\n\n    Example:\n\n    ```mlir\n    // Scalar hyperbolic tangent value.\n    %a = math.tanh %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "math.trunc",
    "summary": "trunc of the specified value",
    "description": "The `trunc` operation returns the operand rounded to the nearest integer\n    value in floating-point format. It takes one operand of floating point type\n    (i.e., scalar, tensor or vector) and produces one result of the same type.\n    The operation always rounds to the nearest integer not larger in magnitude\n    than the operand, regardless of the current rounding direction.\n\n    Example:\n\n    ```mlir\n    // Scalar trunc operation.\n    %a = math.trunc %b : f64\n    ```",
    "inputs": [
      { "name": "operand", "type": "FloatLike" }
    ],
    "outputs": [
      { "name": "result", "type": "FloatLike" }
    ],
    "attributes": [
      { "name": "fastmath", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$operand (`fastmath` `` $fastmath^)?\n                          attr-dict `:` type($result)"
  },
  {
    "name": "memref.alloc",
    "summary": "memory allocation operation",
    "description": "The `alloc` operation allocates a region of memory, as specified by its\n    memref type.\n\n    Example:\n\n    ```mlir\n    %0 = memref.alloc() : memref<8x64xf32, 1>\n    ```\n\n    The optional list of dimension operands are bound to the dynamic dimensions\n    specified in its memref type. In the example below, the ssa value '%d' is\n    bound to the second dimension of the memref (which is dynamic).\n\n    ```mlir\n    %0 = memref.alloc(%d) : memref<8x?xf32, 1>\n    ```\n\n    The optional list of symbol operands are bound to the symbols of the\n    memrefs affine map. In the example below, the ssa value '%s' is bound to\n    the symbol 's0' in the affine map specified in the allocs memref type.\n\n    ```mlir\n    %0 = memref.alloc()[%s] : memref<8x64xf32,\n                              affine_map<(d0, d1)[s0] -> ((d0 + s0), d1)>, 1>\n    ```\n\n    This operation returns a single ssa value of memref type, which can be used\n    by subsequent load and store operations.\n\n    The optional `alignment` attribute may be specified to ensure that the\n    region of memory that will be indexed is aligned at the specified byte\n    boundary.\n\n    ```mlir\n    %0 = memref.alloc()[%s] {alignment = 8} :\n      memref<8x64xf32, affine_map<(d0, d1)[s0] -> ((d0 + s0), d1)>, 1>\n    ```",
    "inputs": [
      { "name": "dynamicSizes", "type": "Variadic" },
      { "name": "symbolOperands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "memref", "type": "Res" }
    ],
    "attributes": [
      { "name": "alignment", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "`(`$dynamicSizes`)` (`` `[` $symbolOperands^ `]`)? attr-dict `:` type($memref)"
  },
  {
    "name": "memref.alloca",
    "summary": "stack memory allocation operation",
    "description": "The `alloca` operation allocates memory on the stack, to be automatically\n    released when control transfers back from the region of its closest\n    surrounding operation with an\n    [`AutomaticAllocationScope`](../Traits/#automaticallocationscope) trait.\n    The amount of memory allocated is specified by its memref and additional\n    operands. For example:\n\n    ```mlir\n    %0 = memref.alloca() : memref<8x64xf32>\n    ```\n\n    The optional list of dimension operands are bound to the dynamic dimensions\n    specified in its memref type. In the example below, the SSA value '%d' is\n    bound to the second dimension of the memref (which is dynamic).\n\n    ```mlir\n    %0 = memref.alloca(%d) : memref<8x?xf32>\n    ```\n\n    The optional list of symbol operands are bound to the symbols of the\n    memref's affine map. In the example below, the SSA value '%s' is bound to\n    the symbol 's0' in the affine map specified in the allocs memref type.\n\n    ```mlir\n    %0 = memref.alloca()[%s] : memref<8x64xf32,\n                               affine_map<(d0, d1)[s0] -> ((d0 + s0), d1)>>\n    ```\n\n    This operation returns a single SSA value of memref type, which can be used\n    by subsequent load and store operations. An optional alignment attribute, if\n    specified, guarantees alignment at least to that boundary. If not specified,\n    an alignment on any convenient boundary compatible with the type will be\n    chosen.",
    "inputs": [
      { "name": "dynamicSizes", "type": "Variadic" },
      { "name": "symbolOperands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "memref", "type": "Res" }
    ],
    "attributes": [
      { "name": "alignment", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "`(`$dynamicSizes`)` (`` `[` $symbolOperands^ `]`)? attr-dict `:` type($memref)"
  },
  {
    "name": "memref.alloca_scope",
    "summary": "explicitly delimited scope for stack allocation",
    "description": "The `memref.alloca_scope` operation represents an explicitly-delimited\n    scope for the alloca allocations. Any `memref.alloca` operations that are\n    used within this scope are going to be cleaned up automatically once\n    the control-flow exits the nested region. For example:\n\n    ```mlir\n    memref.alloca_scope {\n      %myalloca = memref.alloca(): memref<4x3xf32>\n      ...\n    }\n    ```\n\n    Here, `%myalloca` memref is valid within the explicitly delimited scope\n    and is automatically deallocated at the end of the given region. Conceptually,\n    `memref.alloca_scope` is a passthrough operation with\n    `AutomaticAllocationScope` that spans the body of the region within the operation.\n\n    `memref.alloca_scope` may also return results that are defined in the nested\n    region. To return a value, one should use `memref.alloca_scope.return`\n    operation:\n\n    ```mlir\n    %result = memref.alloca_scope {\n      ...\n      memref.alloca_scope.return %value\n    }\n    ```\n\n    If `memref.alloca_scope` returns no value, the `memref.alloca_scope.return ` can\n    be left out, and will be inserted implicitly.",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "memref.alloca_scope.return",
    "summary": "terminator for alloca_scope operation",
    "description": "`memref.alloca_scope.return` operation returns zero or more SSA values\n    from the region within `memref.alloca_scope`. If no values are returned,\n    the return operation may be omitted. Otherwise, it has to be present\n    to indicate which values are going to be returned. For example:\n\n    ```mlir\n    memref.alloca_scope.return %value\n    ```",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($results^ `:` type($results))?"
  },
  {
    "name": "memref.assume_alignment",
    "summary": "assumption that gives alignment information to the input memref",
    "description": "The `assume_alignment` operation takes a memref and an integer alignment\n      value. It returns a new SSA value of the same memref type, but associated\n      with the assumption that the underlying buffer is aligned to the given\n      alignment.\n\n      If the buffer isn't aligned to the given alignment, its result is poison.\n      This operation doesn't affect the semantics of a program where the\n      alignment assumption holds true. It is intended for optimization purposes,\n      allowing the compiler to generate more efficient code based on the\n      alignment assumption. The optimization is best-effort.",
    "inputs": [
      { "name": "memref", "type": "AnyMemRef" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyMemRef" }
    ],
    "attributes": [
      { "name": "alignment", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$memref `,` $alignment attr-dict `:` type($memref)"
  },
  {
    "name": "memref.atomic_rmw",
    "summary": "atomic read-modify-write operation",
    "description": "The `memref.atomic_rmw` operation provides a way to perform a read-modify-write\n    sequence that is free from data races. The kind enumeration specifies the\n    modification to perform. The value operand represents the new value to be\n    applied during the modification. The memref operand represents the buffer\n    that the read and write will be performed against, as accessed by the\n    specified indices. The arity of the indices is the rank of the memref. The\n    result represents the latest value that was stored.\n\n    Example:\n\n    ```mlir\n    %x = memref.atomic_rmw \"addf\" %value, %I[%i] : (f32, memref<10xf32>) -> f32\n    ```",
    "inputs": [
      { "name": "value", "type": "AnyTypeOf" },
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "kind", "type": "AtomicRMWKindAttr" }
    ],
    "assemblyFormat": "$kind $value `,` $memref `[` $indices `]` attr-dict `:` `(` type($value) `,`\n    type($memref) `)` `->` type($result)"
  },
  {
    "name": "memref.atomic_yield",
    "summary": "yield operation for GenericAtomicRMWOp",
    "description": "\"memref.atomic_yield\" yields an SSA value from a\n    GenericAtomicRMWOp region.",
    "inputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "$result attr-dict `:` type($result)"
  },
  {
    "name": "memref.cast",
    "summary": "memref cast operation",
    "description": "The `memref.cast` operation converts a memref from one type to an equivalent\n    type with a compatible shape. The source and destination types are\n    compatible if:\n\n    a. Both are ranked memref types with the same element type, address space,\n    and rank and:\n      1. Both have the same layout or both have compatible strided layouts.\n      2. The individual sizes (resp. offset and strides in the case of strided\n         memrefs) may convert constant dimensions to dynamic dimensions and\n         vice-versa.\n\n    If the cast converts any dimensions from an unknown to a known size, then it\n    acts as an assertion that fails at runtime if the dynamic dimensions\n    disagree with resultant destination size.\n\n    Example:\n\n    ```mlir\n    // Assert that the input dynamic shape matches the destination static shape.\n    %2 = memref.cast %1 : memref<?x?xf32> to memref<4x4xf32>\n    // Erase static shape information, replacing it with dynamic information.\n    %3 = memref.cast %1 : memref<4xf32> to memref<?xf32>\n\n    // The same holds true for offsets and strides.\n\n    // Assert that the input dynamic shape matches the destination static stride.\n    %4 = memref.cast %1 : memref<12x4xf32, strided<[?, ?], offset: ?>> to\n                          memref<12x4xf32, strided<[4, 1], offset: 5>>\n    // Erase static offset and stride information, replacing it with\n    // dynamic information.\n    %5 = memref.cast %1 : memref<12x4xf32, strided<[4, 1], offset: 5>> to\n                          memref<12x4xf32, strided<[?, ?], offset: ?>>\n    ```\n\n    b. Either or both memref types are unranked with the same element type, and\n    address space.\n\n    Example:\n\n    ```mlir\n    Cast to concrete shape.\n        %4 = memref.cast %1 : memref<*xf32> to memref<4x?xf32>\n\n    Erase rank information.\n        %5 = memref.cast %1 : memref<4x?xf32> to memref<*xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedOrUnrankedMemRef" }
    ],
    "outputs": [
      { "name": "dest", "type": "AnyRankedOrUnrankedMemRef" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($dest)"
  },
  {
    "name": "memref.collapse_shape",
    "summary": "operation to produce a memref with a smaller rank.",
    "description": "The `memref.collapse_shape` op produces a new view with a smaller rank\n    whose sizes are a reassociation of the original `view`. The operation is\n    limited to such reassociations, where subsequent, contiguous dimensions are\n    collapsed into a single dimension. Such reassociations never require\n    additional allocs or copies.\n\n    Collapsing non-contiguous dimensions is undefined behavior. When a group of\n    dimensions can be statically proven to be non-contiguous, collapses of such\n    groups are rejected in the verifier on a best-effort basis. In the general\n    case, collapses of dynamically-sized dims with dynamic strides cannot be\n    proven to be contiguous or non-contiguous due to limitations in the memref\n    type.\n\n    A reassociation is defined as a continuous grouping of dimensions and is\n    represented with an array of DenseI64ArrayAttr attribute.\n\n    Note: Only the dimensions within a reassociation group must be contiguous.\n    The remaining dimensions may be non-contiguous.\n\n    The result memref type can be zero-ranked if the source memref type is\n    statically shaped with all dimensions being unit extent. In such a case, the\n    reassociation indices must be empty.\n\n    Examples:\n\n    ```mlir\n    // Dimension collapse (i, j) -> i' and k -> k'\n    %1 = memref.collapse_shape %0 [[0, 1], [2]] :\n        memref<?x?x?xf32, stride_spec> into memref<?x?xf32, stride_spec_2>\n    ```\n\n    For simplicity, this op may not be used to cast dynamicity of dimension\n    sizes and/or strides. I.e., a result dimension must be dynamic if and only\n    if at least one dimension in the corresponding reassociation group is\n    dynamic. Similarly, the stride of a result dimension must be dynamic if and\n    only if the corresponding start dimension in the source type is dynamic.\n\n    Note: This op currently assumes that the inner strides are of the\n    source/result layout map are the faster-varying ones.",
    "inputs": [
      { "name": "src", "type": "AnyStridedMemRef" }
    ],
    "attributes": [
      { "name": "reassociation", "type": "IndexListArrayAttr" }
    ],
    "assemblyFormat": "$src $reassociation attr-dict `:` type($src) `into` type($result)"
  },
  {
    "name": "memref.copy",
    "description": "Copies the data from the source to the destination memref.\n\n    Usage:\n\n    ```mlir\n    memref.copy %arg0, %arg1 : memref<?xf32> to memref<?xf32>\n    ```\n\n    Source and destination are expected to have the same element type and shape.\n    Otherwise, the result is undefined. They may have different layouts.",
    "inputs": [
      { "name": "source", "type": "Arg" },
      { "name": "target", "type": "Arg" }
    ],
    "assemblyFormat": "$source `,` $target attr-dict `:` type($source) `to` type($target)"
  },
  {
    "name": "memref.dealloc",
    "summary": "memory deallocation operation",
    "description": "The `dealloc` operation frees the region of memory referenced by a memref\n    which was originally created by the `alloc` operation.\n    The `dealloc` operation should not be called on memrefs which alias an\n    alloc'd memref (e.g. memrefs returned by `view` operations).\n\n    Example:\n\n    ```mlir\n    %0 = memref.alloc() : memref<8x64xf32, affine_map<(d0, d1) -> (d0, d1), 1>>\n    memref.dealloc %0 : memref<8x64xf32,  affine_map<(d0, d1) -> (d0, d1), 1>>\n    ```",
    "inputs": [
      { "name": "memref", "type": "Arg" }
    ],
    "assemblyFormat": "$memref attr-dict `:` type($memref)"
  },
  {
    "name": "memref.dim",
    "summary": "dimension index operation",
    "description": "The `dim` operation takes a memref and a dimension operand of type `index`.\n    It returns the size of the requested dimension of the given memref.\n    If the dimension index is out of bounds the behavior is undefined.\n\n    The specified memref type is that of the first operand.\n\n    Example:\n\n    ```mlir\n    // Always returns 4, can be constant folded:\n    %c0 = arith.constant 0 : index\n    %x = memref.dim %A, %c0 : memref<4 x ? x f32>\n\n    // Returns the dynamic dimension of %A.\n    %c1 = arith.constant 1 : index\n    %y = memref.dim %A, %c1 : memref<4 x ? x f32>\n\n    // Equivalent generic form:\n    %x = \"memref.dim\"(%A, %c0) : (memref<4 x ? x f32>, index) -> index\n    %y = \"memref.dim\"(%A, %c1) : (memref<4 x ? x f32>, index) -> index\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyNon0RankedOrUnrankedMemRef" },
      { "name": "index", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "attr-dict $source `,` $index `:` type($source)"
  },
  {
    "name": "memref.distinct_objects",
    "summary": "assumption that acesses to specific memrefs will never alias",
    "description": "The `distinct_objects` operation takes a list of memrefs and returns the same\n      memrefs, with the additional assumption that accesses to them will never\n      alias with each other. This means that loads and stores to different\n      memrefs in the list can be safely reordered.\n\n      If the memrefs do alias, the load/store behavior is undefined. This\n      operation doesn't affect the semantics of a valid program. It is\n      intended for optimization purposes, allowing the compiler to generate more\n      efficient code based on the non-aliasing assumption. The optimization is\n      best-effort.\n\n      Example:\n\n      ```mlir\n      %1, %2 = memref.distinct_objects %a, %b : memref<?xf32>, memref<?xf32>\n      ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$operands attr-dict `:` type($operands)"
  },
  {
    "name": "memref.dma_start",
    "summary": "non-blocking DMA operation that starts a transfer",
    "description": "Syntax:\n\n    ```\n    operation ::= `memref.dma_start` ssa-use`[`ssa-use-list`]` `,`\n                   ssa-use`[`ssa-use-list`]` `,` ssa-use `,`\n                   ssa-use`[`ssa-use-list`]` (`,` ssa-use `,` ssa-use)?\n                  `:` memref-type `,` memref-type `,` memref-type\n    ```\n\n    DmaStartOp starts a non-blocking DMA operation that transfers data from a\n    source memref to a destination memref. The source and destination memref\n    need not be of the same dimensionality, but need to have the same elemental\n    type. The operands include the source and destination memref's each followed\n    by its indices, size of the data transfer in terms of the number of elements\n    (of the elemental type of the memref), a tag memref with its indices, and\n    optionally at the end, a stride and a number_of_elements_per_stride\n    arguments. The tag location is used by a DmaWaitOp to check for completion.\n    The indices of the source memref, destination memref, and the tag memref\n    have the same restrictions as any load/store. The optional stride arguments\n    should be of 'index' type, and specify a stride for the slower memory space\n    (memory space with a lower memory space id), transferring chunks of\n    number_of_elements_per_stride every stride until %num_elements are\n    transferred. Either both or no stride arguments should be specified. If the\n    source and destination locations overlap the behavior of this operation is\n    not defined.\n\n    For example, a DmaStartOp operation that transfers 256 elements of a memref\n    '%src' in memory space 0 at indices [%i, %j] to memref '%dst' in memory\n    space 1 at indices [%k, %l], would be specified as follows:\n\n    ```mlir\n    %num_elements = arith.constant 256\n    %idx = arith.constant 0 : index\n    %tag = memref.alloc() : memref<1 x i32, affine_map<(d0) -> (d0)>, 4>\n    dma_start %src[%i, %j], %dst[%k, %l], %num_elements, %tag[%idx] :\n      memref<40 x 128 x f32>, affine_map<(d0) -> (d0)>, 0>,\n      memref<2 x 1024 x f32>, affine_map<(d0) -> (d0)>, 1>,\n      memref<1 x i32>, affine_map<(d0) -> (d0)>, 2>\n    ```\n\n    If %stride and %num_elt_per_stride are specified, the DMA is expected to\n    transfer %num_elt_per_stride elements every %stride elements apart from\n    memory space 0 until %num_elements are transferred.\n\n    ```mlir\n    dma_start %src[%i, %j], %dst[%k, %l], %num_elements, %tag[%idx], %stride,\n              %num_elt_per_stride :\n    ```\n\n    * TODO: add additional operands to allow source and destination striding, and\n    multiple stride levels.\n    * TODO: Consider replacing src/dst memref indices with view memrefs.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ]
  },
  {
    "name": "memref.dma_wait",
    "summary": "blocking DMA operation that waits for transfer completion",
    "description": "DmaWaitOp blocks until the completion of a DMA operation associated with the\n   tag element '%tag[%index]'. %tag is a memref, and %index has to be an index\n   with the same restrictions as any load/store index. %num_elements is the\n   number of elements associated with the DMA operation.\n\n   Example:\n\n   ```mlir\n    dma_start %src[%i, %j], %dst[%k, %l], %num_elements, %tag[%index] :\n      memref<2048 x f32>, affine_map<(d0) -> (d0)>, 0>,\n      memref<256 x f32>, affine_map<(d0) -> (d0)>, 1>\n      memref<1 x i32>, affine_map<(d0) -> (d0)>, 2>\n    ...\n    ...\n    dma_wait %tag[%index], %num_elements : memref<1 x i32, affine_map<(d0) -> (d0)>, 2>\n    ```",
    "inputs": [
      { "name": "tagMemRef", "type": "AnyMemRef" },
      { "name": "tagIndices", "type": "Variadic" },
      { "name": "numElements", "type": "Index" }
    ],
    "assemblyFormat": "$tagMemRef `[` $tagIndices `]` `,` $numElements attr-dict `:` type($tagMemRef)"
  },
  {
    "name": "memref.expand_shape",
    "summary": "operation to produce a memref with a higher rank.",
    "description": "The `memref.expand_shape` op produces a new view with a higher rank whose\n    sizes are a reassociation of the original `view`. The operation is limited\n    to such reassociations, where a dimension is expanded into one or multiple\n    contiguous dimensions. Such reassociations never require additional allocs\n    or copies.\n\n    A reassociation is defined as a grouping of dimensions and is represented\n    with an array of DenseI64ArrayAttr attributes.\n\n    Example:\n\n    ```mlir\n    %r = memref.expand_shape %0 [[0, 1], [2]] output_shape [%sz0, %sz1, 32]\n        : memref<?x32xf32> into memref<?x?x32xf32>\n    ```\n\n    If an op can be statically proven to be invalid (e.g, an expansion from\n    `memref<10xf32>` to `memref<2x6xf32>`), it is rejected by the verifier. If\n    it cannot statically be proven invalid (e.g., the full example above; it is\n    unclear whether the first source dimension is divisible by 5), the op is\n    accepted by the verifier. However, if the op is in fact invalid at runtime,\n    the behavior is undefined.\n\n    The source memref can be zero-ranked. In that case, the reassociation\n    indices must be empty and the result shape may only consist of unit\n    dimensions.\n\n    For simplicity, this op may not be used to cast dynamicity of dimension\n    sizes and/or strides. I.e., if and only if a source dimension is dynamic,\n    there must be a dynamic result dimension in the corresponding reassociation\n    group. Same for strides.\n\n    The representation for the output shape supports a partially-static\n    specification via attributes specified through the `static_output_shape`\n    argument.  A special sentinel value `ShapedType::kDynamic` encodes that the\n    corresponding entry has a dynamic value.  There must be exactly as many SSA\n    inputs in `output_shape` as there are `ShapedType::kDynamic` entries in\n    `static_output_shape`.\n\n    Note: This op currently assumes that the inner strides are of the\n    source/result layout map are the faster-varying ones.",
    "inputs": [
      { "name": "src", "type": "AnyStridedMemRef" },
      { "name": "output_shape", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "reassociation", "type": "IndexListArrayAttr" },
      { "name": "static_output_shape", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$src $reassociation `output_shape`\n    custom<DynamicIndexList>($output_shape, $static_output_shape) attr-dict `:`\n    type($src) `into` type($result)"
  },
  {
    "name": "memref.extract_aligned_pointer_as_index",
    "summary": "Extracts a memref's underlying aligned pointer as an index",
    "description": "Extracts the underlying aligned pointer as an index.\n\n    This operation is useful for lowering to lower-level dialects while still\n    avoiding the need to define a pointer type in higher-level dialects such as\n    the memref dialect.\n\n    This operation is intended solely as step during lowering, it has no side\n    effects. A reverse operation that creates a memref from an index interpreted\n    as a pointer is explicitly discouraged.\n\n    Example:\n\n    ```\n      %0 = memref.extract_aligned_pointer_as_index %arg : memref<4x4xf32> -> index\n      %1 = arith.index_cast %0 : index to i64\n      %2 = llvm.inttoptr %1 : i64 to !llvm.ptr\n      call @foo(%2) : (!llvm.ptr) ->()\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedOrUnrankedMemRef" }
    ],
    "outputs": [
      { "name": "aligned_pointer", "type": "Index" }
    ],
    "assemblyFormat": "$source `:` type($source) `->` type(results) attr-dict"
  },
  {
    "name": "memref.extract_strided_metadata",
    "summary": "Extracts a buffer base with offset and strides",
    "description": "Extracts a base buffer, offset and strides. This op allows additional layers\n    of transformations and foldings to be added as lowering progresses from\n    higher-level dialect to lower-level dialects such as the LLVM dialect.\n\n    The op requires a strided memref source operand. If the source operand is not\n    a strided memref, then verification fails.\n\n    This operation is also useful for completeness to the existing memref.dim op.\n    While accessing strides, offsets and the base pointer independently is not\n    available, this is useful for composing with its natural complement op:\n    `memref.reinterpret_cast`.\n\n    Intended Use Cases:\n\n    The main use case is to expose the logic for manipulate memref metadata at a\n    higher level than the LLVM dialect.\n    This makes lowering more progressive and brings the following benefits:\n      - not all users of MLIR want to lower to LLVM and the information to e.g.\n        lower to library calls---like libxsmm---or to SPIR-V was not available.\n      - foldings and canonicalizations can happen at a higher level in MLIR:\n        before this op existed, lowering to LLVM would create large amounts of\n        LLVMIR. Even when LLVM does a good job at folding the low-level IR from\n        a performance perspective, it is unnecessarily opaque and inefficient to\n        send unkempt IR to LLVM.\n\n    Example:\n\n    ```mlir\n      %base, %offset, %sizes:2, %strides:2 =\n        memref.extract_strided_metadata %memref :\n          memref<10x?xf32>, index, index, index, index, index\n\n      // After folding, the type of %m2 can be memref<10x?xf32> and further\n      // folded to %memref.\n      %m2 = memref.reinterpret_cast %base to\n          offset: [%offset],\n          sizes: [%sizes#0, %sizes#1],\n          strides: [%strides#0, %strides#1]\n        : memref<f32> to memref<?x?xf32, offset: ?, strides: [?, ?]>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyStridedMemRef" }
    ],
    "outputs": [
      { "name": "base_buffer", "type": "AnyStridedMemRefOfRank" },
      { "name": "offset", "type": "Index" },
      { "name": "sizes", "type": "Variadic" },
      { "name": "strides", "type": "Variadic" }
    ],
    "assemblyFormat": "$source `:` type($source) `->` type(results) attr-dict"
  },
  {
    "name": "memref.generic_atomic_rmw",
    "summary": "atomic read-modify-write operation with a region",
    "description": "The `memref.generic_atomic_rmw` operation provides a way to perform a\n    read-modify-write sequence that is free from data races. The memref operand\n    represents the buffer that the read and write will be performed against, as\n    accessed by the specified indices. The arity of the indices is the rank of\n    the memref. The result represents the latest value that was stored. The\n    region contains the code for the modification itself. The entry block has\n    a single argument that represents the value stored in `memref[indices]`\n    before the write is performed. No side-effecting ops are allowed in the\n    body of `GenericAtomicRMWOp`.\n\n    Example:\n\n    ```mlir\n    %x = memref.generic_atomic_rmw %I[%i] : memref<10xf32> {\n      ^bb0(%current_value : f32):\n        %c1 = arith.constant 1.0 : f32\n        %inc = arith.addf %c1, %current_value : f32\n        memref.atomic_yield %inc : f32\n    }\n    ```",
    "inputs": [
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "memref.get_global",
    "summary": "get the memref pointing to a global variable",
    "description": "The `memref.get_global` operation retrieves the memref pointing to a\n     named global variable. If the global variable is marked constant, writing\n     to the result memref (such as through a `memref.store` operation) is\n     undefined.\n\n     Example:\n\n     ```mlir\n     %x = memref.get_global @foo : memref<2xf32>\n     ```",
    "outputs": [
      { "name": "result", "type": "AnyStaticShapeMemRef" }
    ],
    "attributes": [
      { "name": "name", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$name `:` type($result) attr-dict"
  },
  {
    "name": "memref.global",
    "summary": "declare or define a global memref variable",
    "description": "The `memref.global` operation declares or defines a named global memref\n    variable. The backing memory for the variable is allocated statically and is\n    described by the type of the variable (which should be a statically shaped\n    memref type). The operation is a declaration if no `initial_value` is\n    specified, else it is a definition. The `initial_value` can either be a unit\n    attribute to represent a definition of an uninitialized global variable, or\n    an elements attribute to represent the definition of a global variable with\n    an initial value. The global variable can also be marked constant using the\n    `constant` unit attribute. Writing to such constant global variables is\n    undefined.\n\n    The global variable can be accessed by using the `memref.get_global` to\n    retrieve the memref for the global variable. Note that the memref\n    for such global variable itself is immutable (i.e., memref.get_global for a\n    given global variable will always return the same memref descriptor).\n\n    Example:\n\n    ```mlir\n    // Private variable with an initial value.\n    memref.global \"private\" @x : memref<2xf32> = dense<0.0,2.0>\n\n    // Private variable with an initial value and an alignment (power of 2).\n    memref.global \"private\" @x : memref<2xf32> = dense<0.0,2.0> {alignment = 64}\n\n    // Declaration of an external variable.\n    memref.global \"private\" @y : memref<4xi32>\n\n    // Uninitialized externally visible variable.\n    memref.global @z : memref<3xf16> = uninitialized\n\n    // Externally visible constant variable.\n    memref.global constant @c : memref<2xi32> = dense<1, 4>\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "type", "type": "MemRefTypeAttr" },
      { "name": "initial_value", "type": "OptionalAttr" },
      { "name": "constant", "type": "UnitAttr" },
      { "name": "alignment", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($sym_visibility^)?\n       (`constant` $constant^)?\n       $sym_name `:`\n       custom<GlobalMemrefOpTypeAndInitialValue>($type, $initial_value)\n       attr-dict"
  },
  {
    "name": "memref.load",
    "summary": "load operation",
    "description": "The `load` op reads an element from a memref at the specified indices.\n\n    The number of indices must match the rank of the memref. The indices must\n    be in-bounds: `0 <= idx < dim_size`.\n\n    Lowerings of `memref.load` may emit attributes, e.g. `inbouds` + `nuw`\n    when converting to LLVM's `llvm.getelementptr`, that would cause undefined\n    behavior if indices are out of bounds or if computing the offset in the\n    memref would cause signed overflow of the `index` type.\n\n    The single result of `memref.load` is a value with the same type as the\n    element type of the memref.\n\n    A set `nontemporal` attribute indicates that this load is not expected to\n    be reused in the cache. For details, refer to the\n    [LLVM load instruction](https://llvm.org/docs/LangRef.html#load-instruction).\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    load operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violations may lead to\n    architecture-specific faults or performance penalties.\n    A value of 0 indicates no specific alignment requirement.\n    Example:\n\n    ```mlir\n    %0 = memref.load %A[%a, %b] : memref<8x?xi32, #layout, memspace0>\n    ```",
    "inputs": [
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "nontemporal", "type": "DefaultValuedOptionalAttr" },
      { "name": "alignment", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$memref `[` $indices `]` attr-dict `:` type($memref)"
  },
  {
    "name": "memref.memory_space_cast",
    "summary": "memref memory space cast operation",
    "description": "This operation casts memref values between memory spaces.\n    The input and result will be memrefs of the same types and shape that alias\n    the same underlying memory, though, for some casts on some targets,\n    the underlying values of the pointer stored in the memref may be affected\n    by the cast.\n\n    The input and result must have the same shape, element type, rank, and layout.\n\n    If the source and target address spaces are the same, this operation is a noop.\n\n    Finally, if the target memory-space is the generic/default memory-space,\n    then it is assumed this cast can be bubbled down safely. See the docs of\n    `MemorySpaceCastOpInterface` interface for more details.\n\n    Example:\n\n    ```mlir\n    // Cast a GPU private memory attribution into a generic pointer\n    %2 = memref.memory_space_cast %1 : memref<?xf32, 5> to memref<?xf32>\n    // Cast a generic pointer to workgroup-local memory\n    %4 = memref.memory_space_cast %3 : memref<5x4xi32> to memref<5x34xi32, 3>\n    // Cast between two non-default memory spaces\n    %6 = memref.memory_space_cast %5\n      : memref<*xmemref<?xf32>, 5> to memref<*xmemref<?xf32>, 3>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedOrUnrankedMemRef" }
    ],
    "outputs": [
      { "name": "dest", "type": "AnyRankedOrUnrankedMemRef" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($dest)"
  },
  {
    "name": "memref.prefetch",
    "summary": "prefetch operation",
    "description": "The \"prefetch\" op prefetches data from a memref location described with\n    subscript indices similar to memref.load, and with three attributes: a\n    read/write specifier, a locality hint, and a cache type specifier as shown\n    below:\n\n    ```mlir\n    memref.prefetch %0[%i, %j], read, locality<3>, data : memref<400x400xi32>\n    ```\n\n    The read/write specifier is either 'read' or 'write', the locality hint\n    ranges from locality<0> (no locality) to locality<3> (extremely local keep\n    in cache). The cache type specifier is either 'data' or 'instr'\n    and specifies whether the prefetch is performed on data cache or on\n    instruction cache.",
    "inputs": [
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "isWrite", "type": "BoolAttr" },
      { "name": "localityHint", "type": "ConfinedAttr" },
      { "name": "isDataCache", "type": "BoolAttr" }
    ]
  },
  {
    "name": "memref.rank",
    "summary": "rank operation",
    "description": "The `memref.rank` operation takes a memref operand and returns its rank.\n\n    Example:\n\n    ```mlir\n    %0 = memref.rank %arg0 : memref<*xf32>\n    %1 = memref.rank %arg1 : memref<?x?xf32>\n    ```",
    "inputs": [
      { "name": "memref", "type": "AnyRankedOrUnrankedMemRef" }
    ],
    "assemblyFormat": "$memref attr-dict `:` type($memref)"
  },
  {
    "name": "memref.realloc",
    "summary": "memory reallocation operation",
    "description": "The `realloc` operation changes the size of a memory region. The memory\n    region is specified by a 1D source memref and the size of the new memory\n    region is specified by a 1D result memref type and an optional dynamic Value\n    of `Index` type. The source and the result memref must be in the same memory\n    space and have the same element type.\n\n    The operation may move the memory region to a new location. In this case,\n    the content of the memory block is preserved up to the lesser of the new\n    and old sizes. If the new size if larger, the value of the extended memory\n    is undefined. This is consistent with the ISO C realloc.\n\n    The operation returns an SSA value for the memref.\n\n    Example:\n\n    ```mlir\n    %0 = memref.realloc %src : memref<64xf32> to memref<124xf32>\n    ```\n\n    The source memref may have a dynamic shape, in which case, the compiler will\n    generate code to extract its size from the runtime data structure for the\n    memref.\n\n    ```mlir\n    %1 = memref.realloc %src : memref<?xf32> to memref<124xf32>\n    ```\n\n    If the result memref has a dynamic shape, a result dimension operand is\n    needed to spefify its dynamic dimension. In the example below, the ssa value\n    '%d' specifies the unknown dimension of the result memref.\n\n    ```mlir\n    %2 = memref.realloc %src(%d) : memref<?xf32> to memref<?xf32>\n    ```\n\n    An optional `alignment` attribute may be specified to ensure that the\n    region of memory that will be indexed is aligned at the specified byte\n    boundary.  This is consistent with the fact that memref.alloc supports such\n    an optional alignment attribute. Note that in ISO C standard, neither alloc\n    nor realloc supports alignment, though there is aligned_alloc but not\n    aligned_realloc.\n\n    ```mlir\n    %3 = memref.realloc %src {alignment = 8} : memref<64xf32> to memref<124xf32>\n    ```\n\n    Referencing the memref through the old SSA value after realloc is undefined\n    behavior.\n\n    ```mlir\n    %new = memref.realloc %old : memref<64xf32> to memref<124xf32>\n    %4 = memref.load %new[%index]   // ok\n    %5 = memref.load %old[%index]   // undefined behavior\n    ```",
    "inputs": [
      { "name": "source", "type": "Arg" },
      { "name": "dynamicResultSize", "type": "Optional" }
    ],
    "attributes": [
      { "name": "alignment", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$source (`(` $dynamicResultSize^ `)`)? attr-dict\n    `:` type($source) `to` type(results)"
  },
  {
    "name": "memref.reinterpret_cast",
    "summary": "memref reinterpret cast operation",
    "description": "Modify offset, sizes and strides of an unranked/ranked memref.\n\n    Example 1:\n\n    Consecutive `reinterpret_cast` operations on memref's with static\n    dimensions.\n\n    We distinguish between *underlying memory* — the sequence of elements as\n    they appear in the contiguous memory of the memref — and the\n    *strided memref*, which refers to the underlying memory interpreted\n    according to specified offsets, sizes, and strides.\n\n    ```mlir\n    %result1 = memref.reinterpret_cast %arg0 to\n      offset: [9],\n      sizes: [4, 4],\n      strides: [16, 2]\n    : memref<8x8xf32, strided<[8, 1], offset: 0>> to\n      memref<4x4xf32, strided<[16, 2], offset: 9>>\n\n    %result2 = memref.reinterpret_cast %result1 to\n      offset: [0],\n      sizes: [2, 2],\n      strides: [4, 2]\n    : memref<4x4xf32, strided<[16, 2], offset: 9>> to\n      memref<2x2xf32, strided<[4, 2], offset: 0>>\n    ```\n\n    The underlying memory of `%arg0` consists of a linear sequence of integers\n    from 1 to 64. Its memref has the following 8x8 elements:\n\n    ```mlir\n    [[1,  2,  3,  4,  5,  6,  7,  8],\n    [9,  10, 11, 12, 13, 14, 15, 16],\n    [17, 18, 19, 20, 21, 22, 23, 24],\n    [25, 26, 27, 28, 29, 30, 31, 32],\n    [33, 34, 35, 36, 37, 38, 39, 40],\n    [41, 42, 43, 44, 45, 46, 47, 48],\n    [49, 50, 51, 52, 53, 54, 55, 56],\n    [57, 58, 59, 60, 61, 62, 63, 64]]\n    ```\n\n    Following the first `reinterpret_cast`, the strided memref elements\n    of `%result1` are:\n\n    ```mlir\n    [[10, 12, 14, 16],\n    [26, 28, 30, 32],\n    [42, 44, 46, 48],\n    [58, 60, 62, 64]]\n    ```\n\n    Note: The offset and strides are relative to the underlying memory of\n    `%arg0`.\n\n    The second `reinterpret_cast` results in the following strided memref\n    for `%result2`:\n\n    ```mlir\n    [[1, 3],\n    [5, 7]]\n    ```\n\n    Notice that it does not matter if you use %result1 or %arg0 as a source\n    for the second `reinterpret_cast` operation. Only the underlying memory\n    pointers will be reused.\n\n    The offset and stride are relative to the base underlying memory of the\n    memref, starting at 1, not at 10 as seen in the output of `%result1`.\n    This behavior contrasts with the `subview` operator, where values are\n    relative to the strided memref (refer to `subview` examples).\n    Consequently, the second `reinterpret_cast` behaves as if `%arg0` were\n    passed directly as its argument.\n\n    Example 2:\n    ```mlir\n    memref.reinterpret_cast %ranked to\n      offset: [0],\n      sizes: [%size0, 10],\n      strides: [1, %stride1]\n    : memref<?x?xf32> to memref<?x10xf32, strided<[1, ?], offset: 0>>\n\n    memref.reinterpret_cast %unranked to\n      offset: [%offset],\n      sizes: [%size0, %size1],\n      strides: [%stride0, %stride1]\n    : memref<*xf32> to memref<?x?xf32, strided<[?, ?], offset: ?>>\n    ```\n\n    This operation creates a new memref descriptor using the base of the\n    source and applying the input arguments to the other metadata.\n    In other words:\n    ```mlir\n    %dst = memref.reinterpret_cast %src to\n      offset: [%offset],\n      sizes: [%sizes],\n      strides: [%strides]\n    ```\n    means that `%dst`'s descriptor will be:\n    ```mlir\n    %dst.base = %src.base\n    %dst.aligned = %src.aligned\n    %dst.offset = %offset\n    %dst.sizes = %sizes\n    %dst.strides = %strides\n    ```",
    "inputs": [
      { "name": "source", "type": "Arg" },
      { "name": "offsets", "type": "Variadic" },
      { "name": "sizes", "type": "Variadic" },
      { "name": "strides", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyStridedMemRef" }
    ],
    "attributes": [
      { "name": "static_offsets", "type": "DenseI64ArrayAttr" },
      { "name": "static_sizes", "type": "DenseI64ArrayAttr" },
      { "name": "static_strides", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$source `to` `offset` `` `:`\n    custom<DynamicIndexList>($offsets, $static_offsets)\n    `` `,` `sizes` `` `:`\n    custom<DynamicIndexList>($sizes, $static_sizes)\n    `` `,` `strides` `` `:`\n    custom<DynamicIndexList>($strides, $static_strides)\n    attr-dict `:` type($source) `to` type($result)"
  },
  {
    "name": "memref.reshape",
    "summary": "memref reshape operation",
    "description": "The `reshape` operation converts a memref from one type to an\n    equivalent type with a provided shape. The data is never copied or\n    modified. The source and destination types are compatible if both have the\n    same element type, same number of elements, address space and identity\n    layout map. The following combinations are possible:\n\n    a. Source type is ranked or unranked. Shape argument has static size.\n    Result type is ranked.\n\n    ```mlir\n    // Reshape statically-shaped memref.\n    %dst = memref.reshape %src(%shape)\n             : (memref<4x1xf32>, memref<1xi32>) to memref<4xf32>\n    %dst0 = memref.reshape %src(%shape0)\n             : (memref<4x1xf32>, memref<2xi32>) to memref<2x2xf32>\n    // Flatten unranked memref.\n    %dst = memref.reshape %src(%shape)\n             : (memref<*xf32>, memref<1xi32>) to memref<?xf32>\n    ```\n\n    b. Source type is ranked or unranked. Shape argument has dynamic size.\n    Result type is unranked.\n\n    ```mlir\n    // Reshape dynamically-shaped 1D memref.\n    %dst = memref.reshape %src(%shape)\n             : (memref<?xf32>, memref<?xi32>) to memref<*xf32>\n    // Reshape unranked memref.\n    %dst = memref.reshape %src(%shape)\n             : (memref<*xf32>, memref<?xi32>) to memref<*xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedOrUnrankedMemRef" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedOrUnrankedMemRef" }
    ],
    "assemblyFormat": "$source `(` $shape `)` attr-dict `:` functional-type(operands, results)",
    "category": "Shape"
  },
  {
    "name": "memref.store",
    "summary": "store operation",
    "description": "The `store` op stores an element into a memref at the specified indices.\n\n    The number of indices must match the rank of the memref. The indices must\n    be in-bounds: `0 <= idx < dim_size`.\n\n    Lowerings of `memref.store` may emit attributes, e.g. `inbouds` + `nuw`\n    when converting to LLVM's `llvm.getelementptr`, that would cause undefined\n    behavior if indices are out of bounds or if computing the offset in the\n    memref would cause signed overflow of the `index` type.\n\n    A set `nontemporal` attribute indicates that this store is not expected to\n    be reused in the cache. For details, refer to the\n    [LLVM store instruction](https://llvm.org/docs/LangRef.html#store-instruction).\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    store operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violations may lead to\n    architecture-specific faults or performance penalties.\n    A value of 0 indicates no specific alignment requirement.\n    Example:\n\n    ```mlir\n    memref.store %val, %A[%a, %b] : memref<8x?xi32, #layout, memspace0>\n    ```",
    "inputs": [
      { "name": "value", "type": "AnyType" },
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "nontemporal", "type": "DefaultValuedOptionalAttr" },
      { "name": "alignment", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$value `,` $memref `[` $indices `]` attr-dict `:` type($memref)"
  },
  {
    "name": "memref.subview",
    "summary": "memref subview operation",
    "description": "The `subview` operation converts a memref type to a memref type which\n    represents a reduced-size view of the original memref as specified by the\n    operation's offsets, sizes and strides arguments.\n\n    The `subview` operation supports the following arguments:\n\n    * source: the \"base\" memref on which to create a \"view\" memref.\n    * offsets: memref-rank number of offsets into the \"base\" memref at which to\n               create the \"view\" memref.\n    * sizes: memref-rank number of sizes which specify the sizes of the result\n             \"view\" memref type.\n    * strides: memref-rank number of strides that compose multiplicatively with\n               the base memref strides in each dimension.\n\n    The representation based on offsets, sizes and strides support a\n    partially-static specification via attributes specified through the\n    `static_offsets`, `static_sizes` and `static_strides` arguments. A special\n    sentinel value `ShapedType::kDynamic` encodes that the corresponding entry\n    has a dynamic value.\n\n    A `subview` operation may additionally reduce the rank of the resulting\n    view by removing dimensions that are statically known to be of size 1.\n\n    In the absence of rank reductions, the resulting memref type is computed\n    as follows:\n    ```\n    result_sizes[i] = size_operands[i]\n    result_strides[i] = src_strides[i] * stride_operands[i]\n    result_offset = src_offset + dot_product(offset_operands, src_strides)\n    ```\n\n    The offset, size and stride operands must be in-bounds with respect to the\n    source memref. When possible, the static operation verifier will detect\n    out-of-bounds subviews. Subviews that cannot be confirmed to be in-bounds\n    or out-of-bounds based on compile-time information are valid. However,\n    performing an out-of-bounds subview at runtime is undefined behavior.\n\n    Example 1:\n\n    Consecutive `subview` operations on memref's with static dimensions.\n\n    We distinguish between *underlying memory* — the sequence of elements as\n    they appear in the contiguous memory of the memref — and the\n    *strided memref*, which refers to the underlying memory interpreted\n    according to specified offsets, sizes, and strides.\n\n    ```mlir\n    %result1 = memref.subview %arg0[1, 1][4, 4][2, 2]\n    : memref<8x8xf32, strided<[8, 1], offset: 0>> to\n      memref<4x4xf32, strided<[16, 2], offset: 9>>\n\n    %result2 = memref.subview %result1[1, 1][2, 2][2, 2]\n    : memref<4x4xf32, strided<[16, 2], offset: 9>> to\n      memref<2x2xf32, strided<[32, 4], offset: 27>>\n    ```\n\n    The underlying memory of `%arg0` consists of a linear sequence of integers\n    from 1 to 64. Its memref has the following 8x8 elements:\n\n    ```mlir\n    [[1,  2,  3,  4,  5,  6,  7,  8],\n    [9,  10, 11, 12, 13, 14, 15, 16],\n    [17, 18, 19, 20, 21, 22, 23, 24],\n    [25, 26, 27, 28, 29, 30, 31, 32],\n    [33, 34, 35, 36, 37, 38, 39, 40],\n    [41, 42, 43, 44, 45, 46, 47, 48],\n    [49, 50, 51, 52, 53, 54, 55, 56],\n    [57, 58, 59, 60, 61, 62, 63, 64]]\n    ```\n\n    Following the first `subview`, the strided memref elements of `%result1`\n    are:\n\n    ```mlir\n    [[10, 12, 14, 16],\n    [26, 28, 30, 32],\n    [42, 44, 46, 48],\n    [58, 60, 62, 64]]\n    ```\n\n    Note: The offset and strides are relative to the strided memref of `%arg0`\n    (compare to the corresponding `reinterpret_cast` example).\n\n    The second `subview` results in the following strided memref for\n    `%result2`:\n\n    ```mlir\n    [[28, 32],\n    [60, 64]]\n    ```\n\n    Unlike the `reinterpret_cast`, the values are relative to the strided\n    memref of the input (`%result1` in this case) and not its\n    underlying memory.\n\n    Example 2:\n\n    ```mlir\n    // Subview of static memref with strided layout at static offsets, sizes\n    // and strides.\n    %1 = memref.subview %0[4, 2][8, 2][3, 2]\n        : memref<64x4xf32, strided<[7, 9], offset: 91>> to\n          memref<8x2xf32, strided<[21, 18], offset: 137>>\n    ```\n\n    Example 3:\n\n    ```mlir\n    // Subview of static memref with identity layout at dynamic offsets, sizes\n    // and strides.\n    %1 = memref.subview %0[%off0, %off1][%sz0, %sz1][%str0, %str1]\n        : memref<64x4xf32> to memref<?x?xf32, strided<[?, ?], offset: ?>>\n    ```\n\n    Example 4:\n\n    ```mlir\n    // Subview of dynamic memref with strided layout at dynamic offsets and\n    // strides, but static sizes.\n    %1 = memref.subview %0[%off0, %off1][4, 4][%str0, %str1]\n        : memref<?x?xf32, strided<[?, ?], offset: ?>> to\n          memref<4x4xf32, strided<[?, ?], offset: ?>>\n    ```\n\n    Example 5:\n\n    ```mlir\n    // Rank-reducing subviews.\n    %1 = memref.subview %0[0, 0, 0][1, 16, 4][1, 1, 1]\n        : memref<8x16x4xf32> to memref<16x4xf32>\n    %3 = memref.subview %2[3, 4, 2][1, 6, 3][1, 1, 1]\n        : memref<8x16x4xf32> to memref<6x3xf32, strided<[4, 1], offset: 210>>\n    ```\n\n    Example 6:\n\n    ```mlir\n    // Identity subview. The subview is the full source memref.\n    %1 = memref.subview %0[0, 0, 0] [8, 16, 4] [1, 1, 1]\n        : memref<8x16x4xf32> to memref<8x16x4xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyMemRef" },
      { "name": "offsets", "type": "Variadic" },
      { "name": "sizes", "type": "Variadic" },
      { "name": "strides", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyMemRef" }
    ],
    "attributes": [
      { "name": "static_offsets", "type": "DenseI64ArrayAttr" },
      { "name": "static_sizes", "type": "DenseI64ArrayAttr" },
      { "name": "static_strides", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$source ``\n    custom<DynamicIndexList>($offsets, $static_offsets)\n    custom<DynamicIndexList>($sizes, $static_sizes)\n    custom<DynamicIndexList>($strides, $static_strides)\n    attr-dict `:` type($source) `to` type($result)"
  },
  {
    "name": "memref.transpose",
    "summary": "`transpose` produces a new strided memref (metadata-only)",
    "description": "The `transpose` op produces a strided memref whose sizes and strides\n    are a permutation of the original `in` memref. This is purely a metadata\n    transformation.\n\n    Example:\n\n    ```mlir\n    %1 = memref.transpose %0 (i, j) -> (j, i) : memref<?x?xf32> to memref<?x?xf32, affine_map<(d0, d1)[s0] -> (d1 * s0 + d0)>>\n    ```",
    "category": "Transform"
  },
  {
    "name": "memref.view",
    "summary": "memref view operation",
    "description": "The \"view\" operation extracts an N-D contiguous memref with empty layout map\n    with arbitrary element type from a 1-D contiguous memref with empty layout\n    map of i8 element  type. The ViewOp supports the following arguments:\n\n    * A single dynamic byte-shift operand must be specified which represents a\n      a shift of the base 1-D memref pointer from which to create the resulting\n      contiguous memref view with identity layout.\n    * A dynamic size operand that must be specified for each dynamic dimension\n      in the resulting view memref type.\n\n    The \"view\" operation gives a structured indexing form to a flat 1-D buffer.\n    Unlike \"subview\" it can perform a type change. The type change behavior\n    requires the op to have special semantics because, e.g. a byte shift of 3\n    cannot be represented as an offset on f64.\n    For now, a \"view\" op:\n\n    1. Only takes a contiguous source memref with 0 offset and empty layout.\n    2. Must specify a byte_shift operand (in the future, a special integer\n       attribute may be added to support the folded case).\n    3. Returns a contiguous memref with 0 offset and empty layout.\n\n    Example:\n\n    ```mlir\n    // Allocate a flat 1D/i8 memref.\n    %0 = memref.alloc() : memref<2048xi8>\n\n    // ViewOp with dynamic offset and static sizes.\n    %1 = memref.view %0[%offset_1024][] : memref<2048xi8> to memref<64x4xf32>\n\n    // ViewOp with dynamic offset and two dynamic size.\n    %2 = memref.view %0[%offset_1024][%size0, %size1] :\n      memref<2048xi8> to memref<?x4x?xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "MemRefRankOf" },
      { "name": "byte_shift", "type": "Index" },
      { "name": "sizes", "type": "Variadic" }
    ],
    "assemblyFormat": "$source `[` $byte_shift `]` `` `[` $sizes `]` attr-dict\n    `:` type($source) `to` type(results)"
  },
  {
    "name": "mhlo.abs",
    "summary": "Absolute value operator",
    "description": "Returns `abs(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.acos",
    "summary": "Acos operation",
    "description": "Performs element-wise acos operation on `operand` tensor and produces a\n    `result` tensor.\n\n    Example:\n    ```mlir\n    %result = mhlo.acos %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.acosh",
    "summary": "Acosh operation",
    "description": "Performs element-wise acosh operation on `operand` tensor and produces a\n    `result` tensor.\n\n    Example:\n    ```mlir\n    %result = mhlo.acosh %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.add",
    "summary": "Addition operator",
    "description": "Returns `lhs + rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.add_dependency",
    "summary": "AddDependency operation",
    "description": "This operation is private to the XLA compiler, so it is does not yet have\n    a specification.\n\n    Informally, this operation two operands: a data operand and a token. The\n    output of the operation is the data operand. When used with AfterAll this\n    operation enables ordering non-side-effecting operations (those that do not\n    produce token values).\n\n    Example:\n    ```mlir\n    %1 = mhlo.add_dependency %arg0, %0 : (tensor<3x4xf32>, !mhlo.token) -> tensor<3x4xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "MHLO_TensorOrAnyToken" },
      { "name": "token", "type": "MHLO_AnyToken" }
    ],
    "outputs": [
      { "name": "output", "type": "MHLO_TensorOrAnyToken" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "mhlo.after_all",
    "summary": "AfterAll operator",
    "description": "AfterAll takes a variadic number of tokens and produces a single token.\n    Tokens are primitive types which can be threaded between side-effecting\n    operations to enforce ordering. AfterAll can be used as a join of tokens\n    for ordering a operation after a set operations.\n\n    See https://www.tensorflow.org/xla/operation_semantics#afterall.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ]
  },
  {
    "name": "mhlo.all_gather",
    "summary": "AllGather operator",
    "description": "Performs concatenation across replicas.\n\n    See https://www.tensorflow.org/xla/operation_semantics#allgather",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "all_gather_dim", "type": "I64Attr" },
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.all_reduce",
    "summary": "AllReduce operator",
    "description": "Performs a custom reduction across replicas.\n\n    See https://www.tensorflow.org/xla/operation_semantics#allreduce.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.all_to_all",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "split_dimension", "type": "I64Attr" },
      { "name": "concat_dimension", "type": "I64Attr" },
      { "name": "split_count", "type": "I64Attr" },
      { "name": "replica_groups", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.and",
    "summary": "Logical and",
    "description": "Returns `logical_and(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_PredOrIntTensor" },
      { "name": "rhs", "type": "HLO_PredOrIntTensor" }
    ]
  },
  {
    "name": "mhlo.asin",
    "summary": "Asin operation",
    "description": "Performs element-wise asin operation on `operand` tensor and produces a\n    `result` tensor.\n\n    Example:\n    ```mlir\n    %result = mhlo.asin %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.asinh",
    "summary": "Asinh operation",
    "description": "Performs element-wise asinh operation on `operand` tensor and produces a\n    `result` tensor.\n\n    Example:\n    ```mlir\n    %result = mhlo.asinh %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.async_done",
    "summary": "AsyncDone operation",
    "description": "This operation is private to the XLA compiler, so it is does not yet have\n    a specification.\n\n    Informally, this operation blocks until the end of an asynchronous computation.\n    It returns the final result of the asynchronous computation.\n\n    See the documentation for AsyncStart for more information.",
    "inputs": [
      { "name": "bundle", "type": "MHLO_AsyncBundle" }
    ]
  },
  {
    "name": "mhlo.async_start",
    "summary": "AsyncStart operation",
    "description": "This operation is private to the XLA compiler, so it is does not yet have\n    a specification.\n\n    Informally, this operation kicks off an asynchronous computation.\n\n    This is used when there are functions that contain both asynchronous waits\n    (such as DMAs) and on-thread computation. For example, a function might\n    consist of a computation, a DMA, another computation, a second DMA, and a\n    final computation. This would be represented as an async_start followed by\n    and async_update and an async_done. The async_start would do the first\n    computation on-thread and then start the DMA. The async_update would wait\n    for the DMA to complete if it wasn't yet done, then execute the second\n    computation in the function, and start the second DMA. Finally, the\n    async_done would wait on this last DMA, and then run the last computation\n    that needs to be run on-thread and return the result of that final\n    computation.\n\n    `operands` are passed to the computation directly\n    `called_computation` is the function that will be run asynchronously\n    `execution_thread` is the name of the thread in which it will be run. The main\n      thread is called \"main\". All threads have names.\n\n    This returns all the state needed between async ops. After buffer\n    assignment, the return values represents the space needed to hold the input,\n    results, and any scratchpads needed or edited by the async op.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "called_computation", "type": "FlatSymbolRefAttr" },
      { "name": "execution_thread", "type": "StrAttr" }
    ]
  },
  {
    "name": "mhlo.async_update",
    "summary": "AsyncUpdate operation",
    "description": "This operation is private to the XLA compiler, so it is does not yet have\n    a specification.\n\n    Informally, this operation blocks on an asynchronous computation until a\n    sync barrier. This returns `bundle` after operating on it.\n\n    See the documentation for AsyncStart for more information.",
    "inputs": [
      { "name": "bundle", "type": "MHLO_AsyncBundle" }
    ]
  },
  {
    "name": "mhlo.atan2",
    "summary": "Atan2 operator",
    "description": "Returns `atan2(lhs/rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.atanh",
    "summary": "Atanh operation",
    "description": "Performs element-wise atanh operation on `operand` tensor and produces a\n    `result` tensor.\n\n    Example:\n    ```mlir\n    %result = mhlo.atanh %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.batch_norm_grad",
    "summary": "Batch Normalization Gradient",
    "description": "Calculates gradients of batch norm.\n\n    See https://www.tensorflow.org/xla/operation_semantics#batchnormgrad",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "scale", "type": "HLO_Tensor" },
      { "name": "mean", "type": "HLO_Tensor" },
      { "name": "variance", "type": "HLO_Tensor" },
      { "name": "grad_output", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "feature_index", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.batch_norm_inference",
    "summary": "Batch Normalization for Inference",
    "description": "Normalizes an array across batch and spatial dimensions.\n\n    See https://www.tensorflow.org/xla/operation_semantics#batchnorminference",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "scale", "type": "HLO_Tensor" },
      { "name": "offset", "type": "HLO_Tensor" },
      { "name": "mean", "type": "HLO_Tensor" },
      { "name": "variance", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "feature_index", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.batch_norm_training",
    "summary": "Batch Normalization for Training",
    "description": "Normalizes an array across batch and spatial dimensions.\n\n    See https://www.tensorflow.org/xla/operation_semantics#batchnormtraining",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "scale", "type": "HLO_Tensor" },
      { "name": "offset", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "feature_index", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.bitcast",
    "summary": "Bitcast operator",
    "description": "This op changes the shape of the input in the way that the physical\n    arrangement of elements are unchanged.\n\n    However, the op needs layout information to make sense of \"physical\n    arrangement of elements\". Layout support in MHLO is currently under\n    exploration.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.bitcast_convert",
    "summary": "BitcastConvert operator",
    "description": "Similar to a 'tf.bitcast' in TensorFlow, performs an element-wise bitcast\n    operation from a data shape to a target shape. The dimensions must match,\n    and the conversion is an element-wise one. Bitcast is implemented as a\n    low-level cast, so machines with different floating-point representations\n    will give different results.\n\n    See https://www.tensorflow.org/xla/operation_semantics#bitcastconverttype.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.broadcast",
    "summary": "Broadcast a tensor to a higher rank by prepending dimensions",
    "description": "Broadcasts the operand tensor to a higher rank by prepending\n    `broadcast_sizes` to the dimensions. The current values of the operand are\n    copied into the other dimensions.\n\n    This is a more limited form of broadcasting, that corresponds to the XLA\n    client Broadcast method. For a more general form of broadcasting, see the\n    BroadcastInDimOp.\n\n    See https://www.tensorflow.org/xla/operation_semantics#broadcast.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "broadcast_sizes", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.broadcast_in_dim",
    "summary": "Broadcast a tensor into the given shape by adding dimensions.",
    "description": "Broadcasts the `operand` tensor to a higher rank. This is not the limited\n    form of broadcasting exposed as the XLA client broadcast op, but rather the\n    more powerful \"InDim\" broadcasting, which is closer to the HLO broadcast op\n    and exposed in the XLA client BroadcastInDim method.\n\n    `broadcast_dimensions` maps the operand dimension number to the target shape\n    dimension number. It must have the same size as the rank of the operand. The\n    mapped dimensions must either be the same size or the dimension being\n    broadcast from must be size 1 (degenerate broadcasting).\n\n    For a scalar (0D tensor) operand, `broadcast_dimensions` must be empty. The\n    The scalar value will be broadcast to every element in the target shape.\n\n    See https://www.tensorflow.org/xla/broadcasting.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "BroadcastDimAttr" }
    ]
  },
  {
    "name": "mhlo.case",
    "summary": "Switch-Case operator",
    "description": "Returns the result of executing `branches[index]`. If\n    `index` is < 0 or >= N, then `branches[N-1] is executed as\n    the default branch.\n\n    Each branch `branches[b]` must take in a single argument of same type as\n    `branch_operands[b]` and will be invoked with `branch_operands[b]`. The type\n    of the returned value of each branch must be the same.\n\n    Note that only one of the branches will be executed depending on the value\n    of index.\n    See https://www.tensorflow.org/xla/operation_semantics#conditional.",
    "inputs": [
      { "name": "index", "type": "I32Tensor" }
    ]
  },
  {
    "name": "mhlo.cbrt",
    "summary": "Cubic root operator",
    "description": "Returns element-wise cubic root of the operand.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.ceil",
    "summary": "Ceil operator",
    "description": "Returns `Ceil(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.cholesky",
    "summary": "Cholesky operator",
    "description": "Computes the Cholesky decomposition of a batch of symmetric (Hermitian)\n  positive definite matrices.\n\n  If lower is true, computes lower-triangular matrices l such that\n  `a=l.Transpose(l)`. If lower is false, computes upper-triangular matrices u such\n  that `a=Transpose(u).u`.\n\n  Input data is read only from the lower/upper triangle of a, depending on the\n  value of lower. Values from the other triangle are ignored. Output data is\n  returned in the same triangle; the values in the other triangle are\n  implementation-defined and may be anything.\n\n  If the rank of a is greater than 2, a is treated as a batch of matrices, where\n  all except the minor 2 dimensions are batch dimensions.\n\n  If a is not symmetric (Hermitian) positive definite, the result is\n  implementation-defined.\n\n    See https://www.tensorflow.org/xla/operation_semantics#cholesky.",
    "inputs": [
      { "name": "a", "type": "HLO_FpOrComplexTensor" }
    ],
    "attributes": [
      { "name": "lower", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "mhlo.clamp",
    "summary": "Clamp operator",
    "description": "Clamps an operand to within the range between a minimum and maximum value.\n\n    Note: All three arrays must be the same shape. Alternatively, as a\n          restricted form of broadcasting, min and/or max can be a scalar (0D\n          tensor) of the element type of the tensor operand.\n\n    See https://www.tensorflow.org/xla/operation_semantics#clamp.",
    "inputs": [
      { "name": "min", "type": "HLO_Tensor" },
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "max", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.collective_broadcast",
    "summary": "CollectiveBroadcast operation",
    "description": "Within each process group in the process grid, send the value of the\n    `operand` tensor from the source process to the target processes and produce a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#collective_broadcast\n\n    Example:\n    ```mlir\n    %result = \"mhlo.collective_broadcast\"(%operand) {\n      replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>,\n      channel_handle = #mhlo.channel_handle<handle = 0, type = 0>\n    } : (tensor<1x2xi64>) -> tensor<1x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "MHLO_Tensor" }
    ],
    "attributes": [
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.collective_permute",
    "summary": "CollectivePermute operator",
    "description": "CollectivePermute is a collective operation that sends and receives data\n    cross replicas.\n    Note that there are the following restrictions on the source_target_pair:\n    - Any two pairs should not have the same target replica id, and they should\n    not have the same source replica id.\n    - If a replica id is not a target in any pair, then the output on that\n    replica is a tensor consists of 0(s) with the same shape as the input.\n\n    See https://www.tensorflow.org/xla/operation_semantics#collectivepermute.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "source_target_pairs", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.compare",
    "summary": "Comparison operator",
    "description": "Compares `lhs` and `rhs` elementwise according to `comparison_direction`\n    and `compare_type`. If unspecified, `compare_type` is FLOAT for float element\n    types, SIGNED for signed element types and UNSIGNED for unsigned element\n    types.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_comparison_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "comparison_direction", "type": "HLO_ComparisonDirectionAttr" },
      { "name": "compare_type", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.complex",
    "summary": "Complex operator",
    "description": "Performs element-wise conversion of a pair of real and imaginary values to\n    a complex value.",
    "inputs": [
      { "name": "lhs", "type": "HLO_FpTensor" },
      { "name": "rhs", "type": "HLO_FpTensor" }
    ]
  },
  {
    "name": "mhlo.composite",
    "summary": "Composite operation",
    "description": "Encapsulates an operation made up (composed) of other StableHLO operations,\n    taking `inputs` and `composite_attributes` and producing `results`. The\n    semantics of the op are implemented by the `decomposition` attribute. The\n    `composite` op can be replaced with its decomposition without changing program\n    semantics. In cases where inlining the decomposition does not provide the same\n    op semantics, prefer using `custom_call`.\n\n    The `version` field (defaults to `0`) is used to denote when a composite's\n    semantics change.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#composite\n\n    Example:\n    ```mlir\n    %results = mhlo.composite \"my.op\" %arg0, %arg1 {\n      decomposition = @my_op,\n      composite_attributes = { my_attribute = \"my_value\" },\n      version = 1 : i32\n    } : (tensor<f32>, tensor<f32>) -> tensor<f32>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" },
      { "name": "composite_attributes", "type": "DefaultValuedOptionalAttr" },
      { "name": "decomposition", "type": "FlatSymbolRefAttr" },
      { "name": "version", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$name $inputs attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "mhlo.compute_reshape_shape",
    "summary": "Compute input for reshape with any dynamic dim resolved",
    "description": "This operation handles the dynamic aspect of a TF/NumPy/CHLO reshape. The\n    dynamic aspect is that a single extent can be -1 and that dimension will\n    instead be computed. This handles the computation and can then be passed to\n    an HLO DynamicReshapeOp to replicate the TF/NumPy reshape behavior.\n\n    This op has undefined behavior if the dimensions do not evenly divide the\n    number of elements, or if there are multiple -1 values. It is an identity op\n    if no dimensions are -1.\n\n    ```\n    %0 = hlo.compute_reshape_shape 12, [2, -1] -> [2, 6]\n    ```",
    "inputs": [
      { "name": "num_elements", "type": "Index" },
      { "name": "dynamic_shape", "type": "DTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "DTensorOf" }
    ],
    "assemblyFormat": "$num_elements `,` $dynamic_shape attr-dict `:` type($num_elements) `,` type($dynamic_shape) `->` type($result)"
  },
  {
    "name": "mhlo.concatenate",
    "summary": "XLA's concatenate op",
    "description": "Concatenates a set of tensors along the specified dimension.\n\n     See https://www.tensorflow.org/xla/operation_semantics#concatenate.",
    "inputs": [
      { "name": "val", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.constant",
    "summary": "Constant operator",
    "description": "Represents a constant value.",
    "outputs": [
      { "name": "output", "type": "HLO_StaticShapeTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "ElementsAttr" }
    ],
    "assemblyFormat": "attr-dict $value"
  },
  {
    "name": "mhlo.convert",
    "summary": "Convert operator",
    "description": "Performs element-wise conversion of values from one type to another, e.g.\n    float to int.\n\n    See https://www.tensorflow.org/xla/operation_semantics#convertelementtype.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.convolution",
    "summary": "Convolution operator",
    "description": "Computes a convolution of the kind used in neural networks.\n\n    See https://www.tensorflow.org/xla/operation_semantics#conv_convolution.",
    "assemblyFormat": "`(`operands`)`\n       `dim_numbers` `=` custom<ConvolutionDimensions>($dimension_numbers) `,`\n       `window` `=` `{` custom<WindowAttributes>($window_strides, $padding,\n                                                 $lhs_dilation, $rhs_dilation,\n                                                 $window_reversal) `}`\n       attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "mhlo.copy",
    "summary": "Copy operator",
    "description": "Returns a copy of `operand`."
  },
  {
    "name": "mhlo.cosh",
    "summary": "Cosh operation",
    "description": "Performs element-wise cosh operation on `operand` tensor and produces a\n    `result` tensor.\n\n    Example:\n    ```mlir\n    %result = mhlo.cosh %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.cosine",
    "summary": "Cos operator",
    "description": "Returns `Cos(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.count_leading_zeros",
    "summary": "Count-leading-zeros (Clz) operator",
    "description": "Returns the number of leading zeros in each operand element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.create_token",
    "summary": "Create Token operator",
    "description": "Produces a HLO token. Tokens are used for ordering side-effecting perations.\n    This is exported to HLO as an AfterAll operation with no operands to\n    generate a token.",
    "outputs": [
      { "name": "output", "type": "HLO_Token" }
    ]
  },
  {
    "name": "mhlo.cross-replica-sum",
    "summary": "Sums input across replicated instances.",
    "description": "For each of the replica groups, operands of the group devices are summed\n     so that each device has the sum.\n\n     For example, suppose there are 8 TPU devices: `[A, B, C, D, E, F, G, H]`.\n     Passing group_assignment=`[[0,2,4,6],[1,3,5,7]]` sets `A, C, E, G` as group 0,\n     and `B, D, F, H` as group 1. Thus we get the outputs:\n     `[A+C+E+G, B+D+F+H, A+C+E+G, B+D+F+H, A+C+E+G, B+D+F+H, A+C+E+G, B+D+F+H]`.\n\n     See https://www.tensorflow.org/xla/operation_semantics#crossreplicasum.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "replica_groups", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.cstr_reshapable",
    "summary": "Compute input for reshape with any dynamic dim resolved",
    "description": "This operation creates a witness on the constraint that a given shape would\n    be a valid reshape for the given number of elements.\n\n    ```\n    %0 = mhlo.cstr_reshapable 12, [2, -1] -> success\n    %1 = mhlo.cstr_reshapable 13, [2, -1] -> failure\n    ```",
    "inputs": [
      { "name": "num_elements", "type": "Index" },
      { "name": "dynamic_shape", "type": "DTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_WitnessType" }
    ],
    "assemblyFormat": "$num_elements `,` $dynamic_shape attr-dict `:` type($num_elements) `,` type($dynamic_shape)"
  },
  {
    "name": "mhlo.custom_call",
    "summary": "CustomCall operator",
    "description": "A custom call invokes code external to XLA. The `args` are passed to the\n    external code, and the external code is expected to produce a result of the\n    given type. The exact mechanism is backend-specific. For example, in the CPU\n    backend, a call instruction is emitted which targets a symbol with the name\n    `call_target_name`.\n\n    `call_target_name` and `backend_config` can be arbitrary strings, but\n    `call_target_name` should be short as it may be used in labels.\n    `backend_config` can encode arbitrarily large amounts of information.\n\n    `has_side_effect` must be true if the custom call has side-effects.\n    `api_version` specifies the version of the API used by the custom call\n    function.\n\n    A custom call may apply functions within the scope of the parent module.\n    They can be referenced using `called_computations` attribute.\n\n    A custom call can also have layout constraints on operands and results which\n    can be specified as optional `operand_layouts` and `result_layouts`\n    attributes. The layout attribute is an array of rank-1 index tensors and the\n    i-th layout attribute specifies the layout for i-th operand/result.\n\n    The `operand_layouts` & `result_layouts` attributes can be specified under\n    the following constraints:\n    1) Either both `operand_layouts` and `result_layouts` are specified or none.\n    2) None of the operands are of tuple type.\n    3) None of the results are of tuple type except the common case of single\n       tuple result packing non-tuple values is allowed. In this case the i-th\n       `result_layouts` attribute specifies the layout of i-th element in the\n       result tuple.\n\n    See https://www.tensorflow.org/xla/operation_semantics#customcall.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "call_target_name", "type": "StrAttr" },
      { "name": "has_side_effect", "type": "DefaultValuedAttr" },
      { "name": "backend_config", "type": "DefaultValuedStrAttr" },
      { "name": "api_version", "type": "DefaultValuedAttr" },
      { "name": "called_computations", "type": "DefaultValuedAttr" },
      { "name": "operand_layouts", "type": "OptionalAttr" },
      { "name": "result_layouts", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.dequantize",
    "summary": "Dequantize operator",
    "description": "Dequantize the quantized input of packed uint32 to bfloat16. Only uint8 or\n    uint16 is supported for the original unpacked input.\n\n    Returns a tensor of shape [d0,..., dn * unpack_size] if unpacked input shape\n    is [d0, ..., dn], where unpack_size = sizeof(unit32) / sizeof(T), where T is\n    the unpacked input type. If transpose_output is true, will return a tensor\n    of shape [dn * unpack_size, dn-1, ..., d1, d0]. transpose_output is faster\n    when input's rank higher than 1. The input needs to be transposed to use\n    transpose_output feature.",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "min_range", "type": "F32Attr" },
      { "name": "max_range", "type": "F32Attr" },
      { "name": "mode", "type": "HLO_DequantizeModeAttr" },
      { "name": "transpose_output", "type": "BoolAttr" },
      { "name": "is_16bits", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "mhlo.divide",
    "summary": "Division operator",
    "description": "Returns `lhs / rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.domain",
    "summary": "Domain operation",
    "description": "This operation is private to the XLA compiler, so it is does not yet have\n    a specification.\n\n    Informally, these operations are used to group instructions with the same\n    DomainMetadata property. ShardingMetadata is the main use case today to\n    group instructions on the same device. Domain instructions provide two\n    major benefits:\n      - Prevent unintentionally optimizing instructions across domains.\n      - Automatically assign the metadata of the instructions created in the domain.\n    Without domain instructions, each HLO optimization pass would have to check\n    and propagate the metadata, which would be easy to miss and also adds\n    complexity to the compiler. Since domain instructions connect two different\n    domains, each domain instruction is associated with two DomainMetadata --\n    one on the operand side and one on the user side of the domain.",
    "inputs": [
      { "name": "operand", "type": "MHLO_TensorOrToken" }
    ],
    "outputs": [
      { "name": "result", "type": "MHLO_TensorOrToken" }
    ],
    "attributes": [
      { "name": "kind", "type": "MHLO_DomainKindAttr" },
      { "name": "entry_metadata", "type": "StrAttr" },
      { "name": "exit_metadata", "type": "StrAttr" }
    ]
  },
  {
    "name": "mhlo.dot",
    "summary": "Dot operator",
    "description": "Performs dot products between vectors, vector/matrix and matrix/matrix\n    multiplication.\n\n    See https://www.tensorflow.org/xla/operation_semantics#dot.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "precision_config", "type": "HLO_PrecisionConfigAttr" }
    ]
  },
  {
    "name": "mhlo.dot_general",
    "summary": "General Dot operator",
    "description": "Performs general dot products between vectors, vector/matrix and\n    matrix/matrix multiplication.\n\n    See https://www.tensorflow.org/xla/operation_semantics#dotgeneral.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" },
      { "name": "dot_dimension_numbers", "type": "DotDimensionNumbers" }
    ],
    "attributes": [
      { "name": "precision_config", "type": "HLO_PrecisionConfigAttr" }
    ]
  },
  {
    "name": "mhlo.dynamic_broadcast_in_dim",
    "summary": "Broadcast a tensor into the given dynamic shape by adding dimensions.",
    "description": "This is a generalization of the BroadcastInDimOp which accepts its output\n    dimensions as an argument. It should eventually supercede the statically\n    shaped original, but is being phased as a separate op in order to support\n    compatibility with lowerings and translations that precede dynamic\n    shapes.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "output_dimensions", "type": "HLO_DimensionTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "BroadcastDimAttr" }
    ]
  },
  {
    "name": "mhlo.dynamic_conv",
    "summary": "Dynamic Convolution operator",
    "description": "The dynamic shape version of ConvOp. Computes a convolution with dynamic padding."
  },
  {
    "name": "mhlo.dynamic_gather",
    "summary": "Dynamic Gather operator",
    "description": "The dynamic shape version of GatherOp. Stitches together several slices of\n    an input array.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "HLO_IntTensor" },
      { "name": "slice_sizes", "type": "HLO_IntTensor" },
      { "name": "dimension_numbers", "type": "GatherDimensionNumbers" }
    ],
    "attributes": [
      { "name": "indices_are_sorted", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "mhlo.dynamic_iota",
    "summary": "Create linear increasing values from 0 to length -1.",
    "description": "Produces an HLO Tensor of the specified shape, with an incremental set of\n    values along the specified dimension starting at 0.\n\n    Requires:\n    - The output length of the tensor result.",
    "inputs": [
      { "name": "output_shape", "type": "HLO_DimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "iota_dimension", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.dynamic_pad",
    "summary": "Dynamic Pad operator",
    "description": "Dynamically Pads the `operand`, with amount of padding added at\n    low-end/high-end/interior is passed through input tensors.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "padding_value", "type": "HLO_Tensor" },
      { "name": "edge_padding_low", "type": "HLO_DimensionTensor" },
      { "name": "edge_padding_high", "type": "HLO_DimensionTensor" },
      { "name": "interior_padding", "type": "HLO_DimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.dynamic_reshape",
    "summary": "Reshape a tensor to a given, possibly dynamic, shape.",
    "description": "Reshapes `operand` to `output_shape`.\n\n    Requires:\n    - The length of `output_shape` is equal to the rank of `result`.\n    - The number of elements in `operand` (that is, the product of extents of\n      its shape) is equal to the number of elements in `output_shape` (that is,\n      the product of values in `output_shape`).",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "output_shape", "type": "HLO_DimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.dynamic_slice",
    "summary": "DynamicSlice operation",
    "description": "Extracts a slice from the `operand` using dynamically-computed starting\n    indices and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dynamic_slice\n\n    Example:\n    ```mlir\n    %result = mhlo.dynamic_slice %operand, %start_indices0, %start_indices1, sizes = [2, 2]\n      : (tensor<4x4xi32>, tensor<i64>, tensor<i64>) -> tensor<2x2xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "MHLO_Tensor" },
      { "name": "start_indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "MHLO_Tensor" }
    ],
    "attributes": [
      { "name": "slice_sizes", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.dynamic_update_slice",
    "summary": "DynamicUpdateSlice operation",
    "description": "Produces a `result` tensor which is equal to the `operand` tensor except\n    that the slice starting at `start_indices` is updated with the values in\n    `update`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dynamic_update_slice\n\n    Example:\n    ```mlir\n    %result = mhlo.dynamic_update_slice %operand, %update, %start_indices0, %start_indices1\n      : (tensor<4x4xi32>, tensor<2x2xi32>, tensor<i64>, tensor<i64>) -> tensor<4x4xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "MHLO_Tensor" },
      { "name": "update", "type": "MHLO_Tensor" },
      { "name": "start_indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "MHLO_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "mhlo.dynamic-slice",
    "summary": "Dynamic Slice operator",
    "description": "Extracts a sub-array from the input array at dynamic start_indices.\n\n    See https://www.tensorflow.org/xla/operation_semantics#dynamicslice.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "slice_sizes", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.dynamic-update-slice",
    "summary": "Dynamic Update Slice operator",
    "description": "DynamicUpdateSlice generates a result which is the value of the input array\n    operand, with a slice update overwritten at start_indices.\n\n    See https://www.tensorflow.org/xla/operation_semantics#dynamicupdateslice.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "update", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.einsum",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "einsum_config", "type": "StrAttr" }
    ]
  },
  {
    "name": "mhlo.erf",
    "summary": "Erf operation",
    "description": "Performs element-wise erf operation on `operand` tensor and produces a\n    `result` tensor.\n\n    Example:\n    ```mlir\n    %result = mhlo.erf %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.exponential",
    "summary": "Exponential operator",
    "description": "Returns `e^(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.exponential_minus_one",
    "summary": "Exponential minus one operator",
    "description": "Returns `e^(operand) - 1` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.fft",
    "summary": "Fast fourier transform operator",
    "description": "Returns the fast-fourier-transform of the input array.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#fft.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "fft_type", "type": "HLO_FftTypeAttr" },
      { "name": "fft_length", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.floor",
    "summary": "Floor operator",
    "description": "Returns `Floor(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.fusion",
    "summary": "Fusion operator",
    "description": "Models the fusion instruction.\n\n    A fusion op is consists of a group of basic ops (represented as a region\n    attached to it). It serves as a hint to the backend that it is beneficial\n    to emit the contained ops into a single loop nest or kernel.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "fusion_kind", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.gather",
    "summary": "Gather operator",
    "description": "Stitches together several slices of `operand` from offsets specified in\n    `start_indices` (each slice at a potentially different runtime offset).\n\n    See https://www.tensorflow.org/xla/operation_semantics#gather.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "HLO_IntTensor" },
      { "name": "dimension_numbers", "type": "GatherDimensionNumbers" }
    ],
    "attributes": [
      { "name": "slice_sizes", "type": "I64ElementsAttr" },
      { "name": "indices_are_sorted", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "mhlo.get_dimension_size",
    "summary": "GetDimensionSize operator",
    "description": "Returns the size of the given dimension of the operand.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#getdimensionsize.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "dimension", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.get_tuple_element",
    "summary": "GetTupleElement operator",
    "description": "Returns a member of a tuple specified by an index.\n\n    See https://www.tensorflow.org/xla/operation_semantics#gettupleelement.",
    "attributes": [
      { "name": "index", "type": "I32Attr" }
    ]
  },
  {
    "name": "mhlo.if",
    "summary": "If operator",
    "description": "Returns the result of executing either a true or false function depending on\n    the result of a condition function.\n\n    See https://www.tensorflow.org/xla/operation_semantics#conditional.",
    "inputs": [
      { "name": "pred", "type": "HLO_PredTensor" }
    ]
  },
  {
    "name": "mhlo.imag",
    "summary": "Imag operator",
    "description": "Returns `Imag(operand)` element-wise.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.infeed",
    "summary": "Infeed operator",
    "description": "Reads a single data item from the implicit Infeed streaming interface of\n    the device, interpreting the data as the given shape, and returns a XlaOp\n    of the data. Multiple Infeed operations are allowed in a computation, but\n    there must be a total order among the Infeed operations.\n\n    Attributes:\n      layout:  Array attribute. Same shape as the output of the infeed, except\n               that every tensor is replaced by a minor_to_major array for the\n               tensor's layout.\n\n    See https://www.tensorflow.org/xla/operation_semantics#infeed.",
    "inputs": [
      { "name": "token", "type": "HLO_Token" }
    ],
    "attributes": [
      { "name": "infeed_config", "type": "DefaultValuedStrAttr" },
      { "name": "layout", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.iota",
    "summary": "Iota operator",
    "description": "Creates a rank 1 array of values starting at zero and incrementing by one.",
    "outputs": [
      { "name": "output", "type": "HLO_IntFpOrComplexTensor" }
    ],
    "attributes": [
      { "name": "iota_dimension", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.is_finite",
    "summary": "IsFinite operator",
    "description": "Tests whether each element of operand is finite, i.e., is not positive or\n    negative infinity, and is not NaN. Returns a tensor of 1-bit integers with\n    the same shape as the input, where each element is nonzero (i.e. true) if\n    and only if the corresponding input element is finite.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "x", "type": "HLO_FpTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "HLO_PredTensor" }
    ]
  },
  {
    "name": "mhlo.log",
    "summary": "Logarithm operator",
    "description": "Returns `log(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.log_plus_one",
    "summary": "Log1p operator",
    "description": "Returns `log(operand+1)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.logistic",
    "summary": "Logistic operator",
    "description": "Returns `logistic(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.map",
    "summary": "Map operator",
    "description": "Applies a scalar function over the given operands arrays, producing an array\n  of the same dimensions where each element is the result of the mapped function\n  applied to the corresponding elements in the input arrays.\n\n  The mapped function is an arbitrary computation with the restriction that it\n  has N inputs of scalar type T and a single output with type S. The output has\n  the same dimensions as the operands except that the element type T is replaced\n  with S.\n\n  See https://www.tensorflow.org/xla/operation_semantics#map.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimensions", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.maximum",
    "summary": "Maximum operator",
    "description": "Returns `max(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.minimum",
    "summary": "Minimum operator",
    "description": "Returns `min(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.minimum_broadcast_shapes",
    "summary": "Minimizes the rank of two or more shapes to be broadcasted",
    "description": "Given two or more 1D tensors representing shapes, returns one 1D tensor for\n    each operand, where operand `i` corresponds to output `i`.\n\n    The returned tensors have the property that they specify a shape which is a\n    reshape of the corresponding input shape, and the broadcasted output shape\n    (using shape::BroadcastOp) of the returned shapes is a reshape of the\n    broadcasted output shape of the input shapes. Among all possibilities with\n    this property, the one is chosen which minimizes the rank of each returned\n    shape.\n\n    The general idea of this op is that it can be used for ops which have a\n    broadcasting semantic to operate on shapes with a possibly smaller rank\n    while preserving equivalence of the computed values. After computing the\n    result of the op using reshaped operands, the result can be reshaped to the\n    result that would have been originally computed.\n\n    Here is an example with two input shapes:\n\n    ```mlir\n    mhlo.minimum_broadcast_shapes [1, 2, 3, 1, 2, 1],\n                                     [1, 1, 1, 2, 3] -> [6, 2, 1], [2, 3]\n    ```\n\n    The broadcasted output shape of the operands is [1, 2, 3, 1, 2, 3], the\n    broadcasted output shape of the outputs is [6, 2, 3]. These two shapes are\n    reshapes of each other, and also each output is a reshape of the\n    corresponding input.",
    "inputs": [
      { "name": "shapes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$shapes attr-dict `:` type($shapes) `->` type($results)"
  },
  {
    "name": "mhlo.multiply",
    "summary": "Multiplication operator",
    "description": "Returns `lhs * rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.negate",
    "summary": "Negation operator",
    "description": "Returns `-operand` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.not",
    "summary": "Not operator",
    "description": "Returns `!operand` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.optimization_barrier",
    "summary": "The `hlo.optimization_barrier` op blocks optimizations.",
    "description": "Blocks any optimization pass from moving computations across the barrier.\n\n    Ensures that all inputs are evaluated before any operators that depend on the barrier's outputs.\n    See\n    https://www.tensorflow.org/xla/operation_semantics#optimizationbarrier",
    "inputs": [
      { "name": "arg", "type": "Variadic" }
    ]
  },
  {
    "name": "mhlo.or",
    "summary": "Logical or",
    "description": "Returns `logical_or(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_PredOrIntTensor" },
      { "name": "rhs", "type": "HLO_PredOrIntTensor" }
    ]
  },
  {
    "name": "mhlo.outfeed",
    "summary": "Outfeed operator",
    "description": "Generates outgoing data transfers for the given data. It takes data and a\n    token type operand and produces a token type value. Tokens are used for\n    ordering side-effecting operations.\n\n    See https://www.tensorflow.org/xla/operation_semantics#outfeed.",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrTuple" },
      { "name": "token", "type": "HLO_Token" }
    ],
    "attributes": [
      { "name": "outfeed_config", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "mhlo.pad",
    "summary": "Pad operator",
    "description": "Pads the `operand` according to TBD.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "padding_value", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "edge_padding_low", "type": "I64ElementsAttr" },
      { "name": "edge_padding_high", "type": "I64ElementsAttr" },
      { "name": "interior_padding", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.partition_id",
    "summary": "PartitionId operation",
    "description": "Produces `partition_id` of the current process.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#partition_id\n\n    Example:\n    ```mlir\n    %result = mhlo.partition_id : tensor<ui32>\n    ```",
    "assemblyFormat": "attr-dict `:` type(results)"
  },
  {
    "name": "mhlo.popcnt",
    "summary": "PopulationCount operator",
    "description": "Returns the number of bits set in each operand element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.power",
    "summary": "Power operator",
    "description": "Returns `lhs ^ rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.print",
    "summary": "Print operation",
    "description": "PrintOp is used to print tensor, which can be used to debug.",
    "inputs": [
      { "name": "input", "type": "HLO_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.ragged_dot",
    "summary": "Ragged matrix multiplication over a single ragged dimension",
    "description": "This operation takes three tensor args---lhs, rhs, and group_sizes---and\n    a \"ragged_dot_dimension_numbers\" attribute. Like dot_general, the lhs and\n    rhs are allowed arbitrary batch and contracting dimensions. Additionally,\n    the lhs is required to have one ragged dimension, and the rhs may have at\n    most one group dimension. The op has three modes, depending on the kind of\n    the lhs ragged dimension.\n\n    In mode 1, the shape-signature is `[b,m,k], [g,b,k,n], [b,g] -> [b,m,n]`.\n    Here the ragged dimension is an lhs non-contracting dimension (`m`). The\n    dimensions `b` and `k` represent batch and contracting dimensions\n    respectively. The rhs is required to have a group dimension (`g`).\n\n    In mode 2, the shape-signature is `[b,m,k], [b,k,n], [b,g] -> [g,b,m,n]`.\n    Here the ragged dimension is an lhs/rhs contracting dimension (`k`).\n\n    In mode 3, the shape-signature is `[b,m,k], [b,k,n], [g] -> [b,m,n]`. Here\n    the ragged dimension is an lhs/rhs batch dimension (`b`).",
    "inputs": [
      { "name": "lhs", "type": "MHLO_Tensor" },
      { "name": "rhs", "type": "MHLO_Tensor" },
      { "name": "group_sizes", "type": "MHLO_Tensor" },
      { "name": "ragged_dot_dimension_numbers", "type": "MHLO_RaggedDotDimensionNumbers" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "precision_config", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.real",
    "summary": "Real operator",
    "description": "Returns `Real(operand)` element-wise.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.real_dynamic_slice",
    "summary": "Real Dynamic Slice operator",
    "description": "The dynamic shape version of SliceOp. Extracts a sub-array from the input\n    array according to start_indices, limit_indices and strides. Expect\n    start_indices/limit_indices/strides to be statically shaped and matching\n    the rank of the input.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "HLO_DimensionTensor" },
      { "name": "limit_indices", "type": "HLO_DimensionTensor" },
      { "name": "strides", "type": "HLO_DimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.recv",
    "summary": "Recv operator",
    "description": "Receives data of the given shape from a Send instruction in another\n    computation that shares the same channel handle. Returns a tuple containing\n    value for the received data and a token. Recv operation represents\n    synchronous communication. However, the instruction is internally decomposed\n    into 2 HLO instructions (Recv and RecvDone) to enable asynchronous data\n    transfers.\n\n    See https://www.tensorflow.org/xla/operation_semantics#recv.",
    "inputs": [
      { "name": "token", "type": "HLO_Token" },
      { "name": "channel_handle", "type": "ChannelHandle" }
    ],
    "attributes": [
      { "name": "is_host_transfer", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "mhlo.reduce",
    "summary": "Reduce operator",
    "description": "Returns the result of executing a reduction function on one or more arrays\n    in parallel.\n\n    See https://www.tensorflow.org/xla/operation_semantics#reduce.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "init_values", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimensions", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.reduce_precision",
    "summary": "Reduce precision operator",
    "description": "Models the effect of converting floating - point values to a lower -\n    precision format(such as IEEE - FP16) and back to the original\n    format. The number of exponent and mantissa bits in the lower -\n    precision format can be specified arbitrarily,\n    although all bit sizes may not be supported on all hardware\n    implementations.\n\n    See https://www.tensorflow.org/xla/operation_semantics#reduceprecision.",
    "inputs": [
      { "name": "operand", "type": "HLO_FpTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "HLO_FpTensor" }
    ],
    "attributes": [
      { "name": "exponent_bits", "type": "I32Attr" },
      { "name": "mantissa_bits", "type": "I32Attr" }
    ]
  },
  {
    "name": "mhlo.reduce_scatter",
    "summary": "ReduceScatter operator",
    "description": "Performs all_reduce followed by a scatter.\n\n     See https://www.tensorflow.org/xla/operation_semantics#reducescatter",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "scatter_dimension", "type": "I64Attr" },
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.reduce_window",
    "summary": "ReduceWindow operator",
    "description": "Returns the result of executing a reduction function over all elements in\n    each window of one or more arrays in parallel.\n\n    See https://www.tensorflow.org/xla/operation_semantics#reducewindow.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "init_values", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "window_dimensions", "type": "I64ElementsAttr" },
      { "name": "window_strides", "type": "OptionalAttr" },
      { "name": "base_dilations", "type": "OptionalAttr" },
      { "name": "window_dilations", "type": "OptionalAttr" },
      { "name": "padding", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.remainder",
    "summary": "Remainder operator",
    "description": "Returns `lhs % rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.replica_id",
    "summary": "ReplicaId operator",
    "description": "Returns the unique ID (int32 scalar) of the replica.\n\n    The unique ID of each replica is an unsigned integer in the interval [0, N),\n    where N is the number of replicas. Since all the replicas are running the\n    same program, a ReplicaId() call in the program will return a different\n    value on each replica.\n\n    See https://www.tensorflow.org/xla/operation_semantics#replicaid."
  },
  {
    "name": "mhlo.reshape",
    "summary": "Reshape operator",
    "description": "Reshapes the dimensions of `operand` into a new configuration.\n\n    See https://www.tensorflow.org/xla/operation_semantics#reshape.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.return",
    "summary": "The `hlo.return` operation terminates a region and returns values.",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "mhlo.reverse",
    "summary": "Reverse operator",
    "description": "Reverses the specified dimensions of `operand` according to the given\n    `dimensions`.\n\n    See https://www.tensorflow.org/xla/operation_semantics#rev_reverse.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "dimensions", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.rng",
    "summary": "Rng operation",
    "description": "Generates random numbers using the `rng_distribution` algorithm and produces\n    a `result` tensor of a given shape `shape`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#rng\n\n    Example:\n    ```mlir\n    %result = mhlo.rng %a, %b, %shape, distribution = NORMAL : (tensor<i32>, tensor<i32>, tensor<2xi64>) -> tensor<3x3xi32>\n    ```",
    "inputs": [
      { "name": "a", "type": "DTensorOf" },
      { "name": "b", "type": "DTensorOf" },
      { "name": "shape", "type": "MHLO_DimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "MHLO_PredIntOrFpTensor" }
    ],
    "attributes": [
      { "name": "rng_distribution", "type": "MHLO_RngDistributionAttr" }
    ]
  },
  {
    "name": "mhlo.rng_bit_generator",
    "summary": "Uniform random number generator operator",
    "description": "Returns an output with a given shape filled with uniform random bits using\n    the specified algorithm (or backend default) and returns an updated state\n    (with the same shape as initial state) and the generated random data.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#rngbitgenerator.",
    "inputs": [
      { "name": "initial_state", "type": "HLO_IntOrFpTensor" }
    ],
    "attributes": [
      { "name": "rng_algorithm", "type": "I32Attr" }
    ]
  },
  {
    "name": "mhlo.rng_normal",
    "summary": "RNG with normal distribution.",
    "description": "Constructs an output of a given shape with random numbers generated\n    following the normal distribution with parameters `mu` and `sigma`. The\n    parameters and output shape have to have a floating point elemental type.\n    The parameters furthermore have to be scalar valued.\n\n    See https://www.tensorflow.org/xla/operation_semantics#rngnormal.",
    "inputs": [
      { "name": "mu", "type": "HLO_FpTensor" },
      { "name": "sigma", "type": "HLO_FpTensor" },
      { "name": "shape", "type": "HLO_DimensionTensor" }
    ]
  },
  {
    "name": "mhlo.rng_uniform",
    "summary": "RNG with uniform distribution.",
    "description": "Constructs an output of a given shape with random numbers generated\n    following the uniform distribution over the interval `[a,b)`. The parameters\n    and output element type have to be a boolean type, an integral type or a\n    floating point types, and the types have to be consistent.\n\n    See https://www.tensorflow.org/xla/operation_semantics#rnguniform.",
    "inputs": [
      { "name": "a", "type": "HLO_PredIntOrFpTensor" },
      { "name": "b", "type": "HLO_PredIntOrFpTensor" },
      { "name": "shape", "type": "HLO_DimensionTensor" }
    ]
  },
  {
    "name": "mhlo.round_nearest_afz",
    "summary": "Round operator",
    "description": "Returns `Round(operand)` element-wise, rounding to nearest integer with\n    half-way cases rounding away from zero.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.round_nearest_even",
    "summary": "RoundNearestEven operation",
    "description": "Performs element-wise rounding towards the nearest integer, breaking ties\n    towards the even integer, on the `operand` tensor and produces a `result`\n    tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#round_nearest_even\n\n    Example:\n    ```mlir\n    %result = mhlo.round_nearest_even %operand : tensor<5xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.rsqrt",
    "summary": "Reciprocal Square-root operator",
    "description": "Returns `1.0 / sqrt(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.scatter",
    "summary": "Scatter operator",
    "description": "Generates a result which is the value of the input array `operand`,\n    with several slices (at indices specified by `scatter_indices`)\n    updated with the values in `updates` using `update_computation`.\n\n    See https://www.tensorflow.org/xla/operation_semantics#scatter.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "scatter_indices", "type": "HLO_Tensor" },
      { "name": "updates", "type": "HLO_Tensor" },
      { "name": "scatter_dimension_numbers", "type": "ScatterDimensionNumbers" }
    ],
    "attributes": [
      { "name": "indices_are_sorted", "type": "DefaultValuedAttr" },
      { "name": "unique_indices", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "mhlo.select",
    "summary": "Select operator",
    "description": "Constructs an output tensor from the elements of `on_true` and `on_false`\n    based on the values of `pred`. All three operands must be of the same shape\n    with the exception of `pred`, which may also be a scalar in which case it is\n    broadcasted.\n\n    See https://www.tensorflow.org/xla/operation_semantics#select.",
    "inputs": [
      { "name": "pred", "type": "HLO_PredTensor" },
      { "name": "on_true", "type": "HLO_Tensor" },
      { "name": "on_false", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.select_and_scatter",
    "summary": "SelectAndScatter operator",
    "description": "Runs a windowed selection `select` function over `operand` with shape\n    `window_dimensions` and stride `window_strides`. This will produce an amount\n    of selected locations whose shape matches `source`. These are then scattered\n    to the output which is initialized with `init_value`.\n    Multiple scattered elements which land in the same output location are\n    combined using the `scatter` function.\n\n    See https://www.tensorflow.org/xla/operation_semantics#selectandscatter.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "source", "type": "HLO_Tensor" },
      { "name": "init_value", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "window_dimensions", "type": "OptionalAttr" },
      { "name": "window_strides", "type": "OptionalAttr" },
      { "name": "padding", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "mhlo.send",
    "summary": "Send operator",
    "description": "Sends the given operand data to a Recv instruction in another computation\n    that shares the same channel handle. Does not return any data. Similar to\n    the Recv operation, Send operation represents synchronous communication,\n    and is internally decomposed into 2 HLO instructions (Send and SendDone) to\n    enable asynchronous data transfers.\n\n    See https://www.tensorflow.org/xla/operation_semantics#send.",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrTuple" },
      { "name": "token", "type": "HLO_Token" },
      { "name": "channel_handle", "type": "ChannelHandle" }
    ],
    "attributes": [
      { "name": "is_host_transfer", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "mhlo.set_dimension_size",
    "summary": "SetDimensionSize operator",
    "description": "Sets the dynamic size of operand's given dimension. Pass through the operand\n    as result, with dynamic dimension tracked by the compiler. Padded values\n    will be ignored by downstream reduction ops.\n\n    See https://www.tensorflow.org/xla/operation_semantics#setdimensionsize.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "size", "type": "I32Tensor" }
    ],
    "attributes": [
      { "name": "dimension", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.shift_left",
    "summary": "Shift Left operator",
    "description": "Returns `lhs << rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.shift_right_arithmetic",
    "summary": "Shift right arithmetic operator",
    "description": "Returns arithmetic `lhs >> rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.shift_right_logical",
    "summary": "Shift right logical operator",
    "description": "Returns logical `lhs >> rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.sign",
    "summary": "Sign operator",
    "description": "Returns `sign(operand)` element-wise, where\n\n    ```\n    sign(x) = -1  : x < 0\n            = -0  : x = -0\n            = NaN : x = NaN\n            = +0  : x = +0\n            = 1   : x > 0\n    ```\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.sine",
    "summary": "Sin operator",
    "description": "Returns `Sin(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.sinh",
    "summary": "Sinh operation",
    "description": "Performs element-wise sinh operation on `operand` tensor and produces a\n    `result` tensor.\n\n    Example:\n    ```mlir\n    %result = mhlo.sinh %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.slice",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "start_indices", "type": "I64ElementsAttr" },
      { "name": "limit_indices", "type": "I64ElementsAttr" },
      { "name": "strides", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.sort",
    "summary": "Sort operator",
    "description": "Sorts the given `operands` at the given `dimension` with the given\n    `comparator`.\n\n    See https://www.tensorflow.org/xla/operation_semantics#sort.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "DefaultValuedAttr" },
      { "name": "is_stable", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "mhlo.sqrt",
    "summary": "Square-root operator",
    "description": "Returns `sqrt(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.stochastic_convert",
    "summary": "StochasticConvert operation",
    "description": "This operation is a work in progress, so it is not yet included in\n    the specification: https://github.com/openxla/stablehlo/issues/295.\n\n    Informally, this operation performs element-wise conversion of values from\n    a bigger type to a smaller one with stochastic rounding using the random\n    number passed in.",
    "inputs": [
      { "name": "operand", "type": "MHLO_FpTensor" },
      { "name": "random", "type": "RankedTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "MHLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.subtract",
    "summary": "Subtraction operator",
    "description": "Returns `lhs - rhs` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ]
  },
  {
    "name": "mhlo.tan",
    "summary": "Tan operation",
    "description": "This operation is a work in progress, so it is not yet included in\n    the specification: https://github.com/openxla/stablehlo/issues/954.\n\n    Informally, this operation returns `Tan(operand)` element-wise.\n\n    Example:\n    ```mlir\n    %0 = mhlo.tan %arg0 : tensor<2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "MHLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "MHLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.tanh",
    "summary": "Tanh operator",
    "description": "Returns `tanh(operand)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_unary_functions.",
    "inputs": [
      { "name": "operand", "type": "TensorType" }
    ]
  },
  {
    "name": "mhlo.topk",
    "summary": "TopK operation",
    "description": "Returns top `k` values and their indices, along the last\n    dimension of the operand if `largest=true` or the bottom `k` values if\n    `largest=false`.\n\n    See:\n    https://www.tensorflow.org/xla/operation_semantics#top-k\n\n    Example:\n    ```mlir\n    %values, %indices = mhlo.topk(%operand, k=5, largest=true)\n      : tensor<100xf32> -> (tensor<5xf32>, tensor<5xi32>)\n    ```",
    "inputs": [
      { "name": "operand", "type": "MHLO_Tensor" }
    ],
    "outputs": [
      { "name": "values", "type": "MHLO_Tensor" },
      { "name": "indices", "type": "MHLO_Tensor" }
    ],
    "attributes": [
      { "name": "k", "type": "I64Attr" },
      { "name": "largest", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "`(`$operand `,` `k` `=` $k (`,` `largest` `=` $largest^)? `)` attr-dict `:`\n    type($operand) `->` `(`type($values)`,` type($indices)`)`"
  },
  {
    "name": "mhlo.torch_index_select",
    "inputs": [
      { "name": "input", "type": "HLO_Tensor" },
      { "name": "index", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "dim", "type": "I64Attr" },
      { "name": "batch_dims", "type": "I64Attr" }
    ]
  },
  {
    "name": "mhlo.trace",
    "summary": "Trace operator",
    "description": "Emits a logging message `tag` with the `operand`.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "tag", "type": "StrAttr" }
    ]
  },
  {
    "name": "mhlo.transpose",
    "summary": "Transpose operator",
    "description": "Permutes the dimensions of `operand` according to the given `permutation`.\n\n    `res_dimensions[i] = operand_dimensions[permutation[i]]`\n\n    See https://www.tensorflow.org/xla/operation_semantics#transpose.",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "permutation", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "mhlo.triangular_solve",
    "summary": "TriangularSolve operator",
    "description": "Solves systems of linear equations with lower or upper triangular\n    coefficient matrices by forward- or back-substitution. Broadcasting along\n    leading dimensions, this routine solves one of the matrix systems\n    op(a) * x = b, or x * op(a) = b, for the variable x, given a and b, where\n    op(a) is either op(a) = a, or op(a) = Transpose(a), or\n    op(a) = Conj(Transpose(a)).\n\n    Input data is read only from the lower/upper triangle of a, depending on the\n    value of lower. Values from the other triangle are ignored. Output data is\n    returned in the same triangle; the values in the other triangle are\n    implementation-defined and may be anything.\n\n    If the rank of a and b are greater than 2, they are treated as batches of\n    matrices, where all except the minor 2 dimensions are batch dimensions. a\n    and b must have equal batch dimensions.\n\n    See https://www.tensorflow.org/xla/operation_semantics#triangularsolve.",
    "inputs": [
      { "name": "a", "type": "HLO_FpOrComplexTensor" },
      { "name": "b", "type": "HLO_FpOrComplexTensor" }
    ],
    "attributes": [
      { "name": "left_side", "type": "BoolAttr" },
      { "name": "lower", "type": "BoolAttr" },
      { "name": "unit_diagonal", "type": "BoolAttr" },
      { "name": "transpose_a", "type": "HLO_TransposeAttr" }
    ]
  },
  {
    "name": "mhlo.tuple",
    "summary": "XLA's tuple op",
    "description": "Groups a set of tensor inputs into a single tuple object.\n\n     See https://www.tensorflow.org/xla/operation_semantics#tuple.",
    "inputs": [
      { "name": "val", "type": "Variadic" }
    ]
  },
  {
    "name": "mhlo.unary_einsum",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "einsum_config", "type": "StrAttr" }
    ]
  },
  {
    "name": "mhlo.uniform_dequantize",
    "summary": "UniformDequantize operation",
    "description": "Performs element-wise conversion of quantized tensor `operand` to a\n    floating-point tensor `result` according to the quantization parameters\n    defined by the `operand` type.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#uniform_dequantize\n\n    Example:\n    ```mlir\n    %result = mhlo.uniform_dequantize %operand : (tensor<16x16x!quant.uniform<i8:f32, 34.0:16>>) -> tensor<16x16xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.uniform_quantize",
    "summary": "UniformQuantize operation",
    "description": "Performs element-wise conversion of floating-point tensor or quantized\n    tensor `operand` to a quantized tensor `result` according to the\n    quantization parameters defined by the `result` type.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#uniform_quantize\n\n    Example:\n    ```mlir\n    %result = mhlo.uniform_quantize %operand : (tensor<16x16xf32>) -> tensor<16x16x!quant.uniform<ui8:f32, 34.0:16>>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict\n      `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "mhlo.while",
    "summary": "While operator",
    "description": "Returns the result of executing a body function until the cond body returns\n    true.\n\n    See https://www.tensorflow.org/xla/operation_semantics#while.",
    "inputs": [
      { "name": "arg", "type": "Variadic" }
    ]
  },
  {
    "name": "mhlo.xla.rng_get_and_update_state",
    "summary": "XlaRngGetAndUpdateState operation",
    "description": "This operation is private to the XLA compiler, so it is does not yet have\n    a specification.\n\n    Informally, this operation represents the change of the global random number\n    generator state for rng instructions. The global state is incremented by\n    delta and the old state is returned.\n\n    The output is currently defined for a single output type. If this changes in\n    the future to support multiple types, lowering to use of a global memref\n    must ensure that a single memref is still used and updated appropriately.",
    "attributes": [
      { "name": "delta", "type": "I64Attr" }
    ],
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "mhlo.xor",
    "summary": "Logical xor",
    "description": "Returns `logical_xor(lhs, rhs)` element-wise.\n\n    See\n    https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations.",
    "inputs": [
      { "name": "lhs", "type": "HLO_PredOrIntTensor" },
      { "name": "rhs", "type": "HLO_PredOrIntTensor" }
    ]
  },
  {
    "name": "michelson.get_amount",
    "summary": "Get the amount of the current transaction.",
    "description": "The `michelson.get_amount` operation returns the amount of the current transaction.\n\n        Example:\n        ```mlir\n        %0 = michelson.get_amount() : !michelson.mutez\n        ```",
    "outputs": [
      { "name": "res", "type": "MichelsonMutez" }
    ]
  },
  {
    "name": "ml_program.func",
    "summary": "Function containing a single `SSACFG` region",
    "description": "This simple function container represents callables in an ML program where\n    the body is an `SSACFG` region. It must be terminated by a `return` op which\n    yields values with the same arity and types as the `FunctionType` results\n    of the containing `func`.\n\n    This op is a `Symbol` but does not introduce a new `SymbolTable`. As such,\n    it cannot represent nested symbols.\n\n    Example:\n\n    ```mlir\n    ml_program.func private @some_extern(i32) -> i32\n    ml_program.func @compute(%arg0 : i32) -> i32 {\n      ml_program.return %arg0 : i32\n    }\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "sym_visibility", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "ml_program.global",
    "summary": "Module level declaration of a global variable",
    "description": "Declares a named global variable (or constant).\n\n    A global contains a value of a specified type which can be accessed at\n    runtime via appropriate load/store operations. It can be mutable or\n    constant, optionally taking an initial value or declared as\n    extern (in which case, the initial value is found in external storage\n    by symbol name).\n\n    Generally, the type of the global and the type of the initial value\n    will be the same. However, for type hierarchies which can have a more\n    generalized bounding type that can be assigned from a narrow type, this\n    is allowed (but not verified).\n\n    Examples:\n\n    ```mlir\n    // Constant global.\n    ml_program.global @foobar(dense<4> : tensor<4xi32>) : tensor<?xi32>\n\n    // Constant with external linkage.\n    ml_program.global mutable @foobar(#ml_program.extern<tensor<4xi32>>)\n      : tensor<?xi32>\n\n    // Mutable global with an undefined initial value.\n    ml_program.global mutable @foobar : tensor<?xi32>\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "type", "type": "TypeAttr" },
      { "name": "is_mutable", "type": "UnitAttr" },
      { "name": "value", "type": "OptionalAttr" },
      { "name": "sym_visibility", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    (`mutable` $is_mutable^)?\n    $sym_name ``\n    custom<TypedInitialValue>($type, $value)\n    attr-dict"
  },
  {
    "name": "ml_program.global_load",
    "summary": "Direct load of a mutable value from a global",
    "description": "Performs a non-atomic, non-volatile, non-synchronized load from a global\n    that may be mutable.\n\n    It is fully expected that these constraints are not suitable for\n    all situations, and alternative ops should be defined and used for more\n    advanced cases.\n\n    This op is side effecting and may not be valid to use in graph regions\n    without additional consideration to evaluation order constraints. See\n    `global_load_graph` for op which allows for explicit ordering constraints.\n\n    Example:\n\n    ```mlir\n    %0 = ml_program.global_load @foobar : tensor<?xi32>\n    ```",
    "inputs": [
      { "name": "global", "type": "Arg" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "$global `:` type($result) attr-dict"
  },
  {
    "name": "ml_program.global_load_const",
    "summary": "Direct load a constant value from a global",
    "description": "Loads a constant (immutable) value from a global directly by symbol.\n\n    This op is only legal for globals that are not mutable and exists because\n    such a load can be considered to have no side effects.\n\n    Example:\n\n    ```mlir\n    %0 = ml_program.global_load_const @foobar : tensor<?xi32>\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "global", "type": "SymbolRefAttr" }
    ],
    "assemblyFormat": "$global `:` type($result) attr-dict"
  },
  {
    "name": "ml_program.global_load_graph",
    "summary": "Direct load of a mutable value from a global in Graph region",
    "description": "Performs a non-atomic, non-volatile, non-synchronized load from a global\n    that may be mutable.\n\n    It is fully expected that these constraints are not suitable for all\n    situations, and alternative ops should be defined and used for more advanced\n    cases.\n\n    This op is side effecting and may not be valid to use in graph regions\n    without additional consideration to evaluation order constraints.\n\n    Example:\n\n    ```mlir\n    %0, %cstr = ml_program.global_load_graph @foobar\n      ordering (%token -> !ml_program.token) : tensor<?xi32>\n    ```",
    "inputs": [
      { "name": "global", "type": "Arg" },
      { "name": "consumeTokens", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" },
      { "name": "produceToken", "type": "MLProgram_TokenType" }
    ],
    "assemblyFormat": "$global `` custom<TokenOrdering>($consumeTokens, type($produceToken)) `:` type($result) attr-dict"
  },
  {
    "name": "ml_program.global_store",
    "summary": "Direct store of a value into a mutable global",
    "description": "Performs a non-atomic, non-volatile, non-synchronized store to a mutable\n    global.\n\n    It is fully expected that these constraints are not suitable for\n    all situations, and alternative ops should be defined and used for more\n    advanced cases.\n\n    This op is side effecting and may not be valid to use in graph regions\n    without additional consideration to evaluation order constraints. See\n    `global_store_graph` for op which allows for explicit ordering constraints.\n\n    Example:\n\n    ```mlir\n    ml_program.global_store @foobar = %0 : tensor<?xi32>\n    ```",
    "inputs": [
      { "name": "global", "type": "Arg" },
      { "name": "value", "type": "AnyType" }
    ],
    "assemblyFormat": "$global `=` $value `:` type($value) attr-dict"
  },
  {
    "name": "ml_program.global_store_graph",
    "summary": "Direct store of a value into a mutable global",
    "description": "Performs a non-atomic, non-volatile, non-synchronized store to a mutable\n    global.\n\n    It is fully expected that these constraints are not suitable for\n    all situations, and alternative ops should be defined and used for more\n    advanced cases.\n\n    This op is side effecting and may not be valid to use in graph regions\n    without additional consideration to evaluation order constraints.\n\n    Example:\n\n    ```mlir\n    %token = ml_program.global_store @foobar = %0 : tensor<?xi32>\n      ordering (%in_token -> !ml_program.token) : tensor<?xi32>\n    ```",
    "inputs": [
      { "name": "global", "type": "Arg" },
      { "name": "value", "type": "AnyType" },
      { "name": "consumeTokens", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "produceToken", "type": "MLProgram_TokenType" }
    ],
    "assemblyFormat": "$global `=` $value `` custom<TokenOrdering>($consumeTokens, type($produceToken)) `:` type($value) attr-dict"
  },
  {
    "name": "ml_program.output",
    "summary": "Outputs values from a subgraph function",
    "description": "The `output` operation terminates a subgraph by yielding values\n    to the caller.\n    The operation takes variable number of operands and produces no results.\n    The operand number and types must match the signature of the function\n    that contains the operation.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "ml_program.return",
    "summary": "Returns values from a `func` function",
    "description": "The `return` operation terminates a `func` function by yielding values\n    to the caller.\n    The operation takes variable number of operands and produces no results.\n    The operand number and types must match the signature of the function\n    that contains the operation.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "ml_program.subgraph",
    "summary": "An function containing a single `Graph` region",
    "description": "This simple function container represents callables in an ML program where\n    the body is a `Graph` region containing a single block. It must be\n    terminated by an `output` op which yields values with the same arity and\n    types as the `FunctionType` results of the containing `subgraph`.\n\n    This op is a `Symbol` but does not introduce a new `SymbolTable`. As such,\n    it cannot represented nested symbols.\n\n    Example:\n\n    ```mlir\n    ml_program.subgraph private @some_extern(i32) -> i32\n    ml_program.subgraph @compute(%arg0 : i32) -> i32 {\n      ml_program.output %arg0 : i32\n    }\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "sym_visibility", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "ml_program.token",
    "summary": "Produces a new token value",
    "description": "Token values are used to chain side effecting ops in a graph so as to\n    establish an execution order. This op produces a token.",
    "outputs": [
      { "name": "token", "type": "MLProgram_TokenType" }
    ],
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "nvgpu.device_async_copy",
    "summary": "device-side asynchronous copy",
    "description": "The `nvgpu.device_async_copy` op initiates an asynchronous copy operation of\n    elements from source (global memory) to the destination (shared memory)\n    without blocking the thread. The async copy is added to a group.\n\n    This op is meant to be used with `nvgpu.device_async_create_group` and\n    `nvgpu.device_async_wait` to synchronize copies as explained in those ops\n    descriptions.\n\n    `bypassL1` attribute is hint to the hardware to bypass the L1 cache during\n    async copy, this hint may be ignored by the hardware.\n\n    `dstElements` attribute is the total number of elements written to\n    destination (shared memory).\n\n    `srcElements` argument is the total number of elements read from\n    source (global memory).\n\n    `srcElements` is an optional argument and when present the op only reads\n    `srcElements` number of elements from the source (global memory) and zero fills\n    the rest of the elements in the destination (shared memory).\n\n    In order to do a copy and wait for the result we need the following\n    combination:\n    ```\n    // copy 1.\n    %cp1 = nvgpu.device_async_copy %A[%c0], %B[%c0], 4 :memref<16xf32> to memref<16xf32, 3>\n    // copy 2.\n    %cp2 = nvgpu.device_async_copy %C[%c0], %D[%c0], 4 : memref<16xf32> to memref<16xf32, 3>\n    // group 1 contains copy 1 and copy 2.\n    %token1 = nvgpu.device_async_create_group %cp1, %cp2\n    // copy 3.\n    %cp3 = nvgpu.device_async_copy %E[%c0], %F[%c0], 4 : memref<16xf32> to memref<16xf32, 3>\n    // group 2 contains copy 3.\n    %token2 = nvgpu.device_async_create_group %cp3\n    // after the wait copy 1 and copy 2 are complete.\n    nvgpu.device_async_wait %token1\n    // after the wait copy 3 is complete.\n    nvgpu.device_async_wait %token2\n    ```\n\n    Example:\n\n    ```mlir\n    %0 = nvgpu.device_async_copy %src[%c0, %c0], %dst[%c0, %c0, %c0], 4 :\n      memref<4x5xf32> to memref<2x7x5xf32, 3>\n    ```",
    "inputs": [
      { "name": "dst", "type": "Arg" },
      { "name": "dstIndices", "type": "Variadic" },
      { "name": "src", "type": "Arg" },
      { "name": "srcIndices", "type": "Variadic" },
      { "name": "srcElements", "type": "Optional" }
    ],
    "outputs": [
      { "name": "asyncToken", "type": "NVGPU_DeviceAsyncToken" }
    ],
    "attributes": [
      { "name": "dstElements", "type": "IndexAttr" },
      { "name": "bypassL1", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$src `[` $srcIndices `]` `,` $dst `[` $dstIndices `]` `,` $dstElements (`,` $srcElements^)?\n      attr-dict `:` type($src) `to` type($dst)"
  },
  {
    "name": "nvgpu.device_async_create_group",
    "summary": "device side asynchronous create group operation",
    "description": "The `nvgpu.device_async_create_group` op creates a group of memory accesses\n    containing all the pending `device_async_copy` operations associated with\n    argument tokens. Each token can only be part of one group.\n\n    It returns a token that can be use to wait until the group fully completes.\n\n    This is meant to be used with `nvgpu.device_async_wait` to synchronize copies\n    as explained in those ops descriptions.\n\n    Groups are executed in the order they are created.\n\n    Example:\n\n    ```mlir\n    %0 = nvgpu.device_async_create_group\n  ```",
    "inputs": [
      { "name": "inputTokens", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "asyncToken", "type": "NVGPU_DeviceAsyncToken" }
    ],
    "assemblyFormat": "$inputTokens attr-dict"
  },
  {
    "name": "nvgpu.device_async_wait",
    "summary": "Wait for async gpu ops to complete.",
    "description": "The `nvgpu.device_async_wait` op will block the execution thread until the group\n    associated with the source token is fully completed.\n\n    The optional `$numGroups` attribute gives an upper bound of the number of\n    groups uncompleted when the wait can unblock the thread. For example,  if\n    16 async groups are pushe and `$numGroups` is set to 12, then the thread\n    will unblock when 12 groups or fewer are in flight (4 groups have\n    completed).\n\n    Example:\n\n    ```mlir\n    nvgpu.device_async_wait %0\n    ```",
    "inputs": [
      { "name": "asyncDependencies", "type": "NVGPU_DeviceAsyncToken" }
    ],
    "attributes": [
      { "name": "numGroups", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$asyncDependencies attr-dict"
  },
  {
    "name": "nvgpu.ldmatrix",
    "description": "The `nvgpu.ldmatrix` op represents loading a matrix fragment from\n    memory to registers. The source and result type must be compatible\n    with lowering to the `nvvm.ldmatrix` instruction. This op represents\n    the distributed version of a `vector.transfer_read` as an intermediate\n    step between lowering from `vector.transfer_read` to `nvvm.ldmatrix`.\n\n    This operation is meant to follow the semantic of described here:\n    https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix\n\n    Example:\n    ```mlir\n    %0 = nvgpu.ldmatrix %sm[%c0, %c0] {numTiles = 4 : i32, transpose = false} :\n      memref<?x?xf16, 3> -> vector<4x2xf16>\n    ```",
    "inputs": [
      { "name": "srcMemref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "res", "type": "AnyVectorOfNonZeroRank" }
    ],
    "attributes": [
      { "name": "transpose", "type": "BoolAttr" },
      { "name": "numTiles", "type": "I32Attr" }
    ],
    "assemblyFormat": "$srcMemref`[` $indices `]` attr-dict `:` type($srcMemref) `->` type($res)"
  },
  {
    "name": "nvgpu.mbarrier.arrive",
    "summary": "Performs arrive operation on the `nvgpu.mbarrier.arrive`.",
    "description": "The Op performs arrive-on operation on the `mbarrier` object and returns a \n    `nvgpu.mbarrier.token`.\n\n    For more information, see\n    https://docs.nvidia.com/cuda/parallel-thread-execution/#arrive-on-operation-on-mbarrier-object\n\n    Example:\n    ```mlir\n      %token = nvgpu.mbarrier.arrive %barrier : !nvgpu.mbarrier.barrier<memorySpace = #gpu.address_space<workgroup>> -> !nvgpu.mbarrier.token\n    ```",
    "inputs": [
      { "name": "barriers", "type": "NVGPU_MBarrierGroup" },
      { "name": "mbarId", "type": "Index" }
    ],
    "outputs": [
      { "name": "token", "type": "NVGPU_MBarrierToken" }
    ],
    "assemblyFormat": "$barriers `[` $mbarId `]` attr-dict `:` type($barriers) `->` type($token)"
  },
  {
    "name": "nvgpu.mbarrier.arrive.expect_tx",
    "summary": "Performs expect_tx operation on the `nvgpu.mbarrier.arrive`",
    "description": "A thread executing the Op performs an expect-tx operation on the mbarrier \n    object at the location specified by the address operand $barrier. The \n    expect-tx operation, with an $txcount argument, increases the tx-count of \n    an mbarrier object by the value specified by $txcount. This makes the \n    current phase of the mbarrier object to expect and track the completion of \n    additional asynchronous transactions.\n    \n    The `$txCount` specifies the number of element to the expect-tx operation.\n\n    Example:\n    ```mlir\n      nvgpu.mbarrier.arrive.expect_tx %barrier, %ic0 : !nvgpu.mbarrier.barrier<memorySpace = #gpu.address_space<workgroup>>\n    ```",
    "inputs": [
      { "name": "barriers", "type": "NVGPU_MBarrierGroup" },
      { "name": "txcount", "type": "Index" },
      { "name": "mbarId", "type": "Index" },
      { "name": "predicate", "type": "Optional" }
    ],
    "assemblyFormat": "$barriers `[` $mbarId `]` `,` $txcount  (`,` `predicate` `=` $predicate^)? attr-dict `:` type($barriers)"
  },
  {
    "name": "nvgpu.mbarrier.arrive.nocomplete",
    "summary": "Performs arrive operation on the `nvgpu.mbarrier.arrive.nocomplete` as non-blocking.",
    "description": "The Op performs arrive-on operation on the `mbarrier` object and returns a \n    `nvgpu.mbarrier.token`.\n\n    The Op does not cause the `nvgpu.mbarrier` to complete its current phase.\n\n    Example:\n    ```mlir\n      %token = nvgpu.mbarrier.arrive.noComplete %barrier, %count : !nvgpu.mbarrier.barrier<memorySpace = #gpu.address_space<workgroup>> -> !nvgpu.mbarrier.token\n    ```",
    "inputs": [
      { "name": "barriers", "type": "NVGPU_MBarrierGroup" },
      { "name": "mbarId", "type": "Index" },
      { "name": "count", "type": "Index" }
    ],
    "outputs": [
      { "name": "token", "type": "NVGPU_MBarrierToken" }
    ],
    "assemblyFormat": "$barriers `[` $mbarId `]` `,` $count attr-dict `:` type($barriers) `->` type($token)"
  },
  {
    "name": "nvgpu.mbarrier.create",
    "summary": "Creates a `nvgpu.mbarrier` object.",
    "description": "The Op generates one or more `mbarrier` object, which is a barrier created in \n    shared memory and supports various synchronization behaviors for threads.\n\n    The `mbarrier` object has the following type and alignment requirements:\n      Type: .b64, Alignment: 8, Memory space: .shared\n    \n    Example:\n    ```mlir\n      %barrier = nvgpu.mbarrier.create -> !nvgpu.mbarrier.barrier<memorySpace = #gpu.address_space<workgroup>>\n    ```",
    "outputs": [
      { "name": "barriers", "type": "NVGPU_MBarrierGroup" }
    ],
    "assemblyFormat": "attr-dict `->` type($barriers)"
  },
  {
    "name": "nvgpu.mbarrier.get",
    "summary": "Return a pointer to an `nvgpu.mbarrier`.",
    "description": "The `nvgpu.mbarrier.get` operation retrieves a pointer to a specific \n    `mbarrier` object from a group of barriers created by the `nvgpu.mbarrier.create` operation.\n\n    Example:\n    ```mlir\n      %mbars = nvgpu.mbarrier.create -> !nvgpu.mbarrier.group<memorySpace = #gpu.address_space<workgroup>, num_barriers = 10>\n      %mbar_pointer = nvgpu.mbarrier.get %mbars[%c2] : !nvgpu.mbarrier.group<memorySpace = #gpu.address_space<workgroup>>\n    ```",
    "inputs": [
      { "name": "barriers", "type": "NVGPU_MBarrierGroup" },
      { "name": "mbarId", "type": "Index" }
    ],
    "outputs": [
      { "name": "mbarrierPointer", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$barriers `[` $mbarId `]` attr-dict `:` type($barriers) `->` type($mbarrierPointer)"
  },
  {
    "name": "nvgpu.mbarrier.init",
    "summary": "Initialize the `nvgpu.mbarrier`.",
    "description": "The Op initializes the `mbarrier` object with the given number of threads.\n\n    Example:\n    ```mlir\n      %num_threads = gpu.block_dim x\n      %barrier = nvgpu.mbarrier.create -> !nvgpu.mbarrier.barrier<memorySpace = #gpu.address_space<workgroup>>\n      nvgpu.mbarrier.init %barrier, %num_threads : !nvgpu.mbarrier.barrier<memorySpace = #gpu.address_space<workgroup>>\n    ```",
    "inputs": [
      { "name": "barriers", "type": "NVGPU_MBarrierGroup" },
      { "name": "count", "type": "Index" },
      { "name": "mbarId", "type": "Index" },
      { "name": "predicate", "type": "Optional" }
    ],
    "assemblyFormat": "$barriers `[` $mbarId `]` `,` $count (`,` `predicate` `=` $predicate^)? attr-dict `:` type($barriers)"
  },
  {
    "name": "nvgpu.mbarrier.test.wait",
    "summary": "Checks if the `nvgpu.mbarrier` has completed its current phase.",
    "description": "Checks whether the mbarrier object has completed the phase. It is is a \n    non-blocking instruction which tests for the completion of the phase.\n\n    Example:\n    ```mlir\n      %isComplete = nvgpu.mbarrier.test.wait %barrier, %token : !nvgpu.mbarrier.barrier<memorySpace = #gpu.address_space<workgroup>>, !nvgpu.mbarrier.token\n    ```",
    "inputs": [
      { "name": "barriers", "type": "NVGPU_MBarrierGroup" },
      { "name": "token", "type": "NVGPU_MBarrierToken" },
      { "name": "mbarId", "type": "Index" }
    ],
    "outputs": [
      { "name": "waitComplete", "type": "I1" }
    ],
    "assemblyFormat": "$barriers `[` $mbarId `]` `,` $token attr-dict `:` type($barriers) `,` type($token)"
  },
  {
    "name": "nvgpu.mbarrier.try_wait.parity",
    "summary": "Waits for the `nvgpu.mbarrier` to complete its current phase.",
    "description": "Checks whether the mbarrier object has completed the phase. It is is a \n    potentially blocking instruction which tests for the completion of the \n    phase. Suspended thread resumes execution when the specified phase completes \n    OR before the phase completes following a system-dependent time limit. \n\n    The `$phaseParity` specifies either even phase (0) or odd phase (1) to \n    wait.\n\n    Example:\n    ```mlir\n      nvgpu.mbarrier.try_wait.parity %barrier, %phaseParity, %ticks : !nvgpu.mbarrier.barrier<memorySpace = #gpu.address_space<workgroup>>\n    ```",
    "inputs": [
      { "name": "barriers", "type": "NVGPU_MBarrierGroup" },
      { "name": "phaseParity", "type": "I1" },
      { "name": "ticks", "type": "Index" },
      { "name": "mbarId", "type": "Index" }
    ],
    "assemblyFormat": "$barriers `[` $mbarId `]` `,` $phaseParity `,` $ticks attr-dict `:` type($barriers)"
  },
  {
    "name": "nvgpu.mma.sp.sync",
    "description": "The `nvgu.mma.sp.sync` operation performs a warp-distributed MMA operation\n  where operand A is \"structured sparse\". In this case, the `matrixA` operand\n  represents the (warp-distributed) non-zero values of operand A, and the\n  `sparse_metadata` operand provides the indices.\n\n  The full description of the sparsity storage format and distribution scheme is\n  described in the PTX docs. This operation is meant to follow the semantic\n  described in the PTX documentation here:\n  https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-for-sparse-mma\n\n  The way the indices are distributed among the threads in a warp is controlled\n  by the optional `sparsity_selector` operand, which is `0` by default. For\n  more information, please consult the PTX documentation linked above.\n\n  Example (targetingthe f16 16x8x32 `mma.sp` PTX instruction):\n\n  ```mlir\n  nvgpu.mma.sp.sync (%a, %b, %c) metadata (%meta) {mmaShape = [16, 8, 32]} :\n    (vector<4x2xf16>, vector<2x2xf16>, vector<2x2xf16>) -> vector<2x2xf16>\n  ```",
    "inputs": [
      { "name": "matrixA", "type": "AnyVectorOfNonZeroRank" },
      { "name": "matrixB", "type": "AnyVectorOfNonZeroRank" },
      { "name": "matrixC", "type": "AnyVectorOfNonZeroRank" },
      { "name": "sparseMetadata", "type": "NVGPU_MmaSparseSyncMetadataType" }
    ],
    "outputs": [
      { "name": "res", "type": "AnyVectorOfNonZeroRank" }
    ],
    "attributes": [
      { "name": "mmaShape", "type": "I64ArrayAttr" },
      { "name": "sparsitySelector", "type": "DefaultValuedAttr" },
      { "name": "tf32Enabled", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`(` $matrixA`,` $matrixB`,` $matrixC `)` `metadata` `(` $sparseMetadata `)` attr-dict\n    `:` `(` type($matrixA) `,` type($matrixB) `,` type($matrixC) `)` `->` type($res)"
  },
  {
    "name": "nvgpu.mma.sync",
    "description": "The `nvgpu.mma.sync` op represents the warp-level matrix-multiply-and-\n    accumulate (mma) operation that is compatible with `nvvm.mma.sync`.\n    The operands and results vector sizes are thread-level onwership to\n    the warp-level mma operation shape. `mmaShape` attribute holds the\n    warp-level matrix-multiply shape.\n\n    The `nvgpu.mma.sync` op serves as an intermediate point between lowering from\n    `vector.contract` to `nvvm.mma.sync`.\n\n    This operation is meant to follow the semantic of described here:\n      https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma\n\n    Example:\n\n    ```mlir\n    %res = nvgpu.mma.sync (%matrixA, %matrixB, %matrixC) {mmaShape = [16, 8, 16]} :\n        (vector<4x2xf16>, vector<2x2xf16>, vector<2x2xf32>) -> vector<2x2xf32>\n    ```",
    "inputs": [
      { "name": "matrixA", "type": "AnyVectorOfNonZeroRank" },
      { "name": "matrixB", "type": "AnyVectorOfNonZeroRank" },
      { "name": "matrixC", "type": "AnyVectorOfNonZeroRank" }
    ],
    "outputs": [
      { "name": "res", "type": "AnyVectorOfNonZeroRank" }
    ],
    "attributes": [
      { "name": "mmaShape", "type": "I64ArrayAttr" },
      { "name": "tf32Enabled", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`(` $matrixA`,` $matrixB`,` $matrixC `)` attr-dict\n    `:` `(` type($matrixA) `,` type($matrixB) `,` type($matrixC) `)` `->` type($res)"
  },
  {
    "name": "nvgpu.rcp",
    "summary": "The reciprocal calculation for vector types",
    "description": "Reciprocal calculation for `vector` types using `nvvm.rcp` OPs.\n\n    Currently, only the `approx` rounding mode and `ftz` are supported, and only for the `f32` type.\n\n    The input and output must be of the same vector type and shape.",
    "inputs": [
      { "name": "in", "type": "VectorOfNonZeroRankOf" }
    ],
    "outputs": [
      { "name": "out", "type": "VectorOfNonZeroRankOf" }
    ],
    "attributes": [
      { "name": "rounding", "type": "DefaultValuedAttr" },
      { "name": "ftz", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$in `{` `rounding` `=` $rounding (`,` `ftz` $ftz^)? `}` \n    attr-dict `:` type($out)"
  },
  {
    "name": "nvgpu.tma.async.load",
    "summary": "TMA asynchronous load",
    "description": "The Op loads a tile memory region from global memory to shared memory by \n    Tensor Memory Access (TMA).\n    \n    `$tensorMapDescriptor` is tensor map descriptor which has information about\n    tile shape. The descriptor is created by `nvgpu.tma.create.descriptor`\n\n    The Op uses `$barrier` mbarrier based completion mechanism.",
    "inputs": [
      { "name": "dst", "type": "Arg" },
      { "name": "barriers", "type": "NVGPU_MBarrierGroup" },
      { "name": "tensorMapDescriptor", "type": "NVGPU_TensorMapDescriptor" },
      { "name": "coordinates", "type": "Variadic" },
      { "name": "mbarId", "type": "Index" },
      { "name": "multicastMask", "type": "Optional" },
      { "name": "predicate", "type": "Optional" }
    ],
    "assemblyFormat": "$tensorMapDescriptor `[` $coordinates `]` `,` $barriers `[` $mbarId `]` \n      `to` $dst\n      (`multicast_mask` `=` $multicastMask^ )?\n      (`,` `predicate` `=` $predicate^)?\n      attr-dict `:` type($tensorMapDescriptor) `,` type($barriers) \n      `->` type($dst)"
  },
  {
    "name": "nvgpu.tma.async.store",
    "summary": "TMA asynchronous store",
    "description": "The Op store a tile memory region from global memory to shared memory by \n    Tensor Memory Access (TMA).\n    \n    `$tensorMapDescriptor` is tensor map descriptor which has information about\n    tile shape. The descriptor is created by `nvgpu.tma.create.descriptor`",
    "inputs": [
      { "name": "src", "type": "Arg" },
      { "name": "tensorMapDescriptor", "type": "Arg" },
      { "name": "coordinates", "type": "Variadic" },
      { "name": "predicate", "type": "Optional" }
    ],
    "assemblyFormat": "$src `to` $tensorMapDescriptor `[` $coordinates `]`\n      (`,` `predicate` `=` $predicate^)?\n      attr-dict `:` type($src)\n      `->` type($tensorMapDescriptor)"
  },
  {
    "name": "nvgpu.tma.create.descriptor",
    "summary": "TMA create descriptor",
    "description": "The Op creates a tensor map descriptor object representing tiled memory \n    region. To do that it calls CUDA Driver's `cuTensorMapEncodeTiled`. The \n    descriptor is used by Tensor Memory Access (TMA).\n\n    The `tensor` is the source tensor to be tiled. \n\n    The `boxDimensions` is the size of the tiled memory region in each dimension.\n\n    For more information see below:\n    https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html",
    "inputs": [
      { "name": "tensor", "type": "AnyUnrankedMemRef" },
      { "name": "boxDimensions", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "tensorMap", "type": "NVGPU_TensorMapDescriptor" }
    ],
    "assemblyFormat": "$tensor `box` `[` $boxDimensions `]` attr-dict `:` type($tensor) `->` type($tensorMap)"
  },
  {
    "name": "nvgpu.tma.fence.descriptor",
    "summary": "Insert fence given `nvgpu.tensormap.descriptor`",
    "description": "The Op fences the given `$tmaDescriptor`. This is necessary if the tensor map\n    descriptor was modified from the host using cudaMemcpy. In this case, the\n    kernel needs a fence after which it is safe to use `tensor.map`.",
    "inputs": [
      { "name": "tensorMapDescriptor", "type": "NVGPU_TensorMapDescriptor" }
    ],
    "assemblyFormat": "$tensorMapDescriptor attr-dict `:` type($tensorMapDescriptor)"
  },
  {
    "name": "nvgpu.tma.prefetch.descriptor",
    "summary": "Prefetch given `nvgpu.tensormap.descriptor`",
    "description": "The Op brings the cache line containing the given `$tmaDescriptor` for \n    subsequent use by the `tma.async.load` instruction.",
    "inputs": [
      { "name": "tensorMapDescriptor", "type": "NVGPU_TensorMapDescriptor" },
      { "name": "predicate", "type": "Optional" }
    ],
    "assemblyFormat": "$tensorMapDescriptor (`,` `predicate` `=` $predicate^)? attr-dict `:` type($tensorMapDescriptor)"
  },
  {
    "name": "nvgpu.warpgroup.generate.descriptor",
    "summary": "Generate a warpgroup matrix descriptor",
    "description": "This Op builds a `nvgpu.warpgroup.descriptor` that is used by \n  `nvgpu.warpgroup.mma` to perform warpgroup-level matrix multiply and \n  accumulate.\n\n  The descriptor specifies the properties of the matrix in shared memory that \n  is a multiplicand in the matrix multiply and accumulate operation.",
    "inputs": [
      { "name": "tensor", "type": "Arg" },
      { "name": "tensorMap", "type": "NVGPU_TensorMapDescriptor" }
    ],
    "outputs": [
      { "name": "descriptor", "type": "NVGPU_WarpgroupMatrixDescriptor" }
    ],
    "assemblyFormat": "$tensor `,` $tensorMap attr-dict `:` type($tensor) `,` type($tensorMap) `->` type($descriptor)"
  },
  {
    "name": "nvgpu.warpgroup.mma",
    "description": "The `nvgpu.warpgroup.mma` op performs the warpgroup-level (4 warps) \n    matrix-multiply-and-accumulate (mma) operation that results in \n    `nvvm.wgmma.mma_async`. \n    \n    The operands are `descriptorA` and `descriptorB` that are wgmma matrix \n    descriptors that shows the properties of the matrix in shared memory. The \n    results are thread-level ownership to the warpgroup-level mma operation \n    shape. The shape is deduced from the descriptor types and output vector.\n\n    The Op encapsulates multiple `nvvm.wgmma.mma_async` operations to complete \n    the given shape. As `nvvm.wgmma.async` Op, or its corresponding PTX \n    instruction, is asynchronous, this Op groups the `nvvm.wgmma.async` and \n    surrounds them between `wgmma.fence.aligned` and \n    `wgmma.commit.group.sync.aligned`, `wgmma.wait.group.sync.aligned` Ops.\n\n    Example:\n    ```mlir\n      %r1,%r2 = nvgpu.warpgroup.mma %descA, %descB, %acc1, %acc2: \n                 !nvgpu.warpgroup.descriptor<tensor = memref<128x64xf16, 3>>, \n                 !nvgpu.warpgroup.descriptor<tensor = memref<64x128xf16, 3>>, \n                 !nvgpu.warpgroup.accumulator<fragmented = vector<64x128xf32>>,\n                 !nvgpu.warpgroup.accumulator<fragmented = vector<64x128xf32>>\n                 -> \n                 !nvgpu.warpgroup.accumulator<fragmented = vector<64x128xf32>>,\n                 !nvgpu.warpgroup.accumulator<fragmented = vector<64x128xf32>>\n    ```",
    "inputs": [
      { "name": "descriptorA", "type": "NVGPU_WarpgroupMatrixDescriptor" },
      { "name": "descriptorB", "type": "NVGPU_WarpgroupMatrixDescriptor" },
      { "name": "matrixC", "type": "NVGPU_WarpgroupAccumulator" }
    ],
    "outputs": [
      { "name": "matrixD", "type": "NVGPU_WarpgroupAccumulator" }
    ],
    "attributes": [
      { "name": "waitGroup", "type": "DefaultValuedOptionalAttr" },
      { "name": "transposeA", "type": "OptionalAttr" },
      { "name": "transposeB", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$descriptorA`,` $descriptorB`,` $matrixC attr-dict\n    `:` type($descriptorA) `,` type($descriptorB) `,` type($matrixC) `->` type($matrixD)"
  },
  {
    "name": "nvgpu.warpgroup.mma.init.accumulator",
    "summary": "Initializes the accumulator matrix",
    "description": "This Op generates and initializes the accumulator matrix for \n    `nvgpu.warpgroup.mma` op to perform matrix-multiply-and-accumulate.",
    "outputs": [
      { "name": "matrixC", "type": "NVGPU_WarpgroupAccumulator" }
    ],
    "assemblyFormat": "attr-dict `->` type($matrixC)"
  },
  {
    "name": "nvgpu.warpgroup.mma.store",
    "description": "The `nvgpu.warpgroup.mma.store` op performs the store of fragmented result \n    in $matrixD to given memref. \n\n    [See the details of register fragment layout for accumulator matrix D]\n    (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#wgmma-64n16-d) \n\n    Note that, the op must be run with warp group.",
    "inputs": [
      { "name": "matrixD", "type": "NVGPU_WarpgroupAccumulator" },
      { "name": "dstMemref", "type": "Arg" }
    ],
    "assemblyFormat": "$matrixD `,` $dstMemref attr-dict `:` type($matrixD) `to` type($dstMemref)"
  },
  {
    "name": "onnx.Abs",
    "summary": "ONNX Abs operation",
    "description": "Absolute takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where absolute value, y = abs(x), is applied to\n  the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Acos",
    "summary": "ONNX Acos operation",
    "description": "Calculates the arccosine (inverse of cosine) of the given input tensor, element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Acosh",
    "summary": "ONNX Acosh operation",
    "description": "Calculates the hyperbolic arccosine of the given input tensor element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Adagrad",
    "summary": "ONNX Adagrad operation",
    "description": "Compute one iteration of ADAGRAD, a stochastic gradient based optimization\n      algorithm. This operator can conduct the optimization of multiple tensor variables.\n  \n      Let's define the behavior of this operator. As you can imagine, ADAGRAD requires\n      some parameters:\n  \n       - The initial learning-rate \\\"R\\\".\n       - The update count \\\"T\\\". That is, the number of training iterations conducted.\n       - A L2-norm regularization coefficient \\\"norm_coefficient\\\".\n       - A learning-rate decay factor \\\"decay_factor\\\".\n       - A small constant \\\"epsilon\\\" to avoid dividing-by-zero.\n  \n      At each ADAGRAD iteration, the optimized tensors are moved along a direction\n      computed based on their estimated gradient and accumulated squared gradient. Assume\n      that only a single tensor \\\"X\\\" is updated by this operator. We need the value of \\\"X\\\",\n      its gradient \\\"G\\\", and its accumulated squared gradient \\\"H\\\". Therefore, variables in\n      this operator's input list are sequentially \\\"R\\\", \\\"T\\\", \\\"X\\\", \\\"G\\\", and \\\"H\\\". Other\n      parameters are given as attributes because they are usually constants. Also, the\n      corresponding output tensors are the new value of \\\"X\\\" (called \\\"X_new\\\"), and then\n      the new accumulated squared gradient (called \\\"H_new\\\"). Those outputs are computed\n      from the given inputs following the pseudo code below.\n  \n      Let \\\"+\\\", \\\"-\\\", \\\"*\\\", and \\\"/\\\" are all element-wise arithmetic operations with\n      numpy-style broadcasting support. The pseudo code to compute those outputs is:\n  \n        // Compute a scalar learning-rate factor. At the first update of X, T is generally\n        // 0 (0-based update index) or 1 (1-based update index).\n        r = R / (1 + T * decay_factor);\n  \n        // Add gradient of 0.5 * norm_coefficient * ||X||_2^2, where ||X||_2 is the 2-norm.\n        G_regularized = norm_coefficient * X + G;\n  \n        // Compute new accumulated squared gradient.\n        H_new = H + G_regularized * G_regularized;\n  \n        // Compute the adaptive part of per-coordinate learning rate. Note that Sqrt(...)\n        // computes element-wise square-root.\n        H_adaptive = Sqrt(H_new) + epsilon\n  \n        // Compute the new value of \\\"X\\\".\n        X_new = X - r * G_regularized / H_adaptive;\n  \n      If one assign this operators to optimize multiple inputs, for example, \\\"X_1\\\" and \\\"X_2\\\", the same\n      pseudo code may be extended to handle all tensors jointly. More specifically, we can view \\\"X\\\" as a\n      concatenation of \\\"X_1\\\" and \\\"X_2\\\" (of course, their gradient and accumulate gradient should\n      be concatenated too) and then just reuse the entire pseudo code.\n  \n      Note that ADAGRAD was first proposed in http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf.\n      In that reference paper, this operator is a special case of the Figure 1's composite mirror\n      descent update.",
    "inputs": [
      { "name": "R", "type": "AnyTypeOf" },
      { "name": "T", "type": "TensorOf" },
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "decay_factor", "type": "DefaultValuedAttr" },
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "norm_coefficient", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Adam",
    "summary": "ONNX Adam operation",
    "description": "Compute one iteration of Adam, a stochastic gradient based optimization\n      algorithm. This operator can conduct the optimization of multiple tensor variables.\n  \n      Let's define the behavior of this operator. First of all, Adam requires\n      some parameters:\n  \n       - The learning-rate \\\"R\\\".\n       - The update count \\\"T\\\". That is, the number of training iterations conducted.\n       - A L2-norm regularization coefficient \\\"norm_coefficient\\\".\n       - A small constant \\\"epsilon\\\" to avoid dividing-by-zero.\n       - Two coefficients, \\\"alpha\\\" and \\\"beta\\\".\n  \n      At each Adam iteration, the optimized tensors are moved along a direction\n      computed based on their exponentially-averaged historical gradient and\n      exponentially-averaged historical squared gradient. Assume that only a tensor\n      \\\"X\\\" is being optimized. The rest of required information is\n  \n       - the value of \\\"X\\\",\n       - \\\"X\\\"'s gradient (denoted by \\\"G\\\"),\n       - \\\"X\\\"'s exponentially-averaged historical gradient (denoted by \\\"V\\\"), and\n       - \\\"X\\\"'s exponentially-averaged historical squared gradient (denoted by \\\"H\\\").\n  \n      Some of those parameters are passed into this operator as input tensors and others\n      are stored as this operator's attributes. Specifically, this operator's input tensor\n      list is [\\\"R\\\", \\\"T\\\", \\\"X\\\", \\\"G\\\", \\\"V\\\", \\\"H\\\"]. That is, \\\"R\\\" is the first input, \\\"T\\\" is\n      the second input, and so on. Other parameters are given as attributes because they\n      are constants. Moreover, the corresponding output tensors are\n  \n       - the new value of \\\"X\\\" (called \\\"X_new\\\"),\n       - the new exponentially-averaged historical gradient (denoted by \\\"V_new\\\"), and\n       - the new exponentially-averaged historical squared gradient (denoted by \\\"H_new\\\").\n  \n      Those outputs are computed following the pseudo code below.\n  \n      Let \\\"+\\\", \\\"-\\\", \\\"*\\\", and \\\"/\\\" are all element-wise arithmetic operations with\n      numpy-style broadcasting support. The pseudo code to compute those outputs is:\n  \n        // Add gradient of 0.5 * norm_coefficient * ||X||_2^2, where ||X||_2 is the 2-norm.\n        G_regularized = norm_coefficient * X + G\n  \n        // Update exponentially-averaged historical gradient.\n        V_new = alpha * V + (1 - alpha) * G_regularized\n  \n        // Update exponentially-averaged historical squared gradient.\n        H_new = beta * H + (1 - beta) * G_regularized * G_regularized\n  \n        // Compute the element-wise square-root of H_new. V_new will be element-wisely\n        // divided by H_sqrt for a better update direction.\n        H_sqrt = Sqrt(H_new) + epsilon\n  \n        // Compute learning-rate. Note that \\\"alpha**T\\\"/\\\"beta**T\\\" is alpha's/beta's T-th power.\n        R_adjusted = T > 0 ? R * Sqrt(1 - beta**T) / (1 - alpha**T) : R\n  \n        // Compute new value of \\\"X\\\".\n        X_new = X - R_adjusted * V_new / H_sqrt\n  \n        // Post-update regularization.\n        X_final = (1 - norm_coefficient_post) * X_new\n  \n      If there are multiple inputs to be optimized, the pseudo code will be applied\n      independently to each of them.",
    "inputs": [
      { "name": "R", "type": "AnyTypeOf" },
      { "name": "T", "type": "TensorOf" },
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" },
      { "name": "beta", "type": "DefaultValuedAttr" },
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "norm_coefficient", "type": "DefaultValuedAttr" },
      { "name": "norm_coefficient_post", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Add",
    "summary": "ONNX Add operation",
    "description": "Performs element-wise binary addition (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).\n  \n  (Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.And",
    "summary": "ONNX And operation",
    "description": "Returns the tensor resulted from performing the `and` logical operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "TensorOf" },
      { "name": "B", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "C", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.ArgMax",
    "summary": "ONNX ArgMax operation",
    "description": "Computes the indices of the max elements of the input tensor's element along the\n  provided axis. The resulting tensor has the same rank as the input if keepdims equals 1.\n  If keepdims equals 0, then the resulting tensor has the reduced dimension pruned.\n  If select_last_index is True (default False), the index of the last occurrence of the max\n  is selected if the max appears more than once in the input. Otherwise the index of the\n  first occurrence is selected.\n  The type of the output tensor is integer.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "select_last_index", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ArgMin",
    "summary": "ONNX ArgMin operation",
    "description": "Computes the indices of the min elements of the input tensor's element along the\n  provided axis. The resulting tensor has the same rank as the input if keepdims equals 1.\n  If keepdims equals 0, then the resulting tensor has the reduced dimension pruned.\n  If select_last_index is True (default False), the index of the last occurrence of the min\n  is selected if the min appears more than once in the input. Otherwise the index of the\n  first occurrence is selected.\n  The type of the output tensor is integer.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "select_last_index", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ArrayFeatureExtractor",
    "summary": "ONNX ArrayFeatureExtractor operation",
    "description": "Select elements of the input tensor based on the indices passed.<br>\n      The indices are applied to the last axes of the tensor.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "Y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Z", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Asin",
    "summary": "ONNX Asin operation",
    "description": "Calculates the arcsine (inverse of sine) of the given input tensor, element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Asinh",
    "summary": "ONNX Asinh operation",
    "description": "Calculates the hyperbolic arcsine of the given input tensor element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Atan",
    "summary": "ONNX Atan operation",
    "description": "Calculates the arctangent (inverse of tangent) of the given input tensor, element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Atanh",
    "summary": "ONNX Atanh operation",
    "description": "Calculates the hyperbolic arctangent of the given input tensor element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.AveragePool",
    "summary": "ONNX AveragePool operation",
    "description": "AveragePool consumes an input tensor X and applies average pooling across\n   the tensor according to kernel sizes, stride sizes, and pad lengths.\n   average pooling consisting of computing the average on all values of a\n   subset of the input tensor according to the kernel size and downsampling the\n   data into the output tensor Y for further processing. The output spatial shape is calculated differently\n   depending on whether explicit padding is used, where pads is employed, or auto padding is used, where auto_pad is utilized.\n   With explicit padding (https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html?highlight=maxpool#torch.nn.MaxPool2d):\n   ```\n   output_spatial_shape[i] = floor((input_spatial_shape[i] + pad_shape[i] - dilation[i] * (kernel_shape[i] - 1) - 1) / strides_spatial_shape[i] + 1)\n   ```\n   or\n   ```\n   output_spatial_shape[i] = ceil((input_spatial_shape[i] + pad_shape[i] - dilation[i] * (kernel_shape[i] - 1) - 1) / strides_spatial_shape[i] + 1)\n   ```\n   if ceil_mode is enabled. `pad_shape[i]` is the sum of pads along axis `i`. Sliding windows that would start in the right padded region are ignored.\n  \n   `auto_pad` is a DEPRECATED attribute. If you are using them currently, the output spatial shape will be following when ceil_mode is enabled:\n   ```\n   VALID: output_spatial_shape[i] = ceil((input_spatial_shape[i] - ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) + 1) / strides_spatial_shape[i])\n   SAME_UPPER or SAME_LOWER: output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides_spatial_shape[i])\n   ```\n   or when ceil_mode is disabled (https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D):\n   ```\n   VALID: output_spatial_shape[i] = floor((input_spatial_shape[i] - ((kernel_spatial_shape[i] - 1) * dilations[i] + 1)) / strides_spatial_shape[i]) + 1\n   SAME_UPPER or SAME_LOWER: output_spatial_shape[i] = floor((input_spatial_shape[i] - 1) / strides_spatial_shape[i]) + 1\n   ```\n   And pad shape will be following if `SAME_UPPER` or `SAME_LOWER`:\n   ```\n   pad_shape[i] = (output_spatial_shape[i] - 1) * strides_spatial_shape[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n   ```\n   The output of each pooling window is divided by the number of elements (exclude pad when attribute count_include_pad is zero).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "auto_pad", "type": "DefaultValuedStrAttr" },
      { "name": "ceil_mode", "type": "DefaultValuedAttr" },
      { "name": "count_include_pad", "type": "DefaultValuedAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.BatchNormalization",
    "summary": "ONNX BatchNormalization operation",
    "description": "Carries out batch normalization as described in the paper\n  https://arxiv.org/abs/1502.03167. Depending on the mode it is being run,\n  There are five required inputs 'X', 'scale', 'B', 'input_mean' and\n  'input_var'.\n  Note that 'input_mean' and 'input_var' are expected to be the estimated\n  statistics in inference mode (training_mode=False, default),\n  and the running statistics in training mode (training_mode=True).\n  There are multiple cases for the number of outputs, which we list below:\n  \n  * Output case #1: Y, running_mean, running_var (training_mode=True)\n  * Output case #2: Y (training_mode=False)\n  \n  When training_mode=False, extra outputs are invalid.\n  The outputs are updated as follows when training_mode=True:\n  ```\n  running_mean = input_mean * momentum + current_mean * (1 - momentum)\n  running_var = input_var * momentum + current_var * (1 - momentum)\n  \n  Y = (X - current_mean) / sqrt(current_var + epsilon) * scale + B\n  ```\n  where:\n  ```\n  current_mean = ReduceMean(X, axis=all_except_channel_index)\n  current_var =  ReduceVar(X, axis=all_except_channel_index)\n  ```\n  Notice that `ReduceVar` refers to the population variance, and it equals to\n  `sum(sqrd(x_i - x_avg)) / N`\n  where `N` is the population size (this formula does not use sample size `N - 1`).\n  \n  The computation of ReduceMean and ReduceVar uses float to avoid overflow for float16 inputs.\n  \n  When training_mode=False:\n  ```\n  Y = (X - input_mean) / sqrt(input_var + epsilon) * scale + B\n  ```\n  \n  For previous (depreciated) non-spatial cases, implementors are suggested\n  to flatten the input shape to (N x C * D1 * D2 * ... * Dn) before a BatchNormalization Op.\n  This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "scale", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" },
      { "name": "input_mean", "type": "AnyTypeOf" },
      { "name": "input_var", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "running_mean", "type": "AnyTypeOf" },
      { "name": "running_var", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "momentum", "type": "DefaultValuedAttr" },
      { "name": "training_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.BatchNormalizationInferenceMode",
    "summary": "ONNX BatchNormalization operation in test mode",
    "description": "Carries out batch normalization as described in the paper\n    https://arxiv.org/abs/1502.03167. Depending on the mode it is being run,\n    there are multiple cases for the number of outputs, which we list below:\n\n    Output case #1: Y, mean, var, saved_mean, saved_var (training mode)\n    Output case #2: Y (test mode)\"\n\n    For previous (depreciated) non-spatial cases, implementors are suggested\n    to flatten the input shape to (N x C*D1*D2 ..*Dn) before a BatchNormalization Op.\n    This operator has **optional** inputs/outputs. See [the doc](IR.md)\n    for more details about the representation of optional arguments.\n    An empty string may be used in the place of an actual argument's name to\n    indicate a missing argument. Trailing optional arguments (those not followed\n    by an argument that is present) may also be simply omitted.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "scale", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" },
      { "name": "mean", "type": "AnyTypeOf" },
      { "name": "var", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "o_Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "momentum", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Bernoulli",
    "summary": "ONNX Bernoulli operation",
    "description": "Draws binary random numbers (0 or 1) from a Bernoulli distribution. The input tensor should be a tensor\n  containing probabilities p (a value in the range [0,1]) to be used for drawing the binary random number,\n  where an output of 1 is produced with probability p and an output of 0 is produced with probability (1-p).\n  \n  This operator is non-deterministic and may not produce the same values in different\n  implementations (even if a seed is specified).",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dtype", "type": "OptionalAttr" },
      { "name": "seed", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Binarizer",
    "summary": "ONNX Binarizer operation",
    "description": "Maps the values of the input tensor to either 0 or 1, element-wise, based on the outcome of a comparison against a threshold value.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "threshold", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.BitShift",
    "summary": "ONNX BitShift operation",
    "description": "Bitwise shift operator performs element-wise operation. For each input element, if the\n  attribute \\\"direction\\\" is \\\"RIGHT\\\", this operator moves its binary representation toward\n  the right side so that the input value is effectively decreased. If the attribute \\\"direction\\\"\n  is \\\"LEFT\\\", bits of binary representation moves toward the left side, which results the\n  increase of its actual value. The input X is the tensor to be shifted and another input\n  Y specifies the amounts of shifting. For example, if \\\"direction\\\" is \\\"Right\\\", X is [1, 4],\n  and S is [1, 1], the corresponding output Z would be [0, 2]. If \\\"direction\\\" is \\\"LEFT\\\" with\n  X=[1, 2] and S=[1, 2], the corresponding output Y would be [2, 8].\n  \n  Because this operator supports Numpy-style broadcasting, X's and Y's shapes are\n  not necessarily identical.\n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Z", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "direction", "type": "StrAttr" }
    ]
  },
  {
    "name": "onnx.BitwiseAnd",
    "summary": "ONNX BitwiseAnd operation",
    "description": "Returns the tensor resulting from performing the bitwise `and` operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.BitwiseNot",
    "summary": "ONNX BitwiseNot operation",
    "description": "Returns the bitwise not of the input tensor element-wise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.BitwiseOr",
    "summary": "ONNX BitwiseOr operation",
    "description": "Returns the tensor resulting from performing the bitwise `or` operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.BitwiseXor",
    "summary": "ONNX BitwiseXor operation",
    "description": "Returns the tensor resulting from performing the bitwise `xor` operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.BlackmanWindow",
    "summary": "ONNX BlackmanWindow operation",
    "description": "Generates a Blackman window as described in the paper https://ieeexplore.ieee.org/document/1455106.",
    "inputs": [
      { "name": "size", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "output_datatype", "type": "DefaultValuedAttr" },
      { "name": "periodic", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Cast",
    "summary": "ONNX Cast operation",
    "description": "The operator casts the elements of a given input tensor to a data type\n  specified by the 'to' argument and returns an output tensor of the same size in\n  the converted type. The 'to' argument must be one of the data types specified\n  in the 'DataType' enum field in the TensorProto message.\n  \n  Casting from string tensor in plain (e.g., \\\"3.14\\\" and \\\"1000\\\") and scientific numeric representations\n  (e.g., \\\"1e-5\\\" and \\\"1E8\\\") to float types is supported. For example, converting string \\\"100.5\\\" to an integer may\n  yield result 100. There are some string literals reserved for special floating-point values;\n  \\\"+INF\\\" (and \\\"INF\\\"), \\\"-INF\\\", and \\\"NaN\\\" are positive infinity, negative infinity, and not-a-number, respectively.\n  Any string which can exactly match \\\"+INF\\\" in a case-insensitive way would be mapped to positive infinite. Similarly,\n  this case-insensitive rule is applied to \\\"INF\\\" and \\\"NaN\\\". When casting from numeric tensors\n  to string tensors, plain floating-point representation (such as \\\"314.15926\\\") would be used.\n  Converting non-numerical-literal string such as \\\"Hello World!\\\" is an undefined behavior. Cases\n  of converting string representing floating-point arithmetic value, such as \\\"2.718\\\", to INT is an undefined behavior.\n  \n  Conversion from a numerical type to any numerical type is always allowed.\n  User must be aware of precision loss and value change caused by range difference between two types.\n  For example, a 64-bit float 3.1415926459 may be round to a 32-bit float 3.141592. Similarly, converting\n  an integer 36 to Boolean may produce 1 because we truncate bits which can't be stored in the targeted type.\n  \n  In more detail, the conversion among numerical types should follow these rules\n  if the destination type is not a float 8 type.\n  \n  * Casting from floating point to:\n    * floating point: +/- infinity if OOR (out of range).\n    * fixed point: undefined if OOR.\n    * bool: +/- 0.0 to False; all else to True.\n  * Casting from fixed point to:\n    * floating point: +/- infinity if OOR. (+ infinity in the case of uint)\n    * fixed point: when OOR, discard higher bits and reinterpret (with respect to two's complement representation for\n      signed types). For example, 200 (int16) -> -56 (int8).\n    * bool: zero to False; nonzero to True.\n  * Casting from bool to:\n    * floating point: `{1.0, 0.0}`.\n    * fixed point: `{1, 0}`.\n    * bool: no change.\n  \n  Float 8 type were introduced to speed up the training of\n  deep models. By default the conversion of a float *x* obeys\n  to the following rules. `[x]` means the value rounded to\n  the target mantissa width.\n  \n  | x | E4M3FN | E4M3FNUZ | E5M2 | E5M2FNUZ |\n  |------|----|----|----|----|\n  | 0 | 0 | 0 | 0 | 0 |\n  |-0 | -0 | 0 | -0 | 0 |\n  | NaN | NaN | NaN | NaN | NaN |\n  | +/- Inf | +/- FLT_MAX | NaN | FLT_MAX | NaN |\n  | [x] > FLT_MAX | FLT_MAX | FLT_MAX | FLT_MAX | FLT_MAX |\n  | [x] < -FLT_MAX | -FLT_MAX | -FLT_MAX | -FLT_MAX | -FLT_MAX |\n  | else | RNE | RNE | RNE | RNE |\n  \n  The behavior changes if the parameter 'saturate' is set to False.\n  The rules then become:\n  \n  | x | E4M3FN | E4M3FNUZ | E5M2 | E5M2FNUZ |\n  |------|----|----|----|----|\n  | 0 | 0 | 0 | 0 | 0 |\n  |-0 | -0 | 0 | -0 | 0 |\n  | NaN | NaN | NaN | NaN | NaN |\n  | +/- Inf | NaN | NaN | +/- Inf | NaN |\n  | [x] > FLT_MAX | NaN | NaN | Inf | NaN |\n  | [x] < -FLT_MAX | NaN | NaN | -Inf | NaN |\n  | else | RNE | RNE | RNE | RNE |",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "saturate", "type": "DefaultValuedAttr" },
      { "name": "to", "type": "TypeAttr" }
    ]
  },
  {
    "name": "onnx.CastLike",
    "summary": "ONNX CastLike operation",
    "description": "The operator casts the elements of a given input tensor (the first input) to\n  the same data type as the elements of the second input tensor.\n  See documentation of the Cast operator for further details.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "target_type", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "saturate", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.CastMap",
    "summary": "ONNX CastMap operation",
    "description": "Converts a map to a tensor.<br>The map key must be an int64 and the values will be ordered\n      in ascending order based on this key.<br>The operator supports dense packing or sparse packing.\n      If using sparse packing, the key cannot exceed the max_map-1 value.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "cast_to", "type": "DefaultValuedStrAttr" },
      { "name": "map_form", "type": "DefaultValuedStrAttr" },
      { "name": "max_map", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.CategoryMapper",
    "summary": "ONNX CategoryMapper operation",
    "description": "Converts strings to integers and vice versa.<br>\n      Two sequences of equal length are used to map between integers and strings,\n      with strings and integers at the same index detailing the mapping.<br>\n      Each operator converts either integers to strings or strings to integers, depending\n      on which default value attribute is provided. Only one default value attribute\n      should be defined.<br>\n      If the string default value is set, it will convert integers to strings.\n      If the int default value is set, it will convert strings to integers.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "cats_int64s", "type": "OptionalAttr" },
      { "name": "cats_strings", "type": "OptionalAttr" },
      { "name": "default_int64", "type": "DefaultValuedAttr" },
      { "name": "default_string", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.Ceil",
    "summary": "ONNX Ceil operation",
    "description": "Ceil takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the ceil is, y = ceil(x), is applied to\n  the tensor elementwise. If x is integral, +0, -0, NaN,  or infinite, x itself is returned.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Celu",
    "summary": "ONNX Celu operation",
    "description": "Continuously Differentiable Exponential Linear Units:\n  Perform the linear unit element-wise on the input tensor X\n  using formula:\n  \n  ```\n  max(0,x) + min(0,alpha*(exp(x/alpha)-1))\n  ```",
    "inputs": [
      { "name": "X", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.CenterCropPad",
    "summary": "ONNX CenterCropPad operation",
    "description": "Center crop or pad an input to given dimensions.\n  \n  The crop/pad dimensions can be specified for a subset of the `axes`. Non-specified dimensions will not be\n  cropped or padded.\n  \n  If the input dimensions are bigger than the crop shape, a centered cropping window is extracted from the input.\n  If the input dimensions are smaller than the crop shape, the input is padded on each side equally,\n  so that the input is centered in the output.",
    "inputs": [
      { "name": "input_data", "type": "AnyTypeOf" },
      { "name": "shape", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output_data", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Clip",
    "summary": "ONNX Clip operation",
    "description": "Clip operator limits the given input within an interval. The interval is\n  specified by the inputs 'min' and 'max'. They default to\n  numeric_limits::lowest() and numeric_limits::max(), respectively.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "min", "type": "AnyTypeOf" },
      { "name": "max", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.ClipV11",
    "summary": "ONNX Clip operation",
    "description": "Clip operator limits the given input within an interval. The interval is\n  specified by the inputs 'min' and 'max'. They default to\n  numeric_limits::lowest() and numeric_limits::max(), respectively.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "min", "type": "AnyTypeOf" },
      { "name": "max", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.ClipV12",
    "summary": "ONNX Clip operation",
    "description": "Clip operator limits the given input within an interval. The interval is\n  specified by the inputs 'min' and 'max'. They default to\n  numeric_limits::lowest() and numeric_limits::max(), respectively.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "min", "type": "AnyTypeOf" },
      { "name": "max", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.ClipV6",
    "summary": "ONNX Clip operation",
    "description": "Clip operator limits the given input within an interval. The interval is\n  specified with arguments 'min' and 'max'. They default to\n  numeric_limits::lowest() and numeric_limits::max() respectively.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "max", "type": "DefaultValuedAttr" },
      { "name": "min", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Col2Im",
    "summary": "ONNX Col2Im operation",
    "description": "The operator rearranges column blocks back into a multidimensional image\n  \n  Col2Im behaves similarly to PyTorch's fold https://pytorch.org/docs/stable/generated/torch.nn.Fold.html,\n  but it only supports *batched* multi-dimensional image tensors.\n  Another implementation in Python with N-dimension support can be found at https://github.com/f-dangel/unfoldNd/.\n  \n  NOTE:\n    Although specifying image_shape looks redundant because it could be calculated from\n    convolution formulas, it is required as input for more advanced scenarios as explained\n    at PyTorch's implementation (https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Col2Im.cpp#L10)",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "image_shape", "type": "TensorOf" },
      { "name": "block_shape", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Compress",
    "summary": "ONNX Compress operation",
    "description": "Selects slices from an input tensor along a given axis where condition evaluates to True for each axis index.\n      In case axis is not provided, input is flattened before elements are selected.\n      Compress behaves like numpy.compress: https://docs.scipy.org/doc/numpy/reference/generated/numpy.compress.html",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "condition", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Concat",
    "summary": "ONNX Concat operation",
    "description": "Concatenate a list of tensors into a single tensor. All input tensors must have the same shape, except for the dimension size of the axis to concatenate on.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "concat_result", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "SI64Attr" }
    ]
  },
  {
    "name": "onnx.ConcatFromSequence",
    "summary": "ONNX ConcatFromSequence operation",
    "description": "Concatenate a sequence of tensors into a single tensor.\n  All input tensors must have the same shape, except for the dimension size of the axis to concatenate on.\n  By default 'new_axis' is 0, the behavior is similar to numpy.concatenate.\n  When 'new_axis' is 1, the behavior is similar to numpy.stack.",
    "inputs": [
      { "name": "input_sequence", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "concat_result", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "SI64Attr" },
      { "name": "new_axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ConcatShapeTranspose",
    "summary": "ONNX merged operation",
    "description": "Merge the following sequence of ops into one op\n    v1 = onnx.concat\n    v2 = onnx.shape(v1)\n    v3 = onnx.transpose(v1)\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "shape", "type": "TensorOf" },
      { "name": "transposed", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "SI64Attr" },
      { "name": "end", "type": "OptionalAttr" },
      { "name": "start", "type": "DefaultValuedAttr" },
      { "name": "perm", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Constant",
    "summary": "ONNX Constant operation",
    "description": "This operator produces a constant tensor. Exactly one of the provided attributes, either value, sparse_value,\n  or value_* must be specified.",
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "sparse_value", "type": "OptionalAttr" },
      { "name": "value", "type": "OptionalAttr" },
      { "name": "value_float", "type": "OptionalAttr" },
      { "name": "value_floats", "type": "OptionalAttr" },
      { "name": "value_int", "type": "OptionalAttr" },
      { "name": "value_ints", "type": "OptionalAttr" },
      { "name": "value_string", "type": "OptionalAttr" },
      { "name": "value_strings", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.ConstantOfShape",
    "summary": "ONNX ConstantOfShape operation",
    "description": "Generate a tensor with given value and shape.",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "value", "type": "OptionalAttr" }
    ],
    "category": "Shape"
  },
  {
    "name": "onnx.Conv",
    "summary": "ONNX Conv operation",
    "description": "The convolution operator consumes an input tensor and a filter, and\n  computes the output.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "W", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "auto_pad", "type": "DefaultValuedStrAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "group", "type": "DefaultValuedAttr" },
      { "name": "kernel_shape", "type": "OptionalAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ],
    "category": "Layer"
  },
  {
    "name": "onnx.ConvInteger",
    "summary": "ONNX ConvInteger operation",
    "description": "The integer convolution operator consumes an input tensor, its zero-point, a filter, and its zero-point,\n  and computes the output. The production MUST never overflow. The accumulation may overflow if and only if in 32 bits.",
    "inputs": [
      { "name": "x", "type": "AnyTypeOf" },
      { "name": "w", "type": "AnyTypeOf" },
      { "name": "x_zero_point", "type": "AnyTypeOf" },
      { "name": "w_zero_point", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "auto_pad", "type": "DefaultValuedStrAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "group", "type": "DefaultValuedAttr" },
      { "name": "kernel_shape", "type": "OptionalAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.ConvTranspose",
    "summary": "ONNX ConvTranspose operation",
    "description": "The convolution transpose operator consumes an input tensor and a filter,\n  and computes the output.\n  \n  If the pads parameter is provided the shape of the output is calculated via the following equation:\n  \n    output_shape[i] = stride[i] * (input_size[i] - 1) + output_padding[i] + ((kernel_shape[i] - 1) * dilations[i] + 1) - pads[start_i] - pads[end_i]\n  \n  output_shape can also be explicitly specified in which case pads values are auto generated using these equations:\n  \n    total_padding[i] = stride[i] * (input_size[i] - 1) + output_padding[i] + ((kernel_shape[i] - 1) * dilations[i] + 1) - output_shape[i]\n    If (auto_pads == SAME_UPPER): pads[start_i] = total_padding[i]/2; pads[end_i] = total_padding[i] - (total_padding[i]/2)\n    Else: pads[start_i] = total_padding[i] - (total_padding[i]/2); pads[end_i] = (total_padding[i]/2).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "W", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "auto_pad", "type": "DefaultValuedStrAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "group", "type": "DefaultValuedAttr" },
      { "name": "kernel_shape", "type": "OptionalAttr" },
      { "name": "output_padding", "type": "OptionalAttr" },
      { "name": "output_shape", "type": "OptionalAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Cos",
    "summary": "ONNX Cos operation",
    "description": "Calculates the cosine of the given input tensor, element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Cosh",
    "summary": "ONNX Cosh operation",
    "description": "Calculates the hyperbolic cosine of the given input tensor element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.CumSum",
    "summary": "ONNX CumSum operation",
    "description": "Performs cumulative sum of the input elements along the given axis.\n  By default, it will do the sum inclusively meaning the first element is copied as is.\n  Through an `exclusive` attribute, this behavior can change to exclude the first element.\n  It can also perform summation in the opposite direction of the axis. For that, set `reverse` attribute to 1.\n  \n  Example:\n  ```\n  input_x = [1, 2, 3]\n  axis=0\n  output = [1, 3, 6]\n  exclusive=1\n  output = [0, 1, 3]\n  exclusive=0\n  reverse=1\n  output = [6, 5, 3]\n  exclusive=1\n  reverse=1\n  output = [5, 3, 0]\n  ```",
    "inputs": [
      { "name": "x", "type": "AnyTypeOf" },
      { "name": "axis", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "exclusive", "type": "DefaultValuedAttr" },
      { "name": "reverse", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Custom",
    "summary": "ONNX Custom operation",
    "description": "CustomOp is not an Op defined in onnx standard and was added to support\n    extention of Op that can be transformed or finally call a user-defined\n    external function.\"\n\n    It allows for calling a user-defined operation, with a single required\n    attribute being a string that names the operation. Other inputs are passed\n    to the user operation.\n\n    The number of inputs and outputs can vary.\n\n    NoneType is allowed for both input and output, as the CustomOp may require\n    a fixed number of inputs/outputs for the external function call.\n\n    In addition to the values passed to the user-defined operation, certain\n    attributes are introduced to facilitate the analysis and transformation of\n    CustomOp.\n\n    Since the compiler does not define the semantics of CustomOp, onnx-mlir\n    cannot infer the shape of its output. Consequently, specific attributes are\n    introduced to specify how shape inference should be performed on a CustomOp.\n    These attributes are:\n      'inputs_for_infer':\n           Optional. The index of inputs used for shape inference.\n           The value of index should be [0, the number of inputs).\n           If not specified, all the inputs of the CustomOp will be used for\n           shape inference.\n      'shape_infer_pattern':\n           Optional. Specify how to propagate the shape info from the inputs\n           (may be limited by inputs_for_infer) to output. Current supported\n           patterns are `SameAs`, `MDBroadcast`.\n      'output_element_type':\n           Optional. The element type for the output tensor. If not specified,\n           follow the shape infer pattern behavior. Usually the element type of\n           the first input is used.\n    Each instance of CustomOp can have its own attributes for shape inference,\n    allowing for customization. However, CustomOps with the same function_name\n    typically behave similarly in terms of shape inference, and therefore have\n    the same attributes.\n\n    The existing shape inference patterns for ONNX ops are reused for CustomOp,\n    with the polymorphism in shape inference based on its attribute values.\n    Due to the current implementation for ONNX Ops, a CustomOp with specified\n    shape inference attributes supports only a single output, rather than\n    variadic outputs.\n\n    When attributes for shape inference are not provided, the shape inference\n    for CustomOp will simply pass through.\n\n    All of these additional attributes are optional, designed to be less\n    intrusive. The .mlir file can remain the same when a new attribute is\n    added.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "function_name", "type": "StrAttr" },
      { "name": "output_element_type", "type": "OptionalAttr" },
      { "name": "shape_infer_pattern", "type": "OptionalAttr" },
      { "name": "inputs_for_infer", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.DeformConv",
    "summary": "ONNX DeformConv operation",
    "description": "Performs deformable convolution as described in https://arxiv.org/abs/1703.06211 and https://arxiv.org/abs/1811.11168.\n  This operator specification supports the general N-D case. Note that most common use cases have 2D or 3D data.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "W", "type": "AnyTypeOf" },
      { "name": "offset", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" },
      { "name": "mask", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "group", "type": "DefaultValuedAttr" },
      { "name": "kernel_shape", "type": "OptionalAttr" },
      { "name": "offset_group", "type": "DefaultValuedAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.DepthToSpace",
    "summary": "ONNX DepthToSpace operation",
    "description": "DepthToSpace rearranges (permutes) data from depth into blocks of spatial data.\n  This is the reverse transformation of SpaceToDepth. More specifically, this op outputs a copy of\n  the input tensor where values from the depth dimension are moved in spatial blocks to the height\n  and width dimensions. By default, `mode` = `DCR`.\n  In the DCR mode, elements along the depth dimension from the input tensor are rearranged in the\n  following order: depth, column, and then row. The output y is computed from the input x as below:\n  \n  ```\n  b, c, h, w = x.shape\n  tmp = np.reshape(x, [b, blocksize, blocksize, c // (blocksize**2), h, w])\n  tmp = np.transpose(tmp, [0, 3, 4, 1, 5, 2])\n  y = np.reshape(tmp, [b, c // (blocksize**2), h * blocksize, w * blocksize])\n  ```\n  \n  In the CRD mode, elements along the depth dimension from the input tensor are rearranged in the\n  following order: column, row, and the depth. The output y is computed from the input x as below:\n  \n  ```\n  b, c, h, w = x.shape\n  tmp = np.reshape(x, [b, c // (blocksize ** 2), blocksize, blocksize, h, w])\n  tmp = np.transpose(tmp, [0, 1, 4, 2, 5, 3])\n  y = np.reshape(tmp, [b, c // (blocksize ** 2), h * blocksize, w * blocksize])\n  ```",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "blocksize", "type": "SI64Attr" },
      { "name": "mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.DequantizeLinear",
    "summary": "ONNX DequantizeLinear operation",
    "description": "The linear dequantization operator. It consumes a quantized tensor, a scale, and a zero point to compute the full precision tensor.\n  The dequantization formula is `y = (x - x_zero_point) * x_scale`. `x_scale` and `x_zero_point` must have same shape, and can be either a scalar\n  for per-tensor / per layer quantization, or a 1-D tensor for per-axis quantization.\n  `x_zero_point` and `x` must have same type. `x` and `y` must have same shape. In the case of dequantizing int32,\n  there's no zero point (zero point is supposed to be 0).\n  `zero-point` is usually not used in the case of float8e4m3fn, float8e4m3fnuz, float8e5m2, float8e5m2fnuz quantization,\n  but the dequantization formula remains the same for consistency and 'x_scale' still determines the output type.",
    "inputs": [
      { "name": "x", "type": "AnyTypeOf" },
      { "name": "x_scale", "type": "AnyTypeOf" },
      { "name": "x_zero_point", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Det",
    "summary": "ONNX Det operation",
    "description": "Det calculates determinant of a square matrix or batches of square matrices.\n  Det takes one input tensor of shape `[*, M, M]`, where `*` is zero or more batch dimensions,\n  and the inner-most 2 dimensions form square matrices.\n  The output is a tensor of shape `[*]`, containing the determinants of all input submatrices.\n  e.g., When the input is 2-D, the output is a scalar(shape is empty: `[]`).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.DFT",
    "summary": "ONNX DFT operation",
    "description": "Computes the discrete Fourier Transform (DFT) of the input.\n  \n  Assuming the input has shape `[M, N]`, where `N` is the dimension over which the\n  DFT is computed and `M` denotes the conceptual \\\"all other dimensions,\\\"\n  the DFT `y[m, k]` of shape `[M, N]` is defined as\n  \n  $$y[m, k] = \\sum_{n=0}^{N-1} e^{-2 \\pi j \\frac{k n}{N} } x[m, n] ,$$\n  \n  and the inverse transform is defined as\n  \n  $$x[m, n] = \\frac{1}{N} \\sum_{k=0}^{N-1} e^{2 \\pi j \\frac{k n}{N} } y[m, k] ,$$\n  \n  where $j$ is the imaginary unit.\n  \n  The actual shape of the output is specified in the \\\"output\\\" section.\n  \n  Reference: https://docs.scipy.org/doc/scipy/tutorial/fft.html",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "dft_length", "type": "AnyTypeOf" },
      { "name": "axis", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "inverse", "type": "DefaultValuedAttr" },
      { "name": "onesided", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.DFTV17",
    "summary": "ONNX DFT operation",
    "description": "Computes the discrete Fourier transform of input.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "dft_length", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "inverse", "type": "DefaultValuedAttr" },
      { "name": "onesided", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.DictVectorizer",
    "summary": "ONNX DictVectorizer operation",
    "description": "Uses an index mapping to convert a dictionary to an array.<br>\n      Given a dictionary, each key is looked up in the vocabulary attribute corresponding to\n      the key type. The index into the vocabulary array at which the key is found is then\n      used to index the output 1-D tensor 'Y' and insert into it the value found in the dictionary 'X'.<br>\n      The key type of the input map must correspond to the element type of the defined vocabulary attribute.\n      Therefore, the output array will be equal in length to the index mapping vector parameter.\n      All keys in the input dictionary must be present in the index mapping vector.\n      For each item in the input dictionary, insert its value in the output array.\n      Any keys not present in the input dictionary, will be zero in the output array.<br>\n      For example: if the ``string_vocabulary`` parameter is set to ``[\\\"a\\\", \\\"c\\\", \\\"b\\\", \\\"z\\\"]``,\n      then an input of ``{\\\"a\\\": 4, \\\"c\\\": 8}`` will produce an output of ``[4, 8, 0, 0]``.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "int64_vocabulary", "type": "OptionalAttr" },
      { "name": "string_vocabulary", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Dim",
    "summary": "ONNX dimensions operation.",
    "description": "This operation is to obtain the dimension of a Tensor;\n\n    ```\n    \"onnx.Dim\"(%tensor) {axis = 0 : si64} : (tensor<?x3x5xf32>) -> tensor<1xi64>\n    ```\n\n    The axis identifies the dimension within the shape which is going to be obtained.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "dim", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.DimGroup",
    "summary": "ONNX dimension group operation.",
    "description": "This operation is to link a compile-time unknown dimension of a Tensor\n    to a group id. Two dimensions that have the same group id are expected\n    to be equal at runtime.\n\n    ```\n    \"onnx.DimGroup\"(%tensor) {axis = 0 : si64, group_id = 1: si64} : (tensor<?x3x5xf32>) -> ()\n    ```\n\n    `axis` identifies the dimension position in the tensor.\n\n    `group_id` identifies the group id of the dimension. It is non-negative.\n    Value -1 for `group_id` means the dimension does not belong to any group.\n\n    This operation is currently used in the pass `--onnx-dim-analysis`\n    for testing the unknown dimension analysis class.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "group_id", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Div",
    "summary": "ONNX Div operation",
    "description": "Performs element-wise binary division (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).\n  \n  (Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Dropout",
    "summary": "ONNX Dropout operation",
    "description": "Dropout takes an input floating-point tensor, an optional input ratio (floating-point scalar) and an optional input training_mode (boolean scalar). It produces two tensor outputs,\n  output (floating-point tensor) and mask (optional `Tensor<bool>`). If `training_mode` is true then the output Y will be a random dropout;\n  Note that this Dropout scales the masked input data by the following equation, so to convert the trained model into inference mode,\n  the user can simply not pass `training_mode` input or set it to false.\n  ```\n  output = scale * data * mask,\n  ```\n  where\n  ```\n  scale = 1. / (1. - ratio).\n  ```\n  This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "ratio", "type": "AnyTypeOf" },
      { "name": "training_mode", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" },
      { "name": "mask", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "seed", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.DynamicQuantizeLinear",
    "summary": "ONNX DynamicQuantizeLinear operation",
    "description": "A Function to fuse calculation for Scale, Zero Point and FP32->8Bit conversion of FP32 Input data.\n  Outputs Scale, ZeroPoint and Quantized Input for a given FP32 Input.\n  Scale is calculated as:\n  ```\n  y_scale = (maximum(0, max(x)) - minimum(0, min(x))) / (qmax - qmin)\n  ```\n  \n  * where qmax and qmin are max and min values for quantization range i.e. [0, 255] in case of uint8\n  * data range is adjusted to include 0.\n  \n  Zero point is calculated as:\n  ```\n  intermediate_zero_point = qmin - min(x)/y_scale\n  y_zero_point = cast(round(saturate(itermediate_zero_point)))\n  ```\n  \n  * where qmax and qmin are max and min values for quantization range .i.e [0, 255] in case of uint8\n  * for saturation, it saturates to [0, 255] if it's uint8, or [-127, 127] if it's int8. Right now only uint8 is supported.\n  * rounding to nearest ties to even.\n  \n  Data quantization formula is:\n  ```\n  y = saturate (round (x / y_scale) + y_zero_point)\n  ```\n  \n  * for saturation, it saturates to [0, 255] if it's uint8, or [-127, 127] if it's int8. Right now only uint8 is supported.\n  * rounding to nearest ties to even.",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" },
      { "name": "y_scale", "type": "TensorOf" },
      { "name": "y_zero_point", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.Einsum",
    "summary": "ONNX Einsum operation",
    "description": "An einsum of the form `term1, term2 -> output-term` produces an output tensor using the following equation\n  \n  ```\n  output[output-term] = reduce-sum( input1[term1] * input2[term2] )\n  ```\n  \n  where the reduce-sum performs a summation over all the indices occurring in the input terms (term1, term2)\n  that do not occur in the output-term.\n  \n  The Einsum operator evaluates algebraic tensor operations on a sequence of tensors, using the Einstein summation\n  convention. The equation string contains a comma-separated sequence of lower case letters. Each term corresponds to\n  an operand tensor, and the characters within the terms correspond to operands dimensions.\n  \n  This sequence may be followed by \\\"->\\\" to separate the left and right hand side of the equation.\n  If the equation contains \\\"->\\\" followed by the right-hand side, the explicit (not classical) form of the Einstein\n  summation is performed, and the right-hand side indices indicate output tensor dimensions. In other cases,\n  output indices are (implicitly) set to the alphabetically sorted sequence of indices appearing exactly once in the\n  equation.\n  \n  When a dimension character is repeated in the left-hand side, it represents summation along the dimension.\n  \n  The equation may contain ellipsis (\\\"...\\\") to enable broadcasting. Ellipsis must indicate a fixed number of dimensions.\n  Specifically, every occurrence of ellipsis in the equation must represent the same number of dimensions.\n  The right-hand side may contain exactly one ellipsis. In implicit mode, the ellipsis dimensions are set to the\n  beginning of the output. The equation string may contain space (U+0020) character.",
    "inputs": [
      { "name": "Inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "Output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "equation", "type": "StrAttr" }
    ]
  },
  {
    "name": "onnx.Elu",
    "summary": "ONNX Elu operation",
    "description": "Elu takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the function `f(x) = alpha * (exp(x) - 1.) for x <\n  0`, `f(x) = x for x >= 0`., is applied to the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.EntryPoint",
    "summary": "Indicate ONNX entry point",
    "description": "The \"onnx.EntryPoint\" function indicates the main entry point of ONNX model.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "attributes": [
      { "name": "func", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "onnx.Equal",
    "summary": "ONNX Equal operation",
    "description": "Returns the tensor resulted from performing the `equal` logical operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.Erf",
    "summary": "ONNX Erf operation",
    "description": "Computes the error function of the given input tensor element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Exp",
    "summary": "ONNX Exp operation",
    "description": "Calculates the exponential of the given input tensor, element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Expand",
    "summary": "ONNX Expand operation",
    "description": "Broadcast the input tensor following the given shape and the broadcast rule.\n  The broadcast rule is similar to numpy.array(input) * numpy.ones(shape):\n  Dimensions are right alignment;\n  Two corresponding dimensions must have the same value, or one of them is equal to 1.\n  Also, this operator is similar to numpy.broadcast_to(input, shape),\n  but the major difference is numpy.broadcast_to() does not allow shape to be smaller than input.size().\n  It is possible that the output.shape is not equal to shape, when some dimensions in shape is equal to 1,\n  or the shape.ndim < input.shape.ndim.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "shape", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.EyeLike",
    "summary": "ONNX EyeLike operation",
    "description": "Generate a 2D tensor (matrix) with ones on the diagonal and zeros everywhere else. Only 2D\n  tensors are supported, i.e. input T1 must be of rank 2. The shape of the output tensor is the\n  same as the input tensor. The data type can be specified by the 'dtype' argument. If\n  'dtype' is not specified, then the type of input tensor is used. By default, the main diagonal\n  is populated with ones, but attribute 'k' can be used to populate upper or lower diagonals.\n  The 'dtype' argument must be one of the data types specified in the 'DataType' enum field in the\n  TensorProto message and be valid as an output type.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dtype", "type": "OptionalAttr" },
      { "name": "k", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.FeatureVectorizer",
    "summary": "ONNX FeatureVectorizer operation",
    "description": "Concatenates input tensors into one continuous output.<br>\n      All input shapes are 2-D and are concatenated along the second dimension. 1-D tensors are treated as [1,C].\n      Inputs are copied to the output maintaining the order of the input arguments.<br>\n      All inputs must be integers or floats, while the output will be all floating point values.",
    "inputs": [
      { "name": "X", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "inputdimensions", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Flatten",
    "summary": "ONNX Flatten operation",
    "description": "Flattens the input tensor into a 2D matrix. If input tensor has shape\n  (d_0, d_1, ... d_n) then the output will have shape\n  (d_0 X d_1 ... d_(axis-1), d_axis X d_(axis+1) ... X dn).",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Floor",
    "summary": "ONNX Floor operation",
    "description": "Floor takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the floor is, y = floor(x), is applied to\n  the tensor elementwise. If x is integral, +0, -0, NaN,  or infinite, x itself is returned.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Gather",
    "summary": "ONNX Gather operation",
    "description": "Given `data` tensor of rank r >= 1, and `indices` tensor of rank q, gather\n  entries of the axis dimension of `data` (by default outer-most one as axis=0) indexed by `indices`, and concatenates\n  them in an output tensor of rank q + (r - 1).\n  \n  If `axis = 0`, let `k = indices[i_{0}, ..., i_{q-1\\}\\]`\n  then `output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2\\}\\] = input[k , j_{0}, ..., j_{r-2\\}\\]`:\n  \n  ```\n  data = [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n  indices = [\n      [0, 1],\n      [1, 2],\n  ]\n  output = [\n      [\n          [1.0, 1.2],\n          [2.3, 3.4],\n      ],\n      [\n          [2.3, 3.4],\n          [4.5, 5.7],\n      ],\n  ]\n  ```\n  \n  If `axis = 1`, let `k = indices[i_{0}, ..., i_{q-1\\}\\]`\n  then `output[j_{0}, i_{0}, ..., i_{q-1}, j_{1}, ..., j_{r-2\\}\\] = input[j_{0}, k, j_{1}, ..., j_{r-2\\}\\]`:\n  \n  ```\n  data = [\n      [1.0, 1.2, 1.9],\n      [2.3, 3.4, 3.9],\n      [4.5, 5.7, 5.9],\n  ]\n  indices = [\n      [0, 2],\n  ]\n  axis = 1,\n  output = [\n          [[1.0, 1.9]],\n          [[2.3, 3.9]],\n          [[4.5, 5.9]],\n  ]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "indices", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ],
    "category": "Tensor"
  },
  {
    "name": "onnx.GatherElements",
    "summary": "ONNX GatherElements operation",
    "description": "GatherElements takes two inputs `data` and `indices` of the same rank r >= 1\n  and an optional attribute `axis` that identifies an axis of `data`\n  (by default, the outer-most axis, that is axis 0). It is an indexing operation\n  that produces its output by indexing into the input data tensor at index\n  positions determined by elements of the `indices` tensor.\n  Its output shape is the same as the shape of `indices` and consists of one value\n  (gathered from the `data`) for each element in `indices`.\n  \n  For instance, in the 3-D case (r = 3), the output produced is determined\n  by the following equations:\n  ```\n  out[i][j][k] = input[index[i][j][k]][j][k] if axis = 0,\n  out[i][j][k] = input[i][index[i][j][k]][k] if axis = 1,\n  out[i][j][k] = input[i][j][index[i][j][k]] if axis = 2,\n  ```\n  \n  This operator is also the inverse of ScatterElements. It is similar to Torch's gather operation.\n  \n  Example 1:\n  ```\n  data = [\n      [1, 2],\n      [3, 4],\n  ]\n  indices = [\n      [0, 0],\n      [1, 0],\n  ]\n  axis = 1\n  output = [\n      [1, 1],\n      [4, 3],\n  ]\n  ```\n  Example 2:\n  ```\n  data = [\n      [1, 2, 3],\n      [4, 5, 6],\n      [7, 8, 9],\n  ]\n  indices = [\n      [1, 2, 0],\n      [2, 0, 0],\n  ]\n  axis = 0\n  output = [\n      [4, 8, 3],\n      [7, 2, 3],\n  ]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "indices", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.GatherND",
    "summary": "ONNX GatherND operation",
    "description": "Given `data` tensor of rank `r` >= 1, `indices` tensor of rank `q` >= 1, and `batch_dims` integer `b`, this operator gathers\n  slices of `data` into an output tensor of rank `q + r - indices_shape[-1] - 1 - b`.\n  \n  `indices` is an q-dimensional integer tensor, best thought of as a `(q-1)`-dimensional tensor of index-tuples into `data`,\n  where each element defines a slice of `data`\n  \n  `batch_dims` (denoted as `b`) is an integer indicating the number of batch dimensions, i.e the leading `b` number of dimensions of\n  `data` tensor and `indices` are representing the batches, and the gather starts from the `b+1` dimension.\n  \n  Some salient points about the inputs' rank and shape:\n  \n  1) r >= 1 and q >= 1 are to be honored. There is no dependency condition to be met between ranks `r` and `q`\n  \n  2) The first `b` dimensions of the shape of `indices` tensor and `data` tensor must be equal.\n  \n  3) b < min(q, r) is to be honored.\n  \n  4) The `indices_shape[-1]` should have a value between 1 (inclusive) and rank `r-b` (inclusive)\n  \n  5) All values in `indices` are expected to be within bounds [-s, s-1] along axis of size `s` (i.e.) `-data_shape[i] <= indices[...,i] <= data_shape[i] - 1`.\n     It is an error if any of the index values are out of bounds.\n  \n  The output is computed as follows:\n  \n  The output tensor is obtained by mapping each index-tuple in the `indices` tensor to the corresponding slice of the input `data`.\n  \n  1) If `indices_shape[-1] > r-b` => error condition\n  \n  2) If `indices_shape[-1] == r-b`, since the rank of `indices` is `q`, `indices` can be thought of as `N` `(q-b-1)`-dimensional tensors\n     containing 1-D tensors of dimension `r-b`, where `N` is an integer equals to the product of 1 and all the elements in the batch dimensions\n     of the indices_shape. Let us think of each such `r-b` ranked tensor as `indices_slice`. Each *scalar value* corresponding to `data[0:b-1,indices_slice]`\n     is filled into the corresponding location of the `(q-b-1)`-dimensional tensor to form the `output` tensor (Example 1 below)\n  \n  3) If `indices_shape[-1] < r-b`, since the rank of `indices` is `q`, `indices` can be thought of as `N` `(q-b-1)`-dimensional tensor\n     containing 1-D tensors of dimension `< r-b`. Let us think of each such tensors as `indices_slice`. Each *tensor slice* corresponding\n     to `data[0:b-1, indices_slice , :]` is filled into the corresponding location of the `(q-b-1)`-dimensional tensor\n     to form the `output` tensor (Examples 2, 3, 4 and 5 below)\n  \n  This operator is the inverse of `ScatterND`.\n  \n  **Example 1**\n  \n  ```\n  batch_dims = 0\n  data    = [[0,1],[2,3]]   # data_shape    = [2, 2]\n  indices = [[0,0],[1,1]]   # indices_shape = [2, 2]\n  output  = [0,3]           # output_shape  = [2]\n  ```\n  \n  **Example 2**\n  \n  ```\n  batch_dims = 0\n  data    = [[0,1],[2,3]]  # data_shape    = [2, 2]\n  indices = [[1],[0]]      # indices_shape = [2, 1]\n  output  = [[2,3],[0,1]]  # output_shape  = [2, 2]\n  ```\n  \n  **Example 3**\n  \n  ```\n  batch_dims = 0\n  data    = [[[0,1],[2,3]],[[4,5],[6,7]]] # data_shape    = [2, 2, 2]\n  indices = [[0,1],[1,0]]                 # indices_shape = [2, 2]\n  output  = [[2,3],[4,5]]                 # output_shape  = [2, 2]\n  ```\n  \n  **Example 4**\n  \n  ```\n  batch_dims = 0\n  data    = [[[0,1],[2,3]],[[4,5],[6,7]]] # data_shape    = [2, 2, 2]\n  indices = [[[0,1]],[[1,0]]]             # indices_shape = [2, 1, 2]\n  output  = [[[2,3]],[[4,5]]]             # output_shape  = [2, 1, 2]\n  ```\n  \n  **Example 5**\n  \n  ```\n  batch_dims = 1\n  data    = [[[0,1],[2,3]],[[4,5],[6,7]]] # data_shape    = [2, 2, 2]\n  indices = [[1],[0]]                     # indices_shape = [2, 1]\n  output  = [[2,3],[4,5]]                 # output_shape  = [2, 2]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "indices", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "batch_dims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Gelu",
    "summary": "ONNX Gelu operation",
    "description": "Gelu takes one input data (Tensor<T>) and produces one\n  output data (Tensor<T>) where the gaussian error linear units function,\n  $y = 0.5 * x * (1 + erf(x/sqrt(2)))$ is applied to the tensor elementwise.\n  If the attribute \\\"approximate\\\" is set to \\\"tanh\\\", the function estimation,\n  $y = 0.5 * x * (1 + Tanh(sqrt(2/\\pi) * (x + 0.044715 * x^3)))$ is used and applied\n  to the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "approximate", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.Gemm",
    "summary": "ONNX Gemm operation",
    "description": "General Matrix multiplication:\n  https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3\n  \n  * A' = transpose(A) if transA else A\n  * B' = transpose(B) if transB else B\n  \n  Compute Y = alpha * A' * B' + beta * C, where input tensor A has shape (M, K) or (K, M),\n  input tensor B has shape (K, N) or (N, K), input tensor C is broadcastable to shape (M, N),\n  and output tensor Y has shape (M, N). A will be transposed before doing the\n  computation if attribute transA is non-zero, same for B and transB.\n  This operator supports **unidirectional broadcasting** (tensor C should be unidirectional broadcastable to tensor A * B); for more details please check [the doc](Broadcasting.md).\n  This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" },
      { "name": "C", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" },
      { "name": "beta", "type": "DefaultValuedAttr" },
      { "name": "transA", "type": "DefaultValuedAttr" },
      { "name": "transB", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.GlobalAveragePool",
    "summary": "ONNX GlobalAveragePool operation",
    "description": "GlobalAveragePool consumes an input tensor X and applies average pooling across\n   the values in the same channel. This is equivalent to AveragePool with kernel size\n   equal to the spatial dimension of input tensor.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.GlobalLpPool",
    "summary": "ONNX GlobalLpPool operation",
    "description": "GlobalLpPool consumes an input tensor X and applies lp pool pooling across\n   the values in the same channel. This is equivalent to LpPool with kernel size\n   equal to the spatial dimension of input tensor.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "p", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.GlobalMaxPool",
    "summary": "ONNX GlobalMaxPool operation",
    "description": "GlobalMaxPool consumes an input tensor X and applies max pooling across\n   the values in the same channel. This is equivalent to MaxPool with kernel size\n   equal to the spatial dimension of input tensor.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Gradient",
    "summary": "ONNX Gradient operation",
    "description": "Gradient operator computes the partial derivatives of a specific tensor w.r.t.\n  some other tensors. This operator is widely used in gradient-based training\n  algorithms. To illustrate its use, let's consider a computation graph,\n  \n  ```\n  X -----.\n         |\n         v\n  W --> Conv --> H --> Gemm --> Y\n                        ^\n                        |\n                        Z\n  ```\n  \n  , where W and Z are trainable tensors. Note that operators' attributes are\n  omitted for the sake of simplicity. Let dY/dW (dY/dZ) be the gradient of\n  Y with respect to W (Z). The user can compute gradient by inserting Gradient\n  operator to form another graph shown below.\n  \n  ```\n  W --> Conv --> H --> Gemm --> Y\n  |      ^              ^\n  |      |              |\n  |      X              Z\n  |      |              |\n  |      |   .----------'\n  |      |   |  (W/Z/X is the 1st/2nd/3rd input of Gradient as shown in\n  |      |   |   \\\"xs\\\" followed by \\\"zs\\\")\n  |      v   v\n  '---> Gradient(xs=[\\\"W\\\", \\\"Z\\\"], zs=[\\\"X\\\"], y=\\\"Y\\\")\n         |   |\n         |   '-----------------------------------> dY/dW (1st output of Gradient)\n         |\n         '---------------------------------------> dY/dZ (2nd output of Gradient)\n  ```\n  \n  By definition, the tensor \\\"y\\\" is a function of independent variables in \\\"xs\\\"\n  and \\\"zs\\\". Since we only compute the gradient of \\\"y\\\" w.r.t. the differentiable\n  variables in \\\"xs\\\", this Gradient only outputs dY/dW and dY/dZ. Note that \\\"H\\\"\n  cannot appear in \\\"xs\\\" and \\\"zs\\\". The reason is that \\\"H\\\" can be determined by\n  tensors \\\"W\\\" and \\\"X\\\" and therefore \\\"H\\\" is not an independent variable.\n  \n  All outputs are optional. If needed, for example, user can assign an empty\n  string to the 1st output name of that Gradient to skip the generation of dY/dW.\n  Note that the concept of optional outputs can also be found in ONNX's RNN, GRU,\n  and LSTM.\n  \n  Gradient operator can compute derivative against intermediate tensors. For\n  example, the gradient of Y with respect to H can be done via\n  \n  ```\n  W --> Conv --> H --> Gemm --> Y\n         ^       |      ^\n         |       |      |\n         X       |      Z\n         .-------'      |\n         |   .----------'\n         |   | (H/Z is the 1st/2nd input of Gradient as shown in \\\"xs\\\")\n         v   v\n        Gradient(xs=[\\\"H\\\", \\\"Z\\\"], y=\\\"Y\\\")\n         |   |\n         |   '-----------------------------------> dY/dH (1st output of Gradient)\n         |\n         '---------------------------------------> dY/dZ (2nd output of Gradient)\n  ```\n  \n  It is possible to represent high-order differentiation using Gradient operators.\n  For example, given the following linear model:\n  \n  ```\n  W --> Gemm --> Y --> Loss --> O\n         ^              ^\n         |              |\n         X              L\n  ```\n  \n  To compute the 2nd order derivative of O with respect to W (denoted by\n  d^2O/dW^2), one can do\n  \n  ```\n  W --> Gemm --> Y --> Loss --> O\n  |      ^              ^\n  |      |              |\n  |      X .------------L\n  |      | |            |\n  |      | |            v\n  +------+-+> Gradient(xs=[\\\"X\\\", \\\"W\\\"], zs=[\\\"L\\\"], y=\\\"O\\\") ---> dO/dX (1st output of Gradient)\n  |      | |    |\n  |      | |    '---> dO/dW (2nd output of Gradient)\n  |      v v\n  '---> Gradient(xs=[\\\"X\\\", \\\"W\\\"], zs=[\\\"L\\\"], y=\\\"dO/dW\\\") ---> d(dO/dW)dX (1st output of\n         |                                                  Gradient)\n         |\n         |\n         '---> d^2O/dW^2 (2nd output of Gradient)\n  ```\n  \n  The tensors named in attributes \\\"xs\\\", \\\"zs\\\", and \\\"y\\\" define the differentiated\n  computation graph, and the inputs to Gradient node define the values at\n  which the gradient is computed. We can feed different tensors to the identified\n  graph. For example, one can compute the gradient of Y with respect to H at\n  a specific value of H, H_1, by providing that value as an input to the Gradient\n  node.\n  \n  ```\n  W --> Conv --> H --> Gemm --> Y\n         ^              ^\n         |              |\n         X              Z\n  \n            Z_1 (2nd input of Gradient)\n             |\n             v\n  H_1 --> Gradient(xs=[\\\"H\\\", \\\"Z\\\"], y=\\\"Y\\\") ---> dY/dH when H = H_1 and Y = Y_1.\n             |\n             '------------------------------> dY/dZ (2nd output of Gradient)\n  ```\n  \n  When the inputs of Gradient are the tensors named in \\\"xs\\\" and \\\"zs\\\", the\n  computation can be optimized. More specifically, intermediate variables in\n  forward pass can be reused if the gradient is computed via reverse-mode\n  auto-differentiation.",
    "inputs": [
      { "name": "Inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "Outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "xs", "type": "StrArrayAttr" },
      { "name": "y", "type": "StrAttr" },
      { "name": "zs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Greater",
    "summary": "ONNX Greater operation",
    "description": "Returns the tensor resulted from performing the `greater` logical operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.GreaterOrEqual",
    "summary": "ONNX GreaterOrEqual operation",
    "description": "Returns the tensor resulted from performing the `greater_equal` logical operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.GridSample",
    "summary": "ONNX GridSample operation",
    "description": "Given an input `X` and a flow-field `grid`, computes the output `Y` using `X` values and pixel locations from the `grid`.\n  For spatial input `X` with shape (N, C, H, W), the `grid` will have shape (N, H_out, W_out, 2),\n  the output `Y` will have shape (N, C, H_out, W_out). For volumetric input `X` with shape (N, C, D, H, W),\n  the `grid` will have shape (N, D_out, H_out, W_out, 3), the output `Y` will have shape (N, C, D_out, H_out, W_out).\n  More generally, for an input `X` of rank r+2 with shape (N, C, d1, d2, ..., dr),\n  the `grid` will have shape (N, D1_out, D2_out, ..., Dr_out, r), the output `Y` will have shape (N, C, D1_out, D2_out, ..., Dr_out).\n  \n  The tensor `X` contains values at centers of square pixels (voxels, etc) locations such as (n, c, d1_in, d2_in, ..., dr_in).\n  The (n, d1_out, d2_out, ..., dr_out, :) values from the tensor `grid` are the normalized positions for interpolating the values\n  at the (n, c, d1_out, d2_out, ..., dr_out) locations from the output tensor `Y` using a specified interpolation method (the mode)\n  and a padding mode (for `grid` positions falling outside the 2-dimensional image).\n  \n  For example, the values in `grid[n, h_out, w_out, :]` are size-2 vectors specifying normalized positions in the 2-dimensional space of `X`.\n  They are used to interpolate output values of `Y[n, c, h_out, w_out]`.\n  \n  The GridSample operator is often used in doing grid generator and sampler in the\n  [Spatial Transformer Networks](https://arxiv.org/abs/1506.02025).\n  See also in [torch.nn.functional.grid_sample](https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "grid", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "align_corners", "type": "DefaultValuedAttr" },
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "padding_mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.GridSampleV16",
    "summary": "ONNX GridSample operation",
    "description": "Given an input `X` and a flow-field `grid`, computes the output `Y` using `X` values and pixel locations from `grid`.\n  Currently, only spatial (4-D) inputs are supported. For input `X` with shape (N, C, H, W) and `grid` with shape (N, H_out, W_out, 2),\n  the output `Y` will have shape (N, C, H_out, W_out).\n  \n  The tensor `X` contains values at centers of square pixels in a H by W 2-dimensional image.\n  The tensor `grid` describes normalized positions where the output `Y` is to be computed\n  using a specified interpolation method (the mode) and a padding mode (for grid positions falling outside the 2-dimensional image).\n  \n  Elements in `grid[N, H_out, W_out]` are size-2 vectors specifying positions in the 2-dimensional space of `X`.\n  They are used to interpolate output values of `Y[N, C, H_out, W_out]`.\n  \n  The GridSample operator is often used in doing grid generator and sampler in the [Spatial Transformer Networks](https://arxiv.org/abs/1506.02025).\n  See also in [torch.nn.functional.grid_sample](https://pytorch.org/docs/master/generated/torch.nn.functional.grid_sample.html#torch-nn-functional-grid-sample).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "grid", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "align_corners", "type": "DefaultValuedAttr" },
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "padding_mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.GroupNormalization",
    "summary": "ONNX GroupNormalization operation",
    "description": "A GroupNormalization function. Carries out group normalization as described in\n  the paper https://arxiv.org/abs/1803.08494\n  \n  This operator transforms input according to\n  ```\n  y = scale * (x - mean) / sqrt(variance + epsilon) + bias,\n  ```\n  where the mean and variance are computed per instance per group of channels, and\n  `scale` and `bias` should be specified for each group of channels. The number of\n  groups `num_groups` should be divisible by the number of channels so that there are\n  an equal number of channels per group.\n  \n  The overall computation has two stages: the first stage normalizes the elements to\n  have zero mean and unit variance for each instance in each group, and the second\n  stage scales and shifts the results of the first stage. The floating-point precision\n  used in the first stage is determined by the `stash_type` attribute. For example,\n  if `stash_type` is 1, the operator casts all input variables to 32-bit float,\n  performs the computation, and finally casts the normalized results back to the\n  original type of `X`. The second stage does not depend on `stash_type`.\n  \n  When the number of groups is the same as the number of channels, this operator is\n  equivalent to InstanceNormalization. When there is only one group, this operator\n  is equivalent to LayerNormalization.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "scale", "type": "AnyTypeOf" },
      { "name": "bias", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "num_groups", "type": "SI64Attr" },
      { "name": "stash_type", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.GroupNormalizationV18",
    "summary": "ONNX GroupNormalization operation",
    "description": "A GroupNormalization function. Carries out group normalization as described in\n  the paper https://arxiv.org/abs/1803.08494\n  \n  This operator transforms input according to\n  ```\n  y = scale * (x - mean) / sqrt(variance + epsilon) + bias,\n  ```\n  where the mean and variance are computed per instance per group of channels, and\n  `scale` and `bias` should be specified for each group of channels. The number of\n  groups `num_groups` should be divisible by the number of channels so that there are\n  an equal number of channels per group.\n  \n  When the number of groups is the same as the number of channels, this operator is\n  equivalent to InstanceNormalization. When there is only one group, this operator\n  is equivalent to LayerNormalization.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "scale", "type": "AnyTypeOf" },
      { "name": "bias", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "num_groups", "type": "SI64Attr" }
    ]
  },
  {
    "name": "onnx.GRU",
    "summary": "ONNX GRU operation",
    "description": "Computes an one-layer GRU. This operator is usually supported via some custom\n  implementation such as CuDNN.\n  \n  Notations:\n  \n  * `X` - input tensor\n  * `z` - update gate\n  * `r` - reset gate\n  * `h` - hidden gate\n  * `t` - time step (t-1 means previous time step)\n  * `W[zrh]` - W parameter weight matrix for update, reset, and hidden gates\n  * `R[zrh]` - R recurrence weight matrix for update, reset, and hidden gates\n  * `Wb[zrh]` - W bias vectors for update, reset, and hidden gates\n  * `Rb[zrh]` - R bias vectors for update, reset, and hidden gates\n  * `WB[zrh]` - W parameter weight matrix for backward update, reset, and hidden gates\n  * `RB[zrh]` - R recurrence weight matrix for backward update, reset, and hidden gates\n  * `WBb[zrh]` - W bias vectors for backward update, reset, and hidden gates\n  * `RBb[zrh]` - R bias vectors for backward update, reset, and hidden gates\n  * `H` - Hidden state\n  * `num_directions` - 2 if direction == bidirectional else 1\n  \n  Activation functions:\n  \n  * Relu(x)                - max(0, x)\n  * Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})\n  * Sigmoid(x)             - 1/(1 + e^{-x})\n  \n  NOTE:\n    Below are optional\n  \n  * Affine(x)              - alpha * x + beta\n  * LeakyRelu(x)           - x if x >= 0 else alpha * x\n  * ThresholdedRelu(x)     - x if x >= alpha else 0\n  * ScaledTanh(x)          - alpha * Tanh(beta * x)\n  * HardSigmoid(x)         - min(max(alpha * x + beta, 0), 1)\n  * Elu(x)                 - x if x >= 0 else alpha * (e^x - 1)\n  * Softsign(x)            - x/(1 + |x|)\n  * Softplus(x)            - log(1 + e^x)\n  \n  Equations (Default: f=Sigmoid, g=Tanh):\n  \n  * zt = f(Xt*(Wz^T) + Ht-1*(Rz^T) + Wbz + Rbz)\n  * rt = f(Xt*(Wr^T) + Ht-1*(Rr^T) + Wbr + Rbr)\n  * ht = g(Xt*(Wh^T) + (rt (.) Ht-1)*(Rh^T) + Rbh + Wbh) # default, when linear_before_reset = 0\n  * ht = g(Xt*(Wh^T) + (rt (.) (Ht-1*(Rh^T) + Rbh)) + Wbh) # when linear_before_reset != 0\n  * Ht = (1 - zt) (.) ht + zt (.) Ht-1\n  This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "W", "type": "AnyTypeOf" },
      { "name": "R", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" },
      { "name": "sequence_lens", "type": "AnyTypeOf" },
      { "name": "initial_h", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "Y_h", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "activation_alpha", "type": "OptionalAttr" },
      { "name": "activation_beta", "type": "OptionalAttr" },
      { "name": "activations", "type": "OptionalAttr" },
      { "name": "clip", "type": "OptionalAttr" },
      { "name": "direction", "type": "DefaultValuedStrAttr" },
      { "name": "hidden_size", "type": "OptionalAttr" },
      { "name": "layout", "type": "DefaultValuedAttr" },
      { "name": "linear_before_reset", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.HammingWindow",
    "summary": "ONNX HammingWindow operation",
    "description": "Generates a Hamming window as described in the paper https://ieeexplore.ieee.org/document/1455106.",
    "inputs": [
      { "name": "size", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "output_datatype", "type": "DefaultValuedAttr" },
      { "name": "periodic", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.HannWindow",
    "summary": "ONNX HannWindow operation",
    "description": "Generates a Hann window as described in the paper https://ieeexplore.ieee.org/document/1455106.",
    "inputs": [
      { "name": "size", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "output_datatype", "type": "DefaultValuedAttr" },
      { "name": "periodic", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Hardmax",
    "summary": "ONNX Hardmax operation",
    "description": "The operator computes the hardmax values for the given input:\n  \n   Hardmax(element in input, axis) = 1 if the element is the first maximum value along the specified axis, 0 otherwise\n  \n  The \\\"axis\\\" attribute indicates the dimension along which Hardmax\n  will be performed. The output tensor has the same shape\n  and contains the Hardmax values of the corresponding input.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.HardSigmoid",
    "summary": "ONNX HardSigmoid operation",
    "description": "HardSigmoid takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the HardSigmoid function, y = max(0, min(1, alpha * x + beta)),\n  is applied to the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" },
      { "name": "beta", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.HardSwish",
    "summary": "ONNX HardSwish operation",
    "description": "HardSwish takes one input data (Tensor<T>) and produces one output data (Tensor<T>) where\n  the HardSwish function, y = x * max(0, min(1, alpha * x + beta)) = x * HardSigmoid<alpha, beta>(x),\n  where alpha = 1/6 and beta = 0.5, is applied to the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Identity",
    "summary": "ONNX Identity operation",
    "description": "Identity operator",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.If",
    "summary": "ONNX If operation",
    "description": "If conditional",
    "inputs": [
      { "name": "cond", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ]
  },
  {
    "name": "onnx.Imputer",
    "summary": "ONNX Imputer operation",
    "description": "Replaces inputs that equal one value with another, leaving all other elements alone.<br>\n      This operator is typically used to replace missing values in situations where they have a canonical\n      representation, such as -1, 0, NaN, or some extreme value.<br>\n      One and only one of imputed_value_floats or imputed_value_int64s should be defined -- floats if the input tensor\n      holds floats, integers if the input tensor holds integers. The imputed values must all fit within the\n      width of the tensor element type. One and only one of the replaced_value_float or replaced_value_int64 should be defined,\n      which one depends on whether floats or integers are being processed.<br>\n      The imputed_value attribute length can be 1 element, or it can have one element per input feature.<br>In other words, if the input tensor has the shape [*,F], then the length of the attribute array may be 1 or F. If it is 1, then it is broadcast along the last dimension and applied to each feature.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "imputed_value_floats", "type": "OptionalAttr" },
      { "name": "imputed_value_int64s", "type": "OptionalAttr" },
      { "name": "replaced_value_float", "type": "DefaultValuedAttr" },
      { "name": "replaced_value_int64", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.InstanceNormalization",
    "summary": "ONNX InstanceNormalization operation",
    "description": "Carries out instance normalization as described in the paper\n  https://arxiv.org/abs/1607.08022.\n  \n  y = scale * (x - mean) / sqrt(variance + epsilon) + B,\n  where mean and variance are computed per instance per channel.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "scale", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.IsInf",
    "summary": "ONNX IsInf operation",
    "description": "Map infinity to true and other values to false.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "detect_negative", "type": "DefaultValuedAttr" },
      { "name": "detect_positive", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.IsNaN",
    "summary": "ONNX IsNaN operation",
    "description": "Returns which elements of the input are NaN.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.LabelEncoder",
    "summary": "ONNX LabelEncoder operation",
    "description": "Maps each element in the input tensor to another value.<br>\n      The mapping is determined by the two parallel attributes, 'keys_*' and\n      'values_*' attribute. The i-th value in the specified 'keys_*' attribute\n      would be mapped to the i-th value in the specified 'values_*' attribute. It\n      implies that input's element type and the element type of the specified\n      'keys_*' should be identical while the output type is identical to the\n      specified 'values_*' attribute. If an input element can not be found in the\n      specified 'keys_*' attribute, the 'default_*' that matches the specified\n      'values_*' attribute may be used as its output value.<br>\n      Let's consider an example which maps a string tensor to an integer tensor.\n      Assume and 'keys_strings' is [\\\"Amy\\\", \\\"Sally\\\"], 'values_int64s' is [5, 6],\n      and 'default_int64' is '-1'.  The input [\\\"Dori\\\", \\\"Amy\\\", \\\"Amy\\\", \\\"Sally\\\",\n      \\\"Sally\\\"] would be mapped to [-1, 5, 5, 6, 6].<br>\n      Since this operator is an one-to-one mapping, its input and output shapes\n      are the same. Notice that only one of 'keys_*'/'values_*' can be set.<br>\n      For key look-up, bit-wise comparison is used so even a float NaN can be\n      mapped to a value in 'values_*' attribute.<br>",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "default_float", "type": "DefaultValuedAttr" },
      { "name": "default_int64", "type": "DefaultValuedAttr" },
      { "name": "default_string", "type": "DefaultValuedStrAttr" },
      { "name": "keys_floats", "type": "OptionalAttr" },
      { "name": "keys_int64s", "type": "OptionalAttr" },
      { "name": "keys_strings", "type": "OptionalAttr" },
      { "name": "values_floats", "type": "OptionalAttr" },
      { "name": "values_int64s", "type": "OptionalAttr" },
      { "name": "values_strings", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.LayerNormalization",
    "summary": "ONNX LayerNormalization operation",
    "description": "This is layer normalization defined in ONNX as function.\n        The overall computation can be split into two stages.\n        The first stage is standardization, which makes the\n        normalized elements have zero mean and unit variances.\n        The computation required by standardization can be\n        described by the following equations.\n        ```\n        Mean = ReduceMean<axes=normalized_axes>(X)\n        D = Sub(X, Mean)\n        DD = Mul(D, D)\n        Var = ReduceMean<axes=normalized_axes>(DD)\n        VarEps = Add(Var, epsilon)\n        StdDev = Sqrt(VarEps)\n        InvStdDev = Reciprocal(StdDev)\n        Normalized = Mul(D, InvStdDev)\n        ```\n        where `normalized_axes` is `[axis, ..., rank of X - 1]`.\n        The variables `Var` and `StdDev` stand for variance and\n        standard deviation, respectively. The second output is\n        `Mean` and the last one is `InvStdDev`.\n        Depending on `stash_type` attribute, the actual computation\n        must happen in different floating-point precision.\n        For example, if `stash_type` is 1, this operator casts\n        all input variables to 32-bit float, perform the computation, and\n        finally cast `Normalized` back to the original type of `X`.\n        The second stage then scales and shifts the outcome of the\n        first stage using\n        ```\n        NormalizedScaled = Mul(Normalized, Scale)\n        Y = Add(NormalizedScaled, B)\n        ```\n        The second stage doesn't depends on `stash_type`.\n        All equations are in [this syntax](https://github.com/onnx/onnx/blob/main/docs/Syntax.md).\n        The same variable (i.e., input, output, and attribute) uses\n        the same name in the equations above and this operator's definition.\n        Let `d[i]` indicate the i-th dimension of `X`.\n        If `X`'s shape is `[d[0], ..., d[axis-1], d[axis], ..., d[rank-1]]`,\n        the shape of `Mean` and `InvStdDev` is `[d[0], ..., d[axis-1], 1, ..., 1]`.\n        `Y` and `X` have the same shape. This operator supports unidirectional broadcasting\n        (tensors `Scale` and `B` should be unidirectional broadcastable to tensor `X`);\n        for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "Scale", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "Mean", "type": "AnyTypeOf" },
      { "name": "InvStdDev", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "stash_type", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.LayoutTransform",
    "summary": "An operation that transforms data between different layout formats",
    "description": "An operation that transforms a tensor from a layout to another layout. \n    A layout is defined by an attribute, i.e. `target_layout`, which allows this\n    operation work with an arbitrary layout (e.g. a layout used for accelerators).\n\n    `target_layout` is optional. If it is not given, the input tensor will be\n    transformed to a normal tensor that does not have layout.\n\n    If `target_layout` is the same as the input's layout, this operation will\n    become an no-op by canonicalization. \n\n    The input and output tensors must have the same shape.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "target_layout", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.LeakyRelu",
    "summary": "ONNX LeakyRelu operation",
    "description": "LeakyRelu takes input data (Tensor<T>) and an argument alpha, and produces one\n  output data (Tensor<T>) where the function `f(x) = alpha * x for x < 0`,\n  `f(x) = x for x >= 0`, is applied to the data tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Less",
    "summary": "ONNX Less operation",
    "description": "Returns the tensor resulted from performing the `less` logical operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.LessOrEqual",
    "summary": "ONNX LessOrEqual operation",
    "description": "Returns the tensor resulted from performing the `less_equal` logical operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.LinearClassifier",
    "summary": "ONNX LinearClassifier operation",
    "description": "Linear classifier",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "Z", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "classlabels_ints", "type": "OptionalAttr" },
      { "name": "classlabels_strings", "type": "OptionalAttr" },
      { "name": "coefficients", "type": "F32ArrayAttr" },
      { "name": "intercepts", "type": "OptionalAttr" },
      { "name": "multi_class", "type": "DefaultValuedAttr" },
      { "name": "post_transform", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.LinearRegressor",
    "summary": "ONNX LinearRegressor operation",
    "description": "Generalized linear regression evaluation.<br>\n      If targets is set to 1 (default) then univariate regression is performed.<br>\n      If targets is set to M then M sets of coefficients must be passed in as a sequence\n      and M results will be output for each input n in N.<br>\n      The coefficients array is of length n, and the coefficients for each target are contiguous.\n      Intercepts are optional but if provided must match the number of targets.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "coefficients", "type": "OptionalAttr" },
      { "name": "intercepts", "type": "OptionalAttr" },
      { "name": "post_transform", "type": "DefaultValuedStrAttr" },
      { "name": "targets", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Log",
    "summary": "ONNX Log operation",
    "description": "Calculates the natural log of the given input tensor, element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.LogSoftmax",
    "summary": "ONNX LogSoftmax operation",
    "description": "The operator computes the log of softmax values for the given input:\n  \n   LogSoftmax(input, axis) = Log(Softmax(input, axis=axis))\n  \n  The \\\"axis\\\" attribute indicates the dimension along which LogSoftmax\n  will be performed. The output tensor has the same shape\n  and contains the LogSoftmax values of the corresponding input.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Loop",
    "summary": "ONNX Loop operation",
    "description": "Generic Looping construct. This loop has multiple termination conditions:\n  \n  1) Trip count. Iteration count specified at runtime. Set by\n     specifying the input M. Optional. Set to empty string to omit.\n     Note that a static trip count (specified at graph construction time) can be\n     specified by passing in a constant node for input M.\n  2) Loop termination condition. This is an input to the op that determines\n     whether to run the first iteration and also a loop-carried dependency for\n     the body graph. The body graph must yield a value for the condition variable,\n     whether this input is provided or not.\n  \n  This table summarizes the operating modes of this operator with equivalent\n  C-style code:\n  \n  Operator inputs defined as (max_trip_count, condition_var).\n  \n  * input (\\\"\\\", \\\"\\\"):\n          for (int i=0; ; ++i) {\n            cond = ... // Note this value is ignored, but is required in the body\n          }\n  \n  * input (\\\"\\\", cond) // Note this is analogous to a while loop\n          bool cond = ...;\n          for (int i=0; cond; ++i) {\n            cond = ...;\n          }\n  \n  * input (\\\"\\\", 1) // Note this is analogous to a do-while loop\n          bool cond = true\n          for (int i=0; cond; ++i) {\n            cond = ...;\n          }\n  \n  * input (trip_count, \\\"\\\") // Note this is analogous to a for loop\n          int trip_count = ...\n          for (int i=0; i < trip_count; ++i) {\n            cond = ...; // ignored\n          }\n  \n  * input (trip_count, cond)\n          int trip_count = ...;\n          bool cond = ...;\n          for (int i=0; i < trip_count && cond; ++i) {\n            cond = ...;\n          }\n  \n  \n  *Sample usage - cond as well as trip count*\n  \n      graph predict-net {\n        %a = Constant[value = <Scalar Tensor [3]>]()\n        %b = Constant[value = <Scalar Tensor [6]>]()\n        %keepgoing = Constant[value = <Scalar Tensor [1]>]()\n        %max_trip_count = Constant[value = <Scalar Tensor [10]>]()\n        %keepgoing_out, %b_out, %user_defined_vals = Loop[body = <graph body-net>](%max_trip_count, %keepgoing, %b)\n        return\n      }\n  \n      graph body-net (\n        %i[INT32, scalar]           // iteration number\n        %keepgoing_in[BOOL, scalar] // incoming loop-termination-condition; not used\n        %b_in[INT32, scalar]        // incoming value of loop-carried-dependency b\n      ) {\n        %my_local = Add(%a, %b_in)\n        %b_out = Sub(%a, %b_in) // outgoing value of loop-carried-dependency b\n        %keepgoing_out = Greater(%my_local, %b_out) // outgoing loop-termination-condition\n        %user_defined_val = Add(%b_in, %b_in) // scan-output value to be accumulated\n        return %keepgoing_out, %b_out, %user_defined_val\n      }\n  \n  *Sample equivalent C code*\n  \n      {\n        /* User-defined code (enclosing scope) */\n        int a = 3, b = 6;\n        bool keepgoing = true; // Analogous to input cond\n        /* End user-defined code */\n  \n        /* Implicitly-defined code */\n        const int max_trip_count = 10; // Analogous to input M\n        int user_defined_vals[]; // Imagine this is resizable\n        /* End implicitly-defined code */\n        /* initialize loop-carried variables and scan-output variables */\n        bool keepgoing_out = keepgoing\n        int b_out = b\n  \n        for (int i=0; i < max_trip_count && keepgoing_out; ++i) {\n          /* Implicitly-defined code: bind actual parameter values\n             to formal parameter variables of loop-body */\n          bool keepgoing_in = keepgoing_out;\n          bool b_in = b_out;\n  \n          /* User-defined code (loop body) */\n          int my_local = a + b_in; // Reading value \\\"a\\\" from the enclosing scope is fine\n          b_out = a - b_in;\n          keepgoing_out = my_local > b_out;\n          user_defined_val = b_in + b_in; // b_in and b_out are different variables\n          /* End user-defined code */\n  \n          /* Implicitly defined-code */\n          user_defined_vals[i] = user_defined_val // accumulate scan-output values\n        }\n        // int t = my_local; // Can't do this. my_local is not accessible here.\n  \n        // The values below are bound to the output variables of the loop and therefore accessible\n        // b_out; user_defined_vals; keepgoing_out;\n      }\n  \n  There are several things of note in this code snippet:\n  \n  1) Values from the enclosing scope (i.e. variable \\\"a\\\" here) are in scope and can\n     be referenced in the inputs of the loop.\n  2) Any values computed in the loop body that needs to be used in a subsequent\n     iteration or after the loop are modelled using a pair of variables in the loop-body,\n     consisting of an input variable (eg., b_in) and an output variable (eg., b_out).\n     These are referred to as loop-carried dependences. The loop operation node\n     supplies the input value of the input variable for the first iteration, and\n     returns the output value of the output variable produced by the final\n     iteration.\n  3) Scan_output variables are used to implicitly concatenate values computed across\n     all the iterations. In the above example, the value of user_defined_val computed\n     over all iterations are concatenated and returned as the value of user_defined_vals\n     after the loop.\n  4) Values created in the body cannot be accessed in the enclosing scope,\n     except using the mechanism described above.\n  \n  Note that the semantics of this op support \\\"diagonal\\\" or \\\"wavefront\\\" execution.\n  (See Step 3 here for an example:\n  https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5/).\n  Frontends should emit multi-layer RNNs as a series of While operators (with\n  time being the inner looping dimension), with each successive layer consuming\n  the scan_outputs from the previous layer, possibly going through several\n  point-wise operators (e.g. dropout, residual connections, linear layer).\n  \n  The input/output of subgraph (produced by loop node) matching is based on order instead of name. The implementation will figure out the names based on this order.",
    "inputs": [
      { "name": "M", "type": "AnyTypeOf" },
      { "name": "cond", "type": "AnyTypeOf" },
      { "name": "v_initial", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "v_final_and_scan_outputs", "type": "Variadic" }
    ]
  },
  {
    "name": "onnx.LpNormalization",
    "summary": "ONNX LpNormalization operation",
    "description": "Given a matrix, apply Lp-normalization along the provided axis.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "p", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.LpPool",
    "summary": "ONNX LpPool operation",
    "description": "LpPool consumes an input tensor X and applies Lp pooling across\n   the tensor according to kernel sizes, stride sizes, and pad lengths.\n   Lp pooling consisting of computing the Lp norm on all values of a subset\n   of the input tensor according to the kernel size and downsampling the\n   data into the output tensor Y for further processing. The output spatial shape will be following:\n   ```\n   output_spatial_shape[i] = floor((input_spatial_shape[i] + pad_shape[i] - {kernelSpatialShape}) / strides_spatial_shape[i] + 1)\n   ```\n   or\n   ```\n   output_spatial_shape[i] = ceil((input_spatial_shape[i] + pad_shape[i] - {kernelSpatialShape}) / strides_spatial_shape[i] + 1)\n   ```\n   if ceil_mode is enabled `pad_shape[i]` is the sum of pads along axis `i`.\n  \n   `auto_pad` is a DEPRECATED attribute. If you are using them currently, the output spatial shape will be following:\n   ```\n   VALID: output_spatial_shape[i] = ceil((input_spatial_shape[i] - {kernelSpatialShape} + 1) / strides_spatial_shape[i])\n   SAME_UPPER or SAME_LOWER: output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides_spatial_shape[i])\n   ```\n   And pad shape will be following if `SAME_UPPER` or `SAME_LOWER`:\n   ```\n   pad_shape[i] = (output_spatial_shape[i] - 1) * strides_spatial_shape[i] + {kernelSpatialShape} - input_spatial_shape[i]\n   ```",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "auto_pad", "type": "DefaultValuedStrAttr" },
      { "name": "ceil_mode", "type": "DefaultValuedAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "p", "type": "DefaultValuedAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.LRN",
    "summary": "ONNX LRN operation",
    "description": "Local Response Normalization proposed in the [AlexNet paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).\n  It normalizes over local input regions.\n  The local region is defined across the channels. For an element `X[n, c, d1, ..., dk]` in a tensor\n  of shape `(N x C x D1 x D2, ..., Dk)`, its region is\n  `{X[n, i, d1, ..., dk] | max(0, c - floor((size - 1) / 2)) <= i <= min(C - 1, c + ceil((size - 1) / 2))}`.\n  \n  `square_sum[n, c, d1, ..., dk] = sum(X[n, i, d1, ..., dk] ^ 2)`,\n  where `max(0, c - floor((size - 1) / 2)) <= i <= min(C - 1, c + ceil((size - 1) / 2))`.\n  \n  `Y[n, c, d1, ..., dk] = X[n, c, d1, ..., dk] / (bias + alpha / size * square_sum[n, c, d1, ..., dk] ) ^ beta`",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" },
      { "name": "beta", "type": "DefaultValuedAttr" },
      { "name": "bias", "type": "DefaultValuedAttr" },
      { "name": "size", "type": "SI64Attr" }
    ]
  },
  {
    "name": "onnx.LSTM",
    "summary": "ONNX LSTM operation",
    "description": "Computes an one-layer LSTM. This operator is usually supported via some\n  custom implementation such as CuDNN.\n  \n  Notations:\n  \n  * `X` - input tensor\n  * `i` - input gate\n  * `o` - output gate\n  * `f` - forget gate\n  * `c` - cell gate\n  * `t` - time step (t-1 means previous time step)\n  * `W[iofc]` - W parameter weight matrix for input, output, forget, and cell gates\n  * `R[iofc]` - R recurrence weight matrix for input, output, forget, and cell gates\n  * `Wb[iofc]` - W bias vectors for input, output, forget, and cell gates\n  * `Rb[iofc]` - R bias vectors for input, output, forget, and cell gates\n  * `P[iof]`  - P peephole weight vector for input, output, and forget gates\n  * `WB[iofc]` - W parameter weight matrix for backward input, output, forget, and cell gates\n  * `RB[iofc]` - R recurrence weight matrix for backward input, output, forget, and cell gates\n  * `WBb[iofc]` - W bias vectors for backward input, output, forget, and cell gates\n  * `RBb[iofc]` - R bias vectors for backward input, output, forget, and cell gates\n  * `PB[iof]`  - P peephole weight vector for backward input, output, and forget gates\n  * `H` - Hidden state\n  * `num_directions` - 2 if direction == bidirectional else 1\n  \n  Activation functions:\n  \n  * Relu(x)                - max(0, x)\n  * Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})\n  * Sigmoid(x)             - 1/(1 + e^{-x})\n  \n  NOTE: Below are optional\n  \n  * Affine(x)              - alpha*x + beta\n  * LeakyRelu(x)           - x if x >= 0 else alpha * x\n  * ThresholdedRelu(x)     - x if x >= alpha else 0\n  * ScaledTanh(x)          - alpha*Tanh(beta*x)\n  * HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)\n  * Elu(x)                 - x if x >= 0 else alpha*(e^x - 1)\n  * Softsign(x)            - x/(1 + |x|)\n  * Softplus(x)            - log(1 + e^x)\n  \n  Equations (Default: f=Sigmoid, g=Tanh, h=Tanh):\n  \n  * it = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Pi (.) Ct-1 + Wbi + Rbi)\n  * ft = f(Xt*(Wf^T) + Ht-1*(Rf^T) + Pf (.) Ct-1 + Wbf + Rbf)\n  * ct = g(Xt*(Wc^T) + Ht-1*(Rc^T) + Wbc + Rbc)\n  * Ct = ft (.) Ct-1 + it (.) ct\n  * ot = f(Xt*(Wo^T) + Ht-1*(Ro^T) + Po (.) Ct + Wbo + Rbo)\n  * Ht = ot (.) h(Ct)\n  This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "W", "type": "AnyTypeOf" },
      { "name": "R", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" },
      { "name": "sequence_lens", "type": "AnyTypeOf" },
      { "name": "initial_h", "type": "AnyTypeOf" },
      { "name": "initial_c", "type": "AnyTypeOf" },
      { "name": "P", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "Y_h", "type": "AnyTypeOf" },
      { "name": "Y_c", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "activation_alpha", "type": "OptionalAttr" },
      { "name": "activation_beta", "type": "OptionalAttr" },
      { "name": "activations", "type": "OptionalAttr" },
      { "name": "clip", "type": "OptionalAttr" },
      { "name": "direction", "type": "DefaultValuedStrAttr" },
      { "name": "hidden_size", "type": "OptionalAttr" },
      { "name": "input_forget", "type": "DefaultValuedAttr" },
      { "name": "layout", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.MatMul",
    "summary": "ONNX MatMul operation",
    "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.MatMulInteger",
    "summary": "ONNX MatMulInteger operation",
    "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n  The production MUST never overflow. The accumulation may overflow if and only if in 32 bits.",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" },
      { "name": "a_zero_point", "type": "AnyTypeOf" },
      { "name": "b_zero_point", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.Max",
    "summary": "ONNX Max operation",
    "description": "Element-wise max of each of the input tensors (with Numpy-style broadcasting support).\n  All inputs and outputs must have the same data type.\n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "data_0", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "max", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.MaxPool",
    "summary": "ONNX MaxPool operation",
    "description": "MaxPool consumes an input tensor X and applies max pooling across\n   the tensor according to kernel sizes, stride sizes, and pad lengths.\n   max pooling consisting of computing the max on all values of a\n   subset of the input tensor according to the kernel size and downsampling the\n   data into the output tensor Y for further processing. The output spatial shape is calculated differently\n   depending on whether explicit padding is used, where pads is employed, or auto padding is used, where auto_pad is utilized.\n   With explicit padding (https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html?highlight=maxpool#torch.nn.MaxPool2d):\n   ```\n   output_spatial_shape[i] = floor((input_spatial_shape[i] + pad_shape[i] - dilation[i] * (kernel_shape[i] - 1) - 1) / strides_spatial_shape[i] + 1)\n   ```\n   or\n   ```\n   output_spatial_shape[i] = ceil((input_spatial_shape[i] + pad_shape[i] - dilation[i] * (kernel_shape[i] - 1) - 1) / strides_spatial_shape[i] + 1)\n   ```\n   if ceil_mode is enabled. `pad_shape[i]` is the sum of pads along axis `i`. Sliding windows that would start in the right padded region are ignored.\n  \n   `auto_pad` is a DEPRECATED attribute. If you are using them currently, the output spatial shape will be following when ceil_mode is enabled:\n   ```\n   VALID: output_spatial_shape[i] = ceil((input_spatial_shape[i] - ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) + 1) / strides_spatial_shape[i])\n   SAME_UPPER or SAME_LOWER: output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides_spatial_shape[i])\n   ```\n   or when ceil_mode is disabled (https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D):\n   ```\n   VALID: output_spatial_shape[i] = floor((input_spatial_shape[i] - ((kernel_spatial_shape[i] - 1) * dilations[i] + 1)) / strides_spatial_shape[i]) + 1\n   SAME_UPPER or SAME_LOWER: output_spatial_shape[i] = floor((input_spatial_shape[i] - 1) / strides_spatial_shape[i]) + 1\n   ```\n   And pad shape will be following if `SAME_UPPER` or `SAME_LOWER`:\n   ```\n   pad_shape[i] = (output_spatial_shape[i] - 1) * strides_spatial_shape[i] + ((kernel_spatial_shape[i] - 1) * dilations[i] + 1) - input_spatial_shape[i]\n   ```\n   The output of each pooling window is maximum number of elements exclude pad.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "Indices", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "auto_pad", "type": "DefaultValuedStrAttr" },
      { "name": "ceil_mode", "type": "DefaultValuedAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "storage_order", "type": "DefaultValuedAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.MaxPoolSingleOut",
    "summary": "ONNX MaxPool operation with a single output.",
    "description": "ONNX MaxPool operation with a single output.\n    See ONNXMaxPoolOp for a full description of the MaxPool semantics.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "o_Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "auto_pad", "type": "DefaultValuedStrAttr" },
      { "name": "ceil_mode", "type": "DefaultValuedAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "kernel_shape", "type": "DefaultValuedAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "storage_order", "type": "DefaultValuedAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.MaxRoiPool",
    "summary": "ONNX MaxRoiPool operation",
    "description": "ROI max pool consumes an input tensor X and region of interests (RoIs) to\n   apply max pooling across each RoI, to produce output 4-D tensor of shape\n   (num_rois, channels, pooled_shape[0], pooled_shape[1]).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "rois", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "pooled_shape", "type": "I64ArrayAttr" },
      { "name": "spatial_scale", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.MaxUnpool",
    "summary": "ONNX MaxUnpool operation",
    "description": "MaxUnpool essentially computes the partial inverse of the MaxPool op.\n   The input information to this op is typically the output information from a MaxPool op. The first\n   input tensor X is the tensor that needs to be unpooled, which is typically the pooled tensor (first output)\n   from MaxPool. The second input tensor, I, contains the indices to the (locally maximal) elements corresponding\n   to the elements in the first input tensor X. Input tensor I is typically the second output of the MaxPool op.\n   The third (optional) input is a tensor that specifies the output size of the unpooling operation.\n  \n  MaxUnpool is intended to do 'partial' inverse of the MaxPool op. 'Partial' because all the non-maximal\n   values from the original input to MaxPool are set to zero in the output of the MaxUnpool op. Pooling\n   the result of an unpooling operation should give back the original input to the unpooling op.\n  \n  MaxUnpool can produce the same output size for several input sizes, which makes unpooling op ambiguous.\n   The third input argument, output_size, is meant to disambiguate the op and produce output tensor of\n   known/predictable size.\n  \n  In addition to the inputs, MaxUnpool takes three attributes, namely kernel_shape, strides, and pads,\n   which define the exact unpooling op. The attributes typically have the same values as the corresponding\n   pooling op that the unpooling op is trying to invert.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "I", "type": "TensorOf" },
      { "name": "output_shape", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Mean",
    "summary": "ONNX Mean operation",
    "description": "Element-wise mean of each of the input tensors (with Numpy-style broadcasting support).\n  All inputs and outputs must have the same data type.\n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "data_0", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "mean", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.MeanVarianceNormalization",
    "summary": "ONNX MeanVarianceNormalization operation",
    "description": "A MeanVarianceNormalization Function: Perform mean variance normalization\n        on the input tensor X using formula: `(X-EX)/sqrt(E(X-EX)^2)`",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.MelWeightMatrix",
    "summary": "ONNX MelWeightMatrix operation",
    "description": "Generate a MelWeightMatrix that can be used to re-weight a Tensor containing a linearly sampled frequency spectra (from DFT or STFT) into num_mel_bins frequency information based on the [lower_edge_hertz, upper_edge_hertz] range on the mel scale.\n  This function defines the mel scale in terms of a frequency in hertz according to the following formula:\n  \n      mel(f) = 2595 * log10(1 + f/700)\n  \n  In the returned matrix, all the triangles (filterbanks) have a peak value of 1.0.\n  \n  The returned MelWeightMatrix can be used to right-multiply a spectrogram S of shape [frames, num_spectrogram_bins] of linear scale spectrum values (e.g. STFT magnitudes) to generate a \\\"mel spectrogram\\\" M of shape [frames, num_mel_bins].",
    "inputs": [
      { "name": "num_mel_bins", "type": "AnyTypeOf" },
      { "name": "dft_length", "type": "AnyTypeOf" },
      { "name": "sample_rate", "type": "AnyTypeOf" },
      { "name": "lower_edge_hertz", "type": "AnyTypeOf" },
      { "name": "upper_edge_hertz", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "output_datatype", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Min",
    "summary": "ONNX Min operation",
    "description": "Element-wise min of each of the input tensors (with Numpy-style broadcasting support).\n  All inputs and outputs must have the same data type.\n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "data_0", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "min", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Mish",
    "summary": "ONNX Mish operation",
    "description": "Mish: A Self Regularized Non-Monotonic Neural Activation Function.\n  \n  Perform the linear unit element-wise on the input tensor X using formula:\n  \n  ```\n  mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n  ```",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Mod",
    "summary": "ONNX Mod operation",
    "description": "Performs element-wise binary modulus (with Numpy-style broadcasting support).\n    The sign of the remainder is the same as that of the Divisor.\n  \n    Mod operator can also behave like C fmod() or numpy.fmod. In this case, the sign of the remainder however, will be the same as the Dividend\n    (in contrast to integer mod). To force a behavior like numpy.fmod() an 'fmod' Attribute is provided.\n    This attribute is set to 0 by default causing the behavior to be like integer mod.\n    Setting this attribute to 1 causes the remainder to be calculated similar to that of numpy.fmod().\n  \n    If the input type is floating point, then `fmod` attribute must be set to 1.\n  \n    In case of dividend being zero, the results will be platform dependent.\n  \n    This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "fmod", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Momentum",
    "summary": "ONNX Momentum operation",
    "description": "Compute one iteration of stochastic gradient update with momentum.\n      This operator can conduct the optimization of multiple tensor variables.\n  \n      Let's define the behavior of this operator. As you can imagine, SG with momentum requires\n      several parameters:\n  \n       - The learning-rate \\\"R\\\".\n       - The update count \\\"T\\\". That is, the number of conducted training iterations. It should\n         be zero in the first training iteration.\n       - A L2-norm regularization coefficient \\\"norm_coefficient\\\".\n       - A decay coefficient of previous accumulated gradient (i.e., momentum) \\\"alpha\\\".\n       - The scaling coefficient of current gradient \\\"beta\\\".\n       - An attribute to choose either standard momentum or Nesterov's momentum \\\"mode\\\" should\n         be used.\n  \n      For the sake of simplicity, assume that there is only one tensor (called \\\"X\\\") to be optimized.\n      Other necessary inputs are \\\"X\\\"'s gradient (called \\\"G\\\") and \\\"X\\\"'s momentum (called \\\"V\\\"). This\n      Momentum operator maps all these inputs to the new value of \\\"X\\\" (called \\\"X_new\\\") and its new\n      momentum (called \\\"V_new\\\").\n  \n      This operator supports two different momentum algorithms. Set the attribute \\\"mode\\\" to\n      \\\"nesterov\\\" if Nesterov's momentum is desired. Otherwise, set the attribute \\\"model\\\" to\n      \\\"standard\\\" to use standard momentum. Computation details are described subsequently.\n  \n      Let \\\"+\\\", \\\"-\\\", \\\"*\\\", and \\\"/\\\" are all element-wise operations with numpy-style broadcasting.\n  \n      Pseudo code for SG with standard momentum:\n  \n        // Add gradient of 0.5 * norm_coefficient * ||X||^2, where ||X|| is the sum of squared\n        // values of all elements in X.\n        G_regularized = norm_coefficient * X + G\n  \n        // In the first training iteration, beta should always be 1.\n        beta_adjusted = T > 0 ? beta : 1\n  \n        // Compute the current momentum based on previous momentum and the current gradient.\n        V_new = alpha * V + beta_adjusted * G_regularized\n  \n        // Update X.\n        X_new = X - R * V_new\n  \n      Pseudo code for SG with Nesterov's momentum:\n  \n        // Add gradient of 0.5 * norm_coefficient * ||X||^2, where ||X|| is the sum of squared\n        // values of all elements in X.\n        G_regularized = norm_coefficient * X + G;\n  \n        // In the first training iteration, beta should always be 1.\n        beta_adjusted = T > 0 ? beta : 1\n  \n        // Compute the current momentum based on previous momentum and the current gradient.\n        V_new = alpha * V + beta_adjusted * G_regularized;\n  \n        // Compute final update direction and then update X.\n        X_new = X - R * (G_regularized + alpha * V_new)\n  \n      If one assign this operators to optimize multiple inputs, for example, \\\"X_1\\\" and \\\"X_2\\\". The same\n      pseudo code would be extended to handle all tensors jointly. More specifically, we can view \\\"X\\\" as a\n      concatenation of \\\"X_1\\\" and \\\"X_2\\\" (of course, their gradient and accumulate gradient should\n      be concatenated too) and then our pseudo code becomes applicable.",
    "inputs": [
      { "name": "R", "type": "AnyTypeOf" },
      { "name": "T", "type": "TensorOf" },
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "alpha", "type": "F32Attr" },
      { "name": "beta", "type": "F32Attr" },
      { "name": "mode", "type": "StrAttr" },
      { "name": "norm_coefficient", "type": "F32Attr" }
    ]
  },
  {
    "name": "onnx.Mul",
    "summary": "ONNX Mul operation",
    "description": "Performs element-wise binary multiplication (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).\n  \n  (Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Multinomial",
    "summary": "ONNX Multinomial operation",
    "description": "Generate a tensor of samples from a multinomial distribution according to the probabilities\n  of each of the possible outcomes.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dtype", "type": "DefaultValuedAttr" },
      { "name": "sample_size", "type": "DefaultValuedAttr" },
      { "name": "seed", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Neg",
    "summary": "ONNX Neg operation",
    "description": "Neg takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where each element flipped sign, y = -x, is applied to\n  the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.NegativeLogLikelihoodLoss",
    "summary": "ONNX NegativeLogLikelihoodLoss operation",
    "description": "A NegativeLogLikelihoodLoss operator computes (weighted) negative log likelihood loss.\n  Its \\\"input\\\" tensor has the shape of (N, C, d1, d2, ..., dk) where k >= 0.\n  The \\\"input\\\" tensor contains log-probabilities for input[n, :, d_1, d_2,..., d_k] being in a class of [0, C).\n  The operator's \\\"target\\\" input tensor has the shape of (N, d1, d2, ..., dk). It encodes class labels (one of C classes)\n  or it may contain a special value (indicated by an attribute ignore_index) for N x d1 x d2 x ... x dk samples.\n  The loss value for input[n, :, d_1, d_2,...d_k] being classified as class c = target[n][d_1][d_2]...[d_k] is computed as:\n  \n  ```\n  loss[n][d_1][d_2]...[d_k] = -input[n][c][d_1][d_2]...[d_k].\n  ```\n  \n  When an optional \\\"weight\\\" is provided, the sample loss is calculated as:\n  \n  ```\n  loss[n][d_1][d_2]...[d_k] = -input[n][c][d_1][d_2]...[d_k] * weight[c].\n  ```\n  \n  loss is zero for the case when target-value equals ignore_index.\n  \n  ```\n  loss[n][d_1][d_2]...[d_k] = 0, when target[n][d_1][d_2]...[d_k] = ignore_index\n  ```\n  \n  If \\\"reduction\\\" attribute is set to \\\"none\\\", the operator's output will be the above loss with shape (N, d1, d2, ..., dk).\n  If \\\"reduction\\\" attribute is set to \\\"mean\\\" (the default attribute value), the output loss is (weight) averaged:\n  \n  ```\n  mean(loss), if \\\"weight\\\" is not provided,\n  ```\n  \n  or if weight is provided,\n  \n  ```\n  sum(loss) / sum(weight[target[n][d_1][d_2]...[d_k]]]), for all samples.\n  ```\n  \n  If \\\"reduction\\\" attribute is set to \\\"sum\\\", the output is a scalar: `sum(loss)`.\n  \n  See also https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss.\n  \n  Example 1:\n  \n  ```\n  // negative log likelihood loss, \\\"none\\\" reduction\n  N, C, d1 = 2, 3, 2\n  input = [[[1.0, 2.0], [2.0, 2.0], [3.0, 2.0]],\n            [[0.0, 1.0], [2.0, 2.0], [1.0, 2]]]\n  target = [[2, 1], [0, 2]]\n  \n  loss = np.zeros((N, d1))\n  for n in range(N):\n      for d_1 in range(d1):\n          c = target[n][d_1]\n          loss[n][d_1] = -input[n][c][d_1]\n  \n  // print(loss)\n  // [[-3. -2.]\n  //  [-0. -2.]]\n  ```\n  \n  Example 2:\n  \n  ```\n  // weighted negative log likelihood loss, sum reduction\n  N, C, d1 = 2, 3, 2\n  input = [[[1.0, 2.0], [2.0, 2.0], [3.0, 2.0]],\n          [[0.0, 1.0], [2.0, 2.0], [1.0, 2]]]\n  target = [[2, 1], [0, 2]]\n  weight = [0.2, 0.3, 0.1]\n  loss = np.zeros((N, d1))\n  for n in range(N):\n      for d_1 in range(d1):\n          c = target[n][d_1]\n          loss[n][d_1] = -input[n][c][d_1] * weight[c]\n  \n  loss = np.sum(loss)\n  // print(loss)\n  // -1.1\n  ```\n  \n  Example 3:\n  \n  ```\n  // weighted negative log likelihood loss, mean reduction\n  N, C, d1 = 2, 3, 2\n  input = [[[1.0, 2.0], [2.0, 2.0], [3.0, 2.0]],\n          [[0.0, 1.0], [2.0, 2.0], [1.0, 2]]]\n  target = [[2, 1], [0, 2]]\n  weight = [0.2, 0.3, 0.1]\n  loss = np.zeros((N, d1))\n  weight_total = 0\n  for n in range(N):\n      for d_1 in range(d1):\n          c = target[n][d_1]\n          loss[n][d_1] = -input[n][c][d_1] * weight[c]\n          weight_total = weight_total + weight[c]\n  \n  loss = np.sum(loss) / weight_total\n  // print(loss)\n  // -1.57\n  ```",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "target", "type": "AnyTypeOf" },
      { "name": "weight", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "loss", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "ignore_index", "type": "OptionalAttr" },
      { "name": "reduction", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.NonMaxSuppression",
    "summary": "ONNX NonMaxSuppression operation",
    "description": "Filter out boxes that have high intersection-over-union (IOU) overlap with previously selected boxes.\n  Bounding boxes with score less than score_threshold are removed. Bounding box format is indicated by attribute center_point_box.\n  Note that this algorithm is agnostic to where the origin is in the coordinate system and more generally is invariant to\n  orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system\n  result in the same boxes being selected by the algorithm.\n  The selected_indices output is a set of integers indexing into the input collection of bounding boxes representing the selected boxes.\n  The bounding box coordinates corresponding to the selected indices can then be obtained using the Gather or GatherND operation.",
    "inputs": [
      { "name": "boxes", "type": "TensorOf" },
      { "name": "scores", "type": "TensorOf" },
      { "name": "max_output_boxes_per_class", "type": "AnyTypeOf" },
      { "name": "iou_threshold", "type": "AnyTypeOf" },
      { "name": "score_threshold", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "selected_indices", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "center_point_box", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.NonZero",
    "summary": "ONNX NonZero operation",
    "description": "Returns the indices of the elements that are non-zero\n      (in row-major order - by dimension).\n      NonZero behaves similar to numpy.nonzero:\n      https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html,\n      but for scalar input, NonZero produces output shape (0, N) instead of (1, N), which is different from Numpy's behavior.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.Normalizer",
    "summary": "ONNX Normalizer operation",
    "description": "Normalize the input.  There are three normalization modes, which have the corresponding formulas,\n      defined using element-wise infix operators '/' and '^' and tensor-wide functions 'max' and 'sum':<br>\n  <br>\n      Max: Y = X / max(X)<br>\n      L1:  Y = X / sum(X)<br>\n      L2:  Y = sqrt(X^2 / sum(X^2)}<br>\n      In all modes, if the divisor is zero, Y == X.\n  <br>\n      For batches, that is, [N,C] tensors, normalization is done along the C axis. In other words, each row\n      of the batch is normalized independently.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "norm", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.Not",
    "summary": "ONNX Not operation",
    "description": "Returns the negation of the input tensor element-wise.",
    "inputs": [
      { "name": "X", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.NoValue",
    "summary": "An operation representing the absence of a value.",
    "description": "This operation can be used to represent the absence of a value. It is typically\n    used as an argument to operators that have optional parameters.\n\n    Example:\n    ```MLIR\n      %cst = \"onnx.NoValue\"() {value} : () -> none\n      %0, %1 = \"onnx.Split\"(%arg0, %cst) { axis=1 : si64 } : (tensor<?xf32>, none) -> (tensor<*xf32>, tensor<*xf32>)\n    ```\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "outputs": [
      { "name": "none_val", "type": "NoneType" }
    ],
    "attributes": [
      { "name": "value", "type": "UnitAttr" }
    ]
  },
  {
    "name": "onnx.OneHot",
    "summary": "ONNX OneHot operation",
    "description": "Produces a one-hot tensor based on inputs.\n      The locations represented by the index values in the 'indices' input tensor will have 'on_value'\n      and the other locations will have 'off_value' in the output tensor, where 'on_value' and 'off_value'\n      are specified as part of required input argument 'values', which is a two-element tensor of format\n      [off_value, on_value]. The rank of the output tensor will be one greater than the rank of the\n      input tensor. The additional dimension is for one-hot representation. The additional dimension will\n      be inserted at the position specified by 'axis'. If 'axis' is not specified then then additional\n      dimension will be inserted as the innermost dimension, i.e. axis=-1. The size of the additional\n      dimension is specified by required scalar input 'depth'. The type of the output tensor is the same\n      as the type of the 'values' input. Any entries in the 'indices' input tensor with values outside\n      the range [-depth, depth-1] will result in one-hot representation with all 'off_value' values in the\n      output tensor.\n  \n      when axis = 0:\n      output[input[i, j, k], i, j, k] = 1 for all i, j, k and 0 otherwise.\n  \n      when axis = -1:\n      output[i, j, k, input[i, j, k]] = 1 for all i, j, k and 0 otherwise.",
    "inputs": [
      { "name": "indices", "type": "AnyTypeOf" },
      { "name": "depth", "type": "AnyTypeOf" },
      { "name": "values", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.OneHotEncoder",
    "summary": "ONNX OneHotEncoder operation",
    "description": "Replace each input element with an array of ones and zeros, where a single\n      one is placed at the index of the category that was passed in. The total category count\n      will determine the size of the extra dimension of the output array Y.<br>\n      For example, if we pass a tensor with a single value of 4, and a category count of 8,\n      the output will be a tensor with ``[0,0,0,0,1,0,0,0]``.<br>\n      This operator assumes every input feature is from the same set of categories.<br>\n      If the input is a tensor of float, int32, or double, the data will be cast\n      to integers and the cats_int64s category list will be used for the lookups.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "cats_int64s", "type": "OptionalAttr" },
      { "name": "cats_strings", "type": "OptionalAttr" },
      { "name": "zeros", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Optional",
    "summary": "ONNX Optional operation",
    "description": "Constructs an optional-type value containing either an empty optional of a certain type specified by the attribute,\n  or a non-empty value containing the input element.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "type", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.OptionalGetElement",
    "summary": "ONNX OptionalGetElement operation",
    "description": "If the input is a tensor or sequence type, it returns the input.\n  If the input is an optional type, it outputs the element in the input.\n  It is an error if the input is an empty optional-type (i.e. does not have an element) and the behavior is undefined in this case.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.OptionalHasElement",
    "summary": "ONNX OptionalHasElement operation",
    "description": "Returns true if (1) the input is an optional-type and contains an element,\n  or, (2) the input is a tensor or sequence type.\n  If the input is not provided or is an empty optional-type, this op returns false.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.Or",
    "summary": "ONNX Or operation",
    "description": "Returns the tensor resulted from performing the `or` logical operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "TensorOf" },
      { "name": "B", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "C", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.Pad",
    "summary": "ONNX Pad operation",
    "description": "Given a tensor containing the data to be padded (`data`), a tensor containing the number of start and end pad values for axis (`pads`), (optionally) a `mode`, and (optionally) `constant_value`,\n  a padded tensor (`output`) is generated.\n  \n  The three supported `modes` are (similar to corresponding modes supported by `numpy.pad`):\n  \n  1) `constant`(default) - pads with a given constant value as specified by `constant_value` (which defaults to 0, empty string, or False)\n  \n  2) `reflect` - pads with the reflection of the vector mirrored on the first and last values of the vector along each axis\n  \n  3) `edge` - pads with the edge values of array\n  \n  4) `wrap` - wrap-around padding as if the data tensor forms a torus\n  \n  \n  Example 1 (`constant` mode):\n  \n  Insert 0 pads to the beginning of the second dimension.\n  \n  ```\n  data = [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n  \n  pads = [0, 2, 0, 0]\n  \n  mode = 'constant'\n  \n  constant_value = 0.0\n  \n  output = [\n      [0.0, 0.0, 1.0, 1.2],\n      [0.0, 0.0, 2.3, 3.4],\n      [0.0, 0.0, 4.5, 5.7],\n  ]\n  ```\n  \n  Example 2 (`reflect` mode):\n  \n  ```\n  data = [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n  \n  pads = [0, 2, 0, 0]\n  \n  mode = 'reflect'\n  \n  output = [\n      [1.0, 1.2, 1.0, 1.2],\n      [2.3, 3.4, 2.3, 3.4],\n      [4.5, 5.7, 4.5, 5.7],\n  ]\n  ```\n  \n  Example 3 (`edge` mode):\n  \n  ```\n  data = [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n  \n  pads = [0, 2, 0, 0]\n  \n  mode = 'edge'\n  \n  output = [\n      [1.0, 1.0, 1.0, 1.2],\n      [2.3, 2.3, 2.3, 3.4],\n      [4.5, 4.5, 4.5, 5.7],\n  ]\n  ```\n  \n  Example 4 (`wrap` mode):\n  \n  ```\n  data = [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n  \n  pads = [2, 1, 1, 1]\n  \n  mode = 'wrap'\n  \n  output = [\n      [3.4, 2.3, 3.4, 2.3],\n      [5.7, 4.5, 5.7, 4.5],\n      [1.2, 1.0, 1.2, 1.0],\n      [3.4, 2.3, 3.4, 2.3],\n      [5.7, 4.5, 5.7, 4.5],\n      [1.2, 1.0, 1.2, 1.0],\n  ]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "pads", "type": "TensorOf" },
      { "name": "constant_value", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedStrAttr" }
    ],
    "category": "Transform"
  },
  {
    "name": "onnx.PadV11",
    "summary": "ONNX Pad operation",
    "description": "Given a tensor containing the data to be padded (`data`), a tensor containing the number of start and end pad values for axis (`pads`), (optionally) a `mode`, and (optionally) `constant_value`,\n  a padded tensor (`output`) is generated.\n  \n  The three supported `modes` are (similar to corresponding modes supported by `numpy.pad`):\n  \n  1) `constant`(default) - pads with a given constant value as specified by `constant_value` (which defaults to 0)\n  \n  2) `reflect` - pads with the reflection of the vector mirrored on the first and last values of the vector along each axis\n  \n  3) `edge` - pads with the edge values of array\n  \n  \n  Example 1 (`constant` mode):\n    Insert 0 pads to the beginning of the second dimension.\n  \n    data =\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ]\n  \n    pads = [0, 2, 0, 0]\n  \n    mode = 'constant'\n  \n    constant_value = 0.0\n  \n    output =\n    [\n        [0.0, 0.0, 1.0, 1.2],\n        [0.0, 0.0, 2.3, 3.4],\n        [0.0, 0.0, 4.5, 5.7],\n    ]\n  \n  \n  Example 2 (`reflect` mode):\n    data =\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ]\n  \n    pads = [0, 2, 0, 0]\n  \n    mode = 'reflect'\n  \n    output =\n    [\n        [1.0, 1.2, 1.0, 1.2],\n        [2.3, 3.4, 2.3, 3.4],\n        [4.5, 5.7, 4.5, 5.7],\n    ]\n  \n  \n  Example 3 (`edge` mode):\n    data =\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ]\n  \n    pads = [0, 2, 0, 0]\n  \n    mode = 'edge'\n  \n    output =\n    [\n        [1.0, 1.0, 1.0, 1.2],\n        [2.3, 2.3, 2.3, 3.4],\n        [4.5, 4.5, 4.5, 5.7],\n    ]",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "pads", "type": "TensorOf" },
      { "name": "constant_value", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.PadV13",
    "summary": "ONNX Pad operation",
    "description": "Given a tensor containing the data to be padded (`data`), a tensor containing the number of start and end pad values for axis (`pads`), (optionally) a `mode`, and (optionally) `constant_value`,\n  a padded tensor (`output`) is generated.\n  \n  The three supported `modes` are (similar to corresponding modes supported by `numpy.pad`):\n  \n  1) `constant`(default) - pads with a given constant value as specified by `constant_value` (which defaults to 0, empty string, or False)\n  \n  2) `reflect` - pads with the reflection of the vector mirrored on the first and last values of the vector along each axis\n  \n  3) `edge` - pads with the edge values of array\n  \n  \n  Example 1 (`constant` mode):\n    Insert 0 pads to the beginning of the second dimension.\n  \n    data =\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ]\n  \n    pads = [0, 2, 0, 0]\n  \n    mode = 'constant'\n  \n    constant_value = 0.0\n  \n    output =\n    [\n        [0.0, 0.0, 1.0, 1.2],\n        [0.0, 0.0, 2.3, 3.4],\n        [0.0, 0.0, 4.5, 5.7],\n    ]\n  \n  \n  Example 2 (`reflect` mode):\n    data =\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ]\n  \n    pads = [0, 2, 0, 0]\n  \n    mode = 'reflect'\n  \n    output =\n    [\n        [1.0, 1.2, 1.0, 1.2],\n        [2.3, 3.4, 2.3, 3.4],\n        [4.5, 5.7, 4.5, 5.7],\n    ]\n  \n  \n  Example 3 (`edge` mode):\n    data =\n    [\n        [1.0, 1.2],\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ]\n  \n    pads = [0, 2, 0, 0]\n  \n    mode = 'edge'\n  \n    output =\n    [\n        [1.0, 1.0, 1.0, 1.2],\n        [2.3, 2.3, 2.3, 3.4],\n        [4.5, 4.5, 4.5, 5.7],\n    ]",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "pads", "type": "TensorOf" },
      { "name": "constant_value", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.PadV18",
    "summary": "ONNX Pad operation",
    "description": "Given a tensor containing the data to be padded (`data`), a tensor containing the number of start and end pad values for axis (`pads`), (optionally) a `mode`, and (optionally) `constant_value`,\n  a padded tensor (`output`) is generated.\n  \n  The three supported `modes` are (similar to corresponding modes supported by `numpy.pad`):\n  \n  1) `constant`(default) - pads with a given constant value as specified by `constant_value` (which defaults to 0, empty string, or False)\n  \n  2) `reflect` - pads with the reflection of the vector mirrored on the first and last values of the vector along each axis\n  \n  3) `edge` - pads with the edge values of array\n  \n  \n  Example 1 (`constant` mode):\n  \n  Insert 0 pads to the beginning of the second dimension.\n  \n  ```\n  data = [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n  \n  pads = [0, 2, 0, 0]\n  \n  mode = 'constant'\n  \n  constant_value = 0.0\n  \n  output = [\n      [0.0, 0.0, 1.0, 1.2],\n      [0.0, 0.0, 2.3, 3.4],\n      [0.0, 0.0, 4.5, 5.7],\n  ]\n  ```\n  \n  Example 2 (`reflect` mode):\n  \n  ```\n  data = [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n  \n  pads = [0, 2, 0, 0]\n  \n  mode = 'reflect'\n  \n  output = [\n      [1.0, 1.2, 1.0, 1.2],\n      [2.3, 3.4, 2.3, 3.4],\n      [4.5, 5.7, 4.5, 5.7],\n  ]\n  ```\n  \n  Example 3 (`edge` mode):\n  \n  ```\n  data = [\n      [1.0, 1.2],\n      [2.3, 3.4],\n      [4.5, 5.7],\n  ]\n  \n  pads = [0, 2, 0, 0]\n  \n  mode = 'edge'\n  \n  output = [\n      [1.0, 1.0, 1.0, 1.2],\n      [2.3, 2.3, 2.3, 3.4],\n      [4.5, 4.5, 4.5, 5.7],\n  ]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "pads", "type": "TensorOf" },
      { "name": "constant_value", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.PadV2",
    "summary": "ONNX Pad operation",
    "description": "Given `data` tensor, pads, mode, and value.\n  Example:\n    Insert 0 pads to the beginning of the second dimension.\n    data = [\n        [1.0, 1.2],\n        [2.3, 3.4],\n        [4.5, 5.7],\n    ]\n    pads = [0, 2, 0, 0]\n    output = [\n        [\n            [0.0, 0.0, 1.0, 1.2],\n            [0.0, 0.0, 2.3, 3.4],\n            [0.0, 0.0, 4.5, 5.7],\n        ],\n    ]",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "value", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Pow",
    "summary": "ONNX Pow operation",
    "description": "Pow takes input data (Tensor<T>) and exponent Tensor, and\n  produces one output data (Tensor<T>) where the function `f(x) = x^exponent`,\n  is applied to the data tensor elementwise.\n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Z", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.PRelu",
    "summary": "ONNX PRelu operation",
    "description": "PRelu takes input data (Tensor<T>) and slope tensor as input, and produces one\n  output data (Tensor<T>) where the function `f(x) = slope * x for x < 0`,\n  `f(x) = x for x >= 0`., is applied to the data tensor elementwise.\n  This operator supports **unidirectional broadcasting** (tensor slope should be unidirectional broadcastable to input tensor X); for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "slope", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.PrintSignature",
    "summary": "ONNX Op to print type signature or data of its input operands",
    "description": "Print type signature or data of the input operands of this op.\n    The parameter op_name specifies a string to be printed before the tensors.\n    and usually the op_name and onnx_node_name are used.\n    This operation is introduced early so as to preserve the name of the original ONNX op.\n    The argument print_data control whether the data of the tensors to be printed.\n    When print_data == 1, the data of the tensor will be printed. Otherwise, just shape.\n    The argument input specifies the tensor to be printed. They could be a list\n    of the inputs and outputs of an ONNX op.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "op_name", "type": "StrAttr" },
      { "name": "print_data", "type": "SI64Attr" }
    ]
  },
  {
    "name": "onnx.QLinearConv",
    "summary": "ONNX QLinearConv operation",
    "description": "The convolution operator consumes a quantized input tensor, its scale and zero point,\n  a quantized filter, its scale and zero point, and output's scale and zero point,\n  and computes the quantized output. Each scale and zero-point pair must have same shape.\n  It means they must be either scalars (per tensor) or 1-D tensors (per output channel).\n  Each input or output and its related zero point must have same type.\n  When bias is present it must be quantized using scale = input scale * weight scale and\n  zero point as 0.",
    "inputs": [
      { "name": "x", "type": "AnyTypeOf" },
      { "name": "x_scale", "type": "TensorOf" },
      { "name": "x_zero_point", "type": "AnyTypeOf" },
      { "name": "w", "type": "AnyTypeOf" },
      { "name": "w_scale", "type": "TensorOf" },
      { "name": "w_zero_point", "type": "AnyTypeOf" },
      { "name": "y_scale", "type": "TensorOf" },
      { "name": "y_zero_point", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "auto_pad", "type": "DefaultValuedStrAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "group", "type": "DefaultValuedAttr" },
      { "name": "kernel_shape", "type": "OptionalAttr" },
      { "name": "pads", "type": "OptionalAttr" },
      { "name": "strides", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.QLinearMatMul",
    "summary": "ONNX QLinearMatMul operation",
    "description": "Matrix product that behaves like [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n  It consumes two quantized input tensors, their scales and zero points, scale and zero point of output,\n  and computes the quantized output. The quantization formula is y = saturate((x / y_scale) + y_zero_point).\n  For (x / y_scale), it is rounding to nearest ties to even. Refer to https://en.wikipedia.org/wiki/Rounding for details.\n  Scale and zero point must have same shape. They must be either scalar (per tensor) or N-D tensor\n  (per row for 'a' and per column for 'b'). Scalar refers to per tensor quantization whereas N-D refers to per row\n  or per column quantization. If the input is 2D of shape [M, K] then zero point and scale tensor may be\n  an M element vector [v_1, v_2, ..., v_M] for per row quantization and K element vector of shape [v_1, v_2, ..., v_K]\n  for per column quantization. If the input is N-D tensor with shape [D1, D2, M, K] then zero point and scale tensor may\n  have shape [D1, D2, M, 1] for per row quantization and shape [D1, D2, 1, K] for per column quantization.\n  Production must never overflow, and accumulation may overflow if and only if in 32 bits.",
    "inputs": [
      { "name": "a", "type": "AnyTypeOf" },
      { "name": "a_scale", "type": "TensorOf" },
      { "name": "a_zero_point", "type": "AnyTypeOf" },
      { "name": "b", "type": "AnyTypeOf" },
      { "name": "b_scale", "type": "TensorOf" },
      { "name": "b_zero_point", "type": "AnyTypeOf" },
      { "name": "y_scale", "type": "TensorOf" },
      { "name": "y_zero_point", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.QuantizeLinear",
    "summary": "ONNX QuantizeLinear operation",
    "description": "The linear quantization operator. It consumes a high precision tensor, a scale, and a zero point to compute the low precision / quantized tensor.\n  The scale factor and zero point must have same shape, and can be either a scalar for per-tensor / per layer quantization, or a 1-D tensor for per-axis quantization.\n  The quantization formula is `y = saturate ((x / y_scale) + y_zero_point)`.\n  For saturation, it saturates to [0, 255] if it's uint8, or [-128, 127] if it's int8.\n  For (x / y_scale), it's rounding to the nearest even. Refer to https://en.wikipedia.org/wiki/Rounding for details.\n  'y_zero_point' and 'y' must have same type.\n  'y_zero_point' is usually not used for quantization to float8e4m3fn, float8e4m3fnuz, float8e5m2, float8e5m2fnuz,\n  but the quantization formula remains the same for consistency and\n  the type of the attribute 'y_zero_point' still determines the quantization type.",
    "inputs": [
      { "name": "x", "type": "AnyTypeOf" },
      { "name": "y_scale", "type": "AnyTypeOf" },
      { "name": "y_zero_point", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "saturate", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.RandomNormal",
    "summary": "ONNX RandomNormal operation",
    "description": "Generate a tensor with random values drawn from a normal distribution. The shape\n  of the tensor is specified by the `shape` argument and the parameter of the normal distribution\n  specified by `mean` and `scale`.\n  \n  The data type is specified by the 'dtype' argument. The 'dtype' argument must\n  be one of the data types specified in the 'DataType' enum field in the\n  TensorProto message.",
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dtype", "type": "DefaultValuedAttr" },
      { "name": "mean", "type": "DefaultValuedAttr" },
      { "name": "scale", "type": "DefaultValuedAttr" },
      { "name": "seed", "type": "OptionalAttr" },
      { "name": "shape", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "onnx.RandomNormalLike",
    "summary": "ONNX RandomNormalLike operation",
    "description": "Generate a tensor with random values drawn from a normal distribution.\n  The shape of the output tensor is copied from the shape of the input tensor,\n  and the parameters of the normal distribution are specified by `mean` and `scale`.\n  \n  The data type is specified by the 'dtype' argument, or copied from the input tensor if not provided.\n  The 'dtype' argument must be one of the data types specified in the 'DataType' enum field in the\n  TensorProto message, and be valid as an output type.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dtype", "type": "OptionalAttr" },
      { "name": "mean", "type": "DefaultValuedAttr" },
      { "name": "scale", "type": "DefaultValuedAttr" },
      { "name": "seed", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.RandomUniform",
    "summary": "ONNX RandomUniform operation",
    "description": "Generate a tensor with random values drawn from a uniform distribution. The shape\n  of the tensor is specified by the `shape` argument and the range by `low` and `high`.\n  \n  The data type is specified by the 'dtype' argument. The 'dtype' argument must\n  be one of the data types specified in the 'DataType' enum field in the\n  TensorProto message.",
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dtype", "type": "DefaultValuedAttr" },
      { "name": "high", "type": "DefaultValuedAttr" },
      { "name": "low", "type": "DefaultValuedAttr" },
      { "name": "seed", "type": "OptionalAttr" },
      { "name": "shape", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "onnx.RandomUniformLike",
    "summary": "ONNX RandomUniformLike operation",
    "description": "Generate a tensor with random values drawn from a uniform distribution.\n  The shape of the output tensor is copied from the shape of the input tensor,\n  and the parameters of the uniform distribution are specified by `low` and `high`.\n  \n  The data type is specified by the 'dtype' argument, or copied from the input tensor if not provided.\n  The 'dtype' argument must be one of the data types specified in the 'DataType' enum field in the\n  TensorProto message and be valid as an output type.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dtype", "type": "OptionalAttr" },
      { "name": "high", "type": "DefaultValuedAttr" },
      { "name": "low", "type": "DefaultValuedAttr" },
      { "name": "seed", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Range",
    "summary": "ONNX Range operation",
    "description": "Generate a tensor containing a sequence of numbers that begin at `start` and extends by increments of `delta`\n  up to `limit` (exclusive).\n  \n  The number of elements in the output of range is computed as below:\n  \n  ```\n  number_of_elements = max( ceil( (limit - start) / delta ) , 0 )\n  ```\n  \n  The pseudocode determining the contents of the output is shown below:\n  \n  ```\n  for(int i=0; i<number_of_elements; ++i) {\n    output[i] =  start + (i * delta);\n  }\n  ```\n  \n  Example 1\n  \n  ```\n  Inputs: start = 3, limit = 9, delta = 3\n  Output: [3, 6]\n  ```\n  \n  Example 2\n  \n  ```\n  Inputs: start = 10, limit = 4, delta = -2\n  Output: [10, 8, 6]\n  ```",
    "inputs": [
      { "name": "start", "type": "AnyTypeOf" },
      { "name": "limit", "type": "AnyTypeOf" },
      { "name": "delta", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Reciprocal",
    "summary": "ONNX Reciprocal operation",
    "description": "Reciprocal takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the reciprocal is, y = 1/x, is applied to\n  the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.ReduceL1",
    "summary": "ONNX ReduceL1 operation",
    "description": "Computes the L1 norm of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 0.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceL1V13",
    "summary": "ONNX ReduceL1 operation",
    "description": "Computes the L1 norm of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 0.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceL2",
    "summary": "ONNX ReduceL2 operation",
    "description": "Computes the L2 norm of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 0.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceL2V13",
    "summary": "ONNX ReduceL2 operation",
    "description": "Computes the L2 norm of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 0.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceLogSum",
    "summary": "ONNX ReduceLogSum operation",
    "description": "Computes the log sum of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields minus infinity (if supported by the datatype) or undefined otherwise.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceLogSumExp",
    "summary": "ONNX ReduceLogSumExp operation",
    "description": "Computes the log sum exponent of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields minus infinity (if supported by the datatype) or undefined otherwise.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceLogSumExpV13",
    "summary": "ONNX ReduceLogSumExp operation",
    "description": "Computes the log sum exponent of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields minus infinity (if supported by the datatype) or undefined otherwise.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceLogSumV13",
    "summary": "ONNX ReduceLogSum operation",
    "description": "Computes the log sum of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields minus infinity (if supported by the datatype) or undefined otherwise.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceMax",
    "summary": "ONNX ReduceMax operation",
    "description": "Computes the max of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields minus infinity (if supported by the datatype) or the minimum value of the data type otherwise.\n  \n  \n  If the input data type is Boolean, the comparison should consider `False < True`.\n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceMaxV13",
    "summary": "ONNX ReduceMax operation",
    "description": "Computes the max of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields minus infinity (if supported by the datatype) or the minimum value of the data type otherwise.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceMaxV18",
    "summary": "ONNX ReduceMax operation",
    "description": "Computes the max of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields minus infinity (if supported by the datatype) or the minimum value of the data type otherwise.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceMean",
    "summary": "ONNX ReduceMean operation",
    "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields undefined.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceMeanV13",
    "summary": "ONNX ReduceMean operation",
    "description": "Computes the mean of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields undefined.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceMin",
    "summary": "ONNX ReduceMin operation",
    "description": "Computes the min of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields plus infinity (if supported by the datatype) or the maximum value of the data type otherwise.\n  \n  \n  If the input data type is Boolean, the comparison should consider `False < True`.\n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceMinV13",
    "summary": "ONNX ReduceMin operation",
    "description": "Computes the min of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields plus infinity (if supported by the datatype) or the maximum value of the data type otherwise.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceMinV18",
    "summary": "ONNX ReduceMin operation",
    "description": "Computes the min of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields plus infinity (if supported by the datatype) or the maximum value of the data type otherwise.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceProd",
    "summary": "ONNX ReduceProd operation",
    "description": "Computes the product of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 1.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceProdV13",
    "summary": "ONNX ReduceProd operation",
    "description": "Computes the product of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 1.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceSum",
    "summary": "ONNX ReduceSum operation",
    "description": "Computes the sum of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 0.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceSumSquare",
    "summary": "ONNX ReduceSumSquare operation",
    "description": "Computes the sum square of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 0.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "noop_with_empty_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceSumSquareV13",
    "summary": "ONNX ReduceSumSquare operation",
    "description": "Computes the sum square of the input tensor's elements along the provided axes. The resulting\n  tensor has the same rank as the input if `keepdims` equals 1. If `keepdims` equals 0, then\n  the resulting tensor has the reduced dimension pruned. Input tensors of rank zero are\n  valid. Reduction over an empty set of values yields 0.\n  \n  \n  The above behavior is similar to numpy, with the exception that numpy defaults `keepdims`\n  to `False` instead of `True`.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.ReduceSumV11",
    "summary": "ONNX ReduceSum operation",
    "description": "Computes the sum of the input tensor's element along the provided axes. The resulting\n  tensor has the same rank as the input if keepdims equals 1. If keepdims equal 0, then\n  the resulted tensor have the reduced dimension pruned.\n  \n  The above behavior is similar to numpy, with the exception that numpy defaults keepdims to\n  False instead of True.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "reduced", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Relu",
    "summary": "ONNX Relu operation",
    "description": "Relu takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the rectified linear function, y = max(0, x), is applied to\n  the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "category": "Activation"
  },
  {
    "name": "onnx.Reshape",
    "summary": "ONNX Reshape operation",
    "description": "Reshape the input tensor similar to numpy.reshape.\n  First input is the data tensor, second input is a shape tensor which specifies the output shape. It outputs the reshaped tensor.\n  At most one dimension of the new shape can be -1. In this case, the value is\n  inferred from the size of the tensor and the remaining dimensions. A dimension\n  could also be 0, in which case the actual dimension value is unchanged (i.e. taken\n  from the input tensor). If 'allowzero' is set, and the new shape includes 0, the\n  dimension will be set explicitly to zero (i.e. not taken from input tensor).\n  Shape (second input) could be an empty shape, which means converting to a scalar.\n  The input tensor's shape and the output tensor's shape are required to have the same number of elements.\n  \n  If the attribute 'allowzero' is set, it is invalid for the specified shape to\n  contain both a zero value and -1, as the value of the dimension corresponding\n  to -1 cannot be determined uniquely.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "shape", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "reshaped", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "allowzero", "type": "DefaultValuedAttr" }
    ],
    "category": "Shape"
  },
  {
    "name": "onnx.Resize",
    "summary": "ONNX Resize operation",
    "description": "Resize the input tensor. In general, it calculates every value in the output tensor as a weighted average of neighborhood (a.k.a. sampling locations) in the input tensor.\n  Each dimension value of the output tensor is:\n  ```\n  output_dimension = floor(input_dimension * (roi_end - roi_start) * scale)\n  ```\n  if input \\\\\"sizes\\\\\" is not specified.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "roi", "type": "AnyTypeOf" },
      { "name": "scales", "type": "AnyTypeOf" },
      { "name": "sizes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "antialias", "type": "DefaultValuedAttr" },
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "coordinate_transformation_mode", "type": "DefaultValuedStrAttr" },
      { "name": "cubic_coeff_a", "type": "DefaultValuedAttr" },
      { "name": "exclude_outside", "type": "DefaultValuedAttr" },
      { "name": "extrapolation_value", "type": "DefaultValuedAttr" },
      { "name": "keep_aspect_ratio_policy", "type": "DefaultValuedStrAttr" },
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "nearest_mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.ResizeV10",
    "summary": "ONNX Resize operation",
    "description": "Resize the input tensor.\n  Each dimension value of the output tensor is:\n    output_dimension = floor(input_dimension * scale).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "scales", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.ResizeV11",
    "summary": "ONNX Resize operation",
    "description": "Resize the input tensor. In general, it calculates every value in the output tensor as a weighted average of neighborhood (a.k.a. sampling locations) in the input tensor.\n  Each dimension value of the output tensor is:\n    output_dimension = floor(input_dimension * (roi_end - roi_start) * scale) if input \\\\\"sizes\\\\\" is not specified.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "roi", "type": "AnyTypeOf" },
      { "name": "scales", "type": "TensorOf" },
      { "name": "sizes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "coordinate_transformation_mode", "type": "DefaultValuedStrAttr" },
      { "name": "cubic_coeff_a", "type": "DefaultValuedAttr" },
      { "name": "exclude_outside", "type": "DefaultValuedAttr" },
      { "name": "extrapolation_value", "type": "DefaultValuedAttr" },
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "nearest_mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.ResizeV13",
    "summary": "ONNX Resize operation",
    "description": "Resize the input tensor. In general, it calculates every value in the output tensor as a weighted average of neighborhood (a.k.a. sampling locations) in the input tensor.\n  Each dimension value of the output tensor is:\n    output_dimension = floor(input_dimension * (roi_end - roi_start) * scale) if input \\\\\"sizes\\\\\" is not specified.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "roi", "type": "AnyTypeOf" },
      { "name": "scales", "type": "AnyTypeOf" },
      { "name": "sizes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "coordinate_transformation_mode", "type": "DefaultValuedStrAttr" },
      { "name": "cubic_coeff_a", "type": "DefaultValuedAttr" },
      { "name": "exclude_outside", "type": "DefaultValuedAttr" },
      { "name": "extrapolation_value", "type": "DefaultValuedAttr" },
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "nearest_mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.ResizeV18",
    "summary": "ONNX Resize operation",
    "description": "Resize the input tensor. In general, it calculates every value in the output tensor as a weighted average of neighborhood (a.k.a. sampling locations) in the input tensor.\n  Each dimension value of the output tensor is: <br/>\n    `output_dimension = floor(input_dimension * (roi_end - roi_start) * scale)` <br/>\n  if input \\\\\"sizes\\\\\" is not specified.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "roi", "type": "AnyTypeOf" },
      { "name": "scales", "type": "AnyTypeOf" },
      { "name": "sizes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "antialias", "type": "DefaultValuedAttr" },
      { "name": "axes", "type": "OptionalAttr" },
      { "name": "coordinate_transformation_mode", "type": "DefaultValuedStrAttr" },
      { "name": "cubic_coeff_a", "type": "DefaultValuedAttr" },
      { "name": "exclude_outside", "type": "DefaultValuedAttr" },
      { "name": "extrapolation_value", "type": "DefaultValuedAttr" },
      { "name": "keep_aspect_ratio_policy", "type": "DefaultValuedStrAttr" },
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "nearest_mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.Return",
    "summary": "Function return operation",
    "description": "The `onnx.Return` operation represents a return operation within a function.\n    The operation takes variable number of operands and produces no results.\n    The operand number and types must match the signature of the function\n    that contains the operation, with the exception that shaped types may have\n    more specific shapes than the function signature result types, which allows\n    rewrites of defining ops of operands to make their result shapes more specific.\n    This operation terminates a func::FuncOp in the ONNX dialect and is replaced\n    by func::ReturnOp in StandardFuncReturnPass before lowering to Krnl or other\n    dialects.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "onnx.ReverseSequence",
    "summary": "ONNX ReverseSequence operation",
    "description": "Reverse batch of sequences having different lengths specified by `sequence_lens`.\n  \n  For each slice i iterating on batch axis, the operator reverses the first sequence_lens[i] elements on time axis,\n  and copies elements whose index's beyond sequence_lens[i] to the output. So the output slice i contains reversed\n  sequences on the first sequence_lens[i] elements, then have original values copied for the other elements.\n  \n  Example 1:\n    input = [[0.0, 4.0, 8.0,  12.0],\n             [1.0, 5.0, 9.0,  13.0],\n             [2.0, 6.0, 10.0, 14.0],\n             [3.0, 7.0, 11.0, 15.0]]\n    sequence_lens = [4, 3, 2, 1]\n    time_axis = 0\n    batch_axis = 1\n  \n    output = [[3.0, 6.0, 9.0,  12.0],\n              [2.0, 5.0, 8.0,  13.0],\n              [1.0, 4.0, 10.0, 14.0],\n              [0.0, 7.0, 11.0, 15.0]]\n  \n  Example 2:\n    input = [[0.0,  1.0,  2.0,  3.0 ],\n             [4.0,  5.0,  6.0,  7.0 ],\n             [8.0,  9.0,  10.0, 11.0],\n             [12.0, 13.0, 14.0, 15.0]]\n    sequence_lens = [1, 2, 3, 4]\n    time_axis = 1\n    batch_axis = 0\n  \n    output = [[0.0,  1.0,  2.0,  3.0 ],\n              [5.0,  4.0,  6.0,  7.0 ],\n              [10.0, 9.0,  8.0,  11.0],\n              [15.0, 14.0, 13.0, 12.0]]",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "sequence_lens", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "batch_axis", "type": "DefaultValuedAttr" },
      { "name": "time_axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.RMSLayerNormalization",
    "summary": "ONNX RMSLayerNormalization operation",
    "description": "This is RMS layer normalization defined in ONNX as function.\n        The overall computation can be split into two stages.\n        The first stage is an approximate standardization, which makes the\n        normalized elements have zero mean and unit variances.\n        See Equation (4) in [this paper](https://arxiv.org/pdf/1910.07467.pdf).\n        The computation required by standardization can be\n        described by the following equations.\n        ```\n        DD = Mul(X, X)\n        Var = ReduceMean<axes=normalized_axes>(DD)\n        VarEps = Add(Var, epsilon)\n        StdDev = Sqrt(VarEps)\n        InvStdDev = Reciprocal(StdDev)\n        Normalized = Mul(X, InvStdDev)\n        ```\n        where `normalized_axes` is `[axis, ..., rank of X - 1]`.\n        The variables `Var` and `StdDev` stand for approximate variance and\n        standard deviation, respectively.\n        Depending on `stash_type` attribute, the actual computation\n        must happen in different floating-point precision.\n        For example, if `stash_type` is 1, this operator casts\n        all input variables to 32-bit float, perform the computation, and\n        finally cast `Normalized` back to the original type of `X`.\n        The second stage then scales and shifts the outcome of the\n        first stage using\n        ```\n        NormalizedScaled = Mul(Normalized, Scale)\n        Y = Add(NormalizedScaled, B)\n        ```\n        The second stage doesn't depends on `stash_type`.\n        All equations are in [this syntax](https://github.com/onnx/onnx/blob/main/docs/Syntax.md).\n        The same variable (i.e., input, output, and attribute) uses\n        the same name in the equations above and this operator's definition.\n        Let `d[i]` indicate the i-th dimension of `X`.\n        If `X`'s shape is `[d[0], ..., d[axis-1], d[axis], ..., d[rank-1]]`,\n        the shape of `Mean` and `InvStdDev` is `[d[0], ..., d[axis-1], 1, ..., 1]`.\n        `Y` and `X` have the same shape.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "Scale", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "InvStdDev", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "stash_type", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.RNN",
    "summary": "ONNX RNN operation",
    "description": "Computes an one-layer simple RNN. This operator is usually supported\n  via some custom implementation such as CuDNN.\n  \n  Notations:\n  \n  * `X` - input tensor\n  * `i` - input gate\n  * `t` - time step (t-1 means previous time step)\n  * `Wi` - W parameter weight matrix for input gate\n  * `Ri` - R recurrence weight matrix for input gate\n  * `Wbi` - W parameter bias vector for input gate\n  * `Rbi` - R parameter bias vector for input gate\n  * `WBi` - W parameter weight matrix for backward input gate\n  * `RBi` - R recurrence weight matrix for backward input gate\n  * `WBbi` - WR bias vectors for backward input gate\n  * `RBbi` - RR bias vectors for backward input gate\n  * `H` - Hidden state\n  * `num_directions` - 2 if direction == bidirectional else 1\n  \n  Activation functions:\n  \n  * Relu(x)                - max(0, x)\n  * Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})\n  * Sigmoid(x)             - 1/(1 + e^{-x})\n  \n  NOTE: Below are optional\n  \n  * Affine(x)              - alpha*x + beta\n  * LeakyRelu(x)           - x if x >= 0 else alpha * x\n  * ThresholdedRelu(x)     - x if x >= alpha else 0\n  * ScaledTanh(x)          - alpha*Tanh(beta*x)\n  * HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)\n  * Elu(x)                 - x if x >= 0 else alpha*(e^x - 1)\n  * Softsign(x)            - x/(1 + |x|)\n  * Softplus(x)            - log(1 + e^x)\n  \n  Equations (Default: f=Tanh):\n  \n  * Ht = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Wbi + Rbi)\n  This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "W", "type": "AnyTypeOf" },
      { "name": "R", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" },
      { "name": "sequence_lens", "type": "AnyTypeOf" },
      { "name": "initial_h", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "Y_h", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "activation_alpha", "type": "OptionalAttr" },
      { "name": "activation_beta", "type": "OptionalAttr" },
      { "name": "activations", "type": "DefaultValuedAttr" },
      { "name": "clip", "type": "OptionalAttr" },
      { "name": "direction", "type": "DefaultValuedStrAttr" },
      { "name": "hidden_size", "type": "OptionalAttr" },
      { "name": "layout", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.RoiAlign",
    "summary": "ONNX RoiAlign operation",
    "description": "Region of Interest (RoI) align operation described in the\n  [Mask R-CNN paper](https://arxiv.org/abs/1703.06870).\n  RoiAlign consumes an input tensor X and region of interests (rois)\n  to apply pooling across each RoI; it produces a 4-D tensor of shape\n  (num_rois, C, output_height, output_width).\n  \n  RoiAlign is proposed to avoid the misalignment by removing\n  quantizations while converting from original image into feature\n  map and from feature map into RoI feature; in each ROI bin,\n  the value of the sampled locations are computed directly\n  through bilinear interpolation.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "rois", "type": "AnyTypeOf" },
      { "name": "batch_indices", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "coordinate_transformation_mode", "type": "DefaultValuedStrAttr" },
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "output_height", "type": "DefaultValuedAttr" },
      { "name": "output_width", "type": "DefaultValuedAttr" },
      { "name": "sampling_ratio", "type": "DefaultValuedAttr" },
      { "name": "spatial_scale", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Round",
    "summary": "ONNX Round operation",
    "description": "Round takes one input Tensor and rounds the values, element-wise, meaning\n  it finds the nearest integer for each value.\n  In case of halves, the rule is to round them to the nearest even integer.\n  If input x is integral, +0, -0, NaN,  or infinite, x itself is returned.\n  The output tensor has the same shape and type as the input.\n  \n  Examples:\n  ```\n  round([0.9]) = [1.0]\n  round([2.5]) = [2.0]\n  round([2.3]) = [2.0]\n  round([1.5]) = [2.0]\n  round([-4.5]) = [-4.0]\n  ```",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Scaler",
    "summary": "ONNX Scaler operation",
    "description": "Rescale input data, for example to standardize features by removing the mean and scaling to unit variance.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "offset", "type": "OptionalAttr" },
      { "name": "scale", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Scan",
    "summary": "ONNX Scan operation",
    "description": "Scan can be used to iterate over one or more scan_input tensors,\n  constructing zero or more scan_output tensors. It combines ideas from general recurrences,\n  functional programming constructs such as scan, fold, map, and zip, and is intended to enable\n  generalizations of RNN-like constructs for sequence-to-sequence processing.\n  Other tensors (referred to as state_variables here) can be used to carry a state\n  when iterating from one element to another (similar to hidden-state in RNNs, also referred\n  to as loop-carried dependences in the context of loops).\n  Many common usages involve a single scan_input tensor (where functionality\n  similar to scan, fold and map can be obtained). When more than one scan_input is used,\n  a behavior similar to zip is obtained.\n  \n  The attribute body must be a graph, specifying the computation to be performed in\n  every iteration. It takes as input the current values of the state_variables and\n  the current iterated element of the scan_inputs. It must return the (updated) values\n  of the state_variables and zero or more scan_output_element tensors. The values of the\n  scan_output_element tensors are concatenated over all the iterations to produce the\n  scan_output values of the scan construct (similar to the concatenated intermediate\n  hidden-state values of RNN-like constructs). All the output tensors (state_variables as\n  well as scan_output_element tensors) are required to have the same shape in each iteration\n  of the loop (a restriction imposed to enable efficient memory allocation).\n  \n  Note that the iterated element passed to the body subgraph does not have a sequence\n  axis. It will have a rank one less than the rank of the corresponding scan_input.\n  \n  The scan operation returns the final values of the state_variables as well as the\n  scan_outputs.\n  \n  The optional attribute scan_input_directions specifies the direction (forward or backward)\n  for each scan input. If this attribute is omitted, all sequences are scanned in the forward\n  direction. A bidirectional scan may be performed by specifying the same tensor input twice\n  in the scan_inputs, once with a forward direction, and once with a backward direction.\n  \n  The scan_output of the operation is produced by concatenating the scan_output_element\n  values produced by the body in each iteration.  The optional attribute scan_output_directions\n  specifies the direction in which scan_output is constructed (by appending or prepending the\n  scan_output_element to scan_output in each iteration) for each scan_output. If this attribute\n  is omitted, the scan_output_element is appended to the scan_output in each iteration.\n  \n  The optional attribute scan_input_axes specifies the axis to be scanned for each scan_input.\n  If omitted, every scan_input will be scanned in axis 0. For example, if axis 0 is the\n  batch axis and axis 1 is the time axis (to be scanned), specify an axis value of 1.\n  Note that scanning a non-zero axis may be less efficient than scanning axis zero.\n  \n  The optional attribute scan_output_axes specifies the axis along which the scan_outputs\n  are accumulated for each scan_output. For example, if axis 1 is the time axis (to be\n  scanned) for both inputs and outputs, specify a scan_input axis and scan_output axis\n  value of 1.\n  \n  Note that because of the ONNX restriction that only the last parameter of an operator can\n  be variadic, the initial-states and scan-inputs are listed together as one input parameter.\n  Similarly, the final-states and scan-outputs are listed together as one output parameter.\n  The attribute num_scan_inputs indicates the number M of scan-inputs.\n  \n  The behavior of\n  \n      Scan <\n          num_scan_inputs = m,\n          body = loop-body,\n          scan_input_axes = [axis_1, ..., axis_m]\n      > (init_1, ..., init_n, scan_1, ..., scan_m)\n  \n  is equivalent to the following pseudo-code:\n  \n      // scan_i.shape[axis_i] denotes the (max) sequence-length of scan_i\n      // scan_i.shape[axis_i] is required to be equal to scan_j.shape[axis_j] for all i,j.\n      sequence_length = scan_1.shape[axis_1];\n  \n      // initialize state-variables\n      st_1 = init_1; ... st_n = init_n;\n      // initialize scan-output variables: [] denotes an empty tensor\n      scan_out_1 = []; ...; scan_out_k = [];\n      // identify number of iterations:\n  \n      // execute loop\n      for (int t = 0; t < sequence_length; ++t) {\n          // generate the scan-input elements: the notation T<axis=k>[t] indicates the sub-tensor\n          // of rank one less than T obtained by indexing T at position t along axis k.\n          si_1 = scan_1<axis=axis_1>[t];\n          ... ;\n          si_m = scan_m<axis=axis_m>[t];\n          // execute loop-body\n          st_1, ..., st_n, so_1, ..., so_k = loop-body(st_1, ..., st_n, si_1, ..., si_m)\n          // accumulate the scan-output elements\n          scan_out_1 = Concat<axis=0>(scan_out_1, so_1); ... ; scan_out_k = Concat<axis=0>(scan_out_k, so_k);\n      }\n  \n      return st_1, ..., st_n, scan_out_1, ..., scan_out_k;\n  \n  *Sample usage: Encoding RNN using a Scan*\n  \n  The following example shows how a simple RNN over an input tensor %X, with weight tensor %Wi,\n  recurrence weight tensor %Ri, bias tensors %Wbi and %Rbi, and initial hidden-state %H_0 can\n  be encoded as a ScanLoop. Note that the loop-body is a nested graph, and it directly computes\n  %Wi, %Ri, %Wbi, and %Rbi (typically constants or initializers in the body graph). If these\n  values are computed in the outer graph, they need to be passed in as extra state_variables.\n  \n      graph rnn-encoding {\n        %H_0 = ...\n        %X = ...\n        %Y_h, %Y = Scan[body = <graph rnn-cell-1>, num_scan_inputs=1](%H_0, %X)\n        return %Y, %Y_h\n      }\n  \n      graph rnn-cell-1 (\n        %H_tminus1[FLOAT, tensor]\n        %X_t[FLOAT, tensor]\n      ) {\n        %Wi = ...\n        %Ri = ...\n        %Wbi = ...\n        %Rbi = ...\n        %t1 = X_t * (Wi^T)\n        %t2 = H_tminus1*(Ri^T)\n        %t3 = Add(%t1, %t2)\n        %t4 = Add(%t3, %Wbi)\n        %t5 = Add(%t4, %Rbi)\n        %Ht = Tanh(%t5)\n        %Accumulate = Identity(%Ht)\n        return %Ht, %Accumulate\n      }",
    "inputs": [
      { "name": "initial_state_and_scan_inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "final_state_and_scan_outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "num_scan_inputs", "type": "SI64Attr" },
      { "name": "scan_input_axes", "type": "OptionalAttr" },
      { "name": "scan_input_directions", "type": "OptionalAttr" },
      { "name": "scan_output_axes", "type": "OptionalAttr" },
      { "name": "scan_output_directions", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Scatter",
    "summary": "ONNX Scatter operation",
    "description": "This operator is deprecated. Please use ScatterElements, which provides the same functionality.\n  \n  Scatter takes three inputs `data`, `updates`, and `indices` of the same\n  rank r >= 1 and an optional attribute axis that identifies an axis of `data`\n  (by default, the outer-most axis, that is axis 0). The output of the operation\n  is produced by creating a copy of the input `data`, and then updating its value\n  to values specified by `updates` at specific index positions specified by\n  `indices`. Its output shape is the same as the shape of `data`.\n  \n  For each entry in `updates`, the target index in `data` is obtained by combining\n  the corresponding entry in `indices` with the index of the entry itself: the\n  index-value for dimension = axis is obtained from the value of the corresponding\n  entry in `indices` and the index-value for dimension != axis is obtained from the\n  index of the entry itself.\n  \n  For instance, in a 2-D tensor case, the update corresponding to the [i][j] entry\n  is performed as below:\n  ```\n    output[indices[i][j]][j] = updates[i][j] if axis = 0,\n    output[i][indices[i][j]] = updates[i][j] if axis = 1,\n  ```\n  \n  This operator is the inverse of GatherElements. It is similar to Torch's Scatter operation.\n  \n  Example 1:\n  ```\n    data = [\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n    ]\n    indices = [\n        [1, 0, 2],\n        [0, 2, 1],\n    ]\n    updates = [\n        [1.0, 1.1, 1.2],\n        [2.0, 2.1, 2.2],\n    ]\n    output = [\n        [2.0, 1.1, 0.0]\n        [1.0, 0.0, 2.2]\n        [0.0, 2.1, 1.2]\n    ]\n  ```\n  Example 2:\n  ```\n    data = [[1.0, 2.0, 3.0, 4.0, 5.0]]\n    indices = [[1, 3]]\n    updates = [[1.1, 2.1]]\n    axis = 1\n    output = [[1.0, 1.1, 3.0, 2.1, 5.0]]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "indices", "type": "AnyTypeOf" },
      { "name": "updates", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ],
    "category": "Tensor"
  },
  {
    "name": "onnx.ScatterElements",
    "summary": "ONNX ScatterElements operation",
    "description": "ScatterElements takes three inputs `data`, `updates`, and `indices` of the same\n  rank r >= 1 and an optional attribute axis that identifies an axis of `data`\n  (by default, the outer-most axis, that is axis 0). The output of the operation\n  is produced by creating a copy of the input `data`, and then updating its value\n  to values specified by `updates` at specific index positions specified by\n  `indices`. Its output shape is the same as the shape of `data`.\n  \n  For each entry in `updates`, the target index in `data` is obtained by combining\n  the corresponding entry in `indices` with the index of the entry itself: the\n  index-value for dimension = axis is obtained from the value of the corresponding\n  entry in `indices` and the index-value for dimension != axis is obtained from the\n  index of the entry itself.\n  \n  `reduction` allows specification of an optional reduction operation, which is applied to all values in `updates`\n  tensor into `output` at the specified `indices`.\n  In cases where `reduction` is set to \\\"none\\\", indices should not have duplicate entries: that is, if idx1 != idx2,\n  then indices[idx1] != indices[idx2]. For instance, in a 2-D tensor case, the update\n  corresponding to the [i][j] entry is performed as below:\n  ```\n  output[indices[i][j]][j] = updates[i][j] if axis = 0,\n  output[i][indices[i][j]] = updates[i][j] if axis = 1,\n  ```\n  When `reduction` is set to some reduction function `f`, the update corresponding to the [i][j] entry is performed as below:\n  ```\n  output[indices[i][j]][j] = f(output[indices[i][j]][j], updates[i][j]) if axis = 0,\n  output[i][indices[i][j]] = f(output[i][indices[i][j]], updates[i][j]) if axis = 1,\n  ```\n  where the `f` is `+`, `*`, `max` or `min` as specified.\n  \n  This operator is the inverse of GatherElements. It is similar to Torch's Scatter operation.\n  \n  (Opset 18 change): Adds max/min to the set of allowed reduction ops.\n  \n  Example 1:\n  ```\n  data = [\n      [0.0, 0.0, 0.0],\n      [0.0, 0.0, 0.0],\n      [0.0, 0.0, 0.0],\n  ]\n  indices = [\n      [1, 0, 2],\n      [0, 2, 1],\n  ]\n  updates = [\n      [1.0, 1.1, 1.2],\n      [2.0, 2.1, 2.2],\n  ]\n  output = [\n      [2.0, 1.1, 0.0]\n      [1.0, 0.0, 2.2]\n      [0.0, 2.1, 1.2]\n  ]\n  ```\n  Example 2:\n  ```\n  data = [[1.0, 2.0, 3.0, 4.0, 5.0]]\n  indices = [[1, 3]]\n  updates = [[1.1, 2.1]]\n  axis = 1\n  output = [[1.0, 1.1, 3.0, 2.1, 5.0]]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "indices", "type": "AnyTypeOf" },
      { "name": "updates", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "reduction", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.ScatterND",
    "summary": "ONNX ScatterND operation",
    "description": "ScatterND takes three inputs `data` tensor of rank r >= 1, `indices` tensor of rank q >= 1,\n  and `updates` tensor of rank q + r - indices.shape[-1] - 1. The output of the operation\n  is produced by creating a copy of the input `data`, and then updating its value to values\n  specified by `updates` at specific index positions specified by `indices`. Its output shape\n  is the same as the shape of `data`.\n  \n  `indices` is an integer tensor. Let k denote indices.shape[-1], the last dimension in the shape of `indices`.\n  `indices` is treated as a (q-1)-dimensional tensor of k-tuples, where each k-tuple is a partial-index into `data`.\n  Hence, k can be a value at most the rank of `data`. When k equals rank(data), each update entry specifies an\n  update to a single element of the tensor. When k is less than rank(data) each update entry specifies an\n  update to a slice of the tensor. Index values are allowed to be negative, as per the usual\n  convention for counting backwards from the end, but are expected in the valid range.\n  \n  `updates` is treated as a (q-1)-dimensional tensor of replacement-slice-values. Thus, the\n  first (q-1) dimensions of updates.shape must match the first (q-1) dimensions of indices.shape.\n  The remaining dimensions of `updates` correspond to the dimensions of the\n  replacement-slice-values. Each replacement-slice-value is a (r-k) dimensional tensor,\n  corresponding to the trailing (r-k) dimensions of `data`.  Thus, the shape of `updates`\n  must equal indices.shape[0:q-1] ++ data.shape[k:r-1], where ++ denotes the concatenation\n  of shapes.\n  \n  The `output` is calculated via the following equation:\n  \n  ```\n  output = np.copy(data)\n  update_indices = indices.shape[:-1]\n  for idx in np.ndindex(update_indices):\n      output[indices[idx]] = updates[idx]\n  ```\n  \n  The order of iteration in the above loop is not specified.\n  In particular, indices should not have duplicate entries: that is, if idx1 != idx2, then indices[idx1] != indices[idx2].\n  This ensures that the output value does not depend on the iteration order.\n  \n  `reduction` allows specification of an optional reduction operation, which is applied to all values in `updates`\n  tensor into `output` at the specified `indices`.\n  In cases where `reduction` is set to \\\"none\\\", indices should not have duplicate entries: that is, if idx1 != idx2,\n  then indices[idx1] != indices[idx2]. This ensures that the output value does not depend on the iteration order.\n  When `reduction` is set to some reduction function `f`, `output` is calculated as follows:\n  \n  ```\n  output = np.copy(data)\n  update_indices = indices.shape[:-1]\n  for idx in np.ndindex(update_indices):\n      output[indices[idx]] = f(output[indices[idx]], updates[idx])\n  ```\n  \n  where the `f` is `+`, `*`, `max` or `min` as specified.\n  \n  This operator is the inverse of GatherND.\n  \n  (Opset 18 change): Adds max/min to the set of allowed reduction ops.\n  \n  Example 1:\n  ```\n  data    = [1, 2, 3, 4, 5, 6, 7, 8]\n  indices = [[4], [3], [1], [7]]\n  updates = [9, 10, 11, 12]\n  output  = [1, 11, 3, 10, 9, 6, 7, 12]\n  ```\n  \n  Example 2:\n  ```\n  data    = [[[1, 2, 3, 4], [5, 6, 7, 8], [8, 7, 6, 5], [4, 3, 2, 1]],\n              [[1, 2, 3, 4], [5, 6, 7, 8], [8, 7, 6, 5], [4, 3, 2, 1]],\n              [[8, 7, 6, 5], [4, 3, 2, 1], [1, 2, 3, 4], [5, 6, 7, 8]],\n              [[8, 7, 6, 5], [4, 3, 2, 1], [1, 2, 3, 4], [5, 6, 7, 8]]]\n  indices = [[0], [2]]\n  updates = [[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],\n              [[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]]]\n  output  = [[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],\n              [[1, 2, 3, 4], [5, 6, 7, 8], [8, 7, 6, 5], [4, 3, 2, 1]],\n              [[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],\n              [[8, 7, 6, 5], [4, 3, 2, 1], [1, 2, 3, 4], [5, 6, 7, 8]]]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "indices", "type": "TensorOf" },
      { "name": "updates", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "reduction", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.Selu",
    "summary": "ONNX Selu operation",
    "description": "Selu takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the scaled exponential linear unit function,\n  `y = gamma * (alpha * e^x - alpha) for x <= 0`, `y = gamma * x for x > 0`,\n  is applied to the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" },
      { "name": "gamma", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.SequenceAt",
    "summary": "ONNX SequenceAt operation",
    "description": "Outputs a tensor copy from the tensor at 'position' in 'input_sequence'.\n  Accepted range for 'position' is in `[-n, n - 1]`, where `n` is the number of tensors in 'input_sequence'.\n  Negative value means counting positions from the back.",
    "inputs": [
      { "name": "input_sequence", "type": "AnyTypeOf" },
      { "name": "position", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "tensor", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.SequenceConstruct",
    "summary": "ONNX SequenceConstruct operation",
    "description": "Construct a tensor sequence containing 'inputs' tensors.\n  All tensors in 'inputs' must have the same data type.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output_sequence", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.SequenceEmpty",
    "summary": "ONNX SequenceEmpty operation",
    "description": "Construct an empty tensor sequence, with given data type.",
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "dtype", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.SequenceErase",
    "summary": "ONNX SequenceErase operation",
    "description": "Outputs a tensor sequence that removes the tensor at 'position' from 'input_sequence'.\n  Accepted range for 'position' is in `[-n, n - 1]`, where `n` is the number of tensors in 'input_sequence'.\n  Negative value means counting positions from the back.\n  'position' is optional, by default it erases the last tensor from 'input_sequence'.",
    "inputs": [
      { "name": "input_sequence", "type": "AnyTypeOf" },
      { "name": "position", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output_sequence", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.SequenceInsert",
    "summary": "ONNX SequenceInsert operation",
    "description": "Outputs a tensor sequence that inserts 'tensor' into 'input_sequence' at 'position'.\n  'tensor' must have the same data type as 'input_sequence'.\n  Accepted range for 'position' is in `[-n, n]`, where `n` is the number of tensors in 'input_sequence'.\n  Negative value means counting positions from the back.\n  'position' is optional, by default it inserts 'tensor' to the back of 'input_sequence'.",
    "inputs": [
      { "name": "input_sequence", "type": "AnyTypeOf" },
      { "name": "tensor", "type": "AnyTypeOf" },
      { "name": "position", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output_sequence", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.SequenceLength",
    "summary": "ONNX SequenceLength operation",
    "description": "Produces a scalar(tensor of empty shape) containing the number of tensors in 'input_sequence'.",
    "inputs": [
      { "name": "input_sequence", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "length", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.SequenceMap",
    "summary": "ONNX SequenceMap operation",
    "description": "Applies a sub-graph to each sample in the input sequence(s).\n  \n  Inputs can be either tensors or sequences, with the exception of the first input which must\n  be a sequence. The length of the first input sequence will determine the number of samples in the\n  outputs. Any other sequence inputs should have the same number of samples. The number of inputs\n  and outputs, should match the one of the subgraph.\n  \n  For each i-th element in the output, a sample will be extracted from the input sequence(s) at\n  the i-th position and the sub-graph will be applied to it.\n  The outputs will contain the outputs of the sub-graph for each sample, in the same order as in\n  the input.\n  \n  This operator assumes that processing each sample is independent and could executed in parallel\n  or in any order. Users cannot expect any specific ordering in which each subgraph is computed.",
    "inputs": [
      { "name": "input_sequence", "type": "AnyTypeOf" },
      { "name": "additional_inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "out_sequence", "type": "Variadic" }
    ]
  },
  {
    "name": "onnx.Shape",
    "summary": "ONNX Shape operation",
    "description": "Takes a tensor as input and outputs an 1D int64 tensor containing the shape of the input tensor.\n  Optional attributes start and end can be used to compute a slice of the input tensor's shape.\n  If start axis is omitted, the slice starts from axis 0.\n  The end axis, if specified, is exclusive (and the returned value will not include the size of that axis).\n  If the end axis is omitted, the axes upto the last one will be included.\n  Negative axes indicate counting back from the last axis.\n  Note that axes will be clamped to the range [0, r-1], where r is the\n  rank of the input tensor if they are out-of-range (after adding r in the case of\n  negative axis). Thus, specifying any end value > r is equivalent to specifying an end\n  value of r, and specifying any start value < -r is equivalent to specifying a start\n  value of 0.\n  \n  Examples:\n  \n  ```\n  Input tensor with shape: [2, 3, 4]\n  No attributes specified.\n  Output: [2, 3, 4]\n  ```\n  \n  ```\n  Input tensor with shape: [2, 3, 4]\n  start: -1\n  Output: [4]\n  ```\n  \n  ```\n  Input tensor with shape: [2, 3, 4]\n  end: -1\n  Output: [2, 3]\n  ```\n  \n  ```\n  Input tensor with shape: [2, 3, 4]\n  start: 1\n  end: 2\n  Output: [3]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "shape", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "end", "type": "OptionalAttr" },
      { "name": "start", "type": "DefaultValuedAttr" }
    ],
    "category": "Shape"
  },
  {
    "name": "onnx.ShapeTransform",
    "summary": "ONNX Element-wise shape transformation operation",
    "description": "This operator transforms a tensor into another tensor whose shape is changed\n    by a given affine map. This is elemement-wise transformation, so each element\n    in the input will be copied to an element in the output via the affine map.\n    The affine map must be bijective.\n\n    For example, the following code is using `onnx.ShapeTransform` to reshape\n    a tensor from 2D to 4D.\n    ```mlir\n    #reshape = affine_map(d0, d1) -> (d0/32, d0%32, d1/64, d1%64)\n    %Y = onnx.ShapeTransform(%arg0) {index_map = #reshape} :  (tensor<128x128xf32>) -> tensor<4x32x2x64xf32>\n    ```\n\n    `onnx.ShapeTransform` will be finally materialized into an `affine.for` via\n    lowering to `krnl` dialect, e.g.\n    ```mlir\n    %alloc = memref.alloc() {alignment = 16 : i64} : memref<4x32x2x64xf32>\n    affine.for %arg1 = 0 to 128 {\n      affine.for %arg2 = 0 to 128 {\n        %0 = affine.load %arg0[%arg1, %arg2] : memref< 128x128xf32 >\n        affine.store %0, %alloc[%arg1 / 32, %arg1 % 32, %arg2 / 64, %arg2 % 64] : memref<4x32x2x64xf32>\n      }\n    }\n    ```\n\n    When being canonicalized, ShapeTransform operations are composed into\n    a new ShapeTransform operation by composing their affine maps.\n\n    At this moment, this operation only supports static dimensions.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "index_map", "type": "AffineMapAttr" }
    ]
  },
  {
    "name": "onnx.Shrink",
    "summary": "ONNX Shrink operation",
    "description": "Shrink takes one input data (Tensor<numeric>) and produces one Tensor output,\n  having same datatype and shape with input. It has two attributes, lambd and\n  bias. The formula of this operator is: If x < -lambd, y = x + bias;\n  If x > lambd, y = x - bias; Otherwise, y = 0.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "bias", "type": "DefaultValuedAttr" },
      { "name": "lambd", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Sigmoid",
    "summary": "ONNX Sigmoid operation",
    "description": "Sigmoid takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the sigmoid function, y = 1 / (1 + exp(-x)), is applied to the\n  tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "category": "Activation"
  },
  {
    "name": "onnx.Sign",
    "summary": "ONNX Sign operation",
    "description": "Calculate the sign of the given input tensor element-wise.\n  If input > 0, output 1. if input < 0, output -1. if input == 0, output 0.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Sin",
    "summary": "ONNX Sin operation",
    "description": "Calculates the sine of the given input tensor, element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Sinh",
    "summary": "ONNX Sinh operation",
    "description": "Calculates the hyperbolic sine of the given input tensor element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Size",
    "summary": "ONNX Size operation",
    "description": "Takes a tensor as input and outputs a int64 scalar that equals to the total number of elements of the input tensor.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "size", "type": "TensorOf" }
    ],
    "category": "Shape"
  },
  {
    "name": "onnx.Slice",
    "summary": "ONNX Slice operation",
    "description": "Produces a slice of the input tensor along multiple axes. Similar to numpy:\n  https://numpy.org/doc/stable/user/basics.indexing.html?highlight=slice#slicing-and-striding\n  \n  Slice uses the `starts`, `ends`, `axes` and `steps` inputs to select a sub-tensor\n  of its input `data` tensor.\n  \n  An effective `starts[i]`, `ends[i]`, and `steps[i]` must be computed for each `i`\n  in `[0, ... r-1]` where `r = rank(input)` as follows:\n  \n  If `axes` are omitted, they are set to `[0, ..., r-1]`.\n  If `steps` are omitted, they are set to `[1, ..., 1]` of length `len(starts)`\n  \n  The effective values are initialized as `start[i] = 0`, `ends[i] = dims[i]` where\n  `dims` are the dimensions of `input` and `steps[i] = 1`.\n  \n  All negative elements of `axes` are made non-negative by adding `r` to them, where\n  `r =rank(input)`.\n  \n  All negative values in `starts[i]` and `ends[i]` have `dims[axes[i]]` added to them,\n  where `dims` are the dimensions of `input`. Then `start[axes[i]]` is the adjusted\n  `starts[i]` is clamped into the range `[0, dims[axes[i]]]` for positive stepping\n  and `[0, dims[axes[i]]-1]` for negative stepping.\n  \n  The clamping for the adjusted `ends[i]` depends on the sign of `steps[i]` and must\n  accommodate copying 0 through `dims[axes[i]]` elements, so for positive stepping\n  `ends[axes[i]]` is clamped to `[0, dims[axes[i]]]`, while for negative stepping it\n  is clamped to `[-1, dims[axes[i]]-1]`.\n  \n  Finally, `steps[axes[i]] = steps[i]`.\n  \n  For slicing to the end of a dimension with unknown size, it is recommended to pass\n  in `INT_MAX` when slicing forward and 'INT_MIN' when slicing backward.\n  \n  Example 1:\n  \n  ```\n  data = [\n      [1, 2, 3, 4],\n      [5, 6, 7, 8],\n  ]\n  axes = [0, 1]\n  starts = [1, 0]\n  ends = [2, 3]\n  steps = [1, 2]\n  result = [\n      [5, 7],\n  ]\n  ```\n  \n  Example 2:\n  \n  ```\n  data = [\n      [1, 2, 3, 4],\n      [5, 6, 7, 8],\n  ]\n  starts = [0, 1]\n  ends = [-1, 1000]\n  result = [\n      [2, 3, 4],\n  ]\n  ```",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "starts", "type": "AnyTypeOf" },
      { "name": "ends", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" },
      { "name": "steps", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "category": "Tensor"
  },
  {
    "name": "onnx.Softmax",
    "summary": "ONNX Softmax operation",
    "description": "The operator computes the normalized exponential values for the given input:\n  \n   Softmax(input, axis) = Exp(input) / ReduceSum(Exp(input), axis=axis, keepdims=1) \n  \n  The \\\"axis\\\" attribute indicates the dimension along which Softmax\n  will be performed. The output tensor has the same shape\n  and contains the Softmax values of the corresponding input.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ],
    "category": "Activation"
  },
  {
    "name": "onnx.SoftmaxCrossEntropyLoss",
    "summary": "ONNX SoftmaxCrossEntropyLoss operation",
    "description": "Loss function that measures the softmax cross entropy\n  between 'scores' and 'labels'.\n  This operator first computes a loss tensor whose shape is identical to the labels input.\n  If the input is 2-D with shape (N, C), the loss tensor may be a N-element vector L = (l_1, l_2, ..., l_N).\n  If the input is N-D tensor with shape (N, C, D1, D2, ..., Dk),\n  the loss tensor L may have (N, D1, D2, ..., Dk) as its shape and L[i,][j_1][j_2]...[j_k] denotes a scalar element in L.\n  After L is available, this operator can optionally do a reduction operator.\n  \n  * shape(scores): (N, C) where C is the number of classes, or (N, C, D1, D2,..., Dk),\n    with K >= 1 in case of K-dimensional loss.\n  * shape(labels): (N) where each value is 0 <= labels[i] <= C-1, or (N, D1, D2,..., Dk),\n    with K >= 1 in case of K-dimensional loss.\n  \n  The loss for one sample, l_i, can calculated as follows:\n  ```\n  l[i][d1][d2]...[dk] = -y[i][c][d1][d2]..[dk], where i is the index of classes.\n  ```\n  or\n  ```\n  l[i][d1][d2]...[dk] = -y[i][c][d1][d2]..[dk] * weights[c], if 'weights' is provided.\n  ```\n  \n  loss is zero for the case when label-value equals ignore_index.\n  ```\n  l[i][d1][d2]...[dk]  = 0, when labels[n][d1][d2]...[dk] = ignore_index\n  ```\n  \n  where:\n  ```\n  p = Softmax(scores)\n  y = Log(p)\n  c = labels[i][d1][d2]...[dk]\n  ```\n  \n  Finally, L is optionally reduced:\n  \n  * If reduction = 'none', the output is L with shape (N, D1, D2, ..., Dk).\n  * If reduction = 'sum', the output is scalar: Sum(L).\n  * If reduction = 'mean', the output is scalar: ReduceMean(L), or if weight is provided: `ReduceSum(L) / ReduceSum(W)`,\n    where tensor W is of shape `(N, D1, D2, ..., Dk)` and `W[n][d1][d2]...[dk] = weights[labels[i][d1][d2]...[dk]]`.",
    "inputs": [
      { "name": "scores", "type": "AnyTypeOf" },
      { "name": "labels", "type": "AnyTypeOf" },
      { "name": "weights", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" },
      { "name": "log_prob", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "ignore_index", "type": "OptionalAttr" },
      { "name": "reduction", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.SoftmaxV11",
    "summary": "ONNX Softmax operation",
    "description": "The operator computes the softmax (normalized exponential) values for each layer in the batch\n   of the given input.\n  \n  The input does not need to explicitly be a 2D vector; rather, it will be\n  coerced into one. For an arbitrary n-dimensional tensor\n  input \\in [a_0, a_1, ..., a_{k-1}, a_k, ..., a_{n-1\\}\\] and k is\n  the axis provided, then input will be coerced into a 2-dimensional tensor with\n  dimensions [a_0 * ... * a_{k-1}, a_k * ... * a_{n-1\\}\\]. For the default\n  case where axis=1, this means the input tensor will be coerced into a 2D tensor\n  of dimensions [a_0, a_1 * ... * a_{n-1\\}\\], where a_0 is often the batch size.\n  In this situation, we must have a_0 = N and a_1 * ... * a_{n-1} = D.\n  Each of these dimensions must be matched correctly, or else the operator\n  will throw errors. The output tensor has the same shape\n  and contains the softmax values of the corresponding input.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Softplus",
    "summary": "ONNX Softplus operation",
    "description": "Softplus takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the softplus function, y = ln(exp(x) + 1), is applied to\n  the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Softsign",
    "summary": "ONNX Softsign operation",
    "description": "Calculates the softsign (x/(1+|x|)) of the given input tensor element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.SpaceToDepth",
    "summary": "ONNX SpaceToDepth operation",
    "description": "SpaceToDepth rearranges blocks of spatial data into depth. More specifically,\n  this op outputs a copy of the input tensor where values from the height and width dimensions\n  are moved to the depth dimension.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "blocksize", "type": "SI64Attr" }
    ]
  },
  {
    "name": "onnx.Split",
    "summary": "ONNX Split operation",
    "description": "Split a tensor into a list of tensors, along the specified 'axis'.\n  Either input 'split' or the attribute 'num_outputs' should be specified, but not both.\n  If the attribute 'num_outputs' is specified, then the tensor is split into equal sized parts.\n  If the tensor is not evenly splittable into `num_outputs`, the last chunk will be smaller.\n  If the input 'split' is specified, it indicates the sizes of each output in the split.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "split", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "num_outputs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.SplitToSequence",
    "summary": "ONNX SplitToSequence operation",
    "description": "Split a tensor into a sequence of tensors, along the specified 'axis'.\n  Lengths of the parts can be specified using the optional argument 'split'.\n  If the argument `split' is not specified, a default scalar value of 1\n  is used as the value of `split'.\n  'split' must contain only positive numbers.\n  'split' is either a scalar (tensor of empty shape), or a 1-D tensor.\n  If 'split' is a scalar, then 'input' will be split into chunks all of size 'split'\n  if possible. The last chunk alone may be smaller than 'split' if the 'input' size\n  along the given axis 'axis' is not divisible by 'split'.\n  If 'split' is a 1-dimensional tensor, the input tensor is split into 'size(split)' chunks,\n  with lengths of the parts on 'axis' specified in 'split'. In this scenario, the sum of entries\n  in 'split' must be equal to the dimension size of input tensor on 'axis'.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "split", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output_sequence", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.SplitV11",
    "summary": "ONNX Split operation",
    "description": "Split a tensor into a list of tensors, along the specified\n  'axis'. Lengths of the parts can be specified using argument 'split'.\n  Otherwise, the tensor is split to equal sized parts.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "split", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.SplitV13",
    "summary": "ONNX Split operation",
    "description": "Split a tensor into a list of tensors, along the specified\n  'axis'. Lengths of the parts can be specified using input 'split'.\n  Otherwise, the tensor is split to equal sized parts.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "split", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Sqrt",
    "summary": "ONNX Sqrt operation",
    "description": "Square root takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the square root is, y = x^0.5, is applied to\n  the tensor elementwise. If x is negative, then it will return NaN.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Squeeze",
    "summary": "ONNX Squeeze operation",
    "description": "Remove single-dimensional entries from the shape of a tensor.\n  Takes an input `axes` with a list of axes to squeeze.\n  If `axes` is not provided, all the single dimensions will be removed from\n  the shape. If an axis is selected with shape entry not equal to one, an error is raised.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "squeezed", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.SqueezeV11",
    "summary": "ONNX Squeeze operation",
    "description": "Remove single-dimensional entries from the shape of a tensor.\n  Takes a  parameter `axes` with a list of axes to squeeze.\n  If `axes` is not provided, all the single dimensions will be removed from\n  the shape. If an axis is selected with shape entry not equal to one, an error is raised.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "squeezed", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.STFT",
    "summary": "ONNX STFT operation",
    "description": "Computes the Short-time Fourier Transform of the signal.",
    "inputs": [
      { "name": "signal", "type": "AnyTypeOf" },
      { "name": "frame_step", "type": "AnyTypeOf" },
      { "name": "window", "type": "AnyTypeOf" },
      { "name": "frame_length", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "onesided", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.StringNormalizer",
    "summary": "ONNX StringNormalizer operation",
    "description": "StringNormalization performs string operations for basic cleaning.\n  This operator has only one input (denoted by X) and only one output\n  (denoted by Y). This operator first examines the elements in the X,\n  and removes elements specified in \\\"stopwords\\\" attribute.\n  After removing stop words, the intermediate result can be further lowercased,\n  uppercased, or just returned depending the \\\"case_change_action\\\" attribute.\n  This operator only accepts [C]- and [1, C]-tensor.\n  If all elements in X are dropped, the output will be the empty value of string tensor with shape [1]\n  if input shape is [C] and shape [1, 1] if input shape is [1, C].",
    "inputs": [
      { "name": "X", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "case_change_action", "type": "DefaultValuedStrAttr" },
      { "name": "is_case_sensitive", "type": "DefaultValuedAttr" },
      { "name": "locale", "type": "OptionalAttr" },
      { "name": "stopwords", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Sub",
    "summary": "ONNX Sub operation",
    "description": "Performs element-wise binary subtraction (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).\n  \n  (Opset 14 change): Extend supported types to include uint8, int8, uint16, and int16.",
    "inputs": [
      { "name": "A", "type": "AnyTypeOf" },
      { "name": "B", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "C", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Sum",
    "summary": "ONNX Sum operation",
    "description": "Element-wise sum of each of the input tensors (with Numpy-style broadcasting support).\n  All inputs and outputs must have the same data type.\n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "data_0", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "sum", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.SVMClassifier",
    "summary": "ONNX SVMClassifier operation",
    "description": "Support Vector Machine classifier",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "Z", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "classlabels_ints", "type": "OptionalAttr" },
      { "name": "classlabels_strings", "type": "OptionalAttr" },
      { "name": "coefficients", "type": "OptionalAttr" },
      { "name": "kernel_params", "type": "OptionalAttr" },
      { "name": "kernel_type", "type": "DefaultValuedStrAttr" },
      { "name": "post_transform", "type": "DefaultValuedStrAttr" },
      { "name": "prob_a", "type": "OptionalAttr" },
      { "name": "prob_b", "type": "OptionalAttr" },
      { "name": "rho", "type": "OptionalAttr" },
      { "name": "support_vectors", "type": "OptionalAttr" },
      { "name": "vectors_per_class", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.SVMRegressor",
    "summary": "ONNX SVMRegressor operation",
    "description": "Support Vector Machine regression prediction and one-class SVM anomaly detection.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "coefficients", "type": "OptionalAttr" },
      { "name": "kernel_params", "type": "OptionalAttr" },
      { "name": "kernel_type", "type": "DefaultValuedStrAttr" },
      { "name": "n_supports", "type": "DefaultValuedAttr" },
      { "name": "one_class", "type": "DefaultValuedAttr" },
      { "name": "post_transform", "type": "DefaultValuedStrAttr" },
      { "name": "rho", "type": "OptionalAttr" },
      { "name": "support_vectors", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Tan",
    "summary": "ONNX Tan operation",
    "description": "Calculates the tangent of the given input tensor, element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Tanh",
    "summary": "ONNX Tanh operation",
    "description": "Calculates the hyperbolic tangent of the given input tensor element-wise.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "category": "Activation"
  },
  {
    "name": "onnx.TfIdfVectorizer",
    "summary": "ONNX TfIdfVectorizer operation",
    "description": "This transform extracts n-grams from the input sequence and save them as a vector. Input can\n  be either a 1-D or 2-D tensor. For 1-D input, output is the n-gram representation of that input.\n  For 2-D input, the output is also a  2-D tensor whose i-th row is the n-gram representation of the i-th input row.\n  More specifically, if input shape is [C], the corresponding output shape would be [max(ngram_indexes) + 1].\n  If input shape is [N, C], this operator produces a [N, max(ngram_indexes) + 1]-tensor.\n  \n  In contrast to standard n-gram extraction, here, the indexes of extracting an n-gram from the original\n  sequence are not necessarily consecutive numbers. The discontinuity between indexes are controlled by the number of skips.\n  If the number of skips is 2, we should skip two tokens when scanning through the original sequence.\n  Let's consider an example. Assume that input sequence is [94, 17, 36, 12, 28] and the number of skips is 2.\n  The associated 2-grams are [94, 12] and [17, 28] respectively indexed by [0, 3] and [1, 4].\n  If the number of skips becomes 0, the 2-grams generated are [94, 17], [17, 36], [36, 12], [12, 28]\n  indexed by [0, 1], [1, 2], [2, 3], [3, 4], respectively.\n  \n  The output vector (denoted by Y) stores the count of each n-gram;\n  Y[ngram_indexes[i]] indicates the times that the i-th n-gram is found. The attribute ngram_indexes is used to determine the mapping\n  between index i and the corresponding n-gram's output coordinate. If pool_int64s is [94, 17, 17, 36], ngram_indexes is [1, 0],\n  ngram_counts=[0, 0], then the Y[0] (first element in Y) and Y[1] (second element in Y) are the counts of [17, 36] and [94, 17],\n  respectively. An n-gram which cannot be found in pool_strings/pool_int64s should be ignored and has no effect on the output.\n  Note that we may consider all skips up to S when generating the n-grams.\n  \n  The examples used above are true if mode is \\\"TF\\\". If mode is \\\"IDF\\\", all the counts larger than 1 would be truncated to 1 and\n  the i-th element in weights would be used to scale (by multiplication) the count of the i-th n-gram in pool. If mode is \\\"TFIDF\\\",\n  this operator first computes the counts of all n-grams and then scale them by the associated values in the weights attribute.\n  \n  Only one of pool_strings and pool_int64s can be set. If pool_int64s is set, the input should be an integer tensor.\n  If pool_strings is set, the input must be a string tensor.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "max_gram_length", "type": "SI64Attr" },
      { "name": "max_skip_count", "type": "SI64Attr" },
      { "name": "min_gram_length", "type": "SI64Attr" },
      { "name": "mode", "type": "StrAttr" },
      { "name": "ngram_counts", "type": "I64ArrayAttr" },
      { "name": "ngram_indexes", "type": "I64ArrayAttr" },
      { "name": "pool_int64s", "type": "OptionalAttr" },
      { "name": "pool_strings", "type": "OptionalAttr" },
      { "name": "weights", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.ThresholdedRelu",
    "summary": "ONNX ThresholdedRelu operation",
    "description": "ThresholdedRelu takes one input data (Tensor<T>) and produces one output data\n  (Tensor<T>) where the rectified linear function, y = x for x > alpha, y = 0 otherwise,\n  is applied to the tensor elementwise.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Tile",
    "summary": "ONNX Tile operation",
    "description": "Constructs a tensor by tiling a given tensor.\n  This is the same as function `tile` in Numpy, but no broadcast.\n  For example A = [[1, 2], [3, 4]], B = [1, 2], tile(A, B) = [[1, 2, 1, 2], [3, 4, 3, 4]]",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "repeats", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.TopK",
    "summary": "ONNX TopK operation",
    "description": "Retrieve the top-K largest or smallest elements along a specified axis. Given an input tensor of\n  shape [a_0, a_1, ..., a_{n-1\\}\\] and integer argument k, return two outputs:\n  \n  * Value tensor of shape [a_0, a_1, ..., a_{axis-1}, k, a_{axis+1}, ... a_{n-1\\}\\]\n    which contains the values of the top k elements along the specified axis\n  * Index tensor of shape [a_0, a_1, ..., a_{axis-1}, k, a_{axis+1}, ... a_{n-1\\}\\] which\n    contains the indices of the top k elements (original indices from the input\n    tensor).\n  \n  * If \\\"largest\\\" is 1 (the default value) then the k largest elements are returned.\n  * If \\\"sorted\\\" is 1 (the default value) then the resulting k elements will be sorted.\n  * If \\\"sorted\\\" is 0, order of returned 'Values' and 'Indices' are undefined.\n  \n  Given two equivalent values, this operator uses the indices along the axis as\n  a tiebreaker. That is, the element with the lower index will appear first.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "K", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Values", "type": "AnyTypeOf" },
      { "name": "Indices", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "largest", "type": "DefaultValuedAttr" },
      { "name": "sorted", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Transpose",
    "summary": "ONNX Transpose operation",
    "description": "Transpose the input tensor similar to numpy.transpose. For example, when\n  perm=(1, 0, 2), given an input tensor of shape (1, 2, 3), the output shape\n  will be (2, 1, 3).",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "transposed", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "perm", "type": "OptionalAttr" }
    ],
    "category": "Transform"
  },
  {
    "name": "onnx.TreeEnsembleClassifier",
    "summary": "ONNX TreeEnsembleClassifier operation",
    "description": "Tree Ensemble classifier.  Returns the top class for each of N inputs.<br>\n      The attributes named 'nodes_X' form a sequence of tuples, associated by\n      index into the sequences, which must all be of equal length. These tuples\n      define the nodes.<br>\n      Similarly, all fields prefixed with 'class_' are tuples of votes at the leaves.\n      A leaf may have multiple votes, where each vote is weighted by\n      the associated class_weights index.<br>\n      One and only one of classlabels_strings or classlabels_int64s\n      will be defined. The class_ids are indices into this list.",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "Z", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "base_values", "type": "OptionalAttr" },
      { "name": "class_ids", "type": "OptionalAttr" },
      { "name": "class_nodeids", "type": "OptionalAttr" },
      { "name": "class_treeids", "type": "OptionalAttr" },
      { "name": "class_weights", "type": "OptionalAttr" },
      { "name": "classlabels_int64s", "type": "OptionalAttr" },
      { "name": "classlabels_strings", "type": "OptionalAttr" },
      { "name": "nodes_falsenodeids", "type": "OptionalAttr" },
      { "name": "nodes_featureids", "type": "OptionalAttr" },
      { "name": "nodes_hitrates", "type": "OptionalAttr" },
      { "name": "nodes_missing_value_tracks_true", "type": "OptionalAttr" },
      { "name": "nodes_modes", "type": "OptionalAttr" },
      { "name": "nodes_nodeids", "type": "OptionalAttr" },
      { "name": "nodes_treeids", "type": "OptionalAttr" },
      { "name": "nodes_truenodeids", "type": "OptionalAttr" },
      { "name": "nodes_values", "type": "OptionalAttr" },
      { "name": "post_transform", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.TreeEnsembleRegressor",
    "summary": "ONNX TreeEnsembleRegressor operation",
    "description": "Tree Ensemble regressor.  Returns the regressed values for each input in N.<br>\n      All args with nodes_ are fields of a tuple of tree nodes, and\n      it is assumed they are the same length, and an index i will decode the\n      tuple across these inputs.  Each node id can appear only once\n      for each tree id.<br>\n      All fields prefixed with target_ are tuples of votes at the leaves.<br>\n      A leaf may have multiple votes, where each vote is weighted by\n      the associated target_weights index.<br>\n      All trees must have their node ids start at 0 and increment by 1.<br>\n      Mode enum is BRANCH_LEQ, BRANCH_LT, BRANCH_GTE, BRANCH_GT, BRANCH_EQ, BRANCH_NEQ, LEAF",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "aggregate_function", "type": "DefaultValuedStrAttr" },
      { "name": "base_values", "type": "OptionalAttr" },
      { "name": "n_targets", "type": "OptionalAttr" },
      { "name": "nodes_falsenodeids", "type": "OptionalAttr" },
      { "name": "nodes_featureids", "type": "OptionalAttr" },
      { "name": "nodes_hitrates", "type": "OptionalAttr" },
      { "name": "nodes_missing_value_tracks_true", "type": "OptionalAttr" },
      { "name": "nodes_modes", "type": "OptionalAttr" },
      { "name": "nodes_nodeids", "type": "OptionalAttr" },
      { "name": "nodes_treeids", "type": "OptionalAttr" },
      { "name": "nodes_truenodeids", "type": "OptionalAttr" },
      { "name": "nodes_values", "type": "OptionalAttr" },
      { "name": "post_transform", "type": "DefaultValuedStrAttr" },
      { "name": "target_ids", "type": "OptionalAttr" },
      { "name": "target_nodeids", "type": "OptionalAttr" },
      { "name": "target_treeids", "type": "OptionalAttr" },
      { "name": "target_weights", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "onnx.Trilu",
    "summary": "ONNX Trilu operation",
    "description": "Given a 2-D matrix or batches of 2-D matrices, returns the upper or lower triangular part of the tensor(s).\n  The attribute \\\"upper\\\" determines whether the upper or lower part is retained. If set to true,\n  the upper triangular matrix is retained. Lower triangular matrix is retained otherwise.\n  Default value for the \\\"upper\\\" attribute is true.\n  Trilu takes one input tensor of shape [*, N, M], where * is zero or more batch dimensions. The upper triangular part consists\n  of the elements on and above the given diagonal (k). The lower triangular part consists of elements on and below the diagonal.\n  All other elements in the matrix are set to zero.\n  If k = 0, the triangular part on and above/below the main diagonal is retained.\n  If upper is set to true, a positive k retains the upper triangular matrix excluding the main diagonal and (k-1) diagonals above it.\n  A negative k value retains the main diagonal and |k| diagonals below it.\n  If upper is set to false, a positive k retains the lower triangular matrix including the main diagonal and k diagonals above it.\n  A negative k value excludes the main diagonal and (|k|-1) diagonals below it.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" },
      { "name": "k", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "upper", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Unique",
    "summary": "ONNX Unique operation",
    "description": "Find the unique elements of a tensor. When an optional attribute 'axis' is provided, unique subtensors sliced along the 'axis' are returned.\n  Otherwise the input tensor is flattened and unique values of the flattened tensor are returned.\n  \n  This operator returns the unique values or sliced unique subtensors of the input tensor and three optional outputs.\n  The first output tensor 'Y' contains all unique values or subtensors of the input.\n  The second optional output tensor 'indices' contains indices of 'Y' elements' first occurrence in 'X'.\n  The third optional output tensor 'inverse_indices' contains, for elements of 'X', its corresponding indices in 'Y'.\n  The fourth optional output tensor 'counts' contains the count of each element of 'Y' in the input.\n  \n  Outputs are either sorted in ascending order or optionally in the order of the first occurrence of the values in the input.\n  \n  https://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html\n  \n  Example 1:\n  ```\n  input_X = [2, 1, 1, 3, 4, 3]\n  attribute_sorted = 0\n  attribute_axis = None\n  output_Y = [2, 1, 3, 4]\n  output_indices = [0, 1, 3, 4]\n  output_inverse_indices = [0, 1, 1, 2, 3, 2]\n  output_counts = [1, 2, 2, 1]\n  ```\n  \n  Example 2:\n  ```\n  input_X = [[1, 3], [2, 3]]\n  attribute_sorted = 1\n  attribute_axis = None\n  output_Y = [1, 2, 3]\n  output_indices = [0, 2, 1]\n  output_inverse_indices = [0, 2, 1, 2]\n  output_counts = [1, 1, 2]\n  ```\n  \n  Example 3:\n  ```\n  input_X = [[1, 0, 0], [1, 0, 0], [2, 3, 4]]\n  attribute_sorted = 1\n  attribute_axis = 0\n  output_Y = [[1, 0, 0], [2, 3, 4]]\n  output_indices = [0, 2]\n  output_inverse_indices = [0, 0, 1]\n  output_counts = [2, 1]\n  ```\n  \n  Example 4:\n  ```\n  input_x = [[[1., 1.], [0., 1.], [2., 1.], [0., 1.]],\n              [[1., 1.], [0., 1.], [2., 1.], [0., 1.]]]\n  attribute_sorted = 1\n  attribute_axis = 1\n  ```\n  \n  intermediate data are presented below for better understanding:\n  there are 4 subtensors sliced along axis 1 of input_x (shape = (2, 4, 2)):\n  ```\n  A: [[1, 1], [1, 1]],\n     [[0, 1], [0, 1]],\n     [[2, 1], [2, 1]],\n     [[0, 1], [0, 1]].\n  ```\n  \n  there are 3 unique subtensors:\n  ```\n  [[1, 1], [1, 1]],\n  [[0, 1], [0, 1]],\n  [[2, 1], [2, 1]].\n  ```\n  \n  sorted unique subtensors:\n  ```\n  B: [[0, 1], [0, 1]],\n     [[1, 1], [1, 1]],\n     [[2, 1], [2, 1]].\n  ```\n  \n  output_Y is constructed from B:\n  ```\n  [[[0. 1.], [1. 1.], [2. 1.]],\n   [[0. 1.], [1. 1.], [2. 1.]]]\n  ```\n  \n  output_indices is to map from B to A:\n  ```\n  [1, 0, 2]\n  ```\n  \n  output_inverse_indices is to map from A to B:\n  ```\n  [1, 0, 2, 0]\n  ```\n  \n  output_counts:\n  ```\n  [2, 1, 1]\n  ```",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" },
      { "name": "indices", "type": "AnyTypeOf" },
      { "name": "inverse_indices", "type": "AnyTypeOf" },
      { "name": "counts", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "OptionalAttr" },
      { "name": "sorted", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "onnx.Unsqueeze",
    "summary": "ONNX Unsqueeze operation",
    "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\n  Takes one required input `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n  \n  For example, given an input tensor (`data`) of shape [3, 4, 5], then\n  Unsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n  \n  The input `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\n  The rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\n  Each value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\n  The order of values in `axes` does not matter and can come in any order.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "axes", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "expanded", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.UnsqueezeV11",
    "summary": "ONNX Unsqueeze operation",
    "description": "Insert single-dimensional entries to the shape of an input tensor (`data`).\n  Takes one required argument `axes` - which contains a list of dimension indices and this operator will insert a dimension of value `1` into the corresponding index of the output tensor (`expanded`).\n  \n  For example:\n    Given an input tensor (`data`) of shape [3, 4, 5], then\n    Unsqueeze(data, axes=[0, 4]) outputs a tensor (`expanded`) containing same data as `data` but with shape [1, 3, 4, 5, 1].\n  \n  The attribute `axes` should not contain any duplicate entries. It is an error if it contains duplicates.\n  The rank of the output tensor (`output_rank`) is the rank of the input tensor (`data`) plus the number of values in `axes`.\n  Each value in `axes` should be within the (inclusive) range [-output_rank , output_rank - 1].\n  The order of values in `axes` does not matter and can come in any order.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "expanded", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "axes", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "onnx.Upsample",
    "summary": "ONNX Upsample operation",
    "description": "Upsample the input tensor.\n  Each dimension value of the output tensor is:\n    output_dimension = floor(input_dimension * scale).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "scales", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "onnx.UpsampleV7",
    "summary": "ONNX Upsample operation",
    "description": "Upsample the input tensor.\n  Each dimension value of the output tensor is:\n    output_dimension = floor(input_dimension * scale).",
    "inputs": [
      { "name": "X", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedStrAttr" },
      { "name": "scales", "type": "F32ArrayAttr" }
    ]
  },
  {
    "name": "onnx.Where",
    "summary": "ONNX Where operation",
    "description": "Return elements, either from X or Y, depending on condition.\n  Where behaves like\n  [numpy.where](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html)\n  with three parameters.\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "condition", "type": "TensorOf" },
      { "name": "X", "type": "AnyTypeOf" },
      { "name": "Y", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTypeOf" }
    ]
  },
  {
    "name": "onnx.Xor",
    "summary": "ONNX Xor operation",
    "description": "Returns the tensor resulted from performing the `xor` logical operation\n  elementwise on the input tensors `A` and `B` (with Numpy-style broadcasting support).\n  \n  This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).",
    "inputs": [
      { "name": "A", "type": "TensorOf" },
      { "name": "B", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "C", "type": "TensorOf" }
    ]
  },
  {
    "name": "onnx.Yield",
    "summary": "ONNX yield operation",
    "description": "The `onnx.Yield` operation represents a yield operation within an ONNX subgraph.\n    The operation takes variable number of operands and produces no results.\n\n    This operation is not part of the standard and was added to assist onnx-mlir.\n    It terminates a ONNXLoop/Scan/IfOp region.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "onnx.ZipMap",
    "summary": "ONNX ZipMap operation",
    "description": "Creates a map from the input and the attributes.<br>\n      The values are provided by the input tensor, while the keys are specified by the attributes.\n      Must provide keys in either classlabels_strings or classlabels_int64s (but not both).<br>\n      The columns of the tensor correspond one-by-one to the keys specified by the attributes. There must be as many columns as keys.<br>",
    "inputs": [
      { "name": "X", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "Z", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "classlabels_int64s", "type": "OptionalAttr" },
      { "name": "classlabels_strings", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "pdl.apply_native_constraint",
    "summary": "Apply a native constraint to a set of provided entities",
    "description": "`pdl.apply_native_constraint` operations apply a native C++ constraint, that\n    has been registered externally with the consumer of PDL, to a given set of\n    entities and optionally return a number of values.\n\n    Example:\n\n    ```mlir\n    // Apply `myConstraint` to the entities defined by `input`, `attr`, and `op`.\n    pdl.apply_native_constraint \"myConstraint\"(%input, %attr, %op : !pdl.value, !pdl.attribute, !pdl.operation)\n    // Apply constraint `with_result` to `root`. This constraint returns an attribute.\n    %attr = pdl.apply_native_constraint \"with_result\"(%root : !pdl.operation) : !pdl.attribute\n    ```",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" },
      { "name": "isNegated", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$name `(` $args `:` type($args) `)` (`:`  type($results)^ )? attr-dict"
  },
  {
    "name": "pdl.apply_native_rewrite",
    "summary": "Apply a native rewrite method inside of pdl.rewrite region",
    "description": "`pdl.apply_native_rewrite` operations apply a native C++ function, that has\n    been registered externally with the consumer of PDL, to perform a rewrite\n    and optionally return a number of values. The native function may accept any\n    number of arguments. This operation is used within a pdl.rewrite region to enable\n    the interleaving of native rewrite methods with other pdl constructs.\n\n    Example:\n\n    ```mlir\n    // Apply a native rewrite method that returns an attribute.\n    %ret = pdl.apply_native_rewrite \"myNativeFunc\"(%arg0, %attr1) : !pdl.attribute\n    ```\n\n    ```c++\n    // The native rewrite as defined in C++:\n    static Attribute myNativeFunc(PatternRewriter &rewriter, Value arg0, Attribute arg1) {\n      // Just return the second arg.\n      return arg1;\n    }\n\n    void registerNativeRewrite(PDLPatternModule &pdlModule) {\n      pdlModule.registerRewriteFunction(\"myNativeFunc\", myNativeFunc);\n    }\n    ```",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$name (`(` $args^ `:` type($args) `)`)? (`:` type($results)^)? attr-dict"
  },
  {
    "name": "pdl.attribute",
    "summary": "Define an input attribute in a pattern",
    "description": "`pdl.attribute` operations capture named attribute edges into an operation.\n    Instances of this operation define, and partially constrain, attributes of a\n    given operation. A `pdl.attribute` may partially constrain the input by\n    specifying an expected attribute value type (via a `pdl.type` operation), or\n    a constant value for the attribute (via `val`). Only one of these may be set\n    for a given input, as the type of the constant value provides the type. When\n    defined within a `pdl.rewrite` region, the constant value must be specified.\n\n    Example:\n\n    ```mlir\n    // Define an attribute:\n    %attr = pdl.attribute\n\n    // Define an attribute with an expected type:\n    %type = pdl.type : i32\n    %attr = pdl.attribute : %type\n\n    // Define an attribute with a constant value:\n    %attr = pdl.attribute = \"hello\"\n    ```",
    "inputs": [
      { "name": "valueType", "type": "Optional" }
    ],
    "outputs": [
      { "name": "attr", "type": "PDL_Attribute" }
    ],
    "attributes": [
      { "name": "value", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`:` $valueType^)? (`=` $value^)? attr-dict-with-keyword"
  },
  {
    "name": "pdl.erase",
    "summary": "Mark an input operation as `erased`",
    "description": "`pdl.erase` operations are used within `pdl.rewrite` regions to specify that\n    an input operation should be marked as erased. The semantics of this\n    operation correspond with the `eraseOp` method on a `PatternRewriter`.\n\n    Example:\n\n    ```mlir\n    pdl.erase %root\n    ```",
    "inputs": [
      { "name": "opValue", "type": "PDL_Operation" }
    ],
    "assemblyFormat": "$opValue attr-dict"
  },
  {
    "name": "pdl.operand",
    "summary": "Define an external input operand in a pattern",
    "description": "`pdl.operand` operations capture external operand edges into an operation\n    node that originate from operations or block arguments not otherwise\n    specified within the pattern (i.e. via `pdl.result` or `pdl.results`). These\n    operations define individual operands of a given operation. A `pdl.operand`\n    may partially constrain an operand by specifying an expected value type\n    (via a `pdl.type` operation).\n\n    Example:\n\n    ```mlir\n    // Define an external operand:\n    %operand = pdl.operand\n\n    // Define an external operand with an expected type:\n    %type = pdl.type : i32\n    %operand = pdl.operand : %type\n    ```",
    "inputs": [
      { "name": "valueType", "type": "Optional" }
    ],
    "outputs": [
      { "name": "value", "type": "PDL_Value" }
    ],
    "assemblyFormat": "(`:` $valueType^)? attr-dict"
  },
  {
    "name": "pdl.operands",
    "summary": "Define a range of input operands in a pattern",
    "description": "`pdl.operands` operations capture external operand range edges into an\n    operation node that originate from operations or block arguments not\n    otherwise specified within the pattern (i.e. via `pdl.result` or\n    `pdl.results`). These operations define groups of input operands into a\n    given operation. A `pdl.operands` may partially constrain a set of input\n    operands by specifying expected value types (via `pdl.types` operations).\n\n    Example:\n\n    ```mlir\n    // Define a range of input operands:\n    %operands = pdl.operands\n\n    // Define a range of input operands with expected types:\n    %types = pdl.types : [i32, i64, i32]\n    %typed_operands = pdl.operands : %types\n    ```",
    "inputs": [
      { "name": "valueType", "type": "Optional" }
    ],
    "outputs": [
      { "name": "value", "type": "PDL_RangeOf" }
    ],
    "assemblyFormat": "(`:` $valueType^)? attr-dict"
  },
  {
    "name": "pdl.operation",
    "summary": "Define an operation within a pattern",
    "description": "`pdl.operation` operations define operation nodes within a pattern. Within\n    a match sequence, i.e. when directly nested within a `pdl.pattern`, these\n    operations correspond to input operations, or those that already existing\n    within the MLIR module. Inside of a `pdl.rewrite`, these operations\n    correspond to operations that should be created as part of the replacement\n    sequence.\n\n    `pdl.operation`s are composed of a name, and a set of attribute, operand,\n    and result type values, that map to what those that would be on a\n    constructed instance of that operation. The results of a `pdl.operation` are\n    a handle to the operation itself. Handles to the results of the operation\n    can be extracted via `pdl.result`.\n\n    Example:\n\n    ```mlir\n    // Define an instance of a `foo.op` operation.\n    %op = pdl.operation \"foo.op\"(%arg0, %arg1 : !pdl.value, !pdl.value)\n      {\"attrA\" = %attr0} -> (%type, %type : !pdl.type, !pdl.type)\n    ```\n\n    When used within a matching context, the name of the operation may be\n    omitted.\n\n    When used within a rewriting context, i.e. when defined within a\n    `pdl.rewrite`, all of the result types must be \"inferable\". This means that\n    the type must be attributable to either a constant type value or the result\n    type of another entity, such as an attribute, the result of a\n    `apply_native_rewrite`, or the result type of another operation. If the\n    result type value does not meet any of these criteria, the operation must\n    override the `InferTypeOpInterface` to ensure that the result types can be\n    inferred.\n\n    The operands of the operation are interpreted in the following ways:\n\n    1) A single !pdl.range<value>:\n\n    In this case, the single range is treated as all of the operands of the\n    operation.\n\n    ```mlir\n    // Define an instance with single range of operands.\n    %op = pdl.operation \"func.return\"(%allArgs : !pdl.range<value>)\n    ```\n\n    2) A variadic number of either !pdl.value or !pdl.range<value>:\n\n    In this case, the inputs are expected to correspond with the operand groups\n    defined on the operation in ODS.\n\n    ```tablgen\n    // Given the following operation definition in ODS:\n    def MyIndirectCallOp {\n      let results = (outs FunctionType:$call, Variadic<AnyType>:$args);\n    }\n    ```\n\n    ```mlir\n    // We can match the operands as so:\n    %op = pdl.operation \"my.indirect_call\"(%call, %args : !pdl.value, !pdl.range<value>)\n    ```\n\n    The results of the operation are interpreted in the following ways:\n\n    1) A single !pdl.range<type>:\n\n    In this case, the single range is treated as all of the result types of the\n    operation.\n\n    ```mlir\n    // Define an instance with single range of types.\n    %allResultTypes = pdl.types\n    %op = pdl.operation \"builtin.unrealized_conversion_cast\" -> (%allResultTypes : !pdl.types)\n    ```\n\n    2) A variadic number of either !pdl.type or !pdl.range<type>:\n\n    In this case, the inputs are expected to correspond with the result groups\n    defined on the operation in ODS.\n\n    ```tablgen\n    // Given the following operation definition in ODS:\n    def MyOp {\n      let results = (outs SomeType:$result, Variadic<SomeType>:$otherResults);\n    }\n    ```\n\n    ```mlir\n    // We can match the results as so:\n    %result = pdl.type\n    %otherResults = pdl.types\n    %op = pdl.operation \"foo.op\" -> (%result, %otherResults : !pdl.type, !pdl.range<type>)\n    ```",
    "inputs": [
      { "name": "operandValues", "type": "Variadic" },
      { "name": "attributeValues", "type": "Variadic" },
      { "name": "typeValues", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "op", "type": "PDL_Operation" }
    ],
    "attributes": [
      { "name": "opName", "type": "OptionalAttr" },
      { "name": "attributeValueNames", "type": "StrArrayAttr" }
    ],
    "assemblyFormat": "($opName^)? (`(` $operandValues^ `:` type($operandValues) `)`)?\n    custom<OperationOpAttributes>($attributeValues, $attributeValueNames)\n    (`->` `(` $typeValues^ `:` type($typeValues) `)`)? attr-dict"
  },
  {
    "name": "pdl.pattern",
    "summary": "Define a rewrite pattern",
    "description": "`pdl.pattern` operations provide a transformable representation for a\n    `RewritePattern`. The attributes on this operation correspond to the various\n    metadata on a `RewritePattern`, such as the benefit. The match section of\n    the pattern is specified within the region body, with the rewrite provided\n    by a terminating `pdl.rewrite`.\n\n    Example:\n\n    ```mlir\n    // Provide a pattern matching \"foo.op\" that replaces the root with its\n    // operand.\n    pdl.pattern : benefit(1) {\n      %resultType = pdl.type\n      %inputOperand = pdl.operand\n      %root = pdl.operation \"foo.op\"(%inputOperand) -> (%resultType)\n      pdl.rewrite %root {\n        pdl.replace %root with (%inputOperand)\n      }\n    }\n    ```",
    "attributes": [
      { "name": "benefit", "type": "ConfinedAttr" },
      { "name": "sym_name", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($sym_name^)? `:` `benefit` `(` $benefit `)` attr-dict-with-keyword $bodyRegion"
  },
  {
    "name": "pdl.range",
    "summary": "Construct a range of pdl entities",
    "description": "`pdl.range` operations construct a range from a given set of PDL entities,\n    which all share the same underlying element type. For example, a\n    `!pdl.range<value>` may be constructed from a list of `!pdl.value`\n    or `!pdl.range<value>` entities.\n\n    Example:\n\n    ```mlir\n    // Construct a range of values.\n    %valueRange = pdl.range %inputValue, %inputRange : !pdl.value, !pdl.range<value>\n\n    // Construct a range of types.\n    %typeRange = pdl.range %inputType, %inputRange : !pdl.type, !pdl.range<type>\n\n    // Construct an empty range of types.\n    %valueRange = pdl.range : !pdl.range<type>\n    ```\n\n    TODO: Range construction is currently limited to rewrites, but it could\n    be extended to constraints under certain circustances; i.e., if we can\n    determine how to extract the underlying elements. If we can't, e.g. if\n    there are multiple sub ranges used for construction, we won't be able\n    to determine their sizes during constraint time.",
    "inputs": [
      { "name": "arguments", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "PDL_RangeOf" }
    ],
    "assemblyFormat": "($arguments^ `:` type($arguments))?\n    custom<RangeType>(ref(type($arguments)), type($result))\n    attr-dict"
  },
  {
    "name": "pdl.replace",
    "summary": "Mark an input operation as `replaced`",
    "description": "`pdl.replace` operations are used within `pdl.rewrite` regions to specify\n    that an input operation should be marked as replaced. The semantics of this\n    operation correspond with the `replaceOp` method on a `PatternRewriter`. The\n    set of replacement values can be either:\n    * a single `Operation` (`replOperation` should be populated)\n      - The operation will be replaced with the results of this operation.\n    * a set of `Value`s (`replValues` should be populated)\n      - The operation will be replaced with these values.\n\n    Example:\n\n    ```mlir\n    // Replace root node with 2 values:\n    pdl.replace %root with (%val0, %val1 : !pdl.value, !pdl.value)\n\n    // Replace root node with a range of values:\n    pdl.replace %root with (%vals : !pdl.range<value>)\n\n    // Replace root with another operation:\n    pdl.replace %root with %otherOp\n    ```",
    "inputs": [
      { "name": "opValue", "type": "PDL_Operation" },
      { "name": "replOperation", "type": "Optional" },
      { "name": "replValues", "type": "Variadic" }
    ],
    "assemblyFormat": "$opValue `with` (`(` $replValues^ `:` type($replValues) `)`)?\n    ($replOperation^)? attr-dict"
  },
  {
    "name": "pdl.result",
    "summary": "Extract a result from an operation",
    "description": "`pdl.result` operations extract result edges from an operation node within\n    a pattern or rewrite region. The provided index is zero-based, and\n    represents the concrete result to extract, i.e. this is not the result index\n    as defined by the ODS definition of the operation.\n\n    Example:\n\n    ```mlir\n    // Extract a result:\n    %operation = pdl.operation ...\n    %pdl_result = pdl.result 1 of %operation\n\n    // Imagine the following IR being matched:\n    %result_0, %result_1 = foo.op ...\n\n    // If the example pattern snippet above were matching against `foo.op` in\n    // the IR snippet, `%pdl_result` would correspond to `%result_1`.\n    ```",
    "inputs": [
      { "name": "parent", "type": "PDL_Operation" }
    ],
    "outputs": [
      { "name": "val", "type": "PDL_Value" }
    ],
    "attributes": [
      { "name": "index", "type": "I32Attr" }
    ],
    "assemblyFormat": "$index `of` $parent attr-dict"
  },
  {
    "name": "pdl.results",
    "summary": "Extract a result group from an operation",
    "description": "`pdl.results` operations extract a result group from an operation within a\n    pattern or rewrite region. If an index is provided, this operation extracts\n    a result group as defined by the ODS definition of the operation. In this\n    case the result of this operation may be either a single `pdl.value` or\n    a `pdl.range<value>`, depending on the constraint of the result in ODS. If\n    no index is provided, this operation extracts the full result range of the\n    operation.\n\n    Example:\n\n    ```mlir\n    // Extract all of the results of an operation:\n    %operation = pdl.operation ...\n    %results = pdl.results of %operation\n\n    // Extract the results in the first result group of an operation, which is\n    // variadic:\n    %operation = pdl.operation ...\n    %results = pdl.results 0 of %operation -> !pdl.range<value>\n\n    // Extract the results in the second result group of an operation, which is\n    // not variadic:\n    %operation = pdl.operation ...\n    %results = pdl.results 1 of %operation -> !pdl.value\n    ```",
    "inputs": [
      { "name": "parent", "type": "PDL_Operation" }
    ],
    "outputs": [
      { "name": "val", "type": "PDL_InstOrRangeOf" }
    ],
    "attributes": [
      { "name": "index", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($index^)? `of` $parent custom<ResultsValueType>(ref($index), type($val))\n    attr-dict"
  },
  {
    "name": "pdl.rewrite",
    "summary": "Specify the rewrite of a matched pattern",
    "description": "`pdl.rewrite` operations terminate the region of a `pdl.pattern` and specify\n    the main rewrite of a `pdl.pattern`, on the optional root operation. The\n    rewrite is specified either via a string name (`name`) to a native\n    rewrite function, or via the region body. The rewrite region, if specified,\n    must contain a single block. If the rewrite is external it functions\n    similarly to `pdl.apply_native_rewrite`, and takes a set of additional\n    positional values defined within the matcher as arguments. If the rewrite is\n    external, the root operation is passed to the native function as the leading\n    arguments. The root operation, if provided, specifies the starting point in\n    the pattern for the subgraph isomorphism search. Pattern matching will proceed\n    from this node downward (towards the defining operation) or upward\n    (towards the users) until all the operations in the pattern have been matched.\n    If the root is omitted, the pdl_interp lowering will automatically select\n    the best root of the pdl.rewrite among all the operations in the pattern.\n\n    Example:\n\n    ```mlir\n    // Specify an external rewrite function:\n    pdl.rewrite %root with \"myExternalRewriter\"(%value : !pdl.value)\n\n    // Specify a rewrite inline using PDL with the given root:\n    pdl.rewrite %root {\n      %op = pdl.operation \"foo.op\"(%arg0, %arg1)\n      pdl.replace %root with %op\n    }\n\n    // Specify a rewrite inline using PDL, automatically selecting root:\n    pdl.rewrite {\n      %op1 = pdl.operation \"foo.op\"(%arg0, %arg1)\n      %op2 = pdl.operation \"bar.op\"(%arg0, %arg1)\n      pdl.replace %root1 with %op1\n      pdl.replace %root2 with %op2\n    }\n    ```",
    "inputs": [
      { "name": "root", "type": "Optional" },
      { "name": "externalArgs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "name", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($root^)? (`with` $name^ (`(` $externalArgs^ `:` type($externalArgs) `)`)?)?\n              ($bodyRegion^)?\n    attr-dict-with-keyword"
  },
  {
    "name": "pdl.type",
    "summary": "Define a type handle within a pattern",
    "description": "`pdl.type` operations capture result type constraints of `Attributes`,\n    `Values`, and `Operations`. Instances of this operation define, and\n    partially constrain, results types of a given entity. A `pdl.type` may\n    partially constrain the result by specifying a constant `Type`.\n\n    Example:\n\n    ```mlir\n    // Define a type:\n    %type = pdl.type\n\n    // Define a type with a constant value:\n    %type = pdl.type : i32\n    ```",
    "outputs": [
      { "name": "result", "type": "PDL_Type" }
    ],
    "attributes": [
      { "name": "constantType", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "attr-dict (`:` $constantType^)?"
  },
  {
    "name": "pdl.types",
    "summary": "Define a range of type handles within a pattern",
    "description": "`pdl.types` operations capture result type constraints of `Value`s, and\n    `Operation`s. Instances of this operation define results types of a given\n    entity. A `pdl.types` may partially constrain the results by specifying\n    an array of `Type`s.\n\n    Example:\n\n    ```mlir\n    // Define a range of types:\n    %types = pdl.types\n\n    // Define a range of types with a range of constant values:\n    %types = pdl.types : [i32, i64, i32]\n    ```",
    "outputs": [
      { "name": "result", "type": "PDL_RangeOf" }
    ],
    "attributes": [
      { "name": "constantTypes", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "attr-dict (`:` $constantTypes^)?"
  },
  {
    "name": "ptr.constant",
    "summary": "Pointer constant operation",
    "description": "The `constant` operation produces a pointer constant. The attribute must be\n    a typed attribute of pointer type.\n\n    Example:\n\n    ```mlir\n    // Create a null pointer\n    %null = ptr.constant #ptr.null : !ptr.ptr<#ptr.generic_space>\n    ```",
    "outputs": [
      { "name": "result", "type": "Ptr_PtrType" }
    ],
    "attributes": [
      { "name": "value", "type": "TypedAttrInterface" }
    ],
    "assemblyFormat": "attr-dict $value"
  },
  {
    "name": "ptr.from_ptr",
    "summary": "Casts a `!ptr.ptr` value to a ptr-like value.",
    "description": "The `from_ptr` operation casts a `ptr` value to a ptr-like object. It's\n    important to note that:\n    - The ptr-like object cannot be a `!ptr.ptr`.\n    - The memory-space of both the `ptr` and ptr-like object must match.\n    - The cast is Pure (no UB and side-effect free).\n\n    The optional `metadata` operand exists to provide any ptr-like metadata\n    that might be required to perform the cast.\n\n    Example:\n\n    ```mlir\n    %typed_ptr = ptr.from_ptr %ptr : !ptr.ptr<#ptr.generic_space> -> !my.ptr<f32, #ptr.generic_space>\n    %memref = ptr.from_ptr %ptr metadata %md : !ptr.ptr<#ptr.generic_space> -> memref<f32, #ptr.generic_space>\n\n    // Cast the `%ptr` to a memref without utilizing metadata.\n    %memref = ptr.from_ptr %ptr : !ptr.ptr<#ptr.generic_space> -> memref<f32, #ptr.generic_space>\n    ```",
    "inputs": [
      { "name": "ptr", "type": "Ptr_PtrType" },
      { "name": "metadata", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "PtrLikeTypeInterface" }
    ],
    "assemblyFormat": "$ptr (`metadata` $metadata^)? attr-dict `:` type($ptr) `->` type($result)"
  },
  {
    "name": "ptr.gather",
    "summary": "Gather operation",
    "description": "The `gather` operation performs conditional loads from multiple memory\n    locations specified by `ptrs` based on a mask `mask`. Elements of the\n    result corresponding to masked-off lanes are taken from the passthrough\n    operand.\n\n    The mask operand is a shaped type of `i1` elements that must have the same\n    shape as the result type.\n\n    Examples:\n    ```mlir\n    // Gather values from multiple memory locations\n    %result = ptr.gather %ptrs, %mask, %passthrough :\n      vector<4x!ptr.ptr<#ptr.generic_space>> -> vector<4xf32>\n\n    // Gather with alignment\n    %result = ptr.gather %ptrs, %mask, %passthrough alignment = 8 :\n      vector<4x!ptr.ptr<#ptr.generic_space>> -> vector<4xf32>\n    ```",
    "inputs": [
      { "name": "ptrs", "type": "Ptr_Ptr1DType" },
      { "name": "mask", "type": "Ptr_Mask1DType" },
      { "name": "passthrough", "type": "Ptr_Any1DType" },
      { "name": "alignment", "type": "AlignmentProp" }
    ],
    "outputs": [
      { "name": "result", "type": "Ptr_Any1DType" }
    ],
    "assemblyFormat": "$ptrs `,` $mask `,` $passthrough (`alignment` `=` $alignment^)?\n    attr-dict `:` type($ptrs) `->` type($result)"
  },
  {
    "name": "ptr.get_metadata",
    "summary": "SSA value representing pointer metadata.",
    "description": "The `get_metadata` operation produces an opaque value that encodes the\n    metadata of the ptr-like type.\n\n    Example:\n\n    ```mlir\n    %metadata = ptr.get_metadata %memref : memref<?x?xf32>\n    ```",
    "inputs": [
      { "name": "ptr", "type": "PtrLikeTypeInterface" }
    ],
    "outputs": [
      { "name": "result", "type": "Ptr_PtrMetadata" }
    ],
    "assemblyFormat": "$ptr attr-dict `:` type($ptr)"
  },
  {
    "name": "ptr.load",
    "description": "The `load` operation is used to read from memory. A load may be marked as\n    atomic, volatile, and/or nontemporal.\n\n    An atomic load only supports a limited set of value types, and requires\n    an explicit alignment.\n\n    Examples:\n    ```mlir\n    // A volatile load of a float variable.\n    %0 = ptr.load volatile %ptr : !ptr.ptr -> f32\n\n    // A nontemporal load of a float variable.\n    %0 = ptr.load %ptr nontemporal : !ptr.ptr -> f32\n\n    // An atomic load of an integer variable.\n    %0 = ptr.load %ptr atomic monotonic alignment = 8 : !ptr.ptr -> i64\n    ```\n\n    See the following link for more details on the meaning of `alignment`,\n    `volatile_`, `nontemporal`, `invariant`, `invariant_group`, `ordering`,\n    and `syncscope`:\n    https://llvm.org/docs/LangRef.html#load-instruction",
    "inputs": [
      { "name": "ptr", "type": "Ptr_PtrType" },
      { "name": "alignment", "type": "AlignmentProp" },
      { "name": "volatile_", "type": "UnitProp" },
      { "name": "nontemporal", "type": "UnitProp" },
      { "name": "invariant", "type": "UnitProp" },
      { "name": "invariantGroup", "type": "UnitProp" },
      { "name": "ordering", "type": "DefaultValuedProp" }
    ],
    "outputs": [
      { "name": "value", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "syncscope", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`volatile` $volatile_^)? $ptr\n    (`atomic` (`syncscope` `(` $syncscope^ `)`)? $ordering^)?\n    oilist(\n      `nontemporal` $nontemporal |\n      `invariant` $invariant |\n      `invariant_group` $invariantGroup |\n      `alignment` `=` $alignment\n    )\n    attr-dict `:` qualified(type($ptr)) `->` type($value)"
  },
  {
    "name": "ptr.masked_load",
    "summary": "Masked load operation",
    "description": "The `masked_load` operation performs a conditional load from memory based\n    on  a mask. Elements of the result corresponding to masked-off lanes are\n    taken from the passthrough operand.\n\n    The mask operand is a shaped type of `i1` elements that must have the same\n    shape as the result type.\n\n    Examples:\n    ```mlir\n    // Masked load with passthrough on vectors\n    %result = ptr.masked_load %ptr, %mask, %passthrough :\n      !ptr.ptr<#ptr.generic_space> -> vector<4xf32>\n\n    // Masked load with passthrough on tensors\n    %result = ptr.masked_load %ptr, %mask, %passthrough :\n      !ptr.ptr<#ptr.generic_space> -> tensor<4xf32>\n    ```",
    "inputs": [
      { "name": "ptr", "type": "Ptr_PtrType" },
      { "name": "mask", "type": "Ptr_Mask1DType" },
      { "name": "passthrough", "type": "Ptr_Any1DType" },
      { "name": "alignment", "type": "AlignmentProp" }
    ],
    "outputs": [
      { "name": "result", "type": "Ptr_Any1DType" }
    ],
    "assemblyFormat": "$ptr `,` $mask `,` $passthrough (`alignment` `=` $alignment^)?\n    attr-dict `:` qualified(type($ptr)) `->` type($result)"
  },
  {
    "name": "ptr.masked_store",
    "summary": "Masked store operation",
    "description": "The `masked_store` operation performs a conditional store to memory based\n    on  a mask. Only elements corresponding to set bits in the mask are written\n    to memory.\n\n    The mask operand is a shaped type of `i1` elements that must have the same\n    shape as the value being stored.\n\n    Examples:\n    ```mlir\n    // Masked store\n    ptr.masked_store %value, %ptr, %mask :\n      vector<4xf32>, !ptr.ptr<#ptr.generic_space>\n\n    // Masked store with alignment\n    ptr.masked_store %value, %ptr, %mask alignment = 8 :\n      vector<4xf32>, !ptr.ptr<#ptr.generic_space>\n    ```",
    "inputs": [
      { "name": "value", "type": "Ptr_Any1DType" },
      { "name": "ptr", "type": "Ptr_PtrType" },
      { "name": "mask", "type": "Ptr_Mask1DType" },
      { "name": "alignment", "type": "AlignmentProp" }
    ],
    "assemblyFormat": "$value `,` $ptr `,` $mask (`alignment` `=` $alignment^)? attr-dict `:`\n    type($value) `,` qualified(type($ptr))"
  },
  {
    "name": "ptr.ptr_add",
    "summary": "Pointer add operation",
    "description": "The `ptr_add` operation adds an int-like offset to one or more pointers to produce one or more new pointers.\n\n    The operation supports both scalar and shaped types with value semantics:\n    - When both base and offset are scalar: produces a single new pointer\n    - When base is shaped and offset is scalar: adds the same offset to each\n    pointer in the base\n    - When base is scalar and offset is shaped: adds the single pointer to each\n    offset in the shaped value\n    - When both are shaped: performs element-wise addition (shapes must be\n    compatible)\n\n    Example:\n\n    ```mlir\n    // Scalar base and offset\n    %x_off  = ptr.ptr_add %x, %off : !ptr.ptr<#ptr.generic_space>, i32\n    %x_off0 = ptr.ptr_add nusw %x, %off : !ptr.ptr<#ptr.generic_space>, i32\n\n    // Shaped base with scalar offset\n    %ptrs_off = ptr.ptr_add %ptrs, %off : vector<4x!ptr.ptr<#ptr.generic_space>>, i32\n\n    // Scalar base with shaped offset\n    %x_offs = ptr.ptr_add %x, %offs : !ptr.ptr<#ptr.generic_space>, vector<4xi32>\n\n    // Both base and offset are shaped\n    %ptrs_offs = ptr.ptr_add %ptrs, %offs : vector<4x!ptr.ptr<#ptr.generic_space>>, vector<4xi32>\n    ```",
    "inputs": [
      { "name": "base", "type": "Ptr_PtrLikeType" },
      { "name": "offset", "type": "Ptr_IntLikeType" },
      { "name": "flags", "type": "DefaultValuedProp" }
    ],
    "outputs": [
      { "name": "result", "type": "Ptr_PtrLikeType" }
    ],
    "assemblyFormat": "($flags^)? $base `,` $offset attr-dict `:` type($base) `,` type($offset)"
  },
  {
    "name": "ptr.ptr_diff",
    "summary": "Pointer difference operation",
    "description": "The `ptr_diff` operation computes the difference between two pointers,\n    returning an integer or index value representing the number of bytes\n    between them.\n\n    The operation supports both scalar and shaped types with value semantics:\n    - When both operands are scalar: produces a single difference value\n    - When both are shaped: performs element-wise subtraction,\n      shapes must be the same\n\n    The operation also supports the following flags:\n    - `none`: No flags are set.\n    - `nuw`: No Unsigned Wrap, if the subtraction causes an unsigned overflow\n      (that is: the result would be negative), the result is a poison value.\n    - `nsw`: No Signed Wrap, if the subtraction causes a signed overflow, the\n      result is a poison value.\n\n    NOTE: The pointer difference is calculated using an integer type specified\n    by the data layout. The final result will be sign-extended or truncated to\n    fit the result type as necessary.\n\n    Example:\n\n    ```mlir\n    // Scalar pointers\n    %diff = ptr.ptr_diff %p1, %p2 : !ptr.ptr<#ptr.generic_space> -> i64\n\n    // Shaped pointers\n    %diffs = ptr.ptr_diff nsw %ptrs1, %ptrs2 :\n      vector<4x!ptr.ptr<#ptr.generic_space>> -> vector<4xi64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "Ptr_PtrLikeType" },
      { "name": "rhs", "type": "Ptr_PtrLikeType" },
      { "name": "flags", "type": "DefaultValuedProp" }
    ],
    "outputs": [
      { "name": "result", "type": "Ptr_IntLikeType" }
    ],
    "assemblyFormat": "($flags^)? $lhs `,` $rhs attr-dict `:` type($lhs) `->` type($result)"
  },
  {
    "name": "ptr.scatter",
    "summary": "Scatter operation",
    "description": "The `scatter` operation performs a conditional store of a value `value` to\n    multiple memory locations specified by `ptrs` based on a mask `mask`.\n\n    Only elements corresponding to set bits in the mask are written to memory.\n    The mask operand is a shaped type of `i1` elements that must have the same\n    shape as the value being stored.\n\n    Examples:\n    ```mlir\n    // Scatter values to multiple memory locations\n    ptr.scatter %value, %ptrs, %mask :\n      vector<4xf32>, vector<4x!ptr.ptr<#ptr.generic_space>>\n\n    // Scatter with alignment\n    ptr.scatter %value, %ptrs, %mask alignment = 8 :\n      vector<4xf32>, vector<4x!ptr.ptr<#ptr.generic_space>>\n    ```",
    "inputs": [
      { "name": "value", "type": "Ptr_Any1DType" },
      { "name": "ptrs", "type": "Ptr_Ptr1DType" },
      { "name": "mask", "type": "Ptr_Mask1DType" },
      { "name": "alignment", "type": "AlignmentProp" }
    ],
    "assemblyFormat": "$value `,` $ptrs `,` $mask  (`alignment` `=` $alignment^)?\n    attr-dict `:` type($value) `,` type($ptrs)"
  },
  {
    "name": "ptr.store",
    "description": "The `store` operation is used to write to memory. A store may be marked as\n    atomic, volatile, and/or nontemporal.\n\n    An atomic store only supports a limited set of value types, and requires\n    an explicit alignment.\n\n    Examples:\n    ```mlir\n    // A volatile store of a float variable.\n    ptr.store volatile %val, %ptr : f32, !ptr.ptr\n\n    // A nontemporal store of a float variable.\n    ptr.store %val, %ptr nontemporal : f32, !ptr.ptr\n\n    // An atomic store of an integer variable.\n    ptr.store %val, %ptr atomic monotonic alignment = 8: i64, !ptr.ptr\n    ```\n\n    See the following link for more details on the meaning of `alignment`,\n    `volatile_`, `nontemporal`, `invariant_group`, `ordering`, and `syncscope`:\n    https://llvm.org/docs/LangRef.html#store-instruction",
    "inputs": [
      { "name": "value", "type": "AnyType" },
      { "name": "ptr", "type": "Ptr_PtrType" },
      { "name": "alignment", "type": "AlignmentProp" },
      { "name": "volatile_", "type": "UnitProp" },
      { "name": "nontemporal", "type": "UnitProp" },
      { "name": "invariantGroup", "type": "UnitProp" },
      { "name": "ordering", "type": "DefaultValuedProp" }
    ],
    "attributes": [
      { "name": "syncscope", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`volatile` $volatile_^)? $value `,` $ptr\n    (`atomic` (`syncscope` `(` $syncscope^ `)`)? $ordering^)?\n    oilist(\n      `nontemporal` $nontemporal |\n      `invariant_group` $invariantGroup |\n      `alignment` `=` $alignment\n    )\n    attr-dict `:` type($value) `,` qualified(type($ptr))"
  },
  {
    "name": "ptr.to_ptr",
    "summary": "Casts a ptr-like value to a `!ptr.ptr` value.",
    "description": "The `to_ptr` operation casts a ptr-like object to a `!ptr.ptr`. It's\n    important to note that:\n    - The ptr-like object cannot be a `!ptr.ptr`.\n    - The memory-space of both the `ptr` and ptr-like object must match.\n    - The cast is side-effect free.\n\n    Example:\n\n    ```mlir\n    %ptr0 = ptr.to_ptr %my_ptr : !my.ptr<f32, #ptr.generic_space> -> !ptr.ptr<#ptr.generic_space>\n    %ptr1 = ptr.to_ptr %memref : memref<f32, #ptr.generic_space> -> !ptr.ptr<#ptr.generic_space>\n    ```",
    "inputs": [
      { "name": "ptr", "type": "PtrLikeTypeInterface" }
    ],
    "outputs": [
      { "name": "result", "type": "Ptr_PtrType" }
    ],
    "assemblyFormat": "$ptr attr-dict `:` type($ptr) `->` type($result)"
  },
  {
    "name": "ptr.type_offset",
    "summary": "Type offset operation",
    "description": "The `type_offset` operation produces an int or index-typed SSA value\n    equal to a target-specific constant representing the offset of a single\n    element of the given type.\n\n    Example:\n\n    ```mlir\n    // Return the offset between two f32 stored in memory\n    %0 = ptr.type_offset f32 : index\n    // Return the offset between two memref descriptors stored in memory\n    %1 = ptr.type_offset memref<12 x f64> : i32\n    ```",
    "outputs": [
      { "name": "result", "type": "AnySignlessIntegerOrIndex" }
    ],
    "attributes": [
      { "name": "elementType", "type": "TypeAttr" }
    ],
    "assemblyFormat": "$elementType attr-dict `:` type($result)"
  },
  {
    "name": "quant.dcast",
    "summary": "Dequantize cast operation",
    "description": "Convert an input quantized value into its expressed floating-point value.\n    The dequantization process consists of the following steps:\n\n    ```\n    def dequantize(quantizedValue: quantizedType) -> expressedType:\n        storedValue = reinterpretCast(quantizedValue, storageType)\n        storedValueFloat = convertIntToFloat(storedValue, expressedType)\n        zeroPointFloat = convertIntToFloat(zeroPoint, expressedType)\n        expressedValue = (storedValueFloat - zeroPointFloat) * scale\n        return expressedValue\n    ```\n\n    Here, `storageType`, `expressedType`, `scale`, and `zeroPoint` are obtained\n    from the corresponding parameters encoded in `quantizedType`. For\n    per-channel quantization, the appropriate `scale` and `zeroPoint` values\n    are used for each tensor element computation according to the channel the\n    element belongs to.\n    \n    The numerical results produced by the algorithm above may vary depending on\n    the rounding methods used by `convertIntToFloat()`, subtraction (`-`), and\n    multiplication (`*`). This operation does not define specific rounding\n    methods; instead, it is the responsibility of a transform pipeline to\n    determine which rounding method to apply when this operation is broken down\n    into lower-level dialects.\n\n    The operation must satisfy the following syntactic constraints:\n\n    - Operand `input` must be a scalar or tensor of type `!quant.uniform`.\n\n    - The result type must be a floating-point scalar or tensor.\n\n    - The `expressedType` parameter of the `!quant.uniform` type of the input\n      must match the floating-point type of the result.\n\n    - The operand and result types must be both scalars or both tensors. If\n      tensors, they must be both ranked or both unranked. If ranked, both must\n      have the same shape, including matching static and dynamic dimensions.\n\n    - If the operand uses per-channel quantization, its `!quant.uniform` type\n      must adhere to the [Per-axis quantization\n      integrity](#per-axis-quantization-integrity) guidelines.\n\n    Examples:\n\n    ```\n    // Dequantize a scalar quantized value\n    %result = quant.dcast %input : !quant.uniform<i8:f32, 2.0> to f32\n\n    // Dequantize a dynamically shaped tensor of quantized values\n    %result = quant.dcast %input : tensor<?x!quant.uniform<i8:f32, 2.0>> to tensor<?xf32>\n\n    // Dequantize an unranked tensor using per-axis quantization information\n    %result = quant.dcast %input : tensor<*x!quant.uniform<i8:f32:1, {2.0, 3.0}>> to tensor<*xf32>\n    ```",
    "inputs": [
      { "name": "input", "type": "quant_QuantizedScalarOrTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "quant_FloatScalarOrTensor" }
    ],
    "assemblyFormat": "$input attr-dict `:` type($input) `to` type($result)"
  },
  {
    "name": "quant.qcast",
    "summary": "Quantize cast operation",
    "description": "Convert a floating-point value to a quantized type. The quantization\n    process consists of the following steps:\n\n    ```\n    def quantize(expressedValue: expressedType) -> quantizedType:\n        zeroPointFloat = convertIntToFloat(zeroPoint, expressedType)\n        scaledValue = expressedValue / scale\n        storedValueFloat = scaledValue + zeroPointFloat\n        storedValue = convertFloatToInt(storedValueFloat, storageType)\n        storedValueClamped = clamp(storedValue, storageMin, storageMax)\n        quantizedValue = reinterpretCast(storedValueClamped, quantizedType)\n        return quantizedValue\n    ```\n\n    Here, `storageType`, `storageMin`, `storageMax`, `expressedType`, `scale`,\n    and `zeroPoint` are obtained from the corresponding parameters encoded in\n    `quantizedType`. For per-channel quantization, the appropriate `scale` and\n    `zeroPoint` values are used for each tensor element computation according\n    to the channel the element belongs to.\n\n    The numerical results produced by the algorithm above may vary depending on\n    the rounding methods used by `convertIntToFloat()`, `convertFloatToInt()`,\n    `clamp()`, division (`/`), and addition (`+`). This operation does not\n    define specific rounding methods; instead, it is the responsibility of a\n    transform pipeline to determine which rounding method to apply when this\n    operation is broken down into lower-level dialects.\n\n    The operation must satisfy the following syntactic constraints:\n\n    - Operand `input` must be a floating-point scalar or tensor.\n\n    - The result type must be a scalar or tensor of type `!quant.uniform`.\n\n    - The `expressedType` parameter in the `!quant.uniform` type of the result\n      must match the floating-point type of the input.\n\n    - The operand and result types must be both scalars or both tensors. If\n      tensors, they must be both ranked or both unranked. If ranked, both must\n      have the same shape, including matching static and dynamic dimensions.\n\n    - If the result uses per-channel quantization, its `!quant.uniform` type\n      must adhere to the [Per-axis quantization\n      integrity](#per-axis-quantization-integrity) guidelines.\n\n    Examples:\n\n    ```\n    // Quantize a scalar floating-point value\n    %result = quant.qcast %input : f32 to !quant.uniform<i8:f32, 2.0>\n\n    // Quantize a dynamically shaped tensor of quantized values\n    %result = quant.qcast %input : tensor<?xf32> to tensor<?x!quant.uniform<i8:f32, 2.0>>\n\n    // Quantize an unranked tensor using per-axis quantization information\n    %result = quant.qcast %input : tensor<*xf32> to tensor<*x!quant.uniform<i8:f32:1, {2.0, 3.0}>>\n    ```",
    "inputs": [
      { "name": "input", "type": "quant_FloatScalarOrTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "quant_QuantizedScalarOrTensor" }
    ],
    "assemblyFormat": "$input attr-dict `:` type($input) `to` type($result)"
  },
  {
    "name": "quant.scast",
    "summary": "Storage cast operation",
    "description": "Convert a value from a quantized type to the corresponding signless integer\n    storage type, or vice versa. This conversion simply involves a\n    reinterpretation of the input bits and does not involve any data\n    manipulation.\n\n    The following syntactic restrictions must be met:\n\n    - Operand `input` must be a scalar or tensor of a signless integer or\n      `!quant.uniform` type.\n\n    - The result must be a scalar or tensor of a signless integer or\n      `!quant.uniform` type.\n\n    - If the operand is a scalar or tensor of type integer, the result must be\n      a scalar or tensor of type `!quant.uniform`, and vice versa.\n\n    - The operand and result must be both scalars or both tensors. If tensors,\n      they must be both ranked or both unranked. If ranked, both must have the\n      same shape, including matching static and dynamic dimensions.\n\n    - The width of the `storageType` parameter of the quantized type of the\n      operand or result must match the width of the signless integer type of\n      the operand or result.\n\n    - If the operand or result uses per-channel quantization, its\n      `!quant.uniform` type must adhere to the [Per-axis quantization\n      integrity](#per-axis-quantization-integrity) guidelines.\n\n    Examples:\n\n    ```\n    // Cast a scalar quantized value into its storage type\n    %result = quant.scast %input : !quant.uniform<i8:f32, 2.0> to i8\n\n    // Cast a dynamically shaped tensor of quantized values into their storage type\n    %result = quant.scast %input : tensor<?x!quant.uniform<i8:f32, 2.0>> to tensor<?xi8>\n\n    // Cast an unranked tensor of signless integers into a quantized type using\n    // per-channel quantization\n    %result = quant.scast %input : tensor<*xi8> to tensor<*x!quant.uniform<i8:f32:1, {2.0, 3.0}>>\n    ```",
    "inputs": [
      { "name": "input", "type": "quant_IntegerOrQuantizedScalarOrTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "quant_IntegerOrQuantizedScalarOrTensor" }
    ],
    "assemblyFormat": "$input attr-dict `:` type($input) `to` type($result)"
  },
  {
    "name": "scf.condition",
    "summary": "loop continuation condition",
    "description": "This operation accepts the continuation (i.e., inverse of exit) condition\n    of the `scf.while` construct. If its first argument is true, the \"after\"\n    region of `scf.while` is executed, with the remaining arguments forwarded\n    to the entry block of the region. Otherwise, the loop terminates.",
    "inputs": [
      { "name": "condition", "type": "I1" },
      { "name": "args", "type": "Variadic" }
    ],
    "assemblyFormat": "`(` $condition `)` attr-dict ($args^ `:` type($args))?"
  },
  {
    "name": "scf.execute_region",
    "summary": "operation that executes its region exactly once",
    "description": "The `scf.execute_region` operation is used to allow multiple blocks within SCF\n    and other operations which can hold only one block.  The `scf.execute_region`\n    operation executes the region held exactly once and cannot have any operands.\n    As such, its region has no arguments. All SSA values that dominate the op can\n    be accessed inside the op. The op's region can have multiple blocks and the\n    blocks can have multiple distinct terminators. Values returned from this op's\n    region define the op's results.\n    The optional 'no_inline' flag can be set to request the ExecuteRegionOp to be\n    preserved as much as possible and not being inlined in the parent block until\n    an explicit lowering step.\n\n    Example:\n\n    ```mlir\n    scf.for %i = 0 to 128 step %c1 {\n      %y = scf.execute_region -> i32 {\n        %x = load %A[%i] : memref<128xi32>\n        scf.yield %x : i32\n      }\n    }\n\n    // the same as above but with no_inline attribute\n    scf.for %i = 0 to 128 step %c1 {\n      %y = scf.execute_region -> i32 no_inline {\n        %x = load %A[%i] : memref<128xi32>\n        scf.yield %x : i32\n      }\n    }\n\n    affine.for %i = 0 to 100 {\n      \"foo\"() : () -> ()\n      %v = scf.execute_region -> i64 {\n        cf.cond_br %cond, ^bb1, ^bb2\n\n      ^bb1:\n        %c1 = arith.constant 1 : i64\n        cf.br ^bb3(%c1 : i64)\n\n      ^bb2:\n        %c2 = arith.constant 2 : i64\n        cf.br ^bb3(%c2 : i64)\n\n      ^bb3(%x : i64):\n        scf.yield %x : i64\n      }\n      \"bar\"(%v) : (i64) -> ()\n    }\n    ```",
    "attributes": [
      { "name": "no_inline", "type": "UnitAttr" }
    ]
  },
  {
    "name": "scf.for",
    "summary": "for operation",
    "description": "The `scf.for` operation represents a loop taking 3 SSA value as operands\n    that represent the lower bound, upper bound and step respectively. The\n    operation defines an SSA value for its induction variable. It has one\n    region capturing the loop body. The induction variable is represented as an\n    argument of this region. This SSA value is a signless integer or index.\n    The step is a value of same type but required to be positive, the lower and\n    upper bounds can be also negative or zero. The lower and upper bounds\n    specify a half-open range: the iteration is executed iff the comparison of\n    induction variable value is less than the upper bound and bigger or equal\n    to the lower bound.\n\n    By default, the integer comparison is signed. If the `unsignedCmp` unit\n    attribute is specified, the integer comparison is unsigned.\n\n    The body region must contain exactly one block that terminates with\n    `scf.yield`. Calling ForOp::build will create such a region and insert\n    the terminator implicitly if none is defined, so will the parsing even in\n    cases when it is absent from the custom format. For example:\n\n    ```mlir\n    // Index case.\n    scf.for %iv = %lb to %ub step %step {\n      ... // body\n    }\n    ...\n    // Unsigned integer case.\n    scf.for unsigned %iv_32 = %lb_32 to %ub_32 step %step_32 : i32 {\n      ... // body\n    }\n    ```\n\n    `scf.for` can also operate on loop-carried variables and returns the final\n    values after loop termination. The initial values of the variables are\n    passed as additional SSA operands to the `scf.for` following the 3 loop\n    control SSA values mentioned above (lower bound, upper bound and step). The\n    operation region has an argument for the induction variable, followed by\n    one argument for each loop-carried variable, representing the value of the\n    variable at the current iteration.\n\n    The region must terminate with a `scf.yield` that passes the current\n    values of all loop-carried variables to the next iteration, or to the\n    `scf.for` result, if at the last iteration. The static type of a\n    loop-carried variable may not change with iterations; its runtime type is\n    allowed to change. Note, that when the loop-carried variables are present,\n    calling ForOp::build will not insert the terminator implicitly. The caller\n    must insert `scf.yield` in that case.\n\n    `scf.for` results hold the final values after the last iteration.\n    For example, to sum-reduce a memref:\n\n    ```mlir\n    func.func @reduce(%buffer: memref<1024xf32>, %lb: index,\n                      %ub: index, %step: index) -> (f32) {\n      // Initial sum set to 0.\n      %sum_0 = arith.constant 0.0 : f32\n      // iter_args binds initial values to the loop's region arguments.\n      %sum = scf.for %iv = %lb to %ub step %step\n          iter_args(%sum_iter = %sum_0) -> (f32) {\n        %t = load %buffer[%iv] : memref<1024xf32>\n        %sum_next = arith.addf %sum_iter, %t : f32\n        // Yield current iteration sum to next iteration %sum_iter or to %sum\n        // if final iteration.\n        scf.yield %sum_next : f32\n      }\n      return %sum : f32\n    }\n    ```\n\n    If the `scf.for` defines any values, a yield must be explicitly present.\n    The number and types of the `scf.for` results must match the initial\n    values in the `iter_args` binding and the yield operands.\n\n    Another example with a nested `scf.if` (see `scf.if` for details) to\n    perform conditional reduction:\n\n    ```mlir\n    func.func @conditional_reduce(%buffer: memref<1024xf32>, %lb: index,\n                                  %ub: index, %step: index) -> (f32) {\n      %sum_0 = arith.constant 0.0 : f32\n      %c0 = arith.constant 0.0 : f32\n      %sum = scf.for %iv = %lb to %ub step %step\n          iter_args(%sum_iter = %sum_0) -> (f32) {\n        %t = load %buffer[%iv] : memref<1024xf32>\n        %cond = arith.cmpf \"ugt\", %t, %c0 : f32\n        %sum_next = scf.if %cond -> (f32) {\n          %new_sum = arith.addf %sum_iter, %t : f32\n          scf.yield %new_sum : f32\n        } else {\n          scf.yield %sum_iter : f32\n        }\n        scf.yield %sum_next : f32\n      }\n      return %sum : f32\n    }\n    ```",
    "inputs": [
      { "name": "lowerBound", "type": "AnySignlessIntegerOrIndex" },
      { "name": "upperBound", "type": "AnySignlessIntegerOrIndex" },
      { "name": "step", "type": "AnySignlessIntegerOrIndex" },
      { "name": "initArgs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "unsignedCmp", "type": "UnitAttr" }
    ]
  },
  {
    "name": "scf.forall",
    "summary": "evaluate a block multiple times in parallel",
    "description": "`scf.forall` is a target-independent multi-dimensional parallel\n    region application operation. It has exactly one block that represents the\n    parallel body and it takes index operands that specify lower bounds, upper\n    bounds and steps.\n\n    The op also takes a variadic number of tensor operands (`shared_outs`).\n    The future buffers corresponding to these tensors are shared among all\n    threads. Shared tensors should be accessed via their corresponding block\n    arguments. If multiple threads write to a shared buffer in a racy\n    fashion, these writes will execute in some unspecified order. Tensors that\n    are not shared can be used inside the body (i.e., the op is not isolated\n    from above); however, if a use of such a tensor bufferizes to a memory\n    write, the tensor is privatized, i.e., a thread-local copy of the tensor is\n    used. This ensures that memory side effects of a thread are not visible to\n    other threads (or in the parent body), apart from explicitly shared tensors.\n\n    The name \"thread\" conveys the fact that the parallel execution is mapped\n    (i.e. distributed) to a set of virtual threads of execution, one function\n    application per thread. Further lowerings are responsible for specifying\n    how this is materialized on concrete hardware resources.\n\n    An optional `mapping` is an attribute array that specifies processing units\n    with their dimension, how it remaps 1-1 to a set of concrete processing\n    element resources (e.g. a CUDA grid dimension or a level of concrete nested\n    async parallelism). It is expressed via any attribute that implements the\n    device mapping interface. It is the reponsibility of the lowering mechanism\n    to interpret the `mapping` attributes in the context of the concrete target\n    the op is lowered to, or to ignore it when the specification is ill-formed\n    or unsupported for a particular target.\n\n    The only allowed terminator is `scf.forall.in_parallel`.\n    `scf.forall` returns one value per `shared_out` operand. The\n    actions of the `scf.forall.in_parallel` terminators specify how to combine the\n    partial results of all parallel invocations into a full value, in some\n    unspecified order. The \"destination\" of each such op must be a `shared_out`\n    block argument of the `scf.forall` op.\n\n    The actions involved in constructing the return values are further described\n    by `tensor.parallel_insert_slice`.\n\n    `scf.forall` acts as an implicit synchronization point.\n\n    When the parallel function body has side effects, their order is unspecified\n    across threads.\n\n    `scf.forall` can be printed in two different ways depending on\n    whether the loop is normalized or not. The loop is 'normalized' when all\n    lower bounds are equal to zero and steps are equal to one. In that case,\n    `lowerBound` and `step` operands will be omitted during printing.\n\n    Normalized loop example:\n\n    ```mlir\n    //\n    // Sequential context.\n    //\n    %matmul_and_pointwise:2 = scf.forall (%thread_id_1, %thread_id_2) in\n        (%num_threads_1, %numthread_id_2) shared_outs(%o1 = %C, %o2 = %pointwise)\n      -> (tensor<?x?xT>, tensor<?xT>) {\n      //\n      // Parallel context, each thread with id = (%thread_id_1, %thread_id_2)\n      // runs its version of the code.\n      //\n      %sA = tensor.extract_slice %A[f((%thread_id_1, %thread_id_2))]:\n        tensor<?x?xT> to tensor<?x?xT>\n      %sB = tensor.extract_slice %B[g((%thread_id_1, %thread_id_2))]:\n        tensor<?x?xT> to tensor<?x?xT>\n      %sC = tensor.extract_slice %o1[h((%thread_id_1, %thread_id_2))]:\n        tensor<?x?xT> to tensor<?x?xT>\n      %sD = linalg.matmul\n        ins(%sA, %sB : tensor<?x?xT>, tensor<?x?xT>)\n        outs(%sC : tensor<?x?xT>)\n\n      %spointwise = subtensor %o2[i((%thread_id_1, %thread_id_2))]:\n        tensor<?xT> to tensor<?xT>\n      %sE = linalg.add ins(%spointwise : tensor<?xT>) outs(%sD : tensor<?xT>)\n\n      scf.forall.in_parallel {\n        tensor.parallel_insert_slice %sD into %o1[h((%thread_id_1, %thread_id_2))]:\n          tensor<?x?xT> into tensor<?x?xT>\n\n        tensor.parallel_insert_slice %spointwise into %o2[i((%thread_id_1, %thread_id_2))]:\n          tensor<?xT> into tensor<?xT>\n      }\n    }\n    // Implicit synchronization point.\n    // Sequential context.\n    //\n    ```\n\n    Loop with loop bounds example:\n\n    ```mlir\n    //\n    // Sequential context.\n    //\n    %pointwise = scf.forall (%i, %j) = (0, 0) to (%dim1, %dim2)\n      step (%tileSize1, %tileSize2) shared_outs(%o1 = %out)\n      -> (tensor<?x?xT>, tensor<?xT>) {\n      //\n      // Parallel context.\n      //\n      %sA = tensor.extract_slice %A[%i, %j][%tileSize1, %tileSize2][1, 1]\n        : tensor<?x?xT> to tensor<?x?xT>\n      %sB = tensor.extract_slice %B[%i, %j][%tileSize1, %tileSize2][1, 1]\n        : tensor<?x?xT> to tensor<?x?xT>\n      %sC = tensor.extract_slice %o[%i, %j][%tileSize1, %tileSize2][1, 1]\n        : tensor<?x?xT> to tensor<?x?xT>\n\n      %add = linalg.map {\"arith.addf\"}\n        ins(%sA, %sB : tensor<?x?xT>, tensor<?x?xT>)\n        outs(%sC : tensor<?x?xT>)\n\n      scf.forall.in_parallel {\n        tensor.parallel_insert_slice %add into\n          %o[%i, %j][%tileSize1, %tileSize2][1, 1]\n          : tensor<?x?xT> into tensor<?x?xT>\n      }\n    }\n    // Implicit synchronization point.\n    // Sequential context.\n    //\n    ```\n\n    Example with mapping attribute:\n\n    ```mlir\n    //\n    // Sequential context. Here `mapping` is expressed as GPU thread mapping\n    // attributes\n    //\n    %matmul_and_pointwise:2 = scf.forall (%thread_id_1, %thread_id_2) in\n        (%num_threads_1, %numthread_id_2) shared_outs(...)\n      -> (tensor<?x?xT>, tensor<?xT>) {\n      //\n      // Parallel context, each thread with id = **(%thread_id_2, %thread_id_1)**\n      // runs its version of the code.\n      //\n       scf.forall.in_parallel {\n         ...\n      }\n    } { mapping = [#gpu.thread<y>, #gpu.thread<x>] }\n    // Implicit synchronization point.\n    // Sequential context.\n    //\n    ```\n\n    Example with privatized tensors:\n\n    ```mlir\n    %t0 = ...\n    %t1 = ...\n    %r = scf.forall ... shared_outs(%o = t0) -> tensor<?xf32> {\n      // %t0 and %t1 are privatized. %t0 is definitely copied for each thread\n      // because the scf.forall op's %t0 use bufferizes to a memory\n      // write. In the absence of other conflicts, %t1 is copied only if there\n      // are uses of %t1 in the body that bufferize to a memory read and to a\n      // memory write.\n      \"some_use\"(%t0)\n      \"some_use\"(%t1)\n    }\n    ```",
    "inputs": [
      { "name": "dynamicLowerBound", "type": "Variadic" },
      { "name": "dynamicUpperBound", "type": "Variadic" },
      { "name": "dynamicStep", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "staticLowerBound", "type": "DenseI64ArrayAttr" },
      { "name": "staticUpperBound", "type": "DenseI64ArrayAttr" },
      { "name": "staticStep", "type": "DenseI64ArrayAttr" },
      { "name": "mapping", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "scf.forall.in_parallel",
    "summary": "terminates a `forall` block",
    "description": "The `scf.forall.in_parallel` is a designated terminator for\n    the `scf.forall` operation.\n\n    It has a single region with a single block that contains a flat list of ops.\n    Each such op participates in the aggregate formation of a single result of\n    the enclosing `scf.forall`.\n    The result number corresponds to the position of the op in the terminator."
  },
  {
    "name": "scf.if",
    "summary": "if-then-else operation",
    "description": "The `scf.if` operation represents an if-then-else construct for\n    conditionally executing two regions of code. The operand to an if operation\n    is a boolean value. For example:\n\n    ```mlir\n    scf.if %b  {\n      ...\n    } else {\n      ...\n    }\n    ```\n\n    `scf.if` may also produce results. Which values are returned depends on\n    which execution path is taken.\n\n    Example:\n\n    ```mlir\n    %x, %y = scf.if %b -> (f32, f32) {\n      %x_true = ...\n      %y_true = ...\n      scf.yield %x_true, %y_true : f32, f32\n    } else {\n      %x_false = ...\n      %y_false = ...\n      scf.yield %x_false, %y_false : f32, f32\n    }\n    ```\n\n    The \"then\" region has exactly 1 block. The \"else\" region may have 0 or 1\n    block. In case the `scf.if` produces results, the \"else\" region must also\n    have exactly 1 block.\n\n    The blocks are always terminated with `scf.yield`. If `scf.if` defines no\n    values, the `scf.yield` can be left out, and will be inserted implicitly.\n    Otherwise, it must be explicit.\n\n    Example:\n\n    ```mlir\n    scf.if %b  {\n      ...\n    }\n    ```\n\n    The types of the yielded values must match the result types of the\n    `scf.if`.",
    "inputs": [
      { "name": "condition", "type": "I1" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "scf.index_switch",
    "summary": "switch-case operation on an index argument",
    "description": "The `scf.index_switch` is a control-flow operation that branches to one of\n    the given regions based on the values of the argument and the cases. The\n    argument is always of type `index`.\n\n    The operation always has a \"default\" region and any number of case regions\n    denoted by integer constants. Control-flow transfers to the case region\n    whose constant value equals the value of the argument. If the argument does\n    not equal any of the case values, control-flow transfer to the \"default\"\n    region.\n\n    Example:\n\n    ```mlir\n    %0 = scf.index_switch %arg0 : index -> i32\n    case 2 {\n      %1 = arith.constant 10 : i32\n      scf.yield %1 : i32\n    }\n    case 5 {\n      %2 = arith.constant 20 : i32\n      scf.yield %2 : i32\n    }\n    default {\n      %3 = arith.constant 30 : i32\n      scf.yield %3 : i32\n    }\n    ```",
    "inputs": [
      { "name": "arg", "type": "Index" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "cases", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$arg attr-dict (`->` type($results)^)?\n    custom<SwitchCases>($cases, $caseRegions) `\\n`\n    `` `default` $defaultRegion"
  },
  {
    "name": "scf.parallel",
    "summary": "parallel for operation",
    "description": "The `scf.parallel` operation represents a loop nest taking 4 groups of SSA\n    values as operands that represent the lower bounds, upper bounds, steps and\n    initial values, respectively. The operation defines a variadic number of\n    SSA values for its induction variables. It has one region capturing the\n    loop body. The induction variables are represented as an argument of this\n    region. These SSA values always have type index, which is the size of the\n    machine word. The steps are values of type index, required to be positive.\n    The lower and upper bounds specify a half-open range: the range includes\n    the lower bound but does not include the upper bound. The initial values\n    have the same types as results of `scf.parallel`. If there are no results,\n    the keyword `init` can be omitted.\n\n    Semantically we require that the iteration space can be iterated in any\n    order, and the loop body can be executed in parallel. If there are data\n    races, the behavior is undefined.\n\n    The parallel loop operation supports reduction of values produced by\n    individual iterations into a single result. This is modeled using the\n    `scf.reduce` terminator operation (see `scf.reduce` for details). The i-th\n    result of an `scf.parallel` operation is associated with the i-th initial\n    value operand, the i-th operand of the `scf.reduce` operation (the value to\n    be reduced) and the i-th region of the `scf.reduce` operation (the reduction\n    function). Consequently, we require that the number of results of an\n    `scf.parallel` op matches the number of initial values and the the number of\n    reductions in the `scf.reduce` terminator.\n\n    The body region must contain exactly one block that terminates with a\n    `scf.reduce` operation. If an `scf.parallel` op has no reductions, the\n    terminator has no operands and no regions. The `scf.parallel` parser will\n    automatically insert the terminator for ops that have no reductions if it is\n    absent.\n\n    Example:\n\n    ```mlir\n    %init = arith.constant 0.0 : f32\n    %r:2 = scf.parallel (%iv) = (%lb) to (%ub) step (%step) init (%init, %init)\n        -> f32, f32 {\n      %elem_to_reduce1 = load %buffer1[%iv] : memref<100xf32>\n      %elem_to_reduce2 = load %buffer2[%iv] : memref<100xf32>\n      scf.reduce(%elem_to_reduce1, %elem_to_reduce2 : f32, f32) {\n        ^bb0(%lhs : f32, %rhs: f32):\n          %res = arith.addf %lhs, %rhs : f32\n          scf.reduce.return %res : f32\n      }, {\n        ^bb0(%lhs : f32, %rhs: f32):\n          %res = arith.mulf %lhs, %rhs : f32\n          scf.reduce.return %res : f32\n      }\n    }\n    ```",
    "inputs": [
      { "name": "lowerBound", "type": "Variadic" },
      { "name": "upperBound", "type": "Variadic" },
      { "name": "step", "type": "Variadic" },
      { "name": "initVals", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "scf.reduce",
    "summary": "reduce operation for scf.parallel",
    "description": "The `scf.reduce` operation is the terminator for `scf.parallel` operations. It can model\n    an arbitrary number of reductions. It has one region per reduction. Each\n    region has one block with two arguments which have the same type as the\n    corresponding operand of `scf.reduce`. The operands of the op are the values\n    that should be reduce; one value per reduction.\n\n    The i-th reduction (i.e., the i-th region and the i-th operand) corresponds\n    the i-th initial value and the i-th result of the enclosing `scf.parallel`\n    op.\n\n    The `scf.reduce` operation contains regions whose entry blocks expect two\n    arguments of the same type as the corresponding operand. As the iteration\n    order of the enclosing parallel loop and hence reduction order is\n    unspecified, the results of the reductions may be non-deterministic unless\n    the reductions are associative and commutative.\n\n    The result of a reduction region (`scf.reduce.return` operand) must have the\n    same type as the corresponding `scf.reduce` operand and the corresponding\n    `scf.parallel` initial value.\n\n    Example:\n\n    ```mlir\n    %operand = arith.constant 1.0 : f32\n    scf.reduce(%operand : f32) {\n      ^bb0(%lhs : f32, %rhs: f32):\n        %res = arith.addf %lhs, %rhs : f32\n        scf.reduce.return %res : f32\n    }\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "(`(` $operands^ `:` type($operands) `)`)? $reductions attr-dict"
  },
  {
    "name": "scf.reduce.return",
    "summary": "terminator for reduce operation",
    "description": "The `scf.reduce.return` operation is a special terminator operation for the block inside\n    `scf.reduce` regions. It terminates the region. It should have the same\n    operand type as the corresponding operand of the enclosing `scf.reduce` op.\n\n    Example:\n\n    ```mlir\n    scf.reduce.return %res : f32\n    ```",
    "inputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "$result attr-dict `:` type($result)"
  },
  {
    "name": "scf.while",
    "summary": "a generic 'while' loop",
    "description": "This operation represents a generic \"while\"/\"do-while\" loop that keeps\n    iterating as long as a condition is satisfied. There is no restriction on\n    the complexity of the condition. It consists of two regions (with single\n    block each): \"before\" region and \"after\" region. The names of regions\n    indicates whether they execute before or after the condition check.\n    Therefore, if the main loop payload is located in the \"before\" region, the\n    operation is a \"do-while\" loop. Otherwise, it is a \"while\" loop.\n\n    The \"before\" region terminates with a special operation, `scf.condition`,\n    that accepts as its first operand an `i1` value indicating whether to\n    proceed to the \"after\" region (value is `true`) or not. The two regions\n    communicate by means of region arguments. Initially, the \"before\" region\n    accepts as arguments the operands of the `scf.while` operation and uses them\n    to evaluate the condition. It forwards the trailing, non-condition operands\n    of the `scf.condition` terminator either to the \"after\" region if the\n    control flow is transferred there or to results of the `scf.while` operation\n    otherwise. The \"after\" region takes as arguments the values produced by the\n    \"before\" region and uses `scf.yield` to supply new arguments for the\n    \"before\" region, into which it transfers the control flow unconditionally.\n\n    A simple \"while\" loop can be represented as follows.\n\n    ```mlir\n    %res = scf.while (%arg1 = %init1) : (f32) -> f32 {\n      // \"Before\" region.\n      // In a \"while\" loop, this region computes the condition.\n      %condition = call @evaluate_condition(%arg1) : (f32) -> i1\n\n      // Forward the argument (as result or \"after\" region argument).\n      scf.condition(%condition) %arg1 : f32\n\n    } do {\n    ^bb0(%arg2: f32):\n      // \"After\" region.\n      // In a \"while\" loop, this region is the loop body.\n      %next = call @payload(%arg2) : (f32) -> f32\n\n      // Forward the new value to the \"before\" region.\n      // The operand types must match the types of the `scf.while` operands.\n      scf.yield %next : f32\n    }\n    ```\n\n    A simple \"do-while\" loop can be represented by reducing the \"after\" block\n    to a simple forwarder.\n\n    ```mlir\n    %res = scf.while (%arg1 = %init1) : (f32) -> f32 {\n      // \"Before\" region.\n      // In a \"do-while\" loop, this region contains the loop body.\n      %next = call @payload(%arg1) : (f32) -> f32\n\n      // And also evaluates the condition.\n      %condition = call @evaluate_condition(%arg1) : (f32) -> i1\n\n      // Loop through the \"after\" region.\n      scf.condition(%condition) %next : f32\n\n    } do {\n    ^bb0(%arg2: f32):\n      // \"After\" region.\n      // Forwards the values back to \"before\" region unmodified.\n      scf.yield %arg2 : f32\n    }\n    ```\n\n    Note that the types of region arguments need not to match with each other.\n    The op expects the operand types to match with argument types of the\n    \"before\" region; the result types to match with the trailing operand types\n    of the terminator of the \"before\" region, and with the argument types of the\n    \"after\" region. The following scheme can be used to share the results of\n    some operations executed in the \"before\" region with the \"after\" region,\n    avoiding the need to recompute them.\n\n    ```mlir\n    %res = scf.while (%arg1 = %init1) : (f32) -> i64 {\n      // One can perform some computations, e.g., necessary to evaluate the\n      // condition, in the \"before\" region and forward their results to the\n      // \"after\" region.\n      %shared = call @shared_compute(%arg1) : (f32) -> i64\n\n      // Evaluate the condition.\n      %condition = call @evaluate_condition(%arg1, %shared) : (f32, i64) -> i1\n\n      // Forward the result of the shared computation to the \"after\" region.\n      // The types must match the arguments of the \"after\" region as well as\n      // those of the `scf.while` results.\n      scf.condition(%condition) %shared : i64\n\n    } do {\n    ^bb0(%arg2: i64) {\n      // Use the partial result to compute the rest of the payload in the\n      // \"after\" region.\n      %res = call @payload(%arg2) : (i64) -> f32\n\n      // Forward the new value to the \"before\" region.\n      // The operand types must match the types of the `scf.while` operands.\n      scf.yield %res : f32\n    }\n    ```\n\n    The custom syntax for this operation is as follows.\n\n    ```\n    op ::= `scf.while` assignments `:` function-type region `do` region\n           `attributes` attribute-dict\n    initializer ::= /* empty */ | `(` assignment-list `)`\n    assignment-list ::= assignment | assignment `,` assignment-list\n    assignment ::= ssa-value `=` ssa-value\n    ```",
    "inputs": [
      { "name": "inits", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "scf.yield",
    "summary": "loop yield and termination operation",
    "description": "The `scf.yield` operation yields an SSA value from the SCF dialect op region and\n    terminates the regions. The semantics of how the values are yielded is\n    defined by the parent operation.\n    If `scf.yield` has any operands, the operands must match the parent\n    operation's results.\n    If the parent operation defines no values, then the `scf.yield` may be\n    left out in the custom syntax and the builders will insert one implicitly.\n    Otherwise, it has to be present in the syntax to indicate which values are\n    yielded.",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($results^ `:` type($results))?"
  },
  {
    "name": "sdfg.alloc",
    "summary": "Array allocation operation",
    "description": "Alloc operation to create arrays and reserve the specified space.\n        For example:\n\n        ```mlir\n            %A = sdfg.alloc() : !sdfg.array<i32>\n        ```",
    "inputs": [
      { "name": "params", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "res", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "name", "type": "OptionalAttr" },
      { "name": "transient", "type": "UnitAttr" }
    ]
  },
  {
    "name": "sdfg.alloc_symbol",
    "summary": "symbol creation operation",
    "description": "Alloc operation to create a new symbol.\n        For example:\n\n        ```mlir\n        sdfg.state @state_0 {\n            ...\n            sdfg.alloc_symbol(\"N\")\n            ...\n        }\n        ```",
    "attributes": [
      { "name": "sym", "type": "StrAttr" }
    ]
  },
  {
    "name": "sdfg.consume",
    "summary": "Consume scope",
    "description": "Describes a consume scope where the region has access to the popped \n        element as well as the processing element.\n        For example:\n\n        ```mlir\n        sdfg.consume{num_pes=5} (%a : !sdfg.stream<i32>) -> (pe: %p, elem: %e) {\n                %c = sdfg.call @add_one(%a) : i32 -> i32\n            ...\n        } \n        ```",
    "inputs": [
      { "name": "stream", "type": "SDFG_StreamType" }
    ],
    "attributes": [
      { "name": "entryID", "type": "I32Attr" },
      { "name": "exitID", "type": "I32Attr" },
      { "name": "num_pes", "type": "OptionalAttr" },
      { "name": "condition", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "sdfg.copy",
    "summary": "Memlet copy operation",
    "description": "Allows a state to copy the contents from one memlet to another.\n        For example:\n\n        ```mlir\n        sdfg.state @state_0 {\n            ...\n            sdfg.copy %a -> %c : !sdfg.memlet<i32>\n            ...\n        }\n        ```",
    "inputs": [
      { "name": "src", "type": "SDFG_ArrayType" },
      { "name": "dest", "type": "SDFG_ArrayType" }
    ]
  },
  {
    "name": "sdfg.edge",
    "summary": "edge operation",
    "description": "Represents an edge from one state to another with assignment and \n        condition attributes.\n        For example:\n\n        ```mlir\n        sdfg.sdfg {\n            ...\n            sdfg.edge{assign=[\"i = 1\"]} @state_0 -> @state_1\n            ...\n        }\n        ```",
    "inputs": [
      { "name": "ref", "type": "Optional" }
    ],
    "attributes": [
      { "name": "src", "type": "FlatSymbolRefAttr" },
      { "name": "dest", "type": "FlatSymbolRefAttr" },
      { "name": "assign", "type": "DefaultValuedAttr" },
      { "name": "condition", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "sdfg.libcall",
    "summary": "library call operation",
    "description": "The `libcall` operation represents a direct call to a library function. \n        For example:\n\n        ```mlir\n        %2 = sdfg.libcall \"dace.libraries.blas.nodes.Dot\" (%0, %1) : (f32, f32) -> f32\n        ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "callee", "type": "StrAttr" }
    ]
  },
  {
    "name": "sdfg.load",
    "summary": "Memlet load operation",
    "description": "Allows a state to load a value from a memlet.\n        For example:\n\n        ```mlir\n        sdfg.state @state_0 {\n            ...\n            %a = sdfg.get_access %A : !sdfg.memlet<i32>\n            %a_1 = sdfg.load %a[0] : !sdfg.memlet<i32> -> i32\n            ...\n        }\n        ```",
    "inputs": [
      { "name": "indices", "type": "Variadic" },
      { "name": "arr", "type": "SDFG_ArrayType" }
    ],
    "outputs": [
      { "name": "res", "type": "AnyType" }
    ]
  },
  {
    "name": "sdfg.map",
    "summary": "Map scope",
    "description": "Describes a map where the region has access to the map symbol variables.\n        For example:\n\n        ```mlir\n        sdfg.map (%i, %j) = (0, 0) to (2, 2) step (1, 1) {\n            ...\n            %a = sdfg.load %A[%i, %j] : !sdfg.array<12x34xi32>\n            ...\n        } \n        ```",
    "inputs": [
      { "name": "ranges", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "entryID", "type": "I32Attr" },
      { "name": "exitID", "type": "I32Attr" },
      { "name": "lowerBounds", "type": "ArrayAttr" },
      { "name": "upperBounds", "type": "ArrayAttr" },
      { "name": "steps", "type": "ArrayAttr" }
    ]
  },
  {
    "name": "sdfg.nested_sdfg",
    "summary": "Nested SDFG region",
    "description": "Describes a nested SDFG where the states are placed in the region. \n        For example:\n\n        ```mlir\n        sdfg.nested_sdfg{entry=@state_0} {\n            sdfg.edge{assign=[\"i = 1\"]} @state_0 -> @state_1\n            ...\n        }\n        ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "ID", "type": "I32Attr" },
      { "name": "entry", "type": "OptionalAttr" },
      { "name": "num_args", "type": "I32Attr" }
    ]
  },
  {
    "name": "sdfg.return",
    "summary": "return operation",
    "description": "The \"return\" operation represents a return operation within a function.\n        The operation takes an optional operand and produces no results.\n        The operand type must match the signature of the function that contains\n        the operation. \n        For example:\n\n        ```mlir\n        func @foo() -> tensor<2xf64> {\n            ...\n            sdfg.return %0 : tensor<2xf64>\n        }\n        ```",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ]
  },
  {
    "name": "sdfg.sdfg",
    "summary": "SDFG region",
    "description": "Describes an SDFG where the states are placed in the region. \n        For example:\n\n        ```mlir\n        sdfg.sdfg{entry=@state_0} {\n            sdfg.edge{assign=[\"i = 1\"]} @state_0 -> @state_1\n            ...\n        }\n        ```",
    "attributes": [
      { "name": "ID", "type": "I32Attr" },
      { "name": "entry", "type": "OptionalAttr" },
      { "name": "num_args", "type": "I32Attr" }
    ]
  },
  {
    "name": "sdfg.state",
    "summary": "State region",
    "description": "Describes a state where the subgraphs are placed in the region. \n        For example:\n\n        ```mlir\n        sdfg.state @state_0{\n            ...\n        } \n        ```",
    "attributes": [
      { "name": "ID", "type": "I32Attr" },
      { "name": "sym_name", "type": "SymbolNameAttr" }
    ]
  },
  {
    "name": "sdfg.store",
    "summary": "Memlet store operation",
    "description": "Allows a state to store a value in a memlet.\n        For example:\n\n        ```mlir\n        sdfg.state @state_0 {\n            ...\n            %1 = arith.constant 1 : i32\n            %a = sdfg.get_access %A : !sdfg.memlet<i32>\n            sdfg.store %1, %a[0] : !sdfg.memlet<i32>\n            ...\n        }\n        ```",
    "inputs": [
      { "name": "indices", "type": "Variadic" },
      { "name": "val", "type": "AnyType" },
      { "name": "arr", "type": "SDFG_ArrayType" }
    ]
  },
  {
    "name": "sdfg.stream_length",
    "summary": "Stream length operation",
    "description": "Returns the length of the stream.\n        For example:\n\n        ```mlir\n        sdfg.state @state_0 {\n            ...\n            %l = sdfg.stream_length %A : !sdfg.stream<i32> -> i32\n            ...\n        }\n        ```",
    "inputs": [
      { "name": "str", "type": "SDFG_StreamType" }
    ]
  },
  {
    "name": "sdfg.stream_pop",
    "summary": "Stream pop operation",
    "description": "Allows a state to pop a value from a stream.\n        For example:\n\n        ```mlir\n        sdfg.state @state_0 {\n            ...\n            %a = sdfg.stream_pop %A : !sdfg.stream<i32> -> i32\n            ...\n        }\n        ```",
    "inputs": [
      { "name": "str", "type": "SDFG_StreamType" }
    ],
    "outputs": [
      { "name": "res", "type": "AnyType" }
    ]
  },
  {
    "name": "sdfg.stream_push",
    "summary": "Stream push operation",
    "description": "Allows a state to push a value into a stream.\n        For example:\n\n        ```mlir\n        sdfg.state @state_0 {\n            ...\n            %1 = arith.constant 1 : i32\n            sdfg.stream_push %1, %A : i32 -> !sdfg.stream<i32>\n            ...\n        }\n        ```",
    "inputs": [
      { "name": "val", "type": "AnyType" },
      { "name": "str", "type": "SDFG_StreamType" }
    ]
  },
  {
    "name": "sdfg.subview",
    "summary": "Subview operation",
    "description": "Return a subview of a memlet with the provided offsets, sizes and \n        strides.\n        For example:\n\n        ```mlir\n        sdfg.state {\n            ...\n            %B = sdfg.subview %A[3, 4, 2][1, 6, 3][1, 1, 1] : !sdfg.array<8x16x4xi32> -> !sdfg.array<6x3xi32>\n            ...\n        }\n        ```",
    "inputs": [
      { "name": "src", "type": "SDFG_ArrayType" }
    ],
    "outputs": [
      { "name": "res", "type": "SDFG_ArrayType" }
    ],
    "attributes": [
      { "name": "offsets", "type": "ArrayAttr" },
      { "name": "sizes", "type": "ArrayAttr" },
      { "name": "strides", "type": "ArrayAttr" }
    ]
  },
  {
    "name": "sdfg.sym",
    "summary": "symbolic expression operation",
    "description": "Describes an arithmetic symbolic expression.\n        For example:\n\n        ```mlir\n        sdfg.state @state_0 {\n            ...\n            %res = sdfg.sym(\"3*N+2\") : i32\n            ...\n        }\n        ```",
    "outputs": [
      { "name": "res", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "expr", "type": "StrAttr" }
    ]
  },
  {
    "name": "sdfg.tasklet",
    "summary": "Tasklet",
    "description": "Describes a tasklet as a pure function. For example:\n\n        ```mlir\n        sdfg.tasklet @add(%a: i32, %b: i32) -> i32{\n            %c = arith.addi %a, %b, : i32\n            sdfg.return %c\n        }\n        ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "ID", "type": "I32Attr" }
    ]
  },
  {
    "name": "sdfg.view_cast",
    "summary": "view node operation",
    "description": "Represents a view node to cast a memlet from one view to another.\n        For example:\n\n        ```mlir\n        sdfg.state {\n            ...\n            %B = sdfg.view_cast %A : !sdfg.memlet<2x16xi32> -> !sdfg.memlet<32xi32>\n            ...\n        }\n        ```",
    "inputs": [
      { "name": "src", "type": "SDFG_ArrayType" }
    ],
    "outputs": [
      { "name": "res", "type": "SDFG_ArrayType" }
    ]
  },
  {
    "name": "shape.add",
    "summary": "Addition of sizes and indices",
    "description": "Adds two sizes or indices. If either operand is an error it will be\n    propagated to the result. The operands can be of type `size` or `index`. If\n    at least one of the operands can hold an error, i.e. if it is of type\n    `size`, the result must be of type `size`. If error propagation is not\n    possible because both operands are of type `index` then the result may be\n    of type `size` or `index`.",
    "inputs": [
      { "name": "lhs", "type": "Shape_SizeOrIndexType" },
      { "name": "rhs", "type": "Shape_SizeOrIndexType" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_SizeOrIndexType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `->` type($result)"
  },
  {
    "name": "shape.any",
    "summary": "Return any combination of the input shapes",
    "description": "This operation takes multiple input shapes or extent tensors and returns\n    some combination of their dimensions. This can be best seen with examples\n    below.\n\n    The result is undefined, but still side-effect free, in cases where the\n    inputs have differing ranks or differ in extents of shared dimensions.\n\n    Example:\n    ```mlir\n    %s0 = shape.any [2,?], [?,3] // [2,3]\n    %s1 = shape.any [?,?], [1,2] // [1,2]\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_ShapeOrExtentTensorType" }
    ],
    "assemblyFormat": "$inputs attr-dict `:` type($inputs) `->` type($result)"
  },
  {
    "name": "shape.assuming",
    "summary": "Execute the region",
    "description": "Executes the region assuming all witnesses are true.\n\n    \"assuming\" operations represent an execution order restriction to the\n    compiler, information for dependent code to rely on (by assuming), and\n    nothing else. They should not exist after a program is fully lowered and\n    ready to execute.",
    "inputs": [
      { "name": "witness", "type": "Shape_WitnessType" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "shape.assuming_all",
    "summary": "Return a logical AND of all witnesses",
    "description": "Used to simplify constraints as any single failing precondition is enough\n    to prevent execution.\n\n    \"assuming\" operations represent an execution order restriction to the\n    compiler, information for dependent code to rely on (by assuming), and\n    nothing else. They should not exist after a program is fully lowered and\n    ready to execute.\n\n    Example:\n    ```mlir\n    %w0 = shape.cstr_broadcastable [2,2], [3,1,2] // Passing\n    %w1 = shape.cstr_broadcastable [2,2], [3,2] // Failure\n    %w2 = shape.cstr_eq [1,2], [1,2], [1,2] // Passing\n    %wf = shape.assuming_all %w0, %w1 // Failure\n    %wt = shape.assuming_all %w0, %w2 // Passing\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_WitnessType" }
    ],
    "assemblyFormat": "$inputs attr-dict"
  },
  {
    "name": "shape.assuming_yield",
    "summary": "Yield operation",
    "description": "This yield operation represents a return operation within the\n    `shape.assuming` operation region. The operation takes variable number of\n    operands and produces no results. The operand number and types must match\n    the number and types of parent `shape.assuming` results.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "shape.broadcast",
    "summary": "Returns the broadcasted output shape of two or more inputs",
    "description": "Returns the broadcasted shape for input shapes or extent tensors. The rest\n    of this description is simplified for the 2 input case but can be extended\n    to more inputs. Both operands can be of type `shape.shape` or\n    `tensor<?xindex>`. The result is of type `shape.shape` and, if both\n    operands are tensors, may be of type `tensor<?xindex>`.\n\n    If the two operand shapes are of different rank the smaller one is padded\n    with 1's from the left. The resulting broadcasted shape is then defined as\n\n        result[i] = lhs[i] if lhs[i] == rhs[i]\n                  = lhs[i] if rhs[i] == 1\n                  = rhs[i] if lhs[i] == 1.\n\n    In case the resulting shape is undefined, i.e. if corresponding extents are\n    different from each other but none is 1, the result is an error shape.\n    Likewise error values are propagated if any of the operands holds an error\n    value. If the result type is an extent tensor (and can therefore not hold\n    the error value) the behavior may be undefined. The optional string\n    attribute can be used to describe the error case.",
    "inputs": [
      { "name": "shapes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_ShapeOrExtentTensorType" }
    ],
    "attributes": [
      { "name": "error", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$shapes attr-dict `:` type($shapes) `->` type($result)"
  },
  {
    "name": "shape.concat",
    "summary": "Concatenates two shapes",
    "description": "Creates a shape whose dimensions consist of first the dimensions from `lhs`\n    followed by the dimensions of `rhs`.\n\n    Example:\n    concat([2,3], [4,5]) -> [2,3,4,5]\n    concat([], []) -> []\n    concat([], [4,5,6]) -> [4,5,6]",
    "inputs": [
      { "name": "lhs", "type": "Shape_ShapeOrExtentTensorType" },
      { "name": "rhs", "type": "Shape_ShapeOrExtentTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_ShapeOrExtentTensorType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `->` type($result)"
  },
  {
    "name": "shape.const_shape",
    "summary": "Creates a constant shape or extent tensor",
    "description": "Creates a constant shape or extent tensor. The individual extents are given\n    as the `shape` attribute. The number of these values equals the shape's\n    rank.\n\n    ```mlir\n    %0 = shape.const_shape [] : !shape.shape\n    %1 = shape.const_shape [1, 2, 3] : !shape.shape\n    %2 = shape.const_shape [4, 5, 6] : tensor<3xindex>\n    ```",
    "outputs": [
      { "name": "result", "type": "Shape_ShapeOrExtentTensorType" }
    ],
    "attributes": [
      { "name": "shape", "type": "IndexElementsAttr" }
    ]
  },
  {
    "name": "shape.const_size",
    "summary": "Creates a constant of type `shape.size`",
    "description": "Creates a `shape.size` type representing the constant size given by `value`.\n\n    ```mlir\n    %x = shape.const_size 10\n    ```",
    "outputs": [
      { "name": "result", "type": "Shape_SizeType" }
    ],
    "attributes": [
      { "name": "value", "type": "IndexAttr" }
    ],
    "assemblyFormat": "$value attr-dict"
  },
  {
    "name": "shape.const_witness",
    "summary": "An operation that returns a statically known witness value",
    "description": "This operation represents a statically known witness result. This can be\n  often used to canonicalize/fold constraint and assuming code that will always\n  pass.\n\n  ```mlir\n  %0 = shape.const_shape [1,2,3]\n  %1 = shape.const_shape [1,2,3]\n  %w0 = shape.cstr_eq(%0, %1) // Can be folded to \"const_witness true\"\n  %w1 = shape.const_witness true\n  %w2 = shape.assuming_all(%w0, %w2) // Can be folded to \"const_witness true\"\n  ```",
    "outputs": [
      { "name": "result", "type": "Shape_WitnessType" }
    ],
    "attributes": [
      { "name": "passing", "type": "BoolAttr" }
    ],
    "assemblyFormat": "$passing attr-dict"
  },
  {
    "name": "shape.cstr_broadcastable",
    "summary": "Determines if 2+ shapes can be successfully broadcasted",
    "description": "Given input shapes or extent tensors, return a witness specifying if they\n    are broadcastable. This broadcastable follows the same logic as what\n    shape.broadcast documents.\n\n    \"cstr\" operations represent runtime assertions.\n\n    Example:\n    ```mlir\n    %w0 = shape.cstr_broadcastable [2,2], [3,1,2] // Passing\n    %w1 = shape.cstr_broadcastable [2,2], [3,2] // Failure\n    ```",
    "inputs": [
      { "name": "shapes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_WitnessType" }
    ],
    "assemblyFormat": "$shapes attr-dict `:` type($shapes)"
  },
  {
    "name": "shape.cstr_eq",
    "summary": "Determines if all input shapes are equal",
    "description": "Given 1 or more input shapes, determine if all shapes are the exact same.\n\n    \"cstr\" operations represent runtime assertions.\n\n    Example:\n    ```mlir\n    %w0 = shape.cstr_eq [1,2], [1,2], [1,2] // Passing\n    %w1 = shape.cstr_eq [2,2], [1,2] // Failure\n    ```",
    "inputs": [
      { "name": "shapes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_WitnessType" }
    ],
    "assemblyFormat": "$shapes attr-dict `:` type($shapes)"
  },
  {
    "name": "shape.cstr_require",
    "summary": "Represents a runtime assertion that an i1 is `true`",
    "description": "Represents a runtime assertion that an i1 is true. It returns a\n    !shape.witness to order this assertion.\n\n    For simplicity, prefer using other cstr_* ops if they are available for a\n    given constraint.\n\n    Example:\n    ```mlir\n    %bool = ...\n    %w0 = shape.cstr_require %bool, \"msg\" // Passing if `%bool` is true.\n    ```\n\n    Since this op can be used to express many different possible assertions\n    (depending on whatever computation calculated `pred`), the `msg`\n    should clarify the nature of the assertion for users.",
    "inputs": [
      { "name": "pred", "type": "I1" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_WitnessType" }
    ],
    "attributes": [
      { "name": "msg", "type": "StrAttr" }
    ],
    "assemblyFormat": "$pred `,` $msg attr-dict"
  },
  {
    "name": "shape.debug_print",
    "summary": "Prints the input shape or size",
    "description": "Prints the input dim or shape and passes through input.\n\n    Note: This is intended for testing and debugging only.",
    "inputs": [
      { "name": "input", "type": "Shape_ShapeOrSizeType" }
    ],
    "outputs": [
      { "name": "output", "type": "Shape_ShapeOrSizeType" }
    ]
  },
  {
    "name": "shape.dim",
    "summary": "Gets the specified extent from the shape of a shaped input",
    "description": "Gets the extent indexed by `dim` from the shape of the `value` operand. If\n    the index is error or out-of-bound then it returns an invalid size if the\n    return type carries error information else the behavior is undefined.\n\n    This is a convenience op that performs the equivalent of getting the extent\n    of a shape (e.g., `dim(x, i) == get_extent(shape_of(x), i)`).",
    "inputs": [
      { "name": "value", "type": "AnyShaped" },
      { "name": "index", "type": "Shape_SizeOrIndexType" }
    ],
    "outputs": [
      { "name": "extent", "type": "Shape_SizeOrIndexType" }
    ],
    "assemblyFormat": "$value `,` $index attr-dict `:` type($value) `,`type($index) `->` type($extent)"
  },
  {
    "name": "shape.div",
    "summary": "Division of sizes and indices",
    "description": "Divides two sizes or indices. If either operand is an error it will be\n    propagated to the result. The operands can be of type `size` or `index`.\n    If at least one of the operands can hold an error, i.e. if it is of type\n    `size`, the result must be of type `size`. If error propagation is not\n    possible because both operands are of type `index` then the result may be\n    of type  `size` or `index`. If both operands and result are of type\n    `index`, their runtime values could be negative. The result is rounded\n    toward negative infinity, i.e. floor(lhs / rhs), such that\n\n        div(lhs, rhs) * rhs + mod(lhs, rhs) = lhs\n\n    always holds. If any of the values is of type `size`, the behavior for\n    negative value is undefined.",
    "inputs": [
      { "name": "lhs", "type": "Shape_SizeOrIndexType" },
      { "name": "rhs", "type": "Shape_SizeOrIndexType" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_SizeOrIndexType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `->` type($result)"
  },
  {
    "name": "shape.from_extent_tensor",
    "summary": "Creates a shape from a tensor of extents",
    "description": "Creates a shape from a 1D integral tensor of extents. The rank of the\n    resulting shape equals the number of elements in the tensor, and the\n    extents match the values of the elements.",
    "inputs": [
      { "name": "input", "type": "DTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_ShapeType" }
    ],
    "assemblyFormat": "$input attr-dict `:` type($input)"
  },
  {
    "name": "shape.from_extents",
    "summary": "Creates a shape from extents",
    "description": "Creates a shape from multiple SSA values representing the extents of\n    the shape.\n\n    ```mlir\n    // Rank 2 shape.\n    %s0 = shape.from_extents %a, %b\n    // Rank 0 shape.\n    %s1 = shape.from_extents\n    ```",
    "inputs": [
      { "name": "extents", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "shape", "type": "Shape_ShapeType" }
    ],
    "assemblyFormat": "$extents attr-dict `:` type($extents)"
  },
  {
    "name": "shape.func",
    "summary": "Shape function",
    "description": "An operation with a name containing a single `SSACFG` region which\n    represents a shape transfer function or helper function for shape transfer\n    function.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "sym_visibility", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "shape.function_library",
    "summary": "Represents shape functions and corresponding ops",
    "description": "Represents a list of shape functions and the ops whose shape transfer\n    functions they represent.\n\n    Example:\n\n    ```mlir\n    shape.function_library {\n      func @same_result_shape(%arg: !shape.value_shape) -> !shape.shape {\n        %0 = shape_of %arg : !shape.value_shape -> !shape.shape\n        return %0 : !shape.shape\n      }\n    } mapping {\n      std.atan = @same_result_shape\n    }\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "mapping", "type": "DictionaryAttr" }
    ]
  },
  {
    "name": "shape.get_extent",
    "summary": "Gets the specified extent from a shape or extent tensor",
    "description": "Gets the extent indexed by `dim` from the `shape` operand. If the shape is\n    an error then it returns an invalid size.",
    "inputs": [
      { "name": "shape", "type": "Shape_ShapeOrExtentTensorType" },
      { "name": "dim", "type": "Shape_SizeOrIndexType" }
    ],
    "outputs": [
      { "name": "extent", "type": "Shape_SizeOrIndexType" }
    ],
    "assemblyFormat": "$shape `,` $dim attr-dict `:` type($shape) `,` type($dim) `->` type($extent)"
  },
  {
    "name": "shape.index_to_size",
    "summary": "Converts a standard index to a shape size",
    "description": "Converts a standard index to a `shape.size`. This operation and its\n    inverse, `size_to_index`, facilitate index conversion between the standard\n    and the shape dialect.\n\n    The behavior is undefined for negative indices.",
    "inputs": [
      { "name": "arg", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_SizeType" }
    ],
    "assemblyFormat": "$arg attr-dict"
  },
  {
    "name": "shape.is_broadcastable",
    "summary": "Determines if 2+ shapes can be successfully broadcasted",
    "description": "Given multiple input shapes or extent tensors, return a predicate\n    specifying if they are broadcastable. This broadcastable follows the same\n    logic as what shape.broadcast documents.\n\n    Concretely, shape.is_broadcastable returning true implies that\n    shape.broadcast will not give an error, and shape.cstr_broadcastable will\n    not result in an assertion failure. Similarly, false implies an error or\n    assertion failure.\n\n    Example:\n    ```mlir\n    %true = shape.is_broadcastable [2,2], [3,1,2]\n    %false = shape.is_broadcastable [2,2], [3,2]\n    ```",
    "inputs": [
      { "name": "shapes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "I1" }
    ],
    "assemblyFormat": "$shapes attr-dict `:` type($shapes)"
  },
  {
    "name": "shape.max",
    "summary": "Elementwise maximum",
    "description": "Computes the elementwise maximum of two sizes or shapes with equal ranks.\n    If either operand is an error, then an error will be propagated to the\n    result. If the input types mismatch or the ranks do not match, then the\n    result is an error.",
    "inputs": [
      { "name": "lhs", "type": "Shape_ShapeOrSizeType" },
      { "name": "rhs", "type": "Shape_ShapeOrSizeType" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_ShapeOrSizeType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `->` type($result)"
  },
  {
    "name": "shape.meet",
    "summary": "Returns the least general shape or size of its operands",
    "description": "An operation that computes the least general shape or dim of input operands.\n    This effectively asserts that corresponding static dimensions are equal.\n    The behavior is to match each element of the shape/size and propagate the\n    most restrictive information, returning an invalid shape if there are\n    contradictory requirements. E.g., using pseudo code\n\n    ```\n    shape.meet([*], [*]) -> [*]\n    shape.meet([*], [1, ?]) -> [1, ?]\n    shape.meet([1, 2], [1, ?]) -> [1, 2]\n    shape.meet([*], [1, 2]) -> [1, 2]\n    shape.meet([], []) -> []\n    shape.meet([], [*]) -> []\n    shape.meet([], [?, ?]) -> [invalid]\n    shape.meet([1, ?], [2, ?, ?]) -> [invalid]\n    ```\n\n    `shape.meet` also allows specifying an optional error string, that may be\n    used to return an error to the user upon mismatch of dimensions.\n\n    ```mlir\n    %c = shape.meet %a, %b, error=\"<reason>\" : !shape.shape, !shape.shape -> !shape.shape\n    ```",
    "inputs": [
      { "name": "arg0", "type": "Shape_AnyShapeOrSizeType" },
      { "name": "arg1", "type": "Shape_AnyShapeOrSizeType" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_AnyShapeOrSizeType" }
    ],
    "attributes": [
      { "name": "error", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$arg0 `,` $arg1 (`,` `error` `=` $error^)? attr-dict `:`\n      type($arg0) `,` type($arg1) `->` type($result)"
  },
  {
    "name": "shape.min",
    "summary": "Elementwise minimum",
    "description": "Computes the elementwise minimum of two sizes or shapes with equal ranks.\n    If either operand is an error, then an error will be propagated to the\n    result. If the input types mismatch or the ranks do not match, then the\n    result is an error.",
    "inputs": [
      { "name": "lhs", "type": "Shape_ShapeOrSizeType" },
      { "name": "rhs", "type": "Shape_ShapeOrSizeType" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_ShapeOrSizeType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `->` type($result)"
  },
  {
    "name": "shape.mul",
    "summary": "Multiplication of sizes and indices",
    "description": "Multiplies two sizes or indices. If either operand is an error it will be\n    propagated to the result. The operands can be of type `size` or `index`. If\n    at least one of the operands can hold an error, i.e. if it is of type\n    `size`, the result must be of type `size`. If error propagation is not\n    possible because both operands are of type `index` then the result may be\n    of type `size` or `index`.",
    "inputs": [
      { "name": "lhs", "type": "Shape_SizeOrIndexType" },
      { "name": "rhs", "type": "Shape_SizeOrIndexType" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_SizeOrIndexType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `->` type($result)"
  },
  {
    "name": "shape.num_elements",
    "summary": "Returns the number of elements for a given shape",
    "description": "Returns the number of elements for a given shape which is the product of\n    its extents. If the argument is of type `shape` then the result will be of\n    type `size` and potential errors will be propagated. Otherwise, if the\n    argument is and extent tensor `tensor<?xindex>` then the result will be of\n    type `index`.",
    "inputs": [
      { "name": "shape", "type": "Shape_ShapeOrExtentTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_SizeOrIndexType" }
    ],
    "assemblyFormat": "$shape attr-dict `:` type($shape) `->` type($result)"
  },
  {
    "name": "shape.rank",
    "summary": "Gets the rank of a shape",
    "description": "Returns the rank of the shape or extent tensor, i.e. the number of extents.",
    "inputs": [
      { "name": "shape", "type": "Shape_ShapeOrExtentTensorType" }
    ],
    "outputs": [
      { "name": "rank", "type": "Shape_SizeOrIndexType" }
    ],
    "assemblyFormat": "$shape attr-dict `:` type($shape) `->` type($rank)"
  },
  {
    "name": "shape.reduce",
    "summary": "Returns an expression reduced over a shape or extent tensor",
    "description": "An operation that takes as input a shape or extent tensor, and a number of\n    initial values. This operation has a region that is applied repeatedly for\n    every extent of the input. Starting with the initial values, the individual\n    extents are then aggregated as defined by the associated region.\n\n    Conceptually this op performs the following reduction:\n\n    ```\n    res[] = init;\n    for (int i = 0, i < shape.rank(); i++) {\n      res = reduce(i, shape[i], res[0], ..., res[n]);\n    }\n    ```\n\n    Where `reduce` represents the region attached and the result of the reduce\n    op is the last computed output of the reduce region. As an example, the\n    number of elements can be computed as follows:\n\n    ```mlir\n    func.func @reduce(%shape : !shape.shape, %init : !shape.size) ->\n        !shape.size {\n      %num_elements = shape.reduce(%shape, %init) -> !shape.size  {\n        ^bb0(%index: index, %dim: !shape.size, %acc: !shape.size):\n          %updated_acc = \"shape.mul\"(%acc, %dim) :\n            (!shape.size, !shape.size) -> !shape.size\n          shape.yield %updated_acc : !shape.size\n      }\n      return %num_elements : !shape.size\n    }\n    ```",
    "inputs": [
      { "name": "shape", "type": "Shape_ShapeOrExtentTensorType" },
      { "name": "initVals", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Variadic" }
    ]
  },
  {
    "name": "shape.return",
    "summary": "Shape function return operation",
    "description": "The `shape.return` operation represents a return operation within a\n    function.  The operation takes variable number of operands and produces no\n    results.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "shape.shape_eq",
    "summary": "Returns whether the input shapes or extent tensors are equal",
    "description": "Takes one or more shape or extent tensor operands and determines whether\n    they are equal. When extent tensors are compared to shapes they are\n    regarded as their equivalent non-error shapes. Error shapes can be tested\n    for equality like any other shape value, meaning that the error value is\n    equal to itself.",
    "inputs": [
      { "name": "shapes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "I1" }
    ],
    "assemblyFormat": "$shapes attr-dict `:` type($shapes)"
  },
  {
    "name": "shape.shape_of",
    "summary": "Returns shape of a value or shaped type operand",
    "description": "The operation takes a value or a shaped operand as an argument and it\n    returns a shape or extent tensor.",
    "inputs": [
      { "name": "arg", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_ShapeOrExtentTensorType" }
    ],
    "assemblyFormat": "$arg attr-dict `:` type($arg) `->` type($result)"
  },
  {
    "name": "shape.size_to_index",
    "summary": "Casts between index types of the shape and standard dialect",
    "description": "Converts a `shape.size` to a standard index. This operation and its\n    inverse, `index_to_size`, facilitate index conversion between the standard\n    and the shape dialect. The behavior is undefined for unknown and invalid\n    arguments.",
    "inputs": [
      { "name": "arg", "type": "Shape_SizeOrIndexType" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$arg attr-dict `:` type($arg)"
  },
  {
    "name": "shape.split_at",
    "summary": "Splits a shape at a given index",
    "description": "Splits a shape at a given dimension `index`, returning two shapes. If\n    `index` is negative, it is treated as indexing from the back of the shape.\n    This negative-handling behavior is important when handling unranked shapes,\n    where the positive index is not necessarily knowable due to a dynamic\n    number of leading dimensions. If the result is in extent tensor form out of\n    bounds indices result in undefined behavior.\n\n    Examples:\n    - split_at([4,5,6], index=0) -> [], [4,5,6]\n    - split_at([4,5,6], index=1) -> [4], [5,6]\n    - split_at([4,5,6], index=2) -> [4,5], [6]\n    - split_at([4,5,6], index=3) -> [4,5,6], []\n    - split_at([4,5,6], index=4) -> error\n    - split_at([4,5,6], index=-1) -> [4,5], [6]\n    - split_at([4,5,6], index=-2) -> [4], [5,6]\n    - split_at([4,5,6], index=-3) -> [], [4,5,6]\n    - split_at([4,5,6], index=-4) -> error\n\n    Requires:\n    - `index` is in the range [-rank(operand),rank(operand)]",
    "inputs": [
      { "name": "operand", "type": "Shape_ShapeOrExtentTensorType" },
      { "name": "index", "type": "Shape_SizeOrIndexType" }
    ],
    "outputs": [
      { "name": "head", "type": "Shape_ShapeOrExtentTensorType" },
      { "name": "tail", "type": "Shape_ShapeOrExtentTensorType" }
    ]
  },
  {
    "name": "shape.to_extent_tensor",
    "summary": "Creates a dimension tensor from a shape",
    "description": "Converts a shape to a 1D integral tensor of extents. The number of elements\n    in the tensor equals the rank of the shape, and the elements equal the\n    extents of the shape.\n\n    If the shape represents an error, this op's behavior is undefined.",
    "inputs": [
      { "name": "input", "type": "Shape_ShapeOrExtentTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "IndexTensor" }
    ],
    "assemblyFormat": "$input attr-dict `:` type($input) `->` type($result)"
  },
  {
    "name": "shape.value_as_shape",
    "summary": "Returns value as a shape",
    "description": "The operations takes a ValueShape and returns a Shape corresponding to the\n    value.  If the input value cannot be shape (e.g., not a 1D tensor of\n    integral value representing sizes) then this propagages the error shape.\n    E.g.,\n\n    ```mlir\n    // The following\n    %0 = arith.constant dense<[1,2]> : tensor<2xi32>\n    %shape = shape.value_as_shape %0 : tensor<2xi32> -> !shape.shape\n    // is equivalent to\n    %shape' = shape.const_shape [1, 2] : !shape.shape\n    ```\n\n    This operation is the complement of `shape_of` wrt ValueShape values.",
    "inputs": [
      { "name": "arg", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_ShapeOrExtentTensorType" }
    ],
    "assemblyFormat": "$arg attr-dict `:` type($arg) `->` type($result)"
  },
  {
    "name": "shape.value_of",
    "summary": "Returns value of a !shape.value_shape operand",
    "description": "The operation takes !shape.value_shape, a.k.a. (value, shape) tuple as an\n    argument, and returns its value. The behavior is undefined for unknown and\n    invalid arguments.",
    "inputs": [
      { "name": "arg", "type": "Shape_ValueShapeType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyShaped" }
    ],
    "assemblyFormat": "$arg attr-dict `:` type($result)"
  },
  {
    "name": "shape.with_shape",
    "summary": "Returns ValueShape with given shape",
    "description": "Returns ValueShape with the shape updated to match the shape operand. That\n    is a new ValueShape tuple is created with value equal to `operand`'s\n    value and shape equal to `shape`. If the ValueShape and given `shape` are\n    non-conformant, then the returned ValueShape will represent an error of\n    this mismatch. Similarly if either inputs are in an error state, then an\n    error is propagated.\n\n    Usage:\n      %0 = shape.with_shape %1, %2 : tensor<...>, !shape.shape\n\n    This is used, for example, where one combines shape function calculations\n    and/or call one shape function from another. E.g.,\n\n    ```mlir\n    func.func @shape_foobah(%a: !shape.value_shape,\n                       %b: !shape.value_shape,\n                       %c: !shape.value_shape) -> !shape.shape {\n      %0 = call @shape_foo(%a, %b) :\n        (!shape.value_shape, !shape.value_shape) -> !shape.shape\n      %1 = shape.with_shape %b, %0 : !shape.value_shape, !shape.shape\n      %2 = call @shape_bah(%c, %1) :\n        (!shape.value_shape, !shape.value_shape) -> !shape.shape\n      return %2 : !shape.shape\n    }\n    ```\n\n    This op need not be a refinement of the shape. In non-error cases the input\n    ValueShape's value and shape are conformant and so too for the output, but\n    the result may be less specified than `operand`'s shape as `shape` is\n    merely used to construct the new ValueShape. If join behavior is desired\n    then a join op should be used.",
    "inputs": [
      { "name": "operand", "type": "AnyTypeOf" },
      { "name": "shape", "type": "Shape_ShapeOrExtentTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Shape_ValueShapeType" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($operand) `,` type($shape)"
  },
  {
    "name": "shape.yield",
    "summary": "Returns the value to parent op",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "shard.all_gather",
    "summary": "All-gather over a device grid.",
    "description": "Concatenates all tensor slices from a device group defined by `grid_axes` along\n    the tensor dimension `gather_axis` and replicates the result across all devices\n    in the group.\n\n    Example:\n    ```mlir\n    shard.grid @grid0(shape = 2x2)\n    ...\n    %1 = shard.all_gather %0 on @grid0 grid_axes = [1] gather_axis = 1\n      : tensor<2x2xi8> -> tensor<2x4xi8>\n    ```\n    Input:\n    ```\n                     +-------+-------+\n    device (0, 0) -> |  1  2 |  5  6 | <- device (0, 1)\n                     |  3  4 |  7  8 |\n                     +-------+-------+\n    device (1, 0) -> |  9 10 | 13 14 | <- device (1, 1)\n                     | 11 12 | 15 16 |\n                     +-------+-------+\n    ```\n    Result:\n    ```\n    gather tensor\n    axis 1\n    ------------>\n    +-------------+\n    |  1  2  5  6 | <- devices (0, 0) and (0, 1)\n    |  3  4  7  8 |\n    +-------------+\n    |  9 10 13 14 | <- devices (1, 0) and (1, 1)\n    | 11 12 15 16 |\n    +-------------+\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyNon0RankedTensor" }
    ],
    "assemblyFormat": "$input `on` $grid (`grid_axes` `=` $grid_axes^)? `gather_axis` `=` $gather_axis\n    attr-dict `:` type($input) `->` type($result)"
  },
  {
    "name": "shard.all_reduce",
    "summary": "All-reduce over a device grid.",
    "description": "Reduces the input tensor across all devices within the groups defined by\n    `grid_axes`, using the specified reduction method. The operation performs an\n    element-wise reduction over the tensor slices from all devices in each group.\n    Each device in a group receives a replicated copy of the reduction result.\n    The accumulation element type is determined by the result type and does not\n    need to match the input element type. Before performing the reduction, each\n    input element is converted to the result element type.\n\n    Attributes:\n    `reduction`: Indicates the reduction method.\n\n    Example:\n    ```\n    %1 = shard.all_reduce %0 on @grid0 grid_axes = [1, 0] reduction = <max>\n      : tensor<3x4xf32> -> tensor<3x4xf64>\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$input `on` $grid (`grid_axes` `=` $grid_axes^)? (`reduction` `=` $reduction^)?\n    attr-dict `:` type($input) `->` type($result)"
  },
  {
    "name": "shard.all_slice",
    "summary": "All-slice over a device grid.",
    "description": "Within each device group defined by `grid_axes`, slices the input tensor along\n    the `slice_axis` dimension. It can be viewed as the inverse of an all-gather if\n    the input data is replicated along the `slice_axis`.\n    Each process simply crops its local data to the slice corresponding to its\n    in-group device index.\n    Notice: `AllSliceOp` does not involve any communication between devices and\n            devices within a group may not have replicated input data.\n\n    Example:\n    ```mlir\n    shard.grid @grid0(shape = 2x2)\n    ...\n    %1 = shard.all_slice %0 on @grid0 grid_axes = [1] slice_axis = 1\n      : tensor<2x4xi8> -> tensor<2x2xi8>\n    ```\n    Input:\n    ```\n    +-------------+\n    |  1  2  5  6 | <- devices (0, 0) and (0, 1)\n    |  3  4  7  8 |\n    +-------------+\n    |  9 10 13 14 | <- devices (1, 0) and (1, 1)\n    | 11 12 15 16 |\n    +-------------+\n    ```\n    Result:\n    ```\n    slice tensor\n    axis 1\n    ------------>\n                     +-------+-------+\n    device (0, 0) -> |  1  2 |  5  6 | <- device (0, 1)\n                     |  3  4 |  7  8 |\n                     +-------+-------+\n    device (1, 0) -> |  9 10 | 13 14 | <- device (1, 1)\n                     | 11 12 | 15 16 |\n                     +-------+-------+\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyNon0RankedTensor" }
    ],
    "assemblyFormat": "$input `on` $grid (`grid_axes` `=` $grid_axes^)? `slice_axis` `=` $slice_axis\n    attr-dict `:` type($input) `->` type($result)"
  },
  {
    "name": "shard.all_to_all",
    "summary": "All-to-all over a device grid.",
    "description": "Each participant logically splits its input along split_axis,\n    then scatters the resulting pieces across the group defined by `grid_axes`.\n    After receiving data pieces from other participants' scatters,\n    it concatenates them along concat_axis to produce the final result.\n\n    Example:\n    ```\n    shard.grid @grid0(shape = 3)\n    ...\n    %1 = shard.all_to_all %0 on @grid0 grid_axes = [0]\n      split_axis = 0 concat_axis = 0\n      : tensor<3x2xi8> -> tensor<3x2xi8>\n    ```\n    Input:\n    ```\n     device  device  device\n     (0)     (1)     (2)\n    +-------+-------+-------+  | split and concat along\n    | 11 12 | 21 22 | 31 32 |  | tensor axis 0\n    | 13 14 | 23 24 | 33 34 |  ↓\n    | 15 16 | 25 26 | 35 36 |\n    +-------+-------+-------+\n    ```\n    Result:\n    ```\n     device  device  device\n     (0)     (1)     (2)\n    +-------+-------+-------+\n    | 11 12 | 13 14 | 15 16 |\n    | 21 22 | 23 24 | 25 26 |\n    | 31 32 | 33 34 | 35 36 |\n    +-------+-------+-------+\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyNon0RankedTensor" }
    ],
    "assemblyFormat": "$input `on` $grid (`grid_axes` `=` $grid_axes^)?\n    `split_axis` `=` $split_axis\n    `concat_axis` `=` $concat_axis\n    attr-dict `:` type($input) `->` type($result)"
  },
  {
    "name": "shard.broadcast",
    "summary": "Broadcast over a device grid.",
    "description": "Copies the input tensor on `root` to all devices in each group defined by\n    `grid_axes`. The `root` device is defined by its in-group multi-index.\n    The contents of input tensors on non-root devices are ignored.\n    \n    Example:\n    ```\n    shard.grid @grid0(shape = 2x2)\n\n    %1 = shard.broadcast %0 on @grid0\n      grid_axes = [0]\n      root = [0]\n      : (tensor<2xi8>) -> tensor<2xi8>\n    ```\n    \n    Input:\n    ```\n                     +-------+-------+                   | broadcast\n    device (0, 0) -> |  1  2 |  3  4 | <- device (0, 1)  | along axis 0\n                     +-------+-------+                   ↓\n    device (1, 0) -> |  *  * |  *  * | <- device (1, 1)\n                     +-------+-------+\n    ```\n\n    Output:\n    ```\n                     +-------+-------+\n    device (0, 0) -> |  1  2 |  3  4 | <- device (0, 1)\n                     +-------+-------+\n    device (1, 0) -> |  1  2 |  3  4 | <- device (1, 1)\n                     +-------+-------+\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "$input `on` $grid (`grid_axes` `=` $grid_axes^)?\n    `root` `=` custom<DynamicIndexList>($root_dynamic, $root)\n    attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "shard.gather",
    "summary": "Gather over a device grid.",
    "description": "Concatenates all tensor slices from a device group defined by `grid_axes` along\n    the tensor dimension `gather_axis` and returns the resulting tensor on each\n    `root` device. The result on all other (non-root) devices is undefined.\n    The `root` device is defined by its in-group multi-index.\n\n    Example:\n    ```mlir\n    shard.grid @grid0(shape = 2x2)\n    ...\n    %1 = shard.gather %0 on @grid0 grid_axes = [1]\n      gather_axis = 1 root = [1]\n      : (tensor<2x2xi8>) -> tensor<2x4xi8>\n    ```\n    Input:\n    ```\n                      gather tensor\n                      axis 1\n                      ------------>\n                     +-------+-------+\n    device (0, 0) -> |  1  2 |  5  6 | <- device (0, 1)\n                     |  3  4 |  7  8 |\n                     +-------+-------+\n    device (1, 0) -> |  9 10 | 13 14 | <- device (1, 1)\n                     | 11 12 | 15 16 |\n                     +-------+-------+\n    ```\n    Result:\n    ```\n    +-------------+\n    |  1  2  5  6 | <- devices (0, 1)\n    |  3  4  7  8 |\n    +-------------+\n    |  9 10 13 14 | <- devices (1, 1)\n    | 11 12 15 16 |\n    +-------------+\n    ```\n    Devices `(0, 0)` and `(1, 0)` have undefined result.",
    "outputs": [
      { "name": "result", "type": "AnyNon0RankedTensor" }
    ],
    "assemblyFormat": "$input `on` $grid (`grid_axes` `=` $grid_axes^)?\n    `gather_axis` `=` $gather_axis\n    `root` `=` custom<DynamicIndexList>($root_dynamic, $root)\n    attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "shard.get_sharding",
    "summary": "Get the sharding of the given tensor.",
    "description": "This operation returns the sharding of the given tensor as a Sharding.",
    "inputs": [
      { "name": "source", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "Shard_Sharding" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source) `->` type($result)"
  },
  {
    "name": "shard.grid",
    "summary": "Description of a device/process grid.",
    "description": "The shard.grid operation is a symbol operation that identifies a specific\n    grid. The operation has three attributes:\n\n    1. `sym_name`: This attribute uniquely identifies the name of the grid.\n    This name serves as a symbolic reference to the grid throughout\n    the MLIR module, allowing for consistent referencing and easier debugging.\n\n    2. `shape`: This attribute represents the shape of the device grid.\n    It uses the same notation as a tensor shape. Also allowing for dynamic\n    dimensions.\n    This flexibility allows for dynamic device assignment or configurations\n    where the exact number of devices might not be determined during compile\n    time.\n    For example `2x?x4`.\n\n    Example:\n    ```\n    // A device grid with 3 axes, the total device number is 4 * 8 * 12\n    // The dimension sizes are 4, 8, 12 \n    shard.grid @grid0(shape = 4x8x12)\n\n    // A device grid with 2 axes, the total device number is unknown\n    // The first dimension size is 4 and the second is unknown\n    shard.grid @grid1(shape = 4x?)\n\n    // A device grid with 2 axes, the total device number is unknown\n    // The first dimension size is unknown and the second is 4\n    shard.grid @grid2(shape = ?x4)\n\n    // A device grid with 2 axes, the number of devices along both axes\n    // is unknown\n    shard.grid @grid3(shape = ?x?)\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "shape", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$sym_name `(` `shape` `=` custom<DimensionList>($shape) `)`\n      attr-dict"
  },
  {
    "name": "shard.grid_shape",
    "summary": "Get the shape of the grid.",
    "outputs": [
      { "name": "result", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "grid", "type": "FlatSymbolRefAttr" },
      { "name": "axes", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$grid (`axes` `=` $axes^)?\n    attr-dict `:` type($result)"
  },
  {
    "name": "shard.neighbors_linear_indices",
    "summary": "For given grid index get the linear indices of the direct neighbor processes along the given split.",
    "description": "Example:\n    ```\n    shard.grid @grid0(shape = 10x20x30)\n    %c1 = arith.constant 1 : index\n    %c2 = arith.constant 2 : index\n    %c3 = arith.constant 3 : index\n    %idx = shard.neighbors_linear_indices on @grid[%c1, %c2, %c3] split_axes = [1] : index\n    ```\n    The above returns two indices, `633` and `693`, which correspond to the\n    index of the previous process `(1, 1, 3)`, and the next process \n    `(1, 3, 3)` along the split axis `1`.\n\n    A negative value is returned if there is no neighbor in the respective\n    direction along the given `split_axes`.",
    "inputs": [
      { "name": "device", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "neighbor_down", "type": "Index" },
      { "name": "neighbor_up", "type": "Index" }
    ],
    "attributes": [
      { "name": "grid", "type": "FlatSymbolRefAttr" },
      { "name": "split_axes", "type": "Shard_GridAxesAttr" }
    ],
    "assemblyFormat": "`on` $grid `[` $device `]`\n      `split_axes` `=` $split_axes\n      attr-dict `:` type(results)"
  },
  {
    "name": "shard.process_linear_index",
    "summary": "Get the linear index of the current device.",
    "description": "Example:\n    ```\n    %idx = shard.process_linear_index on @grid : index\n    ```\n    if `@grid` has shape `(10, 20, 30)`, a device with multi\n    index `(1, 2, 3)` will have linear index `3 + 30*2 + 20*30*1`.",
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "attributes": [
      { "name": "grid", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "`on` $grid attr-dict `:` type($result)"
  },
  {
    "name": "shard.process_multi_index",
    "summary": "Get the multi index of current device along specified grid axes.",
    "description": "It is used in the SPMD format of IR.\n    The `axes` mush be non-negative and less than the total number of grid axes.\n    If the axes are empty then get the index along all axes.",
    "outputs": [
      { "name": "result", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "grid", "type": "FlatSymbolRefAttr" },
      { "name": "axes", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "`on` $grid (`axes` `=` $axes^)?\n    attr-dict `:` type($result)"
  },
  {
    "name": "shard.recv",
    "summary": "Send over a device grid.",
    "description": "Receive tensor from device `source`, which is defined by its in-group\n    multi-index. The groups are defined by `grid_axes`.\n    The content of input tensor is ignored.",
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "$input `on` $grid (`grid_axes` `=` $grid_axes^)?\n    (`source` `=` custom<DynamicIndexList>($source_dynamic, $source)^)?\n    attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "shard.reduce",
    "summary": "Reduce over a device grid.",
    "description": "Reduces the input tensor across all devices within the groups defined by\n    `grid_axes`, using the specified reduction method. The operation performs an\n    element-wise reduction over the tensor slices from all devices in each group.\n    The reduction result will be returned on the `root` device of each group.\n    It is undefined on all other (non-root) devices.\n    The `root` device is defined by its in-group multi-index.\n    The accumulation element type is determined by the result type and does not\n    need to match the input element type. Before performing the reduction, each\n    input element is converted to the result element type.\n\n    Attributes:\n    `reduction`: Indicates the reduction method.\n\n    Example:\n    ```\n    %1 = shard.reduce %0 on @grid0 grid_axes = [1, 0]\n      reduction = <max> root = [2, 3]\n      : (tensor<3x4xf32>) -> tensor<3x4xf64>\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "$input `on` $grid (`grid_axes` `=` $grid_axes^)?\n    (`reduction` `=` $reduction^)?\n    `root` `=` custom<DynamicIndexList>($root_dynamic, $root)\n    attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "shard.reduce_scatter",
    "summary": "Reduce-scatter over a device grid.",
    "description": "Reduces the input tensor across all devices within the groups defined by\n    `grid_axes` using the specified reduction method. The reduction is performed\n    element-wise across the tensor pieces from all devices in the group.\n    After reduction, the reduction result is scattered (split and distributed)\n    across the device group along `scatter_axis`.\n    Example:\n    ```\n    shard.grid @grid0(shape = 2x2)\n    ...\n    %1 = shard.reduce_scatter %0 on @grid0 grid_axes = [1]\n      reduction = <max> scatter_axis = 0\n      : tensor<2x2xf32> -> tensor<1x2xf64>\n    ```\n    Input:\n    ```\n                              device\n                              (0, 1)\n                                 ↓\n                     +-------+-------+  | scatter tensor\n    device (0, 0) -> |  1  2 |  5  6 |  | axis 0\n                     |  3  4 |  7  8 |  ↓\n                     +-------+-------+\n    device (1, 0) -> |  9 10 | 13 14 |\n                     | 11 12 | 15 16 |\n                     +-------+-------+\n                                ↑\n                              device\n                              (1, 1)\n    ```\n    Result:\n    ```\n    +-------+\n    |  5  6 | <- devices (0, 0)\n    +-------+\n    |  7  8 | <- devices (0, 1)\n    +-------+\n    | 13 14 | <- devices (1, 0)\n    +-------+\n    | 15 16 | <- devices (1, 1)\n    +-------+\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "$input `on` $grid (`grid_axes` `=` $grid_axes^)?\n    (`reduction` `=` $reduction^)?\n    `scatter_axis` `=` $scatter_axis\n    attr-dict `:` type($input) `->` type($result)"
  },
  {
    "name": "shard.scatter",
    "summary": "Scatter over a device grid.",
    "description": "For each device group defined by `grid_axes`, the input tensor on the `root`\n    device is split along axis `scatter_axis` and distributed across the group.\n    The content of the input on all other (non-root) devices is ignored.\n    The `root` device is defined by its in-group multi-index.\n\n    Example:\n    ```\n    shard.grid @grid0(shape = 2x2)\n    %1 = shard.scatter %0 on @grid0 grid_axes = [0]\n      scatter_axis = 0\n      root = [1]\n      : (tensor<2x2xi8>) -> tensor<1x2xi8>\n    ```\n\n    Input:\n    ```\n                              device\n                              (0, 1)\n                                 ↓\n                     +-------+-------+  | scatter tensor\n    device (0, 0) -> |  *  * |  *  * |  | axis 0\n                     |  *  * |  *  * |  ↓\n                     +-------+-------+\n    device (1, 0) -> |  1  2 |  5  6 |\n                     |  3  4 |  7  8 |\n                     +-------+-------+\n                                ↑\n                              device\n                              (1, 1)\n    ```\n    \n    Result:\n    ```\n                              device\n                              (0, 1)\n                                 ↓\n                     +-------+-------+\n    device (0, 0) -> |  1  2 |  5  6 |\n                     +-------+-------+ \n    device (1, 0) -> |  3  4 |  7  8 |\n                     +-------+-------+\n                                ↑\n                              device\n                              (1, 1)\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "$input `on` $grid (`grid_axes` `=` $grid_axes^)?\n    `scatter_axis` `=` $scatter_axis\n    `root` `=` custom<DynamicIndexList>($root_dynamic, $root)\n    attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "shard.send",
    "summary": "Send over a device grid.",
    "description": "Send input tensor to device `destination`, which is defined by its in-group\n    multi-index. The groups are defined by `grid_axes`.",
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "$input `on` $grid (`grid_axes` `=` $grid_axes^)?\n    `destination` `=` custom<DynamicIndexList>($destination_dynamic, $destination)\n    attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "shard.shard",
    "summary": "Annotate on how a tensor is sharded across a shard.",
    "description": "The shard.shard operation is designed to specify and guide the sharding\n    behavior of a tensor value across a grid topology. This operation has two\n    operands and two optional attributes:\n\n    1. `input`: This operand represents the tensor value that needs to be\n    annotated for sharding.\n\n    2. `sharding`: This attribute is type of `ShardingType`, which is the core data\n    structure to represent distribution of a tensor on a shard. it is typically defined\n    by an `shard.sharding` operation.\n\n    3. `annotate_for_users`: A unit attribute addressing the scenario when a\n    tensor's sharding annotation differs based on its context of use (either as\n    a result or an operand). If specified, the sharding pertains to specific\n    users of the tensor value, indicating how it should be considered when used\n    as an operand in subsequent operations. If not, the sharding applies to the\n    operation that defines the tensor value.\n\n    Example:\n    ```\n  func.func @only_result_annotated(%arg0 : tensor<4x8xf32>) -> () {\n      %sharding = shard.sharding @grid0 split_axes = [[0]] : !shard.sharding\n      %0 = shard.shard %arg0 to %sharding : tensor<4x8xf32>\n      ...\n    }\n\n    func.func @only_operand_annotated(%arg0 : tensor<4x8xf32>) -> () {\n      %sharding = shard.sharding @grid0 split_axes = [[0]] : !shard.sharding\n      %0 = shard.shard %arg0 to %sharding annotate_for_users : tensor<4x8xf32>\n      ...\n    }\n    \n    func.func @two_operands_annotated(%arg0 : tensor<4x8xf32>, %arg1 : tensor<16x8xf32>) -> () {\n      %sharding = shard.sharding @grid0 split_axes = [[0]] : !shard.sharding\n      %0 = shard.shard %arg0 to %sharding annotate_for_users : tensor<4x8xf32>\n      %1 = shard.shard %arg1 to %sharding annotate_for_users : tensor<16x8xf32>\n      ...\n    }\n\n    // The first shard.shard op applies to %arg0, the second shard.shard op\n    // applies for the operand of op0, the third shard.shard op applies for the\n    // operand of op2\n    func.func @both_result_and_multi_operands_annotated(\n        %arg0 : tensor<4x8xf32>) -> () {\n      %sharding = shard.sharding @grid0 split_axes = [[0]] : !shard.sharding\n      %0 = shard.shard %arg0 to %sharding : tensor<4x8xf32>\n      %sharding1 = shard.sharding @grid0 split_axes = [[1]] : !shard.sharding\n      %1 = shard.shard %0 to %sharding1 annotate_for_users : tensor<4x8xf32>\n      %sharding2 = shard.sharding @grid0 split_axes = [[2]] : !shard.sharding\n      %2 = shard.shard %0 to %sharding2 annotate_for_users : tensor<4x8xf32>\n      \"op0\"(%1) : ...\n      \"op1\"(%2) : ...\n      ...\n    }\n    ```\n\n    The following usages are undefined:\n    ```\n    func.func @annotate_on_same_result_with_different_sharding(\n        %arg0 : tensor<4x8xf32>) -> () {\n      %sharding1 = shard.sharding @grid0 split_axes = [[0]] : !shard.sharding\n      %sharding2 = shard.sharding @grid0 split_axes = [[1]] : !shard.sharding\n      %0 = shard.shard %arg0 to $sharding1 : tensor<4x8xf32>\n      %1 = shard.shard %0 to sharding2 : tensor<4x8xf32>\n      ...\n    }\n\n    func.func @annotate_on_same_result_same_value_with_different_sharding(\n        %arg0 : tensor<4x8xf32>) -> () {\n      %sharding1 = shard.sharding @grid0 split_axes = [[0]] : !shard.sharding\n      %sharding2 = shard.sharding @grid0 split_axes = [[1]] : !shard.sharding\n      %0 = shard.shard %arg0 to %sharding1 : tensor<4x8xf32>\n      %1 = shard.shard %arg0 to %sharding2 : tensor<4x8xf32>\n      ...\n    }\n\n    func.func @annotate_on_same_operand_with_different_sharding(\n        %arg0 : tensor<4x8xf32>) -> () {\n      %sharding1 = shard.sharding @grid0 split_axes = [[0]] : !shard.sharding\n      %sharding2 = shard.sharding @grid0 split_axes = [[1]] : !shard.sharding\n      %0 = shard.shard %arg0 to %sharding1 annotate_for_users : tensor<4x8xf32>\n      %1 = shard.shard %0 to %sharding2 annotate_for_users : tensor<4x8xf32>\n      ...\n    }\n\n    func.func @result_annotated_after_operand(\n        %arg0 : tensor<4x8xf32>) -> () {\n      %sharding1 = shard.sharding @grid0 split_axes = [[0]] : !shard.sharding\n      %sharding2 = shard.sharding @grid0 split_axes = [[1]] : !shard.sharding\n      %0 = shard.shard %arg0 to %sharding1 annotate_for_users : tensor<4x8xf32>\n      %1 = shard.shard %0 to %sharding2 : tensor<4x8xf32>\n      ...\n    }\n    ```",
    "inputs": [
      { "name": "src", "type": "AnyRankedTensor" },
      { "name": "sharding", "type": "Shard_Sharding" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "annotate_for_users", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$src `to` $sharding\n      (`annotate_for_users` $annotate_for_users^)?\n      attr-dict `:` type($result)"
  },
  {
    "name": "shard.shard_shape",
    "summary": "Get the shard shape for a given process/device.",
    "description": "The device/process id is a multi-index of the device/process in the shard.\n    This operation might be used during partition when the shard shape depends\n    on (non-constant) values used in `shard.sharding`.",
    "inputs": [
      { "name": "dims_dynamic", "type": "Variadic" },
      { "name": "sharding", "type": "Shard_Sharding" },
      { "name": "device_dynamic", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dims", "type": "DenseI64ArrayAttr" },
      { "name": "device", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "`dims` `=` custom<DynamicIndexList>($dims_dynamic, $dims)\n      `sharding` `=` $sharding\n      `device` `=` custom<DynamicIndexList>($device_dynamic, $device)\n      attr-dict `:` type(results)"
  },
  {
    "name": "shard.sharding",
    "summary": "Define a sharding of a tensor.",
    "description": "The Sharding specifies how a tensor is sharded and distributed across the\n    process shard. It is typically used in a `shard.shard` operation.\n    The operation has the following attributes and operands:\n\n    1. `grid`: this attribute is a FlatSymbolRefAttr that refers to the device\n    grid where the distributed tensor is placed. The symbol must resolve to a\n    `shard.grid` operation.\n\n    2. `split_axes`: is an array composed of int64_t sub-arrays. The outer array's\n    maximum size is the `rank` of the related tensor. For the i-th sub-array, if\n    its value is [x, y], it indicates that the tensor's i-th dimension is splitted\n    along the x and y axes of the device grid.\n\n    3. [Optional] Sizes of halos to be added for each sharded tensor dimension.\n    `halo_sizes` is provided as a flattened 1d array of i64s, 2 values for each\n    sharded dimension. `halo_sizes = [1, 2]` means that the first sharded dimension\n    gets an additional halo of size 1 at the start of the first dimension and a halo\n    size is 2 at its end. `halo_sizes = [1, 2, 2, 3]` defines halos for the first 2\n    sharded dimensions e.g. the first sharded dimension gets `[1,2]` halos and the\n    seconds gets `[2,3]` halos. `?` indicates dynamic halo sizes.\n    \n    4. [Optional] Offsets for each shard and sharded tensor dimension.\n    `sharded_dims_offsets` is provided as a flattened 1d array of i64s. For each\n    sharded tensor dimension the offsets (starting index) of all shards in that\n    dimension and an additional value for the end of the last shard are provided.\n    For a 1d sharding this means that position `i` has the exclusive prefix sum for\n    shard `i`, and since only contiguous sharding is supported, its inclusive prefix\n    sum is at position 'i+1'.\n    \n    Assuming a 3d-tensor of shape 32x32x32 with the first 2 dimensions being sharded,\n    `sharded_dims_offsets` = [0, 24, 32, 0, 20, 32] means that the first device of\n    the device-grid will get a shard of shape 24x20x32 and the second device will get\n    a shard of shape 8x12x32. `?` indicates dynamic shard dimensions.\n    \n    `halo_sizes` and `sharded_dims_offsets` are mutually exclusive.\n\n    Examples:\n\n    ```\n    shard.grid @grid0(shape = 2x2x4)\n    shard.grid @grid1d_4(shape = 4)\n\n    // The tensor is fully replicated on @grid0.\n    // Currently, there must be at least one sub-array present in axes, even\n    // if it's empty. Otherwise, a parsing error will occur.\n    %sharding0 = shard.sharding @grid0 split_axes = [[]]\n\n    // The tensor is sharded on the first dimension along axis 0 of @grid0\n    %sharding1 = shard.sharding @grid0 split_axes = [[0]]\n\n    // Could be used for a shard.shard op\n    %sharded0 = shard.shard %arg0 to %sharding3 : tensor<4x8xf32>\n\n    // The tensor is sharded on its first dimension along axis 0 of @grid0 and\n    // and it has halo-sizes of 1 and 2 on the sharded dim.\n    %halo_sharding = shard.sharding @grid0 split_axes = [[0]] halo_sizes = [1, 2]\n    %sharded1 = shard.shard %arg0 to %halo_sharding : tensor<4x8xf32>\n    \n    // The tensor is sharded on its second dimension along axis 0 of @grid1d_4\n    // and it has pre-defined shard sizes. The shards of the devices will have\n    // the following shapes: [4x2, 4x3, 4x4, 4x5]\n    %sharding4 = shard.sharding @grid1d_4 split_axes = [[], [0]] sharded_dims_offsets = [0, 2, 5, 9, 14]\n    %sharded2 = shard.shard %arg0 to %sharding4 : tensor<4x14xf32>\n    ```",
    "inputs": [
      { "name": "dynamic_sharded_dims_offsets", "type": "Variadic" },
      { "name": "dynamic_halo_sizes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Shard_Sharding" }
    ],
    "attributes": [
      { "name": "grid", "type": "FlatSymbolRefAttr" },
      { "name": "split_axes", "type": "Shard_GridAxesArrayAttr" },
      { "name": "static_sharded_dims_offsets", "type": "DefaultValuedAttr" },
      { "name": "static_halo_sizes", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$grid\n    `split_axes` `=` $split_axes\n    (`halo_sizes` `=` custom<DynamicIndexList>($dynamic_halo_sizes, $static_halo_sizes)^)?\n    (`sharded_dims_offsets` `=` custom<DynamicIndexList>($dynamic_sharded_dims_offsets, $static_sharded_dims_offsets)^)?\n    attr-dict `:` type($result)"
  },
  {
    "name": "shard.shift",
    "summary": "Shift over a device grid.",
    "description": "Within each device group defined by `grid_axes`, shifts input tensors along the\n    device grid's axis `shift_axis` by the specified offset. The `shift_axis` must\n    be one of the `grid_axes`. If the `rotate` attribute is set, the shift is circular.\n    That is, the offset wraps around according to the group size along `shift_axis`.\n    Otherwise, the results on devices without a corresponding source are undefined.\n\n    Example:\n    ```\n    shard.grid @grid0(shape = 2x4)\n    %1 = shard.shift on @grid0 grid_axes = [1]\n      shift_axis = 1 offset = 2 rotate\n      : tensor<2xi8> -> tensor<2xi8>\n    ```\n\n    Input:\n    ```\n    grid axis 1\n    ----------->\n\n    +----+----+----+----+\n    |  1 |  2 |  3 |  4 |\n    +----+----+----+----+\n    |  5 |  6 |  7 |  8 |\n    +----+----+----+----+\n    ```\n\n    Result:\n    ```\n    +----+----+----+----+\n    |  3 |  4 |  1 |  2 |\n    +----+----+----+----+\n    |  7 |  8 |  5 |  6 |\n    +----+----+----+----+\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "$input `on` $grid (`grid_axes` `=` $grid_axes^)?\n    `shift_axis` `=` $shift_axis\n    `offset` `=` $offset\n    (`rotate` $rotate^)?\n    attr-dict `:` type($input) `->` type($result)"
  },
  {
    "name": "shard.update_halo",
    "summary": "Update halo data.",
    "description": "This operation updates halo regions of shards, e.g. if their sharding\n    specified halos and the actual tensor/memref data might have changed\n    on the remote devices. Changes might be caused by mutating operations\n    and/or if the new halo regions are larger than the existing ones.\n\n    Destination is supposed to be initialized with the local data (not halos).\n\n    Assumes all devices hold tensors with same-sized halo data as specified\n    by `source_halo_sizes/static_source_halo_sizes` and\n    `destination_halo_sizes/static_destination_halo_sizes` in source shard\n    and destination/result shard.\n\n    `split_axes` specifies for each tensor axis along which grid axes its halo\n    data is updated.",
    "inputs": [
      { "name": "destination", "type": "AnyTypeOf" },
      { "name": "halo_sizes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "grid", "type": "FlatSymbolRefAttr" },
      { "name": "split_axes", "type": "Shard_GridAxesArrayAttr" },
      { "name": "static_halo_sizes", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$destination\n    `on` $grid\n    `split_axes` `=` $split_axes\n    (`halo_sizes` `=` custom<DynamicIndexList>($halo_sizes, $static_halo_sizes)^)?\n    attr-dict `:` type($result)"
  },
  {
    "name": "smt.and",
    "summary": "desc",
    "description": "' operator in the\n    [Core theory](https://smtlib.cs.uiowa.edu/Theories/Core.smt2).\n    of the SMT-LIB Standard 2.7.\n\n    It supports a variadic number of operands, but requires at least two.\n    This is because the operator is annotated with the `:left-assoc` attribute\n    which means that `op a b c` is equivalent to `(op (op a b) c)`.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "BoolType" }
    ],
    "assemblyFormat": "$inputs attr-dict"
  },
  {
    "name": "smt.apply_func",
    "summary": "apply a function",
    "description": "This operation performs a function application as described in the\n    [SMT-LIB 2.7 standard](https://smt-lib.org/papers/smt-lib-reference-v2.7-r2025-02-05.pdf).\n    It is part of the language itself rather than a theory or logic.",
    "inputs": [
      { "name": "func", "type": "SMTFuncType" },
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyNonFuncSMTType" }
    ],
    "assemblyFormat": "$func `(` $args `)` attr-dict `:` qualified(type($func))"
  },
  {
    "name": "smt.array.#mnemonic",
    "summary": "construct an array with the given value stored at every index",
    "description": "This operation represents a broadcast of the 'value' operand to all indices\n    of the array. It is equivalent to\n    ```\n    %0 = smt.declare_fun \"array\" : !smt.array<[!smt.int -> !smt.bool]>\n    %1 = smt.forall [\"idx\"] {\n    ^bb0(%idx: !smt.int):\n      %2 = smt.array.select %0[%idx] : !smt.array<[!smt.int -> !smt.bool]>\n      %3 = smt.eq %value, %2 : !smt.bool\n      smt.yield %3 : !smt.bool\n    }\n    smt.assert %1\n    // return %0\n    ```\n\n    In SMT-LIB, this is frequently written as\n    `((as const (Array Int Bool)) value)`.",
    "inputs": [
      { "name": "value", "type": "AnySMTType" }
    ],
    "outputs": [
      { "name": "result", "type": "ArrayType" }
    ],
    "assemblyFormat": "$value attr-dict `:` qualified(type($result))"
  },
  {
    "name": "smt.assert",
    "summary": "assert that a boolean expression holds",
    "inputs": [
      { "name": "input", "type": "BoolType" }
    ],
    "assemblyFormat": "$input attr-dict"
  },
  {
    "name": "smt.bv.#mnemonic",
    "summary": "repeated bit-vector concatenation of one value",
    "description": "This operation is a shorthand for repeated concatenation of the same\n    bit-vector value, i.e.,\n    ```mlir\n    smt.bv.repeat 5 times %a : !smt.bv<4>\n    // is the same as\n    %0 = smt.bv.repeat 4 times %a : !smt.bv<4>\n    smt.bv.concat %a, %0 : !smt.bv<4>, !smt.bv<16>\n    // or also \n    %0 = smt.bv.repeat 4 times %a : !smt.bv<4>\n    smt.bv.concat %0, %a : !smt.bv<16>, !smt.bv<4>\n    ```\n    \n    The semantics are equivalent to the `repeat` operator defined in the SMT-LIB\n    2.7 standard. More precisely in the\n    [theory of FixedSizeBitVectors](https://smtlib.cs.uiowa.edu/Theories/FixedSizeBitVectors.smt2)\n    and the [QF_BV logic](https://smtlib.cs.uiowa.edu/Logics/QF_BV.smt2)\n    describing closed quantifier-free formulas over the theory of fixed-size\n    bit-vectors.",
    "inputs": [
      { "name": "input", "type": "BitVectorType" }
    ],
    "outputs": [
      { "name": "result", "type": "BitVectorType" }
    ]
  },
  {
    "name": "smt.bv2int",
    "summary": "Convert an SMT bit-vector to an SMT integer.",
    "description": "Create an integer from the bit-vector argument `input`. If `is_signed` is\n    present, the bit-vector is treated as two's complement signed.  Otherwise,\n    it is treated as an unsigned integer in the range [0..2^N-1], where N is\n    the number of bits in `input`.",
    "inputs": [
      { "name": "input", "type": "BitVectorType" }
    ],
    "outputs": [
      { "name": "result", "type": "IntType" }
    ],
    "attributes": [
      { "name": "is_signed", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$input (`signed` $is_signed^)? attr-dict `:`\n    qualified(type($input))"
  },
  {
    "name": "smt.check",
    "summary": "check if the current set of assertions is satisfiable",
    "description": "This operation checks if all the assertions in the solver defined by the\n    nearest ancestor operation of type `smt.solver` are consistent. The outcome\n    an be 'satisfiable', 'unknown', or 'unsatisfiable' and the corresponding\n    region will be executed. It is the corresponding construct to the\n    `check-sat` in SMT-LIB.\n\n    Example:\n    ```mlir\n    %0 = smt.check sat {\n      %c1_i32 = arith.constant 1 : i32\n      smt.yield %c1_i32 : i32\n    } unknown {\n      %c0_i32 = arith.constant 0 : i32\n      smt.yield %c0_i32 : i32\n    } unsat {\n      %c-1_i32 = arith.constant -1 : i32\n      smt.yield %c-1_i32 : i32\n    } -> i32\n    ```",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict `sat` $satRegion `unknown` $unknownRegion `unsat` $unsatRegion\n    (`->` qualified(type($results))^ )?"
  },
  {
    "name": "smt.constant",
    "summary": "Produce a constant boolean",
    "description": "Produces the constant expressions 'true' and 'false' as described in the\n    [Core theory](https://smtlib.cs.uiowa.edu/Theories/Core.smt2) of the SMT-LIB\n    Standard 2.7.",
    "outputs": [
      { "name": "result", "type": "BoolType" }
    ],
    "attributes": [
      { "name": "value", "type": "BoolAttr" }
    ],
    "assemblyFormat": "$value attr-dict"
  },
  {
    "name": "smt.declare_fun",
    "summary": "declare a symbolic value of a given sort",
    "description": "This operation declares a symbolic value just as the `declare-const` and\n    `declare-fun` statements in SMT-LIB 2.7. The result type determines the SMT\n    sort of the symbolic value. The returned value can then be used to refer to\n    the symbolic value instead of using the identifier like in SMT-LIB.\n\n    The optionally provided string will be used as a prefix for the newly\n    generated identifier (useful for easier readability when exporting to\n    SMT-LIB). Each `declare` will always provide a unique new symbolic value\n    even if the identifier strings are the same.\n\n    Note that there does not exist a separate operation equivalent to\n    SMT-LIBs `define-fun` since\n    ```\n    (define-fun f (a Int) Int (-a))\n    ```\n    is only syntactic sugar for\n    ```\n    %f = smt.declare_fun : !smt.func<(!smt.int) !smt.int>\n    %0 = smt.forall {\n    ^bb0(%arg0: !smt.int):\n      %1 = smt.apply_func %f(%arg0) : !smt.func<(!smt.int) !smt.int>\n      %2 = smt.int.neg %arg0\n      %3 = smt.eq %1, %2 : !smt.int\n      smt.yield %3 : !smt.bool\n    }\n    smt.assert %0\n    ```\n\n    Note that this operation cannot be marked as Pure since two operations (even\n    with the same identifier string) could then be CSEd, leading to incorrect\n    behavior.",
    "outputs": [
      { "name": "result", "type": "Res" }
    ],
    "attributes": [
      { "name": "namePrefix", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($namePrefix^)? attr-dict `:` qualified(type($result))"
  },
  {
    "name": "smt.distinct",
    "summary": "returns true iff all operands are not identical to any other",
    "description": "This operation compares the operands and returns true iff all operands are\n    not identical to any of the other operands. The semantics are equivalent to\n    the `distinct` operator defined in the SMT-LIB Standard 2.7 in the\n    [Core theory](https://smtlib.cs.uiowa.edu/Theories/Core.smt2).\n\n    Any SMT sort/type is allowed for the operands and it supports a variadic\n    number of operands, but requires at least two. This is because the\n    `distinct` operator is annotated with `:pairwise` which means that\n    `distinct a b c d` is equivalent to\n    ```\n    and (distinct a b) (distinct a c) (distinct a d)\n        (distinct b c) (distinct b d)\n        (distinct c d)\n    ```\n    where `and` is annotated `:left-assoc`, i.e., it can be further rewritten to\n    ```\n    (and (and (and (and (and (distinct a b)\n                             (distinct a c))\n                        (distinct a d))\n                   (distinct b c))\n              (distinct b d))\n         (distinct c d)\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "BoolType" }
    ]
  },
  {
    "name": "smt.eq",
    "summary": "returns true iff all operands are identical",
    "description": "This operation compares the operands and returns true iff all operands are\n    identical. The semantics are equivalent to the `=` operator defined in the\n    SMT-LIB Standard 2.7 in the\n    [Core theory](https://smtlib.cs.uiowa.edu/Theories/Core.smt2).\n\n    Any SMT sort/type is allowed for the operands and it supports a variadic\n    number of operands, but requires at least two. This is because the `=`\n    operator is annotated with `:chainable` which means that `= a b c d` is\n    equivalent to `and (= a b) (= b c) (= c d)` where `and` is annotated\n    `:left-assoc`, i.e., it can be further rewritten to\n    `and (and (= a b) (= b c)) (= c d)`.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "BoolType" }
    ]
  },
  {
    "name": "smt.exists",
    "summary": "exists quantifier",
    "description": "as described in the\n    [SMT-LIB 2.7 standard](https://smt-lib.org/papers/smt-lib-reference-v2.7-r2025-02-05.pdf).\n    It is part of the language itself rather than a theory or logic.\n\n    The operation specifies the name prefixes (as an optional attribute) and\n    types (as the types of the block arguments of the regions) of bound\n    variables that may be used in the 'body' of the operation. If a 'patterns'\n    region is specified, the block arguments must match the ones of the 'body'\n    region and (other than there) must be used at least once in the 'patterns'\n    region. It may also not contain any operations that bind variables, such as\n    quantifiers. While the 'body' region must always yield exactly one\n    `!smt.bool`-typed value, the 'patterns' region can yield an arbitrary number\n    (but at least one) of SMT values.\n\n    The bound variables can be any SMT type except of functions, since SMT only\n    supports first-order logic.\n\n    The 'no_patterns' attribute is only allowed when no 'patterns' region is\n    specified and forbids the solver to generate and use patterns for this\n    quantifier.\n\n    The 'weight' attribute indicates the importance of this quantifier being\n    instantiated compared to other quantifiers that may be present. The default\n    value is zero.\n\n    Both the 'no_patterns' and 'weight' attributes are annotations to the\n    quantifiers body term. Annotations and attributes are described in the\n    standard in sections 3.4, and 3.6 (specifically 3.6.5). SMT-LIB allows\n    adding custom attributes to provide solvers with additional metadata, e.g.,\n    hints such as above mentioned attributes. They are not part of the standard\n    themselves, but supported by common SMT solvers (e.g., Z3).",
    "outputs": [
      { "name": "result", "type": "BoolType" }
    ],
    "attributes": [
      { "name": "weight", "type": "DefaultValuedAttr" },
      { "name": "noPattern", "type": "UnitAttr" },
      { "name": "boundVarNames", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($boundVarNames^)? (`no_pattern` $noPattern^)? (`weight` $weight^)?\n    attr-dict-with-keyword $body (`patterns` $patterns^)?"
  },
  {
    "name": "smt.forall",
    "summary": "forall quantifier",
    "description": "as described in the\n    [SMT-LIB 2.7 standard](https://smt-lib.org/papers/smt-lib-reference-v2.7-r2025-02-05.pdf).\n    It is part of the language itself rather than a theory or logic.\n\n    The operation specifies the name prefixes (as an optional attribute) and\n    types (as the types of the block arguments of the regions) of bound\n    variables that may be used in the 'body' of the operation. If a 'patterns'\n    region is specified, the block arguments must match the ones of the 'body'\n    region and (other than there) must be used at least once in the 'patterns'\n    region. It may also not contain any operations that bind variables, such as\n    quantifiers. While the 'body' region must always yield exactly one\n    `!smt.bool`-typed value, the 'patterns' region can yield an arbitrary number\n    (but at least one) of SMT values.\n\n    The bound variables can be any SMT type except of functions, since SMT only\n    supports first-order logic.\n\n    The 'no_patterns' attribute is only allowed when no 'patterns' region is\n    specified and forbids the solver to generate and use patterns for this\n    quantifier.\n\n    The 'weight' attribute indicates the importance of this quantifier being\n    instantiated compared to other quantifiers that may be present. The default\n    value is zero.\n\n    Both the 'no_patterns' and 'weight' attributes are annotations to the\n    quantifiers body term. Annotations and attributes are described in the\n    standard in sections 3.4, and 3.6 (specifically 3.6.5). SMT-LIB allows\n    adding custom attributes to provide solvers with additional metadata, e.g.,\n    hints such as above mentioned attributes. They are not part of the standard\n    themselves, but supported by common SMT solvers (e.g., Z3).",
    "outputs": [
      { "name": "result", "type": "BoolType" }
    ],
    "attributes": [
      { "name": "weight", "type": "DefaultValuedAttr" },
      { "name": "noPattern", "type": "UnitAttr" },
      { "name": "boundVarNames", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($boundVarNames^)? (`no_pattern` $noPattern^)? (`weight` $weight^)?\n    attr-dict-with-keyword $body (`patterns` $patterns^)?"
  },
  {
    "name": "smt.implies",
    "summary": "boolean implication",
    "description": "This operation performs a boolean implication. The semantics are equivalent\n    to the '=>' operator in the\n    [Core theory](https://smtlib.cs.uiowa.edu/Theories/Core.smt2) of the SMT-LIB\n    Standard 2.7.",
    "inputs": [
      { "name": "lhs", "type": "BoolType" },
      { "name": "rhs", "type": "BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "BoolType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict"
  },
  {
    "name": "smt.int.#mnemonic",
    "summary": "integer comparison",
    "description": "This operation represents the comparison of (infinite-precision) integers.\n    The semantics are equivalent to the `<= (le)`, `< (lt)`, `>= (ge)`, or\n    `> (gt)` operator depending on the predicate (indicated in parentheses) as\n    described in the\n    [SMT Ints theory](https://smtlib.cs.uiowa.edu/Theories/Ints.smt2) of the\n    SMT-LIB 2.7 standard.",
    "inputs": [
      { "name": "pred", "type": "IntPredicate" },
      { "name": "lhs", "type": "IntType" },
      { "name": "rhs", "type": "IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "BoolType" }
    ],
    "assemblyFormat": "$pred $lhs `,` $rhs attr-dict"
  },
  {
    "name": "smt.int2bv",
    "summary": "Convert an integer to an inferred-width bitvector.",
    "description": "Designed to lower directly to an operation of the same name in Z3. The Z3\n    C API describes the semantics as follows:\n    Create an n bit bit-vector from the integer argument t1.\n    The resulting bit-vector has n bits, where the i'th bit (counting from 0\n    to n-1) is 1 if (t1 div 2^i) mod 2 is 1.\n    The node t1 must have integer sort.",
    "inputs": [
      { "name": "input", "type": "IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "BitVectorType" }
    ],
    "assemblyFormat": "$input attr-dict `:` qualified(type($result))"
  },
  {
    "name": "smt.ite",
    "summary": "an if-then-else function",
    "description": "This operation returns its second operand or its third operand depending on\n    whether its first operand is true or not. The semantics are equivalent to\n    the `ite` operator defined in the\n    [Core theory](https://smtlib.cs.uiowa.edu/Theories/Core.smt2) of the SMT-LIB\n    2.7 standard.",
    "inputs": [
      { "name": "cond", "type": "BoolType" },
      { "name": "thenValue", "type": "AnySMTType" },
      { "name": "elseValue", "type": "AnySMTType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnySMTType" }
    ],
    "assemblyFormat": "$cond `,` $thenValue `,` $elseValue attr-dict `:` qualified(type($result))"
  },
  {
    "name": "smt.not",
    "summary": "a boolean negation",
    "description": "This operation performs a boolean negation. The semantics are equivalent to\n    the 'not' operator in the\n    [Core theory](https://smtlib.cs.uiowa.edu/Theories/Core.smt2) of the SMT-LIB\n    Standard 2.7.",
    "inputs": [
      { "name": "input", "type": "BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "BoolType" }
    ],
    "assemblyFormat": "$input attr-dict"
  },
  {
    "name": "smt.or",
    "summary": "desc",
    "description": "' operator in the\n    [Core theory](https://smtlib.cs.uiowa.edu/Theories/Core.smt2).\n    of the SMT-LIB Standard 2.7.\n\n    It supports a variadic number of operands, but requires at least two.\n    This is because the operator is annotated with the `:left-assoc` attribute\n    which means that `op a b c` is equivalent to `(op (op a b) c)`.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "BoolType" }
    ],
    "assemblyFormat": "$inputs attr-dict"
  },
  {
    "name": "smt.pop",
    "summary": "pop a given number of levels from the assertion stack",
    "attributes": [
      { "name": "count", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$count attr-dict"
  },
  {
    "name": "smt.push",
    "summary": "push a given number of levels onto the assertion stack",
    "attributes": [
      { "name": "count", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$count attr-dict"
  },
  {
    "name": "smt.reset",
    "summary": "reset the solver",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "smt.set_logic",
    "summary": "set the logic for the SMT solver",
    "attributes": [
      { "name": "logic", "type": "StrAttr" }
    ],
    "assemblyFormat": "$logic attr-dict"
  },
  {
    "name": "smt.solver",
    "summary": "create a solver instance within a lifespan",
    "description": "This operation defines an SMT context with a solver instance. SMT operations\n    are only valid when being executed between the start and end of the region\n    of this operation. Any invocation outside is undefined. However, they do not\n    have to be direct children of this operation. For example, it is allowed to\n    have SMT operations in a `func.func` which is only called from within this\n    region. No SMT value may enter or exit the lifespan of this region (such\n    that no value created from another SMT context can be used in this scope and\n    the solver can deallocate all state required to keep track of SMT values at\n    the end).\n\n    As a result, the region is comparable to an entire SMT-LIB script, but\n    allows for concrete operations and control-flow. Concrete values may be\n    passed in and returned to influence the computations after the `smt.solver`\n    operation.\n\n    Example:\n    ```mlir\n    %0:2 = smt.solver (%in) {smt.some_attr} : (i8) -> (i8, i32) {\n    ^bb0(%arg0: i8):\n      %c = smt.declare_fun \"c\" : !smt.bool\n      smt.assert %c\n      %1 = smt.check sat {\n        %c1_i32 = arith.constant 1 : i32\n        smt.yield %c1_i32 : i32\n      } unknown {\n        %c0_i32 = arith.constant 0 : i32\n        smt.yield %c0_i32 : i32\n      } unsat {\n        %c-1_i32 = arith.constant -1 : i32\n        smt.yield %c-1_i32 : i32\n      } -> i32\n      smt.yield %arg0, %1 : i8, i32\n    }\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "`(` $inputs `)` attr-dict `:` functional-type($inputs, $results) $bodyRegion"
  },
  {
    "name": "smt.xor",
    "summary": "desc",
    "description": "' operator in the\n    [Core theory](https://smtlib.cs.uiowa.edu/Theories/Core.smt2).\n    of the SMT-LIB Standard 2.7.\n\n    It supports a variadic number of operands, but requires at least two.\n    This is because the operator is annotated with the `:left-assoc` attribute\n    which means that `op a b c` is equivalent to `(op (op a b) c)`.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "BoolType" }
    ],
    "assemblyFormat": "$inputs attr-dict"
  },
  {
    "name": "sparse_tensor.assemble",
    "summary": "Returns a sparse tensor assembled from the given levels and values",
    "description": "Assembles the per-level position and coordinate arrays together with\n    the values arrays into a sparse tensor. The order and types of the\n    provided levels must be consistent with the actual storage layout of\n    the returned sparse tensor described below.\n\n    - `levels: [tensor<? x iType>, ...]`\n      supplies the sparse tensor position and coordinate arrays\n      of the sparse tensor for the corresponding level as specifed by\n      `sparse_tensor::StorageLayout`.\n    - `values : tensor<? x V>`\n      supplies the values array for the stored elements in the sparse tensor.\n\n    This operation can be used to assemble a sparse tensor from an\n    external source; e.g., by passing numpy arrays from Python. It\n    is the user's responsibility to provide input that can be correctly\n    interpreted by the sparsifier, which does not perform any sanity\n    test to verify data integrity.\n\n    Example:\n\n    ```mlir\n    %pos    = arith.constant dense<[0, 3]>                : tensor<2xindex>\n    %index  = arith.constant dense<[[0,0], [1,2], [1,3]]> : tensor<3x2xindex>\n    %values = arith.constant dense<[ 1.1,   2.2,   3.3 ]> : tensor<3xf64>\n    %s = sparse_tensor.assemble (%pos, %index), %values\n       : (tensor<2xindex>, tensor<3x2xindex>), tensor<3xf64> to tensor<3x4xf64, #COO>\n    // yields COO format |1.1, 0.0, 0.0, 0.0|\n    //     of 3x4 matrix |0.0, 0.0, 2.2, 3.3|\n    //                   |0.0, 0.0, 0.0, 0.0|\n    ```",
    "inputs": [
      { "name": "levels", "type": "Variadic" },
      { "name": "values", "type": "RankedTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnySparseTensor" }
    ],
    "assemblyFormat": "` ` `(` $levels       `)` `,` $values attr-dict `:`    `(` type($levels) `)` `,` type($values) `to` type($result)"
  },
  {
    "name": "sparse_tensor.binary",
    "summary": "Binary set operation utilized within linalg.generic",
    "description": "Defines a computation within a `linalg.generic` operation that takes two\n      operands and executes one of the regions depending on whether both operands\n      or either operand is nonzero (i.e. stored explicitly in the sparse storage\n      format).\n\n      Three regions are defined for the operation and must appear in this order:\n      - overlap (elements present in both sparse tensors)\n      - left (elements only present in the left sparse tensor)\n      - right (element only present in the right sparse tensor)\n\n      Each region contains a single block describing the computation and result.\n      Every non-empty block must end with a sparse_tensor.yield and the return\n      type must match the type of `output`. The primary region's block has two\n      arguments, while the left and right region's block has only one argument.\n\n      A region may also be declared empty (i.e. `left={}`), indicating that the\n      region does not contribute to the output. For example, setting both\n      `left={}` and `right={}` is equivalent to the intersection of the two\n      inputs as only the overlap region will contribute values to the output.\n\n      As a convenience, there is also a special token `identity` which can be\n      used in place of the left or right region. This token indicates that\n      the return value is the input value (i.e. func(%x) => return %x).\n      As a practical example, setting `left=identity` and `right=identity`\n      would be equivalent to a union operation where non-overlapping values\n      in the inputs are copied to the output unchanged.\n\n      Due to the possibility of empty regions, i.e. lack of a value for certain\n      cases, the result of this operation may only feed directly into the output\n      of the `linalg.generic` operation or into into a custom reduction\n      `sparse_tensor.reduce` operation that follows in the same region.\n\n      Example of isEqual applied to intersecting elements only:\n\n      ```mlir\n      %C = tensor.empty(...)\n      %0 = linalg.generic #trait\n        ins(%A: tensor<?xf64, #SparseVector>,\n            %B: tensor<?xf64, #SparseVector>)\n        outs(%C: tensor<?xi8, #SparseVector>) {\n        ^bb0(%a: f64, %b: f64, %c: i8) :\n          %result = sparse_tensor.binary %a, %b : f64, f64 to i8\n            overlap={\n              ^bb0(%arg0: f64, %arg1: f64):\n                %cmp = arith.cmpf \"oeq\", %arg0, %arg1 : f64\n                %ret_i8 = arith.extui %cmp : i1 to i8\n                sparse_tensor.yield %ret_i8 : i8\n            }\n            left={}\n            right={}\n          linalg.yield %result : i8\n      } -> tensor<?xi8, #SparseVector>\n      ```\n\n      Example of A+B in upper triangle, A-B in lower triangle:\n\n      ```mlir\n      %C = tensor.empty(...)\n      %1 = linalg.generic #trait\n        ins(%A: tensor<?x?xf64, #CSR>, %B: tensor<?x?xf64, #CSR>\n        outs(%C: tensor<?x?xf64, #CSR> {\n        ^bb0(%a: f64, %b: f64, %c: f64) :\n          %row = linalg.index 0 : index\n          %col = linalg.index 1 : index\n          %result = sparse_tensor.binary %a, %b : f64, f64 to f64\n            overlap={\n              ^bb0(%x: f64, %y: f64):\n                %cmp = arith.cmpi \"uge\", %col, %row : index\n                %upperTriangleResult = arith.addf %x, %y : f64\n                %lowerTriangleResult = arith.subf %x, %y : f64\n                %ret = arith.select %cmp, %upperTriangleResult, %lowerTriangleResult : f64\n                sparse_tensor.yield %ret : f64\n            }\n            left=identity\n            right={\n              ^bb0(%y: f64):\n                %cmp = arith.cmpi \"uge\", %col, %row : index\n                %lowerTriangleResult = arith.negf %y : f64\n                %ret = arith.select %cmp, %y, %lowerTriangleResult : f64\n                sparse_tensor.yield %ret : f64\n            }\n          linalg.yield %result : f64\n      } -> tensor<?x?xf64, #CSR>\n      ```\n\n      Example of set difference. Returns a copy of A where its sparse structure\n      is *not* overlapped by B. The element type of B can be different than A\n      because we never use its values, only its sparse structure:\n\n      ```mlir\n      %C = tensor.empty(...)\n      %2 = linalg.generic #trait\n        ins(%A: tensor<?x?xf64, #CSR>, %B: tensor<?x?xi32, #CSR>\n        outs(%C: tensor<?x?xf64, #CSR> {\n        ^bb0(%a: f64, %b: i32, %c: f64) :\n          %result = sparse_tensor.binary %a, %b : f64, i32 to f64\n            overlap={}\n            left=identity\n            right={}\n          linalg.yield %result : f64\n      } -> tensor<?x?xf64, #CSR>\n      ```",
    "inputs": [
      { "name": "x", "type": "AnyType" },
      { "name": "y", "type": "AnyType" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "left_identity", "type": "UnitAttr" },
      { "name": "right_identity", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$x `,` $y `:` attr-dict type($x) `,` type($y) `to` type($output) `\\n`\n        `overlap` `=` $overlapRegion `\\n`\n        `left` `=` (`identity` $left_identity^):($leftRegion)? `\\n`\n        `right` `=` (`identity` $right_identity^):($rightRegion)?"
  },
  {
    "name": "sparse_tensor.coiterate",
    "summary": "Co-iterates over a set of sparse iteration spaces",
    "description": "The `sparse_tensor.coiterate` operation represents a loop (nest) over\n      a set of iteration spaces. The operation can have multiple regions,\n      with each of them defining a case to compute a result at the current iterations.\n      The case condition is defined solely based on the pattern of specified iterators.\n      For example:\n      ```mlir\n      %ret = sparse_tensor.coiterate (%sp1, %sp2) at(%coord) iter_args(%arg = %init)\n           : (!sparse_tensor.iter_space<#CSR, lvls = 0>,\n              !sparse_tensor.iter_space<#COO, lvls = 0>)\n           -> index\n      case %it1, _ {\n        // %coord is specifed in space %sp1 but *NOT* specified in space %sp2.\n      }\n      case %it1, %it2 {\n        // %coord is specifed in *BOTH* spaces %sp1 and %sp2.\n      }\n      ```\n\n      `sparse_tensor.coiterate` can also operate on loop-carried variables.\n      It returns the final value for each loop-carried variable after loop termination.\n      The initial values of the variables are passed as additional SSA operands\n      to the iterator SSA value and used coordinate SSA values.\n      Each operation region has variadic arguments for specified (used), one argument\n      for each loop-carried variable, representing the value of the variable\n      at the current iteration, followed by a list of arguments for iterators.\n      The body region must contain exactly one block that terminates with\n      `sparse_tensor.yield`.\n\n      The results of an `sparse_tensor.coiterate` hold the final values after\n      the last iteration. If the `sparse_tensor.coiterate` defines any values,\n      a yield must be explicitly present in every region defined in the operation.\n      The number and types of the `sparse_tensor.coiterate` results must match\n      the initial values in the iter_args binding and the yield operands.\n\n\n      A `sparse_tensor.coiterate` example that does elementwise addition between two\n      sparse vectors.\n\n\n      ```mlir\n      %ret = sparse_tensor.coiterate (%sp1, %sp2) at(%coord) iter_args(%arg = %init)\n           : (!sparse_tensor.iter_space<#CSR, lvls = 0>,\n              !sparse_tensor.iter_space<#CSR, lvls = 0>)\n           -> tensor<?xindex, #CSR>\n      case %it1, _ {\n         // v = v1 + 0 = v1\n         %v1 = sparse_tensor.extract_value %t1 at %it1 : index\n         %yield = sparse_tensor.insert %v1 into %arg[%coord]\n         sparse_tensor.yield %yield\n      }\n      case _, %it2 {\n         // v = v2 + 0 = v2\n         %v2 = sparse_tensor.extract_value %t2 at %it2 : index\n         %yield = sparse_tensor.insert %v1 into %arg[%coord]\n         sparse_tensor.yield %yield\n      }\n      case %it1, %it2 {\n         // v = v1 + v2\n         %v1 = sparse_tensor.extract_value %t1 at %it1 : index\n         %v2 = sparse_tensor.extract_value %t2 at %it2 : index\n         %v = arith.addi %v1, %v2 : index\n         %yield = sparse_tensor.insert %v into %arg[%coord]\n         sparse_tensor.yield %yield\n      }\n      ```",
    "inputs": [
      { "name": "iterSpaces", "type": "Variadic" },
      { "name": "initArgs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "crdUsedLvls", "type": "I64BitSetAttr" },
      { "name": "cases", "type": "I64BitSetArrayAttr" }
    ]
  },
  {
    "name": "sparse_tensor.compress",
    "summary": "Compressed an access pattern for insertion",
    "description": "Finishes a single access pattern expansion by moving inserted elements\n    into the sparse storage scheme of the given tensor with the given\n    level-coordinates.  The arity of `lvlCoords` is one less than the\n    level-rank of the tensor, with the coordinate of the innermost\n    level defined through the `added` array.  The `values` and `filled`\n    arrays are reset in a *sparse* fashion by only iterating over set\n    elements through an indirection using the `added` array, so that\n    the operations are kept proportional to the number of nonzeros.\n    See the `sparse_tensor.expand` operation for more details.\n\n    Note that this operation is \"impure\" in the sense that even though\n    the result is modeled through an SSA value, the insertion is eventually\n    done \"in place\", and referencing the old SSA value is undefined behavior.\n\n    Example:\n\n    ```mlir\n    %result = sparse_tensor.compress %values, %filled, %added, %count into %tensor[%i]\n      : memref<?xf64>, memref<?xi1>, memref<?xindex>, tensor<4x4xf64, #CSR>\n    ```",
    "inputs": [
      { "name": "values", "type": "AnyStridedMemRefOfRank" },
      { "name": "filled", "type": "StridedMemRefRankOf" },
      { "name": "added", "type": "StridedMemRefRankOf" },
      { "name": "count", "type": "Index" },
      { "name": "tensor", "type": "AnySparseTensor" },
      { "name": "lvlCoords", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnySparseTensor" }
    ],
    "assemblyFormat": "$values `,` $filled `,` $added `,` $count `into` $tensor `[` $lvlCoords `]` attr-dict `:` type($values) `,` type($filled) `,` type($added) `,` type($tensor)"
  },
  {
    "name": "sparse_tensor.concatenate",
    "summary": "Concatenates a list of tensors into a single tensor.",
    "description": "Concatenates a list input tensors and the output tensor with the same\n     dimension-rank.  The concatenation happens on the specified `dimension`\n     (0 <= dimension < dimRank).  The resulting `dimension` size is the\n     sum of all the input sizes for that dimension, while all the other\n     dimensions should have the same size in the input and output tensors.\n\n     Only statically-sized input tensors are accepted, while the output tensor\n     can be dynamically-sized.\n\n     Example:\n\n     ```mlir\n     %0 = sparse_tensor.concatenate %1, %2 { dimension = 0 : index }\n       : tensor<64x64xf64, #CSR>, tensor<64x64xf64, #CSR> to tensor<128x64xf64, #CSR>\n     ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "dimension", "type": "DimensionAttr" }
    ],
    "assemblyFormat": "$inputs attr-dict `:` type($inputs) `to` type($result)"
  },
  {
    "name": "sparse_tensor.convert",
    "summary": "Converts between different tensor types",
    "description": "Converts one sparse or dense tensor type to another tensor type. The rank\n    of the source and destination types must match exactly, and the dimension\n    sizes must either match exactly or relax from a static to a dynamic size.\n    The sparse encoding of the two types can obviously be completely different.\n    The name `convert` was preferred over `cast`, since the operation may incur\n    a non-trivial cost.\n\n    When converting between two different sparse tensor types, only explicitly\n    stored values are moved from one underlying sparse storage format to\n    the other. When converting from an unannotated dense tensor type to a\n    sparse tensor type, an explicit test for nonzero values is used. When\n    converting to an unannotated dense tensor type, implicit zeroes in the\n    sparse storage format are made explicit. Note that the conversions can have\n    non-trivial costs associated with them, since they may involve elaborate\n    data structure transformations. Also, conversions from sparse tensor types\n    into dense tensor types may be infeasible in terms of storage requirements.\n\n    Trivial dense-to-dense convert will be removed by canonicalization while\n    trivial sparse-to-sparse convert will be removed by the sparse codegen. This\n    is because we use trivial sparse-to-sparse convert to tell bufferization\n    that the sparse codegen will expand the tensor buffer into sparse tensor\n    storage.\n\n    Examples:\n\n    ```mlir\n    %0 = sparse_tensor.convert %a : tensor<32x32xf32> to tensor<32x32xf32, #CSR>\n    %1 = sparse_tensor.convert %a : tensor<32x32xf32> to tensor<?x?xf32, #CSR>\n    %2 = sparse_tensor.convert %b : tensor<8x8xi32, #CSC> to tensor<8x8xi32, #CSR>\n    %3 = sparse_tensor.convert %c : tensor<4x8xf64, #CSR> to tensor<4x?xf64, #CSC>\n\n    // The following conversion is not allowed (since it would require a\n    // runtime assertion that the source's dimension size is actually 100).\n    %4 = sparse_tensor.convert %d : tensor<?xf64> to tensor<100xf64, #SV>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "dest", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($dest)"
  },
  {
    "name": "sparse_tensor.coordinates",
    "summary": "Extracts the `level`-th coordinates array of the `tensor`",
    "description": "Returns the coordinates array of the tensor's storage at the given\n    level.  This is similar to the `bufferization.to_buffer` operation\n    in the sense that it provides a bridge between a tensor world view\n    and a bufferized world view.  Unlike the `bufferization.to_buffer`\n    operation, however, this sparse operation actually lowers into code\n    that extracts the coordinates array from the sparse storage itself\n    (either by calling a support library or through direct code).\n\n    Writing into the result of this operation is undefined behavior.\n\n    Example:\n\n    ```mlir\n    %1 = sparse_tensor.coordinates %0 { level = 1 : index }\n       : tensor<64x64xf64, #CSR> to memref<?xindex>\n    ```",
    "inputs": [
      { "name": "tensor", "type": "AnySparseTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyNon0RankedMemRef" }
    ],
    "attributes": [
      { "name": "level", "type": "LevelAttr" }
    ],
    "assemblyFormat": "$tensor attr-dict `:` type($tensor) `to` type($result)"
  },
  {
    "name": "sparse_tensor.coordinates_buffer",
    "summary": "Extracts the linear coordinates array from a tensor",
    "description": "Returns the linear coordinates array for a sparse tensor with\n    a trailing COO region with at least two levels.  It is an error\n    if the tensor doesn't contain such a COO region.  This is similar\n    to the `bufferization.to_buffer` operation in the sense that it\n    provides a bridge between a tensor world view and a bufferized\n    world view.  Unlike the `bufferization.to_buffer` operation,\n    however, this operation actually lowers into code that extracts\n    the linear coordinates array from the sparse storage scheme that\n    stores the coordinates for the COO region as an array of structures.\n    For example, a 2D COO sparse tensor with two non-zero elements at\n    coordinates (1, 3) and (4, 6) are stored in a linear buffer as\n    (1, 4, 3, 6) instead of two buffer as (1, 4) and (3, 6).\n\n    Writing into the result of this operation is undefined behavior.\n\n    Example:\n\n    ```mlir\n    %1 = sparse_tensor.coordinates_buffer %0\n       : tensor<64x64xf64, #COO> to memref<?xindex>\n    ```",
    "inputs": [
      { "name": "tensor", "type": "AnySparseTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyNon0RankedMemRef" }
    ],
    "assemblyFormat": "$tensor attr-dict `:` type($tensor) `to` type($result)"
  },
  {
    "name": "sparse_tensor.crd_translate",
    "summary": "Performs coordinate translation between level and dimension coordinate space.",
    "description": "Performs coordinate translation between level and dimension coordinate space according\n    to the affine maps defined by $encoder.\n\n    Example:\n\n    ```mlir\n    %l0, %l1, %l2, %l3 = sparse_tensor.crd_translate dim_to_lvl [%d0, %d1] as #BSR\n                       : index, index, index, index\n    ```",
    "inputs": [
      { "name": "in_crds", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "out_crds", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "direction", "type": "SparseTensorCrdTransDirectionAttr" },
      { "name": "encoder", "type": "SparseTensorEncodingAttr" }
    ],
    "assemblyFormat": "$direction `[` $in_crds `]` `as` $encoder attr-dict `:` type($out_crds)"
  },
  {
    "name": "sparse_tensor.disassemble",
    "summary": "Copies the levels and values of the given sparse tensor",
    "description": "The disassemble operation is the inverse of `sparse_tensor::assemble`.\n    It copies the per-level position and coordinate arrays together with\n    the values array of the given sparse tensor into the user-supplied buffers\n    along with the actual length of the memory used in each returned buffer.\n\n    This operation can be used for returning a disassembled MLIR sparse tensor;\n    e.g., copying the sparse tensor contents into pre-allocated numpy arrays\n    back to Python. It is the user's responsibility to allocate large enough\n    buffers of the appropriate types to hold the sparse tensor contents.\n    The sparsifier simply copies all fields of the sparse tensor into the\n    user-supplied buffers without any sanity test to verify data integrity.\n\n    Example:\n\n    ```mlir\n    // input COO format |1.1, 0.0, 0.0, 0.0|\n    //    of 3x4 matrix |0.0, 0.0, 2.2, 3.3|\n    //                  |0.0, 0.0, 0.0, 0.0|\n    %p, %c, %v, %p_len, %c_len, %v_len =\n      sparse_tensor.disassemble %s : tensor<3x4xf64, #COO>\n         out_lvls(%op, %oi : tensor<2xindex>, tensor<3x2xindex>)\n         out_vals(%od : tensor<3xf64>) ->\n           (tensor<2xindex>, tensor<3x2xindex>), tensor<3xf64>, (index, index), index\n    // %p = arith.constant dense<[ 0,              3 ]> : tensor<2xindex>\n    // %c = arith.constant dense<[[0,0], [1,2], [1,3]]> : tensor<3x2xindex>\n    // %v = arith.constant dense<[ 1.1,   2.2,   3.3 ]> : tensor<3xf64>\n    // %p_len = 2\n    // %c_len = 6 (3x2)\n    // %v_len = 3\n    ```",
    "inputs": [
      { "name": "tensor", "type": "AnySparseTensor" },
      { "name": "out_levels", "type": "Variadic" },
      { "name": "out_values", "type": "RankedTensorOf" }
    ],
    "outputs": [
      { "name": "ret_levels", "type": "Variadic" },
      { "name": "ret_values", "type": "RankedTensorOf" },
      { "name": "lvl_lens", "type": "Variadic" },
      { "name": "val_len", "type": "AnyIndexingScalarLike" }
    ],
    "assemblyFormat": "$tensor attr-dict `:` type($tensor)`out_lvls` `(` $out_levels `:` type($out_levels) `)` `out_vals` `(` $out_values `:` type($out_values) `)` `->``(` type($ret_levels) `)` `,` type($ret_values) `,` `(` type($lvl_lens)   `)` `,` type($val_len)"
  },
  {
    "name": "sparse_tensor.expand",
    "summary": "Expands an access pattern for insertion",
    "description": "Performs an access pattern expansion for the innermost levels of the\n    given tensor. This operation is useful to implement kernels in which a\n    sparse tensor appears as output. This technique is known under several\n    different names and using several alternative implementations,\n    for example, phase counter [Gustavson72], expanded or switch array\n    [Pissanetzky84], in phase scan [Duff90], access pattern expansion [Bik96],\n    and workspaces [Kjolstad19].\n\n    The `values` and `filled` arrays must have lengths equal to the\n    level-size of the innermost level (i.e., as if the innermost level\n    were *dense*).  The `added` array and `count` are used to store new\n    level-coordinates when a false value is encountered in the `filled`\n    array.  All arrays should be allocated before the loop (possibly even\n    shared between loops in a future optimization) so that their *dense*\n    initialization can be amortized over many iterations.  Setting and\n    resetting the dense arrays in the loop nest itself is kept *sparse*\n    by only iterating over set elements through an indirection using\n    the added array, so that the operations are kept proportional to\n    the number of nonzeros.\n\n    Note that this operation is \"impure\" in the sense that even though the\n    results are modeled through SSA values, the operation relies on a proper\n    side-effecting context that sets and resets the expanded arrays.\n\n    Example:\n\n    ```mlir\n    %values, %filled, %added, %count = sparse_tensor.expand %tensor\n      : tensor<4x4xf64, #CSR> to memref<?xf64>, memref<?xi1>, memref<?xindex>\n    ```",
    "inputs": [
      { "name": "tensor", "type": "AnySparseTensor" }
    ],
    "outputs": [
      { "name": "values", "type": "AnyStridedMemRefOfRank" },
      { "name": "filled", "type": "StridedMemRefRankOf" },
      { "name": "added", "type": "StridedMemRefRankOf" },
      { "name": "count", "type": "Index" }
    ],
    "assemblyFormat": "$tensor attr-dict `:` type($tensor) `to` type($values) `,` type($filled) `,` type($added)"
  },
  {
    "name": "sparse_tensor.extract_iteration_space",
    "summary": "Extracts an iteration space from a sparse tensor between certain levels",
    "description": "Extracts a `!sparse_tensor.iter_space` from a sparse tensor between\n      certain (consecutive) levels. For sparse levels, it is usually done by\n      loading a postion range from the underlying sparse tensor storage.\n      E.g., for a compressed level, the iteration space is extracted by\n      [pos[i], pos[i+1]) supposing the the parent iterator points at `i`.\n\n      `tensor`: the input sparse tensor that defines the iteration space.\n      `parentIter`: the iterator for the previous level, at which the iteration space\n      at the current levels will be extracted.\n      `loLvl`, `hiLvl`: the level range between [loLvl, hiLvl) in the input tensor that\n      the returned iteration space covers. `hiLvl - loLvl` defines the dimension of the\n      iteration space.\n\n      The type of returned the value is must be\n      `!sparse_tensor.iter_space<#INPUT_ENCODING, lvls = $loLvl to $hiLvl>`.\n      The returned iteration space can then be iterated over by\n      `sparse_tensor.iterate` operations to visit every stored element\n      (usually nonzeros) in the input sparse tensor.\n\n      Example:\n      ```mlir\n      // Extracts a 1-D iteration space from a COO tensor at level 1.\n      %space = sparse_tensor.iteration.extract_space %sp at %it1 lvls = 1\n        : tensor<4x8xf32, #COO>, !sparse_tensor.iterator<#COO, lvls = 0>\n       ->!sparse_tensor.iter_space<#COO, lvls = 1>\n      ```",
    "inputs": [
      { "name": "tensor", "type": "AnySparseTensor" },
      { "name": "parentIter", "type": "Optional" }
    ],
    "outputs": [
      { "name": "extractedSpace", "type": "AnySparseIterSpace" }
    ],
    "attributes": [
      { "name": "loLvl", "type": "LevelAttr" },
      { "name": "hiLvl", "type": "LevelAttr" }
    ],
    "assemblyFormat": "$tensor (`at` $parentIter^)? `lvls` `=` custom<LevelRange>($loLvl, $hiLvl)  attr-dict `:` type($tensor) (`,` type($parentIter)^)? `->` qualified(type($extractedSpace))"
  },
  {
    "name": "sparse_tensor.extract_value",
    "summary": "Extracts a value from a sparse tensor using an iterator.",
    "description": "The `sparse_tensor.extract_value` operation extracts the value\n      pointed to by a sparse iterator from a sparse tensor.\n\n      Example:\n\n      ```mlir\n      %val = sparse_tensor.extract_value %sp at %it\n           : tensor<?x?xf32, #CSR>, !sparse_tensor.iterator<#CSR, lvl = 1>\n      ```",
    "inputs": [
      { "name": "tensor", "type": "AnySparseTensor" },
      { "name": "iterator", "type": "AnySparseIterator" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "$tensor `at` $iterator attr-dict `:` type($tensor)`,` qualified(type($iterator))"
  },
  {
    "name": "sparse_tensor.foreach",
    "summary": "Iterates over elements in a tensor",
    "description": "Iterates over stored elements in a tensor (which are typically, but not always,\n     non-zero for sparse tensors) and executes the block.\n\n     `tensor`: the input tensor to iterate over.\n     `initArgs`: the initial loop argument to carry and update during each iteration.\n     `order`: an optional permutation affine map that specifies the order in which\n     the dimensions are visited (e.g., row first or column first). This is only\n     applicable when the input tensor is a non-annotated dense tensor.\n\n     For an input tensor with dim-rank `n`, the block must take `n + 1`\n     arguments (plus additional loop-carried variables as described below).\n     The first `n` arguments provide the dimension-coordinates of the element\n     being visited, and must all have `index` type.  The `(n+1)`-th argument\n     provides the element's value, and must have the tensor's element type.\n\n     `sparse_tensor.foreach` can also operate on loop-carried variables and returns\n     the final values after loop termination. The initial values of the variables are\n     passed as additional SSA operands to the \"sparse_tensor.foreach\" following the n + 1\n     SSA values mentioned above (n coordinates and 1 value).\n\n     The region must terminate with a \"sparse_tensor.yield\" that passes the current\n     values of all loop-carried variables to the next iteration, or to the\n     result, if at the last iteration. The number and static types of loop-carried\n     variables may not change with iterations.\n\n     For example:\n     ```mlir\n     %c0 = arith.constant 0 : i32\n     %ret = sparse_tensor.foreach in %0 init(%c0): tensor<?x?xi32, #DCSR>, i32 -> i32 do {\n      ^bb0(%arg1: index, %arg2: index, %arg3: i32, %iter: i32):\n        %sum = arith.add %iter, %arg3\n        sparse_tensor.yield %sum\n     }\n     ```\n\n     It is important to note that the generated loop iterates over\n     elements in their storage order.  However, regardless of the\n     storage scheme used by the tensor, the block is always given\n     the dimension-coordinates.\n\n     For example:\n     ```mlir\n     #COL_MAJOR = #sparse_tensor.encoding<{\n       map = (d0, d1) -> (d1 : compressed, d0 : compressed)\n     }>\n\n     // foreach on a column-major sparse tensor\n     sparse_tensor.foreach in %0 : tensor<2x3xf64, #COL_MAJOR> do {\n      ^bb0(%row: index, %col: index, %arg3: f64):\n         // [%row, %col] -> [0, 0], [1, 0], [2, 0], [0, 1], [1, 1], [2, 1]\n     }\n\n     #ROW_MAJOR = #sparse_tensor.encoding<{\n       map = (d0, d1) -> (d0 : compressed, d1 : compressed)\n     }>\n\n     // foreach on a row-major sparse tensor\n     sparse_tensor.foreach in %0 : tensor<2x3xf64, #ROW_MAJOR> do {\n      ^bb0(%row: index, %col: index, %arg3: f64):\n         // [%row, %col] -> [0, 0], [0, 1], [1, 0], [1, 1], [2, 0], [2, 1]\n     }\n\n     // foreach on a row-major dense tensor but visit column first\n     sparse_tensor.foreach in %0 {order=affine_map<(i,j)->(j,i)>}: tensor<2x3xf64> do {\n      ^bb0(%row: index, %col: index, %arg3: f64):\n         // [%row, %col] -> [0, 0], [1, 0], [2, 0], [0, 1], [1, 1], [2, 1]\n     }\n\n     ```",
    "inputs": [
      { "name": "tensor", "type": "AnyRankedTensor" },
      { "name": "initArgs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "order", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`in` $tensor (`init``(`$initArgs^`)`)? attr-dict    `:` type($tensor) (`,` type($initArgs)^)?  (`->` type($results)^)?  `do` $region"
  },
  {
    "name": "sparse_tensor.has_runtime_library",
    "summary": "Indicates whether running in runtime/codegen mode",
    "description": "Returns a boolean value that indicates whether the sparsifier runs in\n    runtime library mode or not. For testing only! This operation is useful\n    for writing test cases that require different code depending on\n    runtime/codegen mode.\n\n    Example:\n\n    ```mlir\n    %has_runtime = sparse_tensor.has_runtime_library\n    scf.if %has_runtime {\n      ...\n    }\n    ```",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "sparse_tensor.iterate",
    "summary": "Iterates over a sparse iteration space",
    "description": "The `sparse_tensor.iterate` operation represents a loop (nest) over\n      the provided iteration space extracted from a specific sparse tensor.\n      The operation defines an SSA value for a sparse iterator that points\n      to the current stored element in the sparse tensor and SSA values\n      for coordinates of the stored element. The coordinates are always\n      converted to `index` type despite of the underlying sparse tensor\n      storage. When coordinates are not used, the SSA values can be skipped\n      by `_` symbols, which usually leads to simpler generated code after\n      sparsification. For example:\n\n      ```mlir\n      // The coordinate for level 0 is not used when iterating over a 2-D\n      // iteration space.\n      %sparse_tensor.iterate %iterator in %space at(_, %crd_1)\n        : !sparse_tensor.iter_space<#CSR, lvls = 0 to 2>\n      ```\n\n      `sparse_tensor.iterate` can also operate on loop-carried variables.\n      It returns the final values after loop termination.\n      The initial values of the variables are passed as additional SSA operands\n      to the iterator SSA value and used coordinate SSA values mentioned\n      above. The operation region has an argument for the iterator, variadic\n      arguments for specified (used) coordiates and followed by one argument\n      for each loop-carried variable, representing the value of the variable\n      at the current iteration.\n      The body region must contain exactly one block that terminates with\n      `sparse_tensor.yield`.\n\n      The results of an `sparse_tensor.iterate` hold the final values after\n      the last iteration. If the `sparse_tensor.iterate` defines any values,\n      a yield must be explicitly present.\n      The number and types of the `sparse_tensor.iterate` results must match\n      the initial values in the iter_args binding and the yield operands.\n\n\n      A nested `sparse_tensor.iterate` example that prints all the coordinates\n      stored in the sparse input:\n\n      ```mlir\n      func.func @nested_iterate(%sp : tensor<4x8xf32, #COO>) {\n        // Iterates over the first level of %sp\n        %l1 = sparse_tensor.extract_iteration_space %sp lvls = 0\n            : tensor<4x8xf32, #COO> -> !sparse_tensor.iter_space<#COO, lvls = 0 to 1>\n        %r1 = sparse_tensor.iterate %it1 in %l1 at (%coord0)\n            : !sparse_tensor.iter_space<#COO, lvls = 0 to 1>  {\n          // Iterates over the second level of %sp\n          %l2 = sparse_tensor.extract_iteration_space %sp at %it1 lvls = 1\n              : tensor<4x8xf32, #COO>, !sparse_tensor.iterator<#COO, lvls = 0 to 1>\n             -> !sparse_tensor.iter_space<#COO, lvls = 1 to 2>\n          %r2 = sparse_tensor.iterate %it2 in %l2 at (coord1)\n              : !sparse_tensor.iter_space<#COO, lvls = 1 to 2>  {\n             vector.print %coord0 : index\n             vector.print %coord1 : index\n          }\n        }\n      }\n\n      ```",
    "inputs": [
      { "name": "iterSpace", "type": "AnySparseIterSpace" },
      { "name": "initArgs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "crdUsedLvls", "type": "I64BitSetAttr" }
    ]
  },
  {
    "name": "sparse_tensor.load",
    "summary": "Rematerializes tensor from underlying sparse storage format",
    "description": "Rematerializes a tensor from the underlying sparse storage format of the\n    given tensor. This is similar to the `bufferization.to_tensor` operation\n    in the sense that it provides a bridge between a bufferized world view\n    and a tensor world view. Unlike the `bufferization.to_tensor` operation,\n    however, this sparse operation is used only temporarily to maintain a\n    correctly typed intermediate representation during progressive\n    bufferization.\n\n    The `hasInserts` attribute denote whether insertions to the underlying\n    sparse storage format may have occurred, in which case the underlying\n    sparse storage format needs to be finalized. Otherwise, the operation\n    simply folds away.\n\n    Note that this operation is \"impure\" in the sense that even though\n    the result is modeled through an SSA value, the operation relies on\n    a proper context of materializing and inserting the tensor value.\n\n    Examples:\n\n    ```mlir\n    %result = sparse_tensor.load %tensor : tensor<8xf64, #SV>\n\n    %1 = sparse_tensor.load %0 hasInserts : tensor<16x32xf32, #CSR>\n    ```",
    "inputs": [
      { "name": "tensor", "type": "AnySparseTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "hasInserts", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$tensor (`hasInserts` $hasInserts^)? attr-dict `:` type($tensor)"
  },
  {
    "name": "sparse_tensor.lvl",
    "summary": "level index operation",
    "description": "The `sparse_tensor.lvl` behaves similar to `tensor.dim` operation.\n    It takes a sparse tensor and a level operand of type `index` and returns\n    the size of the requested level of the given sparse tensor.\n    If the sparse tensor has an identity dimension to level mapping, it returns\n    the same result as `tensor.dim`.\n    If the level index is out of bounds, the behavior is undefined.\n\n    Example:\n\n    ```mlir\n    #BSR = #sparse_tensor.encoding<{\n      map = ( i, j ) ->\n        ( i floordiv 2 : dense,\n          j floordiv 3 : compressed,\n          i mod 2      : dense,\n          j mod 3      : dense\n        )\n    }>\n\n    // Always returns 2 (4 floordiv 2), can be constant folded:\n    %c0 = arith.constant 0 : index\n    %x = sparse_tensor.lvl %A, %c0 : tensor<4x?xf32, #BSR>\n\n    // Return the dynamic dimension of %A computed by %j mod 3.\n    %c1 = arith.constant 1 : index\n    %y = sparse_tensor.lvl %A, %c1 : tensor<4x?xf32, #BSR>\n\n    // Always return 3 (since j mod 3 < 3), can be constant fold\n    %c3 = arith.constant 3 : index\n    %y = sparse_tensor.lvl %A, %c3 : tensor<4x?xf32, #BSR>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnySparseTensor" },
      { "name": "index", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "attr-dict $source `,` $index `:` type($source)"
  },
  {
    "name": "sparse_tensor.new",
    "summary": "Materializes a new sparse tensor from given source",
    "description": "Materializes a sparse tensor with contents taken from an opaque pointer\n    provided by `source`. For targets that have access to a file system,\n    for example, this pointer may be a filename (or file) of a sparse\n    tensor in a particular external storage format. The form of the operation\n    is kept deliberately very general to allow for alternative implementations\n    in the future, such as pointers to buffers or runnable initialization\n    code. The operation is provided as an anchor that materializes a properly\n    typed sparse tensor with inital contents into a computation.\n\n    Reading in a symmetric matrix will result in just the lower/upper triangular\n    part of the matrix (so that only relevant information is stored). Proper\n    symmetry support for operating on symmetric matrices is still TBD.\n\n    Example:\n\n    ```mlir\n    sparse_tensor.new %source : !Source to tensor<1024x1024xf64, #CSR>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnySparseTensor" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($result)"
  },
  {
    "name": "sparse_tensor.number_of_entries",
    "summary": "Returns the number of entries that are stored in the tensor.",
    "description": "Returns the number of entries that are stored in the given sparse tensor.\n    Note that this is typically the number of nonzero elements in the tensor,\n    but since explicit zeros may appear in the storage formats, the more\n    accurate nomenclature is used.\n\n    Example:\n\n    ```mlir\n    %noe = sparse_tensor.number_of_entries %tensor : tensor<64x64xf64, #CSR>\n    ```",
    "inputs": [
      { "name": "tensor", "type": "AnySparseTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$tensor attr-dict `:` type($tensor)"
  },
  {
    "name": "sparse_tensor.out",
    "summary": "Outputs a sparse tensor to the given destination",
    "description": "Outputs the contents of a sparse tensor to the destination defined by an\n    opaque pointer provided by `dest`. For targets that have access to a file\n    system, for example, this pointer may specify a filename (or file) for output.\n    The form of the operation is kept deliberately very general to allow for\n    alternative implementations in the future, such as sending the contents to\n    a buffer defined by a pointer.\n\n    Note that this operation is \"impure\" in the sense that its behavior\n    is solely defined by side-effects and not SSA values.\n\n    Example:\n\n    ```mlir\n    sparse_tensor.out %t, %dest : tensor<1024x1024xf64, #CSR>, !Dest\n    ```",
    "inputs": [
      { "name": "tensor", "type": "AnySparseTensor" },
      { "name": "dest", "type": "AnyType" }
    ],
    "assemblyFormat": "$tensor `,` $dest attr-dict `:` type($tensor) `,` type($dest)"
  },
  {
    "name": "sparse_tensor.positions",
    "summary": "Extracts the `level`-th positions array of the `tensor`",
    "description": "Returns the positions array of the tensor's storage at the given\n    level.  This is similar to the `bufferization.to_buffer` operation\n    in the sense that it provides a bridge between a tensor world view\n    and a bufferized world view.  Unlike the `bufferization.to_buffer`\n    operation, however, this sparse operation actually lowers into code\n    that extracts the positions array from the sparse storage itself\n    (either by calling a support library or through direct code).\n\n    Writing into the result of this operation is undefined behavior.\n\n    Example:\n\n    ```mlir\n    %1 = sparse_tensor.positions %0 { level = 1 : index }\n       : tensor<64x64xf64, #CSR> to memref<?xindex>\n    ```",
    "inputs": [
      { "name": "tensor", "type": "AnySparseTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyNon0RankedMemRef" }
    ],
    "attributes": [
      { "name": "level", "type": "LevelAttr" }
    ],
    "assemblyFormat": "$tensor attr-dict `:` type($tensor) `to` type($result)"
  },
  {
    "name": "sparse_tensor.print",
    "summary": "Prints a sparse tensor (for testing and debugging)",
    "description": "Prints the individual components of a sparse tensors (the positions,\n    coordinates, and values components) to stdout for testing and debugging\n    purposes. This operation lowers to just a few primitives in a light-weight\n    runtime support to simplify supporting this operation on new platforms.\n\n    Example:\n\n    ```mlir\n    sparse_tensor.print %tensor : tensor<1024x1024xf64, #CSR>\n    ```",
    "inputs": [
      { "name": "tensor", "type": "AnySparseTensor" }
    ],
    "assemblyFormat": "$tensor attr-dict `:` type($tensor)"
  },
  {
    "name": "sparse_tensor.push_back",
    "summary": "Pushes a value to the back of a given buffer",
    "description": "Pushes `value` to the end of the given sparse tensor storage buffer\n    `inBuffer` as indicated by the value of `curSize` and returns the\n    new size of the buffer in `newSize` (`newSize = curSize + n`).\n    The capacity of the buffer is recorded in the memref type of `inBuffer`.\n    If the current buffer is full, then `inBuffer.realloc` is called before\n    pushing the data to the buffer. This is similar to std::vector push_back.\n\n    The optional input `n` specifies the number of times to repeately push\n    the value to the back of the tensor. When `n` is a compile-time constant,\n    its value can't be less than 1. If `n` is a runtime value that is less\n    than 1, the behavior is undefined. Although using input `n` is semantically\n    equivalent to calling push_back n times, it gives compiler more chances to\n    to optimize the memory reallocation and the filling of the memory with the\n    same value.\n\n    The `inbounds` attribute tells the compiler that the insertion won't go\n    beyond the current storage buffer. This allows the compiler to not generate\n    the code for capacity check and reallocation. The typical usage will be for\n    \"dynamic\" sparse tensors for which a capacity can be set beforehand.\n\n    Note that this operation is \"impure\" in the sense that even though\n    the result is modeled through an SSA value, referencing the memref\n    through the old SSA value after this operation is undefined behavior.\n\n    Example:\n\n    ```mlir\n    %buf, %newSize = sparse_tensor.push_back %curSize, %buffer, %val\n       : index, memref<?xf64>, f64\n    ```\n\n    ```mlir\n    %buf, %newSize = sparse_tensor.push_back inbounds %curSize, %buffer, %val\n       : xindex, memref<?xf64>, f64\n    ```\n\n    ```mlir\n    %buf, %newSize = sparse_tensor.push_back inbounds %curSize, %buffer, %val, %n\n       : xindex, memref<?xf64>, f64\n    ```",
    "inputs": [
      { "name": "curSize", "type": "Index" },
      { "name": "inBuffer", "type": "StridedMemRefRankOf" },
      { "name": "value", "type": "AnyType" },
      { "name": "n", "type": "Optional" }
    ],
    "outputs": [
      { "name": "outBuffer", "type": "StridedMemRefRankOf" },
      { "name": "newSize", "type": "Index" }
    ],
    "attributes": [
      { "name": "inbounds", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`inbounds` $inbounds^)? $curSize `,` $inBuffer `,` $value (`,` $n^ )?  attr-dict `:` type($curSize) `,` type($inBuffer) `,` type($value) (`,` type($n)^ )?"
  },
  {
    "name": "sparse_tensor.reduce",
    "summary": "Custom reduction operation utilized within linalg.generic",
    "description": "Defines a computation with a `linalg.generic` operation that takes two\n      operands and an identity value and reduces all stored values down to a\n      single result based on the computation in the region.\n\n      The region must contain exactly one block taking two arguments. The block\n      must end with a sparse_tensor.yield and the output must match the input\n      argument types.\n\n      Note that this operation is only required for custom reductions beyond\n      the standard reduction operations (add, sub, or, xor) that can be\n      sparsified by merely reducing the stored values. More elaborate reduction\n      operations (mul, and, min, max, etc.) would need to account for implicit\n      zeros as well. They can still be handled using this custom reduction\n      operation. The `linalg.generic` `iterator_types` defines which indices\n      are being reduced. When the associated operands are used in an operation,\n      a reduction will occur. The use of this explicit `reduce` operation\n      is not required in most cases.\n\n      Example of Matrix->Vector reduction using max(product(x_i), 100):\n\n      ```mlir\n      %cf1 = arith.constant 1.0 : f64\n      %cf100 = arith.constant 100.0 : f64\n      %C = tensor.empty(...)\n      %0 = linalg.generic #trait\n         ins(%A: tensor<?x?xf64, #SparseMatrix>)\n        outs(%C: tensor<?xf64, #SparseVector>) {\n        ^bb0(%a: f64, %c: f64) :\n          %result = sparse_tensor.reduce %c, %a, %cf1 : f64 {\n              ^bb0(%arg0: f64, %arg1: f64):\n                %0 = arith.mulf %arg0, %arg1 : f64\n                %cmp = arith.cmpf \"ogt\", %0, %cf100 : f64\n                %ret = arith.select %cmp, %cf100, %0 : f64\n                sparse_tensor.yield %ret : f64\n            }\n          linalg.yield %result : f64\n      } -> tensor<?xf64, #SparseVector>\n      ```",
    "inputs": [
      { "name": "x", "type": "AnyType" },
      { "name": "y", "type": "AnyType" },
      { "name": "identity", "type": "AnyType" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyType" }
    ],
    "assemblyFormat": "$x `,` $y `,` $identity attr-dict `:` type($output) $region"
  },
  {
    "name": "sparse_tensor.reinterpret_map",
    "summary": "Reinterprets the dimension/level maps of the source tensor",
    "description": "Reinterprets the dimension-to-level and level-to-dimension map specified in\n    `source` according to the type of `dest`.\n    `reinterpret_map` is a no-op and is introduced merely to resolve type conflicts.\n    It does not make any modification to the source tensor and source/dest tensors\n    are considered to be aliases.\n\n    `source` and `dest` tensors are \"reinterpretable\" if and only if they have\n    the exactly same storage at a low level.\n    That is, both `source` and `dest` has the same number of levels and level types,\n    and their shape is consistent before and after `reinterpret_map`.\n\n    Example:\n    ```mlir\n    #CSC = #sparse_tensor.encoding<{\n      map = (d0, d1) -> (d1: dense, d0: compressed)\n    }>\n    #CSR = #sparse_tensor.encoding<{\n      map = (d0, d1) -> (d0: dense, d1: compressed)\n    }>\n    %t1 = sparse_tensor.reinterpret_map %t0 : tensor<3x4xi32, #CSC> to tensor<4x3xi32, #CSR>\n\n    #BSR = #sparse_tensor.encoding<{\n      map = ( i, j ) -> ( i floordiv 2 : dense,\n                          j floordiv 3 : compressed,\n                          i mod 2      : dense,\n                          j mod 3      : dense\n      )\n    }>\n    #DSDD = #sparse_tensor.encoding<{\n      map = (i, j, k, l) -> (i: dense, j: compressed, k: dense, l: dense)\n    }>\n    %t1 = sparse_tensor.reinterpret_map %t0 : tensor<6x12xi32, #BSR> to tensor<3x4x2x3xi32, #DSDD>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnySparseTensor" }
    ],
    "outputs": [
      { "name": "dest", "type": "AnySparseTensor" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($dest)"
  },
  {
    "name": "sparse_tensor.reorder_coo",
    "summary": "Reorder the input COO such that it has the the same order as the output COO",
    "description": "Reorders the input COO to the same order as specified by the output format.\n    E.g., reorder an unordered COO into an ordered one.\n\n    The input and result COO tensor must have the same element type, position type and\n    coordinate type. At the moment, the operation also only supports ordering\n    input and result COO with the same dim2lvl map.\n\n    Example:\n\n    ```mlir\n    %res = sparse_tensor.reorder_coo quick_sort %coo : tensor<?x?xf64 : #Unordered_COO> to\n                                                       tensor<?x?xf64 : #Ordered_COO>\n\n    ```",
    "inputs": [
      { "name": "input_coo", "type": "AnySparseTensor" }
    ],
    "outputs": [
      { "name": "result_coo", "type": "AnySparseTensor" }
    ],
    "attributes": [
      { "name": "algorithm", "type": "SparseTensorSortKindAttr" }
    ],
    "assemblyFormat": "$algorithm $input_coo attr-dict`:` type($input_coo) `to` type($result_coo)"
  },
  {
    "name": "sparse_tensor.select",
    "summary": "Select operation utilized within linalg.generic",
    "description": "Defines an evaluation within a `linalg.generic` operation that takes a single\n      operand and decides whether or not to keep that operand in the output.\n\n      A single region must contain exactly one block taking one argument. The block\n      must end with a sparse_tensor.yield and the output type must be boolean.\n\n      Value threshold is an obvious usage of the select operation. However, by using\n      `linalg.index`, other useful selection can be achieved, such as selecting the\n      upper triangle of a matrix.\n\n      Example of selecting A >= 4.0:\n\n      ```mlir\n      %C = tensor.empty(...)\n      %0 = linalg.generic #trait\n         ins(%A: tensor<?xf64, #SparseVector>)\n        outs(%C: tensor<?xf64, #SparseVector>) {\n        ^bb0(%a: f64, %c: f64) :\n          %result = sparse_tensor.select %a : f64 {\n              ^bb0(%arg0: f64):\n                %cf4 = arith.constant 4.0 : f64\n                %keep = arith.cmpf \"uge\", %arg0, %cf4 : f64\n                sparse_tensor.yield %keep : i1\n            }\n          linalg.yield %result : f64\n      } -> tensor<?xf64, #SparseVector>\n      ```\n\n      Example of selecting lower triangle of a matrix:\n\n      ```mlir\n      %C = tensor.empty(...)\n      %1 = linalg.generic #trait\n         ins(%A: tensor<?x?xf64, #CSR>)\n        outs(%C: tensor<?x?xf64, #CSR>) {\n        ^bb0(%a: f64, %c: f64) :\n          %row = linalg.index 0 : index\n          %col = linalg.index 1 : index\n          %result = sparse_tensor.select %a : f64 {\n              ^bb0(%arg0: f64):\n                %keep = arith.cmpf \"olt\", %col, %row : f64\n                sparse_tensor.yield %keep : i1\n            }\n          linalg.yield %result : f64\n      } -> tensor<?x?xf64, #CSR>\n      ```",
    "inputs": [
      { "name": "x", "type": "AnyType" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyType" }
    ],
    "assemblyFormat": "$x attr-dict `:` type($x) $region"
  },
  {
    "name": "sparse_tensor.slice.offset",
    "summary": "Extracts the offset of the sparse tensor slice at the given dimension",
    "description": "Extracts the offset of the sparse tensor slice at the given dimension.\n\n    Currently, sparse tensor slices are still a work in progress, and only\n    works when runtime library is disabled (i.e., running the sparsifier\n    with `enable-runtime-library=false`).\n\n    Example:\n\n    ```mlir\n    %0 = tensor.extract_slice %s[%v1, %v2][64, 64][1, 1] : tensor<128x128xf64, #DCSR>\n                                                        to tensor<64x64xf64, #Slice>\n\n    %1 = sparse_tensor.slice.offset %0 at 0 : tensor<64x64xf64, #Slice>\n    %2 = sparse_tensor.slice.offset %0 at 1 : tensor<64x64xf64, #Slice>\n    // %1 = %v1\n    // %2 = %v2\n    ```",
    "inputs": [
      { "name": "slice", "type": "AnySparseTensorSlice" }
    ],
    "outputs": [
      { "name": "offset", "type": "Index" }
    ],
    "attributes": [
      { "name": "dim", "type": "IndexAttr" }
    ],
    "assemblyFormat": "$slice `at` $dim attr-dict `:` type($slice)"
  },
  {
    "name": "sparse_tensor.slice.stride",
    "summary": "Extracts the stride of the sparse tensor slice at the given dimension",
    "description": "Extracts the stride of the sparse tensor slice at the given dimension.\n\n    Currently, sparse tensor slices are still a work in progress, and only\n    works when runtime library is disabled (i.e., running the sparsifier\n    with `enable-runtime-library=false`).\n\n    Example:\n\n    ```mlir\n    %0 = tensor.extract_slice %s[%v1, %v2][64, 64][%s1, %s2] : tensor<128x128xf64, #DCSR>\n                                                            to tensor<64x64xf64, #Slice>\n\n    %1 = sparse_tensor.slice.stride %0 at 0 : tensor<64x64xf64, #Slice>\n    %2 = sparse_tensor.slice.stride %0 at 1 : tensor<64x64xf64, #Slice>\n    // %1 = %s1\n    // %2 = %s2\n\n    ```",
    "inputs": [
      { "name": "slice", "type": "AnySparseTensorSlice" }
    ],
    "outputs": [
      { "name": "stride", "type": "Index" }
    ],
    "attributes": [
      { "name": "dim", "type": "IndexAttr" }
    ],
    "assemblyFormat": "$slice `at` $dim attr-dict `:` type($slice)"
  },
  {
    "name": "sparse_tensor.sort",
    "summary": "Sorts the arrays in xs and ys lexicographically on the integral values found in the xs list",
    "description": "Sorts the `xs` values along with some `ys` values that are put in a single linear\n    buffer `xy`.  The affine map attribute `perm_map` specifies the permutation to be\n    applied on the `xs` before comparison, the rank of the permutation map\n    also specifies the number of `xs` values in `xy`.\n    The optional index attribute `ny` provides the number of `ys` values in `xy`.\n    When `ny` is not explicitly specified, its value is 0.\n    This instruction supports a more efficient way to store the COO definition\n    in sparse tensor type.\n\n    The buffer xy should have a dimension not less than n * (rank(perm_map) + ny) while the\n    buffers in `ys` should have a dimension not less than `n`. The behavior of\n    the operator is undefined if this condition is not met.\n\n    Example:\n\n    ```mlir\n    sparse_tensor.sort insertion_sort_stable %n, %x { perm_map = affine_map<(i,j) -> (j,i)> }\n      : memref<?xindex>\n    ```",
    "inputs": [
      { "name": "n", "type": "Index" },
      { "name": "xy", "type": "StridedMemRefRankOf" },
      { "name": "ys", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "perm_map", "type": "AffineMapAttr" },
      { "name": "ny", "type": "OptionalAttr" },
      { "name": "algorithm", "type": "SparseTensorSortKindAttr" }
    ],
    "assemblyFormat": "$algorithm $n`,`$xy (`jointly` $ys^)? attr-dict`:` type($xy) (`jointly` type($ys)^)?"
  },
  {
    "name": "sparse_tensor.storage_specifier.get",
    "description": "Returns the requested field of the given storage_specifier.\n\n    Example of querying the size of the coordinates array for level 0:\n\n    ```mlir\n    %0 = sparse_tensor.storage_specifier.get %arg0 crd_mem_sz at 0\n         : !sparse_tensor.storage_specifier<#COO>\n    ```",
    "inputs": [
      { "name": "specifier", "type": "SparseTensorStorageSpecifier" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "attributes": [
      { "name": "specifierKind", "type": "SparseTensorStorageSpecifierKindAttr" },
      { "name": "level", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$specifier $specifierKind (`at` $level^)? attr-dict`:` qualified(type($specifier))"
  },
  {
    "name": "sparse_tensor.storage_specifier.init",
    "description": "Returns an initial storage specifier value.  A storage specifier\n    value holds the level-sizes, position arrays, coordinate arrays,\n    and the value array.\n    If this is a specifier for slices, it also holds the extra strides/offsets\n    for each tensor dimension.\n\n    TODO: The sparse tensor slice support is currently in a unstable state, and\n    is subject to change in the future.\n\n    Example:\n\n    ```mlir\n    #CSR = #sparse_tensor.encoding<{\n      map = (i, j) -> (i : dense, j : compressed)\n    }>\n    #CSR_SLICE = #sparse_tensor.encoding<{\n      map = (d0 : #sparse_tensor<slice(1, 4, 1)>,\n             d1 : #sparse_tensor<slice(1, 4, 2)>) ->\n            (d0 : dense, d1 : compressed)\n    }>\n\n    %0 = sparse_tensor.storage_specifier.init :  !sparse_tensor.storage_specifier<#CSR>\n    %1 = sparse_tensor.storage_specifier.init with %src\n         : !sparse_tensor.storage_specifier<#CSR> to\n           !sparse_tensor.storage_specifier<#CSR_SLICE>\n    ```",
    "inputs": [
      { "name": "source", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "SparseTensorStorageSpecifier" }
    ],
    "assemblyFormat": "attr-dict (`with` $source^)? `:` (`from` qualified(type($source))^ `to`)? qualified(type($result))"
  },
  {
    "name": "sparse_tensor.storage_specifier.set",
    "description": "Set the field of the storage specifier to the given input value. Returns\n    the updated storage_specifier as a new SSA value.\n\n    Example of updating the sizes of the coordinates array for level 0:\n\n    ```mlir\n    %0 = sparse_tensor.storage_specifier.set %arg0 crd_mem_sz at 0 with %new_sz\n       : !sparse_tensor.storage_specifier<#COO>\n    ```",
    "inputs": [
      { "name": "specifier", "type": "SparseTensorStorageSpecifier" },
      { "name": "value", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "SparseTensorStorageSpecifier" }
    ],
    "attributes": [
      { "name": "specifierKind", "type": "SparseTensorStorageSpecifierKindAttr" },
      { "name": "level", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$specifier $specifierKind (`at` $level^)? `with` $value attr-dict `:` qualified(type($result))"
  },
  {
    "name": "sparse_tensor.unary",
    "summary": "Unary set operation utilized within linalg.generic",
    "description": "Defines a computation with a `linalg.generic` operation that takes a single\n      operand and executes one of two regions depending on whether the operand is\n      nonzero (i.e. stored explicitly in the sparse storage format).\n\n      Two regions are defined for the operation must appear in this order:\n      - present (elements present in the sparse tensor)\n      - absent (elements not present in the sparse tensor)\n\n      Each region contains a single block describing the computation and result.\n      A non-empty block must end with a sparse_tensor.yield and the return type\n      must match the type of `output`. The primary region's block has one\n      argument, while the missing region's block has zero arguments. The\n      absent region may only generate constants or values already computed\n      on entry of the `linalg.generic` operation.\n\n      A region may also be declared empty (i.e. `absent={}`), indicating that the\n      region does not contribute to the output.\n\n      Due to the possibility of empty regions, i.e. lack of a value for certain\n      cases, the result of this operation may only feed directly into the output\n      of the `linalg.generic` operation or into into a custom reduction\n      `sparse_tensor.reduce` operation that follows in the same region.\n\n      Example of A+1, restricted to existing elements:\n\n      ```mlir\n      %C = tensor.empty(...) : tensor<?xf64, #SparseVector>\n      %0 = linalg.generic #trait\n         ins(%A: tensor<?xf64, #SparseVector>)\n        outs(%C: tensor<?xf64, #SparseVector>) {\n        ^bb0(%a: f64, %c: f64) :\n          %result = sparse_tensor.unary %a : f64 to f64\n            present={\n            ^bb0(%arg0: f64):\n              %cf1 = arith.constant 1.0 : f64\n              %ret = arith.addf %arg0, %cf1 : f64\n              sparse_tensor.yield %ret : f64\n            }\n            absent={}\n          linalg.yield %result : f64\n      } -> tensor<?xf64, #SparseVector>\n      ```\n\n      Example returning +1 for existing values and -1 for missing values:\n\n      ```mlir\n      %p1 = arith.constant  1 : i32\n      %m1 = arith.constant -1 : i32\n      %C = tensor.empty(...) : tensor<?xi32, #SparseVector>\n      %1 = linalg.generic #trait\n         ins(%A: tensor<?xf64, #SparseVector>)\n        outs(%C: tensor<?xi32, #SparseVector>) {\n        ^bb0(%a: f64, %c: i32) :\n          %result = sparse_tensor.unary %a : f64 to i32\n            present={\n            ^bb0(%x: f64):\n              sparse_tensor.yield %p1 : i32\n            }\n            absent={\n              sparse_tensor.yield %m1 : i32\n            }\n          linalg.yield %result : i32\n      } -> tensor<?xi32, #SparseVector>\n      ```\n\n      Example showing a structural inversion (existing values become missing in\n      the output, while missing values are filled with 1):\n\n      ```mlir\n      %c1 = arith.constant 1 : i64\n      %C = tensor.empty(...) : tensor<?xi64, #SparseVector>\n      %2 = linalg.generic #trait\n         ins(%A: tensor<?xf64, #SparseVector>)\n        outs(%C: tensor<?xi64, #SparseVector>) {\n        ^bb0(%a: f64, %c: i64) :\n          %result = sparse_tensor.unary %a : f64 to i64\n            present={}\n            absent={\n              sparse_tensor.yield %c1 : i64\n            }\n          linalg.yield %result : i64\n      } -> tensor<?xi64, #SparseVector>\n      ```",
    "inputs": [
      { "name": "x", "type": "AnyType" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyType" }
    ],
    "assemblyFormat": "$x attr-dict `:` type($x) `to` type($output) `\\n`\n        `present` `=` $presentRegion `\\n`\n        `absent` `=` $absentRegion"
  },
  {
    "name": "sparse_tensor.values",
    "summary": "Extracts numerical values array from a tensor",
    "description": "Returns the values array of the sparse storage format for the given\n    sparse tensor, independent of the actual dimension. This is similar to\n    the `bufferization.to_buffer` operation in the sense that it provides a bridge\n    between a tensor world view and a bufferized world view. Unlike the\n    `bufferization.to_buffer` operation, however, this sparse operation actually\n    lowers into code that extracts the values array from the sparse storage\n    scheme (either by calling a support library or through direct code).\n\n    Writing into the result of this operation is undefined behavior.\n\n    Example:\n\n    ```mlir\n    %1 = sparse_tensor.values %0 : tensor<64x64xf64, #CSR> to memref<?xf64>\n    ```",
    "inputs": [
      { "name": "tensor", "type": "AnySparseTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyNon0RankedMemRef" }
    ],
    "assemblyFormat": "$tensor attr-dict `:` type($tensor) `to` type($result)"
  },
  {
    "name": "sparse_tensor.yield",
    "summary": "Yield from sparse_tensor set-like operations",
    "description": "Yields a value from within a `binary`, `unary`, `reduce`,\n      `select` or `foreach` block.\n\n      Example:\n\n      ```mlir\n      %0 = sparse_tensor.unary %a : i64 to i64 {\n        present={\n          ^bb0(%arg0: i64):\n            %cst = arith.constant 1 : i64\n            %ret = arith.addi %arg0, %cst : i64\n            sparse_tensor.yield %ret : i64\n        }\n      }\n      ```",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$results attr-dict `:` type($results)"
  },
  {
    "name": "spirv.AtomicAnd",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by the bitwise AND of Original Value and Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicAnd <Device> <None> %pointer, %value :\n                       !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicCompareExchange",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value from Value only if Original Value equals Comparator,\n    and\n\n    3) store the New Value back through Pointer'only if 'Original Value\n    equaled Comparator.\n\n    The instruction's result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n    Use Equal for the memory semantics of this instruction when Value and\n    Original Value compare equal.\n\n    Use Unequal for the memory semantics of this instruction when Value and\n    Original Value compare unequal. Unequal must not be set to Release or\n    Acquire and Release. In addition, Unequal cannot be set to a stronger\n    memory-order then Equal.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.  This type\n    must also match the type of Comparator.\n\n    Memory is a memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```\n    %0 = spirv.AtomicCompareExchange <Workgroup> <Acquire> <None>\n                                    %pointer, %value, %comparator\n                                    : !spirv.ptr<i32, WorkGroup>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" },
      { "name": "comparator", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "equal_semantics", "type": "SPIRV_MemorySemanticsAttr" },
      { "name": "unequal_semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $equal_semantics $unequal_semantics operands attr-dict `:`\n      type($pointer)"
  },
  {
    "name": "spirv.AtomicCompareExchangeWeak",
    "summary": "Deprecated (use OpAtomicCompareExchange).",
    "description": "Has the same semantics as OpAtomicCompareExchange.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicCompareExchangeWeak <Workgroup> <Acquire> <None>\n                                       %pointer, %value, %comparator\n                                       : !spirv.ptr<i32, WorkGroup>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" },
      { "name": "comparator", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "equal_semantics", "type": "SPIRV_MemorySemanticsAttr" },
      { "name": "unequal_semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $equal_semantics $unequal_semantics operands attr-dict `:`\n      type($pointer)"
  },
  {
    "name": "spirv.AtomicExchange",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value from copying Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction's result is the Original Value.\n\n    Result Type must be a scalar of integer type or floating-point type.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory is a memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicExchange <Workgroup> <Acquire> %pointer, %value,\n                            : !spirv.ptr<i32, WorkGroup>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Numerical" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Numerical" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicIAdd",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by integer addition of Original Value and Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicIAdd <Device> <None> %pointer, %value :\n                        !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicIDecrement",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value through integer subtraction of 1 from Original Value,\n    and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.  The type of the value\n    pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicIDecrement <Device> <None> %pointer :\n                              !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicIIncrement",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value through integer addition of 1 to Original Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.  The type of the value\n    pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicIncrement <Device> <None> %pointer :\n                             !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicISub",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by integer subtraction of Value from Original Value,\n    and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicISub <Device> <None> %pointer, %value :\n                        !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicOr",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by the bitwise OR of Original Value and Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicOr <Device> <None> %pointer, %value :\n                      !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicSMax",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by finding the largest signed integer of Original\n    Value and Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicSMax <Device> <None> %pointer, %value :\n                        !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicSMin",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by finding the smallest signed integer of Original\n    Value and Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicSMin <Device> <None> %pointer, %value :\n                        !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicUMax",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by finding the largest unsigned integer of Original\n    Value and Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicUMax <Device> <None> %pointer, %value :\n                        !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicUMin",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by finding the smallest unsigned integer of Original\n    Value and Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicUMin <Device> <None> %pointer, %value :\n                        !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.AtomicXor",
    "summary": "Perform the following steps atomically with respect to any other atomic\n    accesses within Scope to the same location:",
    "description": "1) load through Pointer to get an Original Value,\n\n    2) get a New Value by the bitwise exclusive OR of Original Value and\n    Value, and\n\n    3) store the New Value back through Pointer.\n\n    The instruction’s result is the Original Value.\n\n    Result Type must be an integer type scalar.\n\n     The type of Value must be the same as Result Type.  The type of the\n    value pointed to by Pointer must be the same as Result Type.\n\n    Memory must be a valid memory Scope.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.AtomicXor <Device> <None> %pointer, %value :\n                       !spirv.ptr<i32, StorageBuffer>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" },
      { "name": "value", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope $semantics operands attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.Bitcast",
    "summary": "Bit pattern-preserving type conversion.",
    "description": "Result Type must be an OpTypePointer, or a scalar or vector of\n    numerical-type.\n\n    Operand must have a type of OpTypePointer, or a scalar or vector of\n    numerical-type. It must be a different type than Result Type.\n\n    If either Result Type or Operand is a pointer, the other must be a\n    pointer (diverges from the SPIR-V spec).\n\n    If Result Type has a different number of components than Operand, the\n    total number of bits in Result Type must equal the total number of bits\n    in Operand. Let L be the type, either Result Type or Operand's type,\n    that has the larger number of components. Let S be the other type, with\n    the smaller number of components. The number of components in L must be\n    an integer multiple of the number of components in S. The first\n    component (that is, the only or lowest-numbered component) of S maps to\n    the first components of L, and so on,  up to the last component of S\n    mapping to the last components of L. Within this mapping, any single\n    component of S (mapping to multiple components of L) maps its lower-\n    ordered bits to the lower-numbered components of L.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.Bitcast %0 : f32 to i32\n    %1 = spirv.Bitcast %0 : vector<2xf32> to i64\n    %1 = spirv.Bitcast %0 : !spirv.ptr<f32, Function> to !spirv.ptr<i32, Function>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrPtr" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrPtr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.BitCount",
    "summary": "Count the number of set bits in an object.",
    "description": "Results are computed per component.\n\n    Result Type must be a scalar or vector of integer type.  The components\n    must be wide enough to hold the unsigned Width of Base as an unsigned\n    value. That is, no sign bit is needed or counted when checking for a\n    wide enough result width.\n\n    Base must be a scalar or vector of integer type.  It must have the same\n    number of components as Result Type.\n\n    The result is the unsigned value that is the number of bits in Base that\n    are 1.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.BitCount %0: i32\n    %3 = spirv.BitCount %1: vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand `:` type($operand) attr-dict"
  },
  {
    "name": "spirv.BitFieldInsert",
    "summary": "Make a copy of an object, with a modified bit field that comes from\n    another object.",
    "description": "Results are computed per component.\n\n    Result Type must be a scalar or vector of integer type.\n\n    The type of Base and Insert must be the same as Result Type.\n\n    Any result bits numbered outside [Offset, Offset + Count -  1]\n    (inclusive) will come from the corresponding bits in Base.\n\n    Any result bits numbered in [Offset, Offset + Count -  1] come, in\n    order, from the bits numbered [0, Count - 1] of Insert.\n\n    Count  must be an integer type scalar. Count is the number of bits taken\n    from Insert. It will be consumed as an unsigned value. Count can be 0,\n    in which case the result will be Base.\n\n    Offset  must be an integer type scalar. Offset is the lowest-order bit\n    of the bit field.  It will be consumed as an unsigned value.\n\n    The resulting value is undefined if Count or Offset or their sum is\n    greater than the number of bits in the result.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.BitFieldInsert %base, %insert, %offset, %count : vector<3xi32>, i8, i8\n    ```",
    "inputs": [
      { "name": "base", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "insert", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "offset", "type": "SPIRV_Integer" },
      { "name": "count", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($base) `,` type($offset) `,` type($count)"
  },
  {
    "name": "spirv.BitFieldSExtract",
    "summary": "Extract a bit field from an object, with sign extension.",
    "description": "Results are computed per component.\n\n    Result Type must be a scalar or vector of integer type.\n\n    The type of Base must be the same as Result Type.\n\n    If Count is greater than 0: The bits of Base numbered in [Offset, Offset\n    + Count -  1] (inclusive) become the bits numbered [0, Count - 1] of the\n    result. The remaining bits of the result will all be the same as bit\n    Offset + Count -  1 of Base.\n\n    Count  must be an integer type scalar. Count is the number of bits\n    extracted from Base. It will be consumed as an unsigned value. Count can\n    be 0, in which case the result will be 0.\n\n    Offset  must be an integer type scalar. Offset is the lowest-order bit\n    of the bit field to extract from Base.  It will be consumed as an\n    unsigned value.\n\n    The resulting value is undefined if Count or Offset or their sum is\n    greater than the number of bits in the result.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.BitFieldSExtract %base, %offset, %count : vector<3xi32>, i8, i8\n    ```",
    "inputs": [
      { "name": "base", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "offset", "type": "SPIRV_Integer" },
      { "name": "count", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($base) `,` type($offset) `,` type($count)"
  },
  {
    "name": "spirv.BitFieldUExtract",
    "summary": "Extract a bit field from an object, without sign extension.",
    "description": "The semantics are the same as with OpBitFieldSExtract with the exception\n    that there is no sign extension. The remaining bits of the result will\n    all be 0.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.BitFieldUExtract %base, %offset, %count : vector<3xi32>, i8, i8\n    ```",
    "inputs": [
      { "name": "base", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "offset", "type": "SPIRV_Integer" },
      { "name": "count", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($base) `,` type($offset) `,` type($count)"
  },
  {
    "name": "spirv.BitReverse",
    "summary": "Reverse the bits in an object.",
    "description": "Results are computed per component.\n\n    Result Type must be a scalar or vector of integer type.\n\n    The type of Base must be the same as Result Type.\n\n    The bit-number n of the result will be taken from bit-number Width - 1 -\n    n of Base, where Width is the OpTypeInt operand of the Result Type.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.BitReverse %0 : i32\n    %3 = spirv.BitReverse %1 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand `:` type($operand) attr-dict"
  },
  {
    "name": "spirv.BitwiseAnd",
    "summary": "Result is 1 if both Operand 1 and Operand 2 are 1. Result is 0 if either\n    Operand 1 or Operand 2 are 0.",
    "description": "Results are computed per component, and within each component, per bit.\n\n    Result Type must be a scalar or vector of integer type.  The type of\n    Operand 1 and Operand 2  must be a scalar or vector of integer type.\n    They must have the same number of components as Result Type. They must\n    have the same component width as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.BitwiseAnd %0, %1 : i32\n    %2 = spirv.BitwiseAnd %0, %1 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.BitwiseOr",
    "summary": "Result is 1 if either Operand 1 or Operand 2 is 1. Result is 0 if both\n    Operand 1 and Operand 2 are 0.",
    "description": "Results are computed per component, and within each component, per bit.\n\n    Result Type must be a scalar or vector of integer type.  The type of\n    Operand 1 and Operand 2  must be a scalar or vector of integer type.\n    They must have the same number of components as Result Type. They must\n    have the same component width as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.BitwiseOr %0, %1 : i32\n    %2 = spirv.BitwiseOr %0, %1 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.BitwiseXor",
    "summary": "Result is 1 if exactly one of Operand 1 or Operand 2 is 1. Result is 0\n    if Operand 1 and Operand 2 have the same value.",
    "description": "Results are computed per component, and within each component, per bit.\n\n    Result Type must be a scalar or vector of integer type.  The type of\n    Operand 1 and Operand 2  must be a scalar or vector of integer type.\n    They must have the same number of components as Result Type. They must\n    have the same component width as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.BitwiseXor %0, %1 : i32\n    %2 = spirv.BitwiseXor %0, %1 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.Branch",
    "summary": "Unconditional branch to target block.",
    "description": "This instruction must be the last instruction in a block.\n\n    #### Example:\n\n    ```mlir\n    spirv.Branch ^target\n    spirv.Branch ^target(%0, %1: i32, f32)\n    ```",
    "inputs": [
      { "name": "targetOperands", "type": "Variadic" }
    ],
    "successors": [
      {
        "name": "target"
      }
    ],
    "assemblyFormat": "$target (`(` $targetOperands^ `:` type($targetOperands) `)`)? attr-dict"
  },
  {
    "name": "spirv.BranchConditional",
    "summary": "If Condition is true, branch to true block, otherwise branch to false\n    block.",
    "description": "Condition must be a Boolean type scalar.\n\n    Branch weights are unsigned 32-bit integer literals. There must be\n    either no Branch Weights or exactly two branch weights. If present, the\n    first is the weight for branching to True Label, and the second is the\n    weight for branching to False Label. The implied probability that a\n    branch is taken is its weight divided by the sum of the two Branch\n    weights. At least one weight must be non-zero. A weight of zero does not\n    imply a branch is dead or permit its removal; branch weights are only\n    hints. The two weights must not overflow a 32-bit unsigned integer when\n    added together.\n\n    This instruction must be the last instruction in a block.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    branch-conditional-op ::= `spirv.BranchConditional` ssa-use\n                              (`[` integer-literal, integer-literal `]`)?\n                              `,` successor `,` successor\n    successor ::= bb-id branch-use-list?\n    branch-use-list ::= `(` ssa-use-list `:` type-list-no-parens `)`\n    ```\n\n    #### Example:\n\n    ```mlir\n    spirv.BranchConditional %condition, ^true_branch, ^false_branch\n    spirv.BranchConditional %condition, ^true_branch(%0: i32), ^false_branch(%1: i32)\n    ```",
    "inputs": [
      { "name": "condition", "type": "SPIRV_Bool" },
      { "name": "trueTargetOperands", "type": "Variadic" },
      { "name": "falseTargetOperands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "branch_weights", "type": "OptionalAttr" }
    ],
    "successors": [
      {
        "name": "trueTarget"
      },
      {
        "name": "falseTarget"
      }
    ]
  },
  {
    "name": "spirv.CompositeConstruct",
    "summary": "Construct a new composite object from a set of constituent objects.",
    "description": "Result Type must be a composite type, whose top-level\n    members/elements/components/columns have the same type as the types of\n    the operands, with one exception. The exception is that for constructing\n    a vector, the operands may also be vectors with the same component type\n    as the Result Type component type. When constructing a vector, the total\n    number of components in all the operands must equal the number of\n    components in Result Type.\n\n    Constituents will become members of a structure, or elements of an\n    array, or components of a vector, or columns of a matrix. There must be\n    exactly one Constituent for each top-level\n    member/element/component/column of the result, with one exception. The\n    exception is that for constructing a vector, a contiguous subset of the\n    scalars consumed can be represented by a vector operand instead. The\n    Constituents must appear in the order needed by the definition of the\n    type of the result. When constructing a vector, there must be at least\n    two Constituent operands.\n\n    #### Example:\n\n    ```mlir\n    %a = spirv.CompositeConstruct %1, %2, %3 : vector<3xf32>\n    %b = spirv.CompositeConstruct %a, %1 : (vector<3xf32>, f32) -> vector<4xf32>\n\n    %c = spirv.CompositeConstruct %1 :\n      (f32) -> !spirv.coopmatrix<4x4xf32, Subgroup, MatrixA>\n\n    %d = spirv.CompositeConstruct %a, %4, %5 :\n      (vector<3xf32>, !spirv.array<4xf32>, !spirv.struct<(f32)>) ->\n        !spirv.struct<(vector<3xf32>, !spirv.array<4xf32>, !spirv.struct<(f32)>)>\n    ```",
    "inputs": [
      { "name": "constituents", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Composite" }
    ],
    "assemblyFormat": "$constituents attr-dict `:` `(` type(operands) `)` `->` type($result)"
  },
  {
    "name": "spirv.CompositeExtract",
    "summary": "Extract a part of a composite object.",
    "description": "Result Type must be the type of object selected by the last provided\n    index.  The instruction result is the extracted object.\n\n    Composite is the composite to extract from.\n\n    Indexes walk the type hierarchy, potentially down to component\n    granularity, to select the part to extract. All indexes must be in\n    bounds.  All composite constituents use zero-based numbering, as\n    described by their OpType… instruction.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    composite-extract-op ::= ssa-id `=` `spirv.CompositeExtract` ssa-use\n                             `[` integer-literal (',' integer-literal)* `]`\n                             `:` composite-type\n    ```\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.Variable : !spirv.ptr<!spirv.array<4x!spirv.array<4xf32>>, Function>\n    %1 = spirv.Load \"Function\" %0 [\"Volatile\"] : !spirv.array<4x!spirv.array<4xf32>>\n    %2 = spirv.CompositeExtract %1[1 : i32] : !spirv.array<4x!spirv.array<4xf32>>\n    ```",
    "inputs": [
      { "name": "composite", "type": "SPIRV_Composite" }
    ],
    "outputs": [
      { "name": "component", "type": "SPIRV_Type" }
    ],
    "attributes": [
      { "name": "indices", "type": "I32ArrayAttr" }
    ]
  },
  {
    "name": "spirv.CompositeInsert",
    "summary": "Make a copy of a composite object, while modifying one part of it.",
    "description": "Result Type must be the same type as Composite.\n\n    Object is the object to use as the modified part.\n\n    Composite is the composite to copy all but the modified part from.\n\n    Indexes walk the type hierarchy of Composite to the desired depth,\n    potentially down to component granularity, to select the part to modify.\n    All indexes must be in bounds. All composite constituents use zero-based\n    numbering, as described by their OpType… instruction. The type of the\n    part selected to modify must match the type of Object.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    composite-insert-op ::= ssa-id `=` `spirv.CompositeInsert` ssa-use, ssa-use\n                            `[` integer-literal (',' integer-literal)* `]`\n                            `:` object-type `into` composite-type\n    ```\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.CompositeInsert %object, %composite[1 : i32] : f32 into !spirv.array<4xf32>\n    ```",
    "inputs": [
      { "name": "object", "type": "SPIRV_Type" },
      { "name": "composite", "type": "SPIRV_Composite" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Composite" }
    ],
    "attributes": [
      { "name": "indices", "type": "I32ArrayAttr" }
    ]
  },
  {
    "name": "spirv.Constant",
    "summary": "Declare a new integer-type or floating-point-type scalar constant.",
    "description": "This op declares a SPIR-V normal constant. SPIR-V has multiple constant\n    instructions covering different constant types:\n\n    * `OpConstantTrue` and `OpConstantFalse` for boolean constants\n    * `OpConstant` for scalar constants\n    * `OpConstantComposite` for composite constants\n    * `OpConstantNull` for null constants\n    * ...\n\n    Having such a plethora of constant instructions renders IR transformations\n    more tedious. Therefore, we use a single `spirv.Constant` op to represent\n    them all. Note that conversion between those SPIR-V constant instructions\n    and this op is purely mechanical; so it can be scoped to the binary\n    (de)serialization process.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    spirv.Constant-op ::= ssa-id `=` `spirv.Constant` attribute-value\n                        (`:` spirv-type)?\n    ```\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.Constant true\n    %1 = spirv.Constant dense<[2.0, 3.0]> : vector<2xf32>\n    %2 = spirv.Constant [dense<3.0> : vector<2xf32>] : !spirv.array<1xvector<2xf32>>\n    ```\n\n    TODO: support constant structs",
    "outputs": [
      { "name": "constant", "type": "SPIRV_Type" }
    ],
    "attributes": [
      { "name": "value", "type": "AnyAttr" }
    ]
  },
  {
    "name": "spirv.ControlBarrier",
    "summary": "Wait for other invocations of this module to reach the current point of\n    execution.",
    "description": "All invocations of this module within Execution scope must reach this\n    point of execution before any invocation will proceed beyond it.\n\n    When Execution is Workgroup or larger, behavior is undefined if this\n    instruction is used in control flow that is non-uniform within\n    Execution. When Execution is Subgroup or Invocation, the behavior of\n    this instruction in non-uniform control flow is defined by the client\n    API.\n\n    If Semantics is not None, this instruction also serves as an\n    OpMemoryBarrier instruction, and must also perform and adhere to the\n    description and semantics of an OpMemoryBarrier instruction with the\n    same Memory and Semantics operands.  This allows atomically specifying\n    both a control barrier and a memory barrier (that is, without needing\n    two instructions). If Semantics is None, Memory is ignored.\n\n    Before version 1.3, it is only valid to use this instruction with\n    TessellationControl, GLCompute, or Kernel execution models. There is no\n    such restriction starting with version 1.3.\n\n    When used with the TessellationControl execution model, it also\n    implicitly synchronizes the Output Storage Class:  Writes to Output\n    variables performed by any invocation executed prior to a\n    OpControlBarrier will be visible to any other invocation after return\n    from that OpControlBarrier.\n\n    #### Example:\n\n    ```mlir\n    spirv.ControlBarrier <Workgroup>, <Device>, <Acquire|UniformMemory>\n    ```",
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "memory_semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$execution_scope `,` $memory_scope `,` $memory_semantics attr-dict"
  },
  {
    "name": "spirv.ConvertFToS",
    "summary": "Convert value numerically from floating point to signed integer, with\n    round toward 0.0.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    Float Value must be a scalar or vector of floating-point type.  It must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.ConvertFToS %0 : f32 to i32\n    %3 = spirv.ConvertFToS %2 : vector<3xf32> to vector<3xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.ConvertFToU",
    "summary": "Convert value numerically from floating point to unsigned integer, with\n    round toward 0.0.",
    "description": "Result Type must be a scalar or vector of integer type, whose Signedness\n    operand is 0.\n\n    Float Value must be a scalar or vector of floating-point type.  It must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.ConvertFToU %0 : f32 to i32\n    %3 = spirv.ConvertFToU %2 : vector<3xf32> to vector<3xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.ConvertPtrToU",
    "summary": "Bit pattern-preserving conversion of a pointer to\n    an unsigned scalar integer of possibly different bit width.",
    "description": "Result Type must be a scalar of integer type, whose Signedness operand is 0.\n\n    Pointer must be a physical pointer type. If the bit width of Pointer is\n    smaller than that of Result Type, the conversion zero extends Pointer.\n    If the bit width of Pointer is larger than that of Result Type,\n    the conversion truncates Pointer.\n\n    For same bit width Pointer and Result Type, this is the same as OpBitcast.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.ConvertPtrToU %0 : !spirv.ptr<i32, Generic> to i32\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Integer" }
    ],
    "assemblyFormat": "$pointer attr-dict `:` type($pointer) `to` type($result)"
  },
  {
    "name": "spirv.ConvertSToF",
    "summary": "Convert value numerically from signed integer to floating point.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    Signed Value must be a scalar or vector of integer type.  It must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.ConvertSToF %0 : i32 to f32\n    %3 = spirv.ConvertSToF %2 : vector<3xi32> to vector<3xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.ConvertUToF",
    "summary": "Convert value numerically from unsigned integer to floating point.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    Unsigned Value must be a scalar or vector of integer type.  It must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.ConvertUToF %0 : i32 to f32\n    %3 = spirv.ConvertUToF %2 : vector<3xi32> to vector<3xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.ConvertUToPtr",
    "summary": "Bit pattern-preserving conversion of an unsigned scalar integer\n    to a pointer.",
    "description": "Result Type must be a physical pointer type.\n\n    Integer Value must be a scalar of integer type, whose Signedness\n    operand is 0. If the bit width of Integer Value is smaller\n    than that of Result Type, the conversion zero extends Integer Value.\n    If the bit width of Integer Value is larger than that of Result Type,\n    the conversion truncates Integer Value.\n\n    For same-width Integer Value and Result Type, this is the same as OpBitcast.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.ConvertUToPtr %0 :  i32 to !spirv.ptr<i32, Generic>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyPtr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.Dot",
    "summary": "Dot product of Vector 1 and Vector 2",
    "description": "Result Type must be a floating point scalar.\n\n    Vector 1 and Vector 2 must be vectors of the same type, and their component\n    type must be Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.Dot %v1, %v2 : vector<4xf32> -> f32\n    ```",
    "inputs": [
      { "name": "vector1", "type": "SPIRV_VectorOf" },
      { "name": "vector2", "type": "SPIRV_VectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyFloat" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($vector1) `->` type($result)"
  },
  {
    "name": "spirv.EntryPoint",
    "summary": "Declare an entry point, its execution model, and its interface.",
    "description": "Execution Model is the execution model for the entry point and its\n    static call tree. See Execution Model.\n\n    Entry Point must be the Result <id> of an OpFunction instruction.\n\n    Name is a name string for the entry point. A module cannot have two\n    OpEntryPoint instructions with the same Execution Model and the same\n    Name string.\n\n    Interface is a list of symbol references to `spirv.GlobalVariable`\n    operations. These declare the set of global variables from a\n    module that form the interface of this entry point. The set of\n    Interface symbols must be equal to or a superset of the\n    `spirv.GlobalVariable`s referenced by the entry point’s static call\n    tree, within the interface’s storage classes.  Before version 1.4,\n    the interface’s storage classes are limited to the Input and\n    Output storage classes. Starting with version 1.4, the interface’s\n    storage classes are all storage classes used in declaring all\n    global variables referenced by the entry point’s call tree.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    execution-model ::= \"Vertex\" | \"TesellationControl\" |\n                        <and other SPIR-V execution models...>\n\n    entry-point-op ::= ssa-id `=` `spirv.EntryPoint` execution-model\n                       symbol-reference (`, ` symbol-reference)*\n    ```\n\n    #### Example:\n\n    ```mlir\n    spirv.EntryPoint \"GLCompute\" @foo\n    spirv.EntryPoint \"Kernel\" @foo, @var1, @var2\n\n    ```",
    "attributes": [
      { "name": "execution_model", "type": "SPIRV_ExecutionModelAttr" },
      { "name": "fn", "type": "FlatSymbolRefAttr" },
      { "name": "interface", "type": "SymbolRefArrayAttr" }
    ]
  },
  {
    "name": "spirv.ExecutionMode",
    "summary": "Declare an execution mode for an entry point.",
    "description": "Entry Point must be the Entry Point <id> operand of an OpEntryPoint\n    instruction.\n\n    Mode is the execution mode. See Execution Mode.\n\n    This instruction is only valid when the Mode operand is an execution\n    mode that takes no Extra Operands, or takes Extra Operands that are not\n    <id> operands.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    execution-mode ::= \"Invocations\" | \"SpacingEqual\" |\n                       <and other SPIR-V execution modes...>\n\n    execution-mode-op ::= `spirv.ExecutionMode ` ssa-use execution-mode\n                          (integer-literal (`, ` integer-literal)* )?\n    ```\n\n    #### Example:\n\n    ```mlir\n    spirv.ExecutionMode @foo \"ContractionOff\"\n    spirv.ExecutionMode @bar \"LocalSizeHint\", 3, 4, 5\n    ```",
    "attributes": [
      { "name": "fn", "type": "FlatSymbolRefAttr" },
      { "name": "execution_mode", "type": "SPIRV_ExecutionModeAttr" },
      { "name": "values", "type": "I32ArrayAttr" }
    ]
  },
  {
    "name": "spirv.FAdd",
    "summary": "Floating-point addition of Operand 1 and Operand 2.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    The types of Operand 1 and Operand 2 both must be the same as Result\n    Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FAdd %0, %1 : f32\n    %5 = spirv.FAdd %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.FConvert",
    "summary": "Convert value numerically from one floating-point width to another\n    width.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    Float Value must be a scalar or vector of floating-point type.  It must\n    have the same number of components as Result Type.  The component width\n    cannot equal the component width in Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.FConvertOp %0 : f32 to f64\n    %3 = spirv.FConvertOp %2 : vector<3xf32> to vector<3xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.FDiv",
    "summary": "Floating-point division of Operand 1 divided by Operand 2.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    The types of Operand 1 and Operand 2 both must be the same as Result\n    Type.\n\n    Results are computed per component.  The resulting value is undefined\n    if Operand 2 is 0.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FDiv %0, %1 : f32\n    %5 = spirv.FDiv %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.FMod",
    "summary": "The floating-point remainder whose sign matches the sign of Operand 2.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    The types of Operand 1 and Operand 2 both must be the same as Result\n    Type.\n\n    Results are computed per component.  The resulting value is undefined\n    if Operand 2 is 0.  Otherwise, the result is the remainder r of Operand\n    1 divided by Operand 2 where if r ≠ 0, the sign of r is the same as the\n    sign of Operand 2.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FMod %0, %1 : f32\n    %5 = spirv.FMod %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.FMul",
    "summary": "Floating-point multiplication of Operand 1 and Operand 2.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    The types of Operand 1 and Operand 2 both must be the same as Result\n    Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FMul %0, %1 : f32\n    %5 = spirv.FMul %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.FNegate",
    "summary": "Inverts the sign bit of Operand. (Note, however, that OpFNegate is still\n    considered a floating-point instruction, and so is subject to the\n    general floating-point rules regarding, for example, subnormals and NaN\n    propagation).",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    The type of Operand must be the same as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.FNegate %0 : f32\n    %3 = spirv.FNegate %2 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.FOrdEqual",
    "summary": "Floating-point comparison for being ordered and equal.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FOrdEqual %0, %1 : f32\n    %5 = spirv.FOrdEqual %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FOrdGreaterThan",
    "summary": "Floating-point comparison if operands are ordered and Operand 1 is\n    greater than  Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FOrdGreaterThan %0, %1 : f32\n    %5 = spirv.FOrdGreaterThan %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FOrdGreaterThanEqual",
    "summary": "Floating-point comparison if operands are ordered and Operand 1 is\n    greater than or equal to Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FOrdGreaterThanEqual %0, %1 : f32\n    %5 = spirv.FOrdGreaterThanEqual %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FOrdLessThan",
    "summary": "Floating-point comparison if operands are ordered and Operand 1 is less\n    than Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FOrdLessThan %0, %1 : f32\n    %5 = spirv.FOrdLessThan %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FOrdLessThanEqual",
    "summary": "Floating-point comparison if operands are ordered and Operand 1 is less\n    than or equal to Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FOrdLessThanEqual %0, %1 : f32\n    %5 = spirv.FOrdLessThanEqual %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FOrdNotEqual",
    "summary": "Floating-point comparison for being ordered and not equal.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FOrdNotEqual %0, %1 : f32\n    %5 = spirv.FOrdNotEqual %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FRem",
    "summary": "The floating-point remainder whose sign matches the sign of Operand 1.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    The types of Operand 1 and Operand 2 both must be the same as Result\n    Type.\n\n    Results are computed per component.  The resulting value is undefined\n    if Operand 2 is 0.  Otherwise, the result is the remainder r of Operand\n    1 divided by Operand 2 where if r ≠ 0, the sign of r is the same as the\n    sign of Operand 1.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FRemOp %0, %1 : f32\n    %5 = spirv.FRemOp %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.FSub",
    "summary": "Floating-point subtraction of Operand 2 from Operand 1.",
    "description": "Result Type must be a scalar or vector of floating-point type.\n\n    The types of Operand 1 and Operand 2 both must be the same as Result\n    Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FRemOp %0, %1 : f32\n    %5 = spirv.FRemOp %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.func",
    "summary": "Declare or define a function",
    "description": "This op declares or defines a SPIR-V function using one region, which\n    contains one or more blocks.\n\n    Different from the SPIR-V binary format, this op is not allowed to\n    implicitly capture global values, and all external references must use\n    function arguments or symbol references. This op itself defines a symbol\n    that is unique in the enclosing module op.\n\n    This op itself takes no operands and generates no results. Its region\n    can take zero or more arguments and return zero or one values.\n\n    From `SPV_KHR_physical_storage_buffer`:\n    If a parameter of function is\n    - a pointer (or contains a pointer) in the PhysicalStorageBuffer storage\n      class, the function parameter must be decorated with exactly one of\n      `Aliased` or `Restrict`.\n    - a pointer (or contains a pointer) and the type it points to is a pointer\n      in the PhysicalStorageBuffer storage class, the function parameter must\n      be decorated with exactly one of `AliasedPointer` or `RestrictPointer`.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    spv-function-control ::= \"None\" | \"Inline\" | \"DontInline\" | ...\n    spv-function-op ::= `spirv.func` function-signature\n                         spv-function-control region\n    ```\n\n    #### Example:\n\n    ```mlir\n    spirv.func @foo() -> () \"None\" { ... }\n    spirv.func @bar() -> () \"Inline|Pure\" { ... }\n\n    spirv.func @aliased_pointer(%arg0: !spirv.ptr<i32, PhysicalStorageBuffer>,\n        { spirv.decoration = #spirv.decoration<Aliased> }) -> () \"None\" { ... }\n\n    spirv.func @restrict_pointer(%arg0: !spirv.ptr<i32, PhysicalStorageBuffer>,\n        { spirv.decoration = #spirv.decoration<Restrict> }) -> () \"None\" { ... }\n\n    spirv.func @aliased_pointee(%arg0: !spirv.ptr<!spirv.ptr<i32,\n        PhysicalStorageBuffer>, Generic> { spirv.decoration =\n        #spirv.decoration<AliasedPointer> }) -> () \"None\" { ... }\n\n    spirv.func @restrict_pointee(%arg0: !spirv.ptr<!spirv.ptr<i32,\n        PhysicalStorageBuffer>, Generic> { spirv.decoration =\n        #spirv.decoration<RestrictPointer> }) -> () \"None\" { ... }\n    ```",
    "attributes": [
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "StrAttr" },
      { "name": "function_control", "type": "SPIRV_FunctionControlAttr" },
      { "name": "linkage_attributes", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "spirv.FunctionCall",
    "summary": "Call a function.",
    "description": "Result Type is the type of the return value of the function. It must be\n    the same as the Return Type operand of the Function Type operand of the\n    Function operand.\n\n    Function is an OpFunction instruction.  This could be a forward\n    reference.\n\n    Argument N is the object to copy to parameter N of Function.\n\n    Note: A forward call is possible because there is no missing type\n    information: Result Type must match the Return Type of the function, and\n    the calling argument types must match the formal parameter types.\n\n    #### Example:\n\n    ```mlir\n    spirv.FunctionCall @f_void(%arg0) : (i32) ->  ()\n    %0 = spirv.FunctionCall @f_iadd(%arg0, %arg1) : (i32, i32) -> i32\n    ```",
    "inputs": [
      { "name": "arguments", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "return_value", "type": "Optional" }
    ],
    "attributes": [
      { "name": "callee", "type": "FlatSymbolRefAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$callee `(` $arguments `)` attr-dict `:`\n      functional-type($arguments, results)"
  },
  {
    "name": "spirv.FUnordEqual",
    "summary": "Floating-point comparison for being unordered or equal.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FUnordEqual %0, %1 : f32\n    %5 = spirv.FUnordEqual %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FUnordGreaterThan",
    "summary": "Floating-point comparison if operands are unordered or Operand 1 is\n    greater than  Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FUnordGreaterThan %0, %1 : f32\n    %5 = spirv.FUnordGreaterThan %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FUnordGreaterThanEqual",
    "summary": "Floating-point comparison if operands are unordered or Operand 1 is\n    greater than or equal to Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FUnordGreaterThanEqual %0, %1 : f32\n    %5 = spirv.FUnordGreaterThanEqual %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FUnordLessThan",
    "summary": "Floating-point comparison if operands are unordered or Operand 1 is less\n    than Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FUnordLessThan %0, %1 : f32\n    %5 = spirv.FUnordLessThan %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FUnordLessThanEqual",
    "summary": "Floating-point comparison if operands are unordered or Operand 1 is less\n    than or equal to Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FUnordLessThanEqual %0, %1 : f32\n    %5 = spirv.FUnordLessThanEqual %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.FUnordNotEqual",
    "summary": "Floating-point comparison for being unordered or not equal.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    floating-point type.  They must have the same type, and they must have\n    the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.FUnordNotEqual %0, %1 : f32\n    %5 = spirv.FUnordNotEqual %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.GenericCastToPtr",
    "summary": "Convert a pointer’s Storage Class to a non-Generic class.",
    "description": "Result Type must be an OpTypePointer. Its Storage Class must be\n    Workgroup, CrossWorkgroup, or Function.\n\n    Pointer must point to the Generic Storage Class.\n\n    Result Type and Pointer must point to the same type.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n       %1 = spirv.GenericCastToPtrOp %0 : !spirv.ptr<f32, Generic> to\n       !spirv.ptr<f32, CrossWorkGroup>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyPtr" }
    ],
    "assemblyFormat": "$pointer attr-dict `:` type($pointer) `to` type($result)"
  },
  {
    "name": "spirv.GenericCastToPtrExplicit",
    "summary": "Attempts to explicitly convert Pointer to Storage storage-class pointer\n    value.",
    "description": "Result Type must be an OpTypePointer. Its Storage Class must be Storage.\n\n    Pointer must have a type of OpTypePointer whose Type is the same as the\n    Type of Result Type.Pointer must point to the Generic Storage Class. If\n    the cast fails, the instruction result is an OpConstantNull pointer in\n    the Storage Storage Class.\n\n    Storage must be one of the following literal values from Storage Class:\n    Workgroup, CrossWorkgroup, or Function.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n       %1 = spirv.GenericCastToPtrExplicitOp %0 : !spirv.ptr<f32, Generic> to\n       !spirv.ptr<f32, CrossWorkGroup>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyPtr" }
    ],
    "assemblyFormat": "$pointer attr-dict `:` type($pointer) `to` type($result)"
  },
  {
    "name": "spirv.GlobalVariable",
    "summary": "Allocate an object in memory at module scope. The object is\n    referenced using a symbol name.",
    "description": "The variable type must be an OpTypePointer. Its type operand is the type of\n    object in memory.\n\n    Storage Class is the Storage Class of the memory holding the object. It\n    cannot be Generic. It must be the same as the Storage Class operand of\n    the variable types. Only those storage classes that are valid at module\n    scope (like Input, Output, StorageBuffer, etc.) are valid.\n\n    Initializer is optional.  If Initializer is present, it will be\n    the initial value of the variable’s memory content. Initializer\n    must be an symbol defined from a constant instruction or other\n    `spirv.GlobalVariable` operation in module scope. Initializer must\n    have the same type as the type of the defined symbol.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    variable-op ::= `spirv.GlobalVariable` spirv-type symbol-ref-id\n                    (`initializer(` symbol-ref-id `)`)?\n                    (`bind(` integer-literal, integer-literal `)`)?\n                    (`built_in(` string-literal `)`)?\n                    attribute-dict?\n    ```\n\n    where `initializer` specifies initializer and `bind` specifies the\n    descriptor set and binding number. `built_in` specifies SPIR-V\n    BuiltIn decoration associated with the op.\n\n    #### Example:\n\n    ```mlir\n    spirv.GlobalVariable @var0 : !spirv.ptr<f32, Input> @var0\n    spirv.GlobalVariable @var1 initializer(@var0) : !spirv.ptr<f32, Output>\n    spirv.GlobalVariable @var2 bind(1, 2) : !spirv.ptr<f32, Uniform>\n    spirv.GlobalVariable @var3 built_in(\"GlobalInvocationId\") : !spirv.ptr<vector<3xi32>, Input>\n    ```",
    "attributes": [
      { "name": "type", "type": "TypeAttr" },
      { "name": "sym_name", "type": "StrAttr" },
      { "name": "initializer", "type": "OptionalAttr" },
      { "name": "location", "type": "OptionalAttr" },
      { "name": "binding", "type": "OptionalAttr" },
      { "name": "descriptor_set", "type": "OptionalAttr" },
      { "name": "built_in", "type": "OptionalAttr" },
      { "name": "linkage_attributes", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "spirv.GroupBroadcast",
    "summary": "Broadcast the Value of the invocation identified by the local id LocalId\n    to the result of all invocations in the group.",
    "description": "All invocations of this module within Execution must reach this point of\n    execution.\n\n    Behavior is undefined if this instruction is used in control flow that\n    is non-uniform within Execution.\n\n    Result Type  must be a scalar or vector of floating-point type, integer\n    type, or Boolean type.\n\n    Execution must be Workgroup or Subgroup Scope.\n\n    The type of Value must be the same as Result Type.\n\n    LocalId must be an integer datatype. It can be a scalar, or a vector\n    with 2 components or a vector with 3 components. LocalId must be the\n    same for all invocations in the group.\n\n    #### Example:\n\n    ```mlir\n    %scalar_value = ... : f32\n    %vector_value = ... : vector<4xf32>\n    %scalar_localid = ... : i32\n    %vector_localid = ... : vector<3xi32>\n    %0 = spirv.GroupBroadcast \"Subgroup\" %scalar_value, %scalar_localid : f32, i32\n    %1 = spirv.GroupBroadcast \"Workgroup\" %vector_value, %vector_localid :\n      vector<4xf32>, vector<3xi32>\n    ```",
    "inputs": [
      { "name": "value", "type": "SPIRV_Type" },
      { "name": "localid", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Type" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" }
    ],
    "assemblyFormat": "$execution_scope operands attr-dict `:` type($value) `,` type($localid)"
  },
  {
    "name": "spirv.GroupFAdd",
    "summary": "A floating-point add group operation specified for all values of X\n    specified by invocations in the group.",
    "description": "Behavior is undefined if not all invocations of this module within\n    Execution reach this point of execution.\n\n    Behavior is undefined unless all invocations within Execution execute\n    the same dynamic instance of this instruction.\n\n    Result Type  must be a scalar or vector of floating-point type.\n\n    Execution is a Scope. It must be either Workgroup or Subgroup.\n\n    The identity I for Operation is 0.\n\n    The type of X must be the same as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.GroupFAdd <Workgroup> <Reduce> %value : f32\n    ```",
    "inputs": [
      { "name": "x", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "group_operation", "type": "SPIRV_GroupOperationAttr" }
    ],
    "assemblyFormat": "$execution_scope $group_operation operands attr-dict `:` type($x)"
  },
  {
    "name": "spirv.GroupFMax",
    "summary": "A floating-point maximum group operation specified for all values of X\n    specified by invocations in the group.",
    "description": "Behavior is undefined if not all invocations of this module within\n    Execution reach this point of execution.\n\n    Behavior is undefined unless all invocations within Execution execute\n    the same dynamic instance of this instruction.\n\n    Result Type  must be a scalar or vector of floating-point type.\n\n    Execution is a Scope. It must be either Workgroup or Subgroup.\n\n    The identity I for Operation is -INF.\n\n    The type of X must be the same as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.GroupFMax <Workgroup> <Reduce> %value : f32\n    ```",
    "inputs": [
      { "name": "x", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "group_operation", "type": "SPIRV_GroupOperationAttr" }
    ],
    "assemblyFormat": "$execution_scope $group_operation operands attr-dict `:` type($x)"
  },
  {
    "name": "spirv.GroupFMin",
    "summary": "A floating-point minimum group operation specified for all values of X\n    specified by invocations in the group.",
    "description": "Behavior is undefined if not all invocations of this module within\n    Execution reach this point of execution.\n\n    Behavior is undefined unless all invocations within Execution execute\n    the same dynamic instance of this instruction.\n\n    Result Type  must be a scalar or vector of floating-point type.\n\n    Execution is a Scope. It must be either Workgroup or Subgroup.\n\n    The identity I for Operation is +INF.\n\n    The type of X must be the same as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.GroupFMin <Workgroup> <Reduce> %value : f32\n    ```",
    "inputs": [
      { "name": "x", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "group_operation", "type": "SPIRV_GroupOperationAttr" }
    ],
    "assemblyFormat": "$execution_scope $group_operation operands attr-dict `:` type($x)"
  },
  {
    "name": "spirv.GroupIAdd",
    "summary": "An integer add group operation specified for all values of X specified\n    by invocations in the group.",
    "description": "Behavior is undefined if not all invocations of this module within\n    Execution reach this point of execution.\n\n    Behavior is undefined unless all invocations within Execution execute\n    the same dynamic instance of this instruction.\n\n    Result Type  must be a scalar or vector of integer type.\n\n    Execution is a Scope. It must be either Workgroup or Subgroup.\n\n    The identity I for Operation is 0.\n\n    The type of X must be the same as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.GroupIAdd <Workgroup> <Reduce> %value : i32\n    ```",
    "inputs": [
      { "name": "x", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "group_operation", "type": "SPIRV_GroupOperationAttr" }
    ],
    "assemblyFormat": "$execution_scope $group_operation operands attr-dict `:` type($x)"
  },
  {
    "name": "spirv.GroupSMax",
    "summary": "A signed integer maximum group operation specified for all values of X\n    specified by invocations in the group.",
    "description": "Behavior is undefined if not all invocations of this module within\n    Execution reach this point of execution.\n\n    Behavior is undefined unless all invocations within Execution execute\n    the same dynamic instance of this instruction.\n\n    Result Type  must be a scalar or vector of integer type.\n\n    Execution is a Scope. It must be either Workgroup or Subgroup.\n\n    The identity I for Operation is INT_MIN when X is 32 bits wide and\n    LONG_MIN when X is 64 bits wide.\n\n    The type of X must be the same as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.GroupSMax <Workgroup> <Reduce> %value : i32\n    ```",
    "inputs": [
      { "name": "x", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "group_operation", "type": "SPIRV_GroupOperationAttr" }
    ],
    "assemblyFormat": "$execution_scope $group_operation operands attr-dict `:` type($x)"
  },
  {
    "name": "spirv.GroupSMin",
    "summary": "A signed integer minimum group operation specified for all values of X\n    specified by invocations in the group.",
    "description": "Behavior is undefined if not all invocations of this module within\n    Execution reach this point of execution.\n\n    Behavior is undefined unless all invocations within Execution execute\n    the same dynamic instance of this instruction.\n\n    Result Type  must be a scalar or vector of integer type.\n\n    Execution is a Scope. It must be either Workgroup or Subgroup.\n\n    The identity I for Operation is INT_MAX when X is 32 bits wide and\n    LONG_MAX when X is 64 bits wide.\n\n    The type of X must be the same as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.GroupSMin <Workgroup> <Reduce> %value : i32\n    ```",
    "inputs": [
      { "name": "x", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "group_operation", "type": "SPIRV_GroupOperationAttr" }
    ],
    "assemblyFormat": "$execution_scope $group_operation operands attr-dict `:` type($x)"
  },
  {
    "name": "spirv.GroupUMax",
    "summary": "An unsigned integer maximum group operation specified for all values of\n    X specified by invocations in the group.",
    "description": "Behavior is undefined if not all invocations of this module within\n    Execution reach this point of execution.\n\n    Behavior is undefined unless all invocations within Execution execute\n    the same dynamic instance of this instruction.\n\n    Result Type  must be a scalar or vector of integer type.\n\n    Execution is a Scope. It must be either Workgroup or Subgroup.\n\n    The identity I for Operation is 0.\n\n    The type of X must be the same as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.GroupUMax <Workgroup> <Reduce> %value : i32\n    ```",
    "inputs": [
      { "name": "x", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "group_operation", "type": "SPIRV_GroupOperationAttr" }
    ],
    "assemblyFormat": "$execution_scope $group_operation operands attr-dict `:` type($x)"
  },
  {
    "name": "spirv.GroupUMin",
    "summary": "An unsigned integer minimum group operation specified for all values of\n    X specified by invocations in the group.",
    "description": "Behavior is undefined if not all invocations of this module within\n    Execution reach this point of execution.\n\n    Behavior is undefined unless all invocations within Execution execute\n    the same dynamic instance of this instruction.\n\n    Result Type  must be a scalar or vector of integer type.\n\n    Execution is a Scope. It must be either Workgroup or Subgroup.\n\n    The identity I for Operation is UINT_MAX when X is 32 bits wide and\n    ULONG_MAX when X is 64 bits wide.\n\n    The type of X must be the same as Result Type.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.GroupUMin <Workgroup> <Reduce> %value : i32\n    ```",
    "inputs": [
      { "name": "x", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "attributes": [
      { "name": "execution_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "group_operation", "type": "SPIRV_GroupOperationAttr" }
    ],
    "assemblyFormat": "$execution_scope $group_operation operands attr-dict `:` type($x)"
  },
  {
    "name": "spirv.IAdd",
    "summary": "Integer addition of Operand 1 and Operand 2.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same number of components as Result\n    Type. They must have the same component width as Result Type.\n\n    The resulting value will equal the low-order N bits of the correct\n    result R, where N is the component width and R is computed with enough\n    precision to avoid overflow and underflow.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.IAdd %0, %1 : i32\n    %5 = spirv.IAdd %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.IAddCarry",
    "summary": "Integer addition of Operand 1 and Operand 2, including the carry.",
    "description": "Result Type must be from OpTypeStruct.  The struct must have two\n    members, and the two members must be the same type.  The member type\n    must be a scalar or vector of integer type, whose Signedness operand is\n    0.\n\n    Operand 1 and Operand 2 must have the same type as the members of Result\n    Type. These are consumed as unsigned integers.\n\n     Results are computed per component.\n\n    Member 0 of the result gets the low-order bits (full component width) of\n    the addition.\n\n    Member 1 of the result gets the high-order (carry) bit of the result of\n    the addition. That is, it gets the value 1 if the addition overflowed\n    the component width, and 0 otherwise.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.IAddCarry %0, %1 : !spirv.struct<(i32, i32)>\n    %2 = spirv.IAddCarry %0, %1 : !spirv.struct<(vector<2xi32>, vector<2xi32>)>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyStruct" }
    ]
  },
  {
    "name": "spirv.IEqual",
    "summary": "Integer comparison for equality.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.IEqual %0, %1 : i32\n    %5 = spirv.IEqual %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.IMul",
    "summary": "Integer multiplication of Operand 1 and Operand 2.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same number of components as Result\n    Type. They must have the same component width as Result Type.\n\n    The resulting value will equal the low-order N bits of the correct\n    result R, where N is the component width and R is computed with enough\n    precision to avoid overflow and underflow.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.IMul %0, %1 : i32\n    %5 = spirv.IMul %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.INotEqual",
    "summary": "Integer comparison for inequality.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.INotEqual %0, %1 : i32\n    %5 = spirv.INotEqual %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.IsFinite",
    "summary": "Result is true if x is an IEEE Finite, otherwise result is false",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    x must be a scalar or vector of floating-point type.  It must have the\n    same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.IsFinite %0: f32\n    %3 = spirv.IsFinite %1: vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand `:` type($operand) attr-dict"
  },
  {
    "name": "spirv.IsInf",
    "summary": "Result is true if x is an IEEE Inf, otherwise result is false",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    x must be a scalar or vector of floating-point type.  It must have the\n    same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.IsInf %0: f32\n    %3 = spirv.IsInf %1: vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand `:` type($operand) attr-dict"
  },
  {
    "name": "spirv.IsNan",
    "summary": "Result is true if x is an IEEE NaN, otherwise result is false.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    x must be a scalar or vector of floating-point type.  It must have the\n    same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.IsNan %0: f32\n    %3 = spirv.IsNan %1: vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand `:` type($operand) attr-dict"
  },
  {
    "name": "spirv.ISub",
    "summary": "Integer subtraction of Operand 2 from Operand 1.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same number of components as Result\n    Type. They must have the same component width as Result Type.\n\n    The resulting value will equal the low-order N bits of the correct\n    result R, where N is the component width and R is computed with enough\n    precision to avoid overflow and underflow.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.ISub %0, %1 : i32\n    %5 = spirv.ISub %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.ISubBorrow",
    "summary": "Result is the unsigned integer subtraction of Operand 2 from Operand 1,\n    and what it needed to borrow.",
    "description": "Result Type must be from OpTypeStruct.  The struct must have two\n    members, and the two members must be the same type.  The member type\n    must be a scalar or vector of integer type, whose Signedness operand is\n    0.\n\n    Operand 1 and Operand 2 must have the same type as the members of Result\n    Type. These are consumed as unsigned integers.\n\n     Results are computed per component.\n\n    Member 0 of the result gets the low-order bits (full component width) of\n    the subtraction. That is, if Operand 1 is larger than Operand 2, member\n    0 gets the full value of the subtraction;  if Operand 2 is larger than\n    Operand 1, member 0 gets 2w + Operand 1 - Operand 2, where w is the\n    component width.\n\n    Member 1 of the result gets 0 if Operand 1 ≥ Operand 2, and gets 1\n    otherwise.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.ISubBorrow %0, %1 : !spirv.struct<(i32, i32)>\n    %2 = spirv.ISubBorrow %0, %1 : !spirv.struct<(vector<2xi32>, vector<2xi32>)>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyStruct" }
    ]
  },
  {
    "name": "spirv.Kill",
    "summary": "Deprecated (use OpTerminateInvocation or OpDemoteToHelperInvocation).",
    "description": "Fragment-shader discard.\n\n    Ceases all further processing in any invocation that executes it: Only\n    instructions these invocations executed before OpKill have observable\n    side effects. If this instruction is executed in non-uniform control\n    flow, all subsequent control flow is non-uniform (for invocations that\n    continue to execute).\n\n    This instruction must be the last instruction in a block.\n\n    This instruction is only valid in the Fragment Execution Model.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    spirv.Kill\n    ```",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "spirv.LogicalAnd",
    "summary": "Result is true if both Operand 1 and Operand 2 are true. Result is false\n    if either Operand 1 or Operand 2 are false.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 must be the same as Result Type.\n\n    The type of Operand 2 must be the same as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.LogicalAnd %0, %1 : i1\n    %2 = spirv.LogicalAnd %0, %1 : vector<4xi1>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.LogicalEqual",
    "summary": "Result is true if Operand 1 and Operand 2 have the same value. Result is\n    false if Operand 1 and Operand 2 have different values.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 must be the same as Result Type.\n\n    The type of Operand 2 must be the same as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.LogicalEqual %0, %1 : i1\n    %2 = spirv.LogicalEqual %0, %1 : vector<4xi1>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.LogicalNot",
    "summary": "Result is true if Operand is false.  Result is false if Operand is true.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand must be the same as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.LogicalNot %0 : i1\n    %2 = spirv.LogicalNot %0 : vector<4xi1>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand `:` type($operand) attr-dict"
  },
  {
    "name": "spirv.LogicalNotEqual",
    "summary": "Result is true if Operand 1 and Operand 2 have different values. Result\n    is false if Operand 1 and Operand 2 have the same value.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 must be the same as Result Type.\n\n    The type of Operand 2 must be the same as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.LogicalNotEqual %0, %1 : i1\n    %2 = spirv.LogicalNotEqual %0, %1 : vector<4xi1>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.LogicalOr",
    "summary": "Result is true if either Operand 1 or Operand 2 is true. Result is false\n    if both Operand 1 and Operand 2 are false.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 must be the same as Result Type.\n\n    The type of Operand 2 must be the same as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.LogicalOr %0, %1 : i1\n    %2 = spirv.LogicalOr %0, %1 : vector<4xi1>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.MemoryBarrier",
    "summary": "Control the order that memory accesses are observed.",
    "description": "Ensures that memory accesses issued before this instruction will be\n    observed before memory accesses issued after this instruction. This\n    control is ensured only for memory accesses issued by this invocation\n    and observed by another invocation executing within Memory scope. If the\n    Vulkan memory model is declared, this ordering only applies to memory\n    accesses that use the NonPrivatePointer memory operand or\n    NonPrivateTexel image operand.\n\n    Semantics declares what kind of memory is being controlled and what kind\n    of control to apply.\n\n    To execute both a memory barrier and a control barrier, see\n    OpControlBarrier.\n\n    #### Example:\n\n    ```mlir\n    spirv.MemoryBarrier \"Device\", \"Acquire|UniformMemory\"\n    ```",
    "attributes": [
      { "name": "memory_scope", "type": "SPIRV_ScopeAttr" },
      { "name": "memory_semantics", "type": "SPIRV_MemorySemanticsAttr" }
    ],
    "assemblyFormat": "$memory_scope `,` $memory_semantics attr-dict"
  },
  {
    "name": "spirv.mlir.addressof",
    "summary": "Get the address of a global variable.",
    "description": "Variables in module scope are defined using symbol names. This op generates\n    an SSA value that can be used to refer to the symbol within function scope\n    for use in ops that expect an SSA value. This operation has no corresponding\n    SPIR-V instruction; it's merely used for modelling purpose in the SPIR-V\n    dialect. Since variables in module scope in SPIR-V dialect are of pointer\n    type, this op returns a pointer type as well, and the type is the same as\n    the variable referenced.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.mlir.addressof @global_var : !spirv.ptr<f32, Input>\n    ```",
    "outputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" }
    ],
    "attributes": [
      { "name": "variable", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$variable attr-dict `:` type($pointer)"
  },
  {
    "name": "spirv.mlir.loop",
    "summary": "Define a structured loop.",
    "description": "SPIR-V can explicitly declare structured control-flow constructs using merge\n    instructions. These explicitly declare a header block before the control\n    flow diverges and a merge block where control flow subsequently converges.\n    These blocks delimit constructs that must nest, and can only be entered\n    and exited in structured ways. See \"2.11. Structured Control Flow\" of the\n    SPIR-V spec for more details.\n\n    Instead of having a `spirv.LoopMerge` op to directly model loop merge\n    instruction for indicating the merge and continue target, we use regions\n    to delimit the boundary of the loop: the merge target is the next op\n    following the `spirv.mlir.loop` op and the continue target is the block that\n    has a back-edge pointing to the entry block inside the `spirv.mlir.loop`'s region.\n    This way it's easier to discover all blocks belonging to a construct and\n    it plays nicer with the MLIR system.\n\n    The `spirv.mlir.loop` region should contain at least four blocks: one entry block,\n    one loop header block, one loop continue block, one loop merge block.\n    The entry block should be the first block and it should jump to the loop\n    header block, which is the second block. The loop merge block should be the\n    last block. The merge block should only contain a `spirv.mlir.merge` op.\n    The continue block should be the second to last block and it should have a\n    branch to the loop header block. The loop continue block should be the only\n    block, except the entry block, branching to the header block.\n\n    Values defined inside the loop regions cannot be directly used\n    outside of them; however, the loop region can yield values. These values are\n    yielded using a `spirv.mlir.merge` op and returned as a result of the loop op.",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "loop_control", "type": "SPIRV_LoopControlAttr" }
    ]
  },
  {
    "name": "spirv.mlir.merge",
    "summary": "A special terminator for merging a structured selection/loop.",
    "description": "We use `spirv.mlir.selection`/`spirv.mlir.loop` for modelling structured selection/loop.\n    This op is a terminator used inside their regions to mean jumping to the\n    merge point, which is the next op following the `spirv.mlir.selection` or\n    `spirv.mlir.loop` op. This op does not have a corresponding instruction in the\n    SPIR-V binary format; it's solely for structural purpose.\n\n    The instruction is also used to yield values from inside the selection/loop region\n    to the outside, as values that were sunk into the region cannot otherwise escape it.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "spirv.mlir.referenceof",
    "summary": "Reference a specialization constant.",
    "description": "Specialization constants in module scope are defined using symbol names.\n    This op generates an SSA value that can be used to refer to the symbol\n    within function scope for use in ops that expect an SSA value.\n    This operation has no corresponding SPIR-V instruction; it's merely used\n    for modelling purpose in the SPIR-V dialect. This op's return type is\n    the same as the specialization constant.\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.mlir.referenceof @spec_const : f32\n    ```\n\n    TODO Add support for composite specialization constants.",
    "outputs": [
      { "name": "reference", "type": "SPIRV_Type" }
    ],
    "attributes": [
      { "name": "spec_const", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$spec_const attr-dict `:` type($reference)"
  },
  {
    "name": "spirv.mlir.selection",
    "summary": "Define a structured selection.",
    "description": "SPIR-V can explicitly declare structured control-flow constructs using merge\n    instructions. These explicitly declare a header block before the control\n    flow diverges and a merge block where control flow subsequently converges.\n    These blocks delimit constructs that must nest, and can only be entered\n    and exited in structured ways. See \"2.11. Structured Control Flow\" of the\n    SPIR-V spec for more details.\n\n    Instead of having a `spirv.SelectionMerge` op to directly model selection\n    merge instruction for indicating the merge target, we use regions to delimit\n    the boundary of the selection: the merge target is the next op following the\n    `spirv.mlir.selection` op. This way it's easier to discover all blocks belonging to\n    the selection and it plays nicer with the MLIR system.\n\n    The `spirv.mlir.selection` region should contain at least two blocks: one selection\n    header block, and one selection merge. The selection header block should be\n    the first block. The selection merge block should be the last block.\n    The merge block should only contain a `spirv.mlir.merge` op.\n\n    Values defined inside the selection regions cannot be directly used\n    outside of them; however, the selection region can yield values. These values are\n    yielded using a `spirv.mlir.merge` op and returned as a result of the selection op.",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "selection_control", "type": "SPIRV_SelectionControlAttr" }
    ]
  },
  {
    "name": "spirv.mlir.yield",
    "summary": "Yields the result computed in `spirv.SpecConstantOperation`'s\n    region back to the parent op.",
    "description": "This op is a special terminator whose only purpose is to terminate\n    an `spirv.SpecConstantOperation`'s enclosed region. It accepts a\n    single operand produced by the preceeding (and only other) instruction\n    in its parent block (see SPIRV_SpecConstantOperation for further\n    details). This op has no corresponding SPIR-V instruction.\n\n    #### Example:\n\n    ```mlir\n    %0 = ... (some op supported by SPIR-V OpSpecConstantOp)\n    spirv.mlir.yield %0\n    ```",
    "inputs": [
      { "name": "operand", "type": "AnyType" }
    ],
    "assemblyFormat": "attr-dict $operand `:` type($operand)"
  },
  {
    "name": "spirv.module",
    "summary": "The top-level op that defines a SPIR-V module",
    "description": "This op defines a SPIR-V module using a MLIR region. The region contains\n    one block. Module-level operations, including functions definitions,\n    are all placed in this block.\n\n    Using an op with a region to define a SPIR-V module enables \"embedding\"\n    SPIR-V modules in other dialects in a clean manner: this op guarantees\n    the validity and serializability of a SPIR-V module and thus serves as\n    a clear-cut boundary.\n\n    This op takes no operands and generates no results. This op should not\n    implicitly capture values from the enclosing environment.\n\n    This op has only one region, which only contains one block. The block\n    has no terminator.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    addressing-model ::= `Logical` | `Physical32` | `Physical64` | ...\n    memory-model ::= `Simple` | `GLSL450` | `OpenCL` | `Vulkan` | ...\n    spv-module-op ::= `spirv.module` addressing-model memory-model\n                      (requires  spirv-vce-attribute)?\n                      (`attributes` attribute-dict)?\n                      region\n    ```\n\n    #### Example:\n\n    ```mlir\n    spirv.module Logical GLSL450  {}\n\n    spirv.module Logical Vulkan\n        requires #spirv.vce<v1.0, [Shader], [SPV_KHR_vulkan_memory_model]>\n        attributes { some_additional_attr = ... } {\n      spirv.func @do_nothing() -> () {\n        spirv.Return\n      }\n    }\n    ```",
    "attributes": [
      { "name": "addressing_model", "type": "SPIRV_AddressingModelAttr" },
      { "name": "memory_model", "type": "SPIRV_MemoryModelAttr" },
      { "name": "vce_triple", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "spirv.Not",
    "summary": "Complement the bits of Operand.",
    "description": "Results are computed per component, and within each component, per bit.\n\n    Result Type must be a scalar or vector of integer type.\n\n    Operand's type  must be a scalar or vector of integer type.  It must\n    have the same number of components as Result Type.  The component width\n    must equal the component width in Result Type.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.Not %0 : i32\n    %3 = spirv.Not %1 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand `:` type($operand) attr-dict"
  },
  {
    "name": "spirv.Ordered",
    "summary": "Result is true if both x == x and y == y are true, where IEEE comparison\n    is used, otherwise result is false.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    x must be a scalar or vector of floating-point type.  It must have the\n    same number of components as Result Type.\n\n    y must have the same type as x.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.Ordered %0, %1 : f32\n    %5 = spirv.Ordered %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.PtrCastToGeneric",
    "summary": "Convert a pointer’s Storage Class to Generic.",
    "description": "Result Type must be an OpTypePointer. Its Storage Class must be Generic.\n\n    Pointer must point to the Workgroup, CrossWorkgroup, or Function Storage\n    Class.\n\n    Result Type and Pointer must point to the same type.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.PtrCastToGenericOp %0 : !spirv.ptr<f32, CrossWorkGroup> to\n         !spirv.ptr<f32, Generic>\n    ```",
    "inputs": [
      { "name": "pointer", "type": "SPIRV_AnyPtr" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyPtr" }
    ],
    "assemblyFormat": "$pointer attr-dict `:` type($pointer) `to` type($result)"
  },
  {
    "name": "spirv.Return",
    "summary": "Return with no value from a function with void return type.",
    "description": "This instruction must be the last instruction in a block.\n\n    #### Example:\n\n    ```mlir\n    spirv.Return\n    ```",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "spirv.ReturnValue",
    "summary": "Return a value from a function.",
    "description": "Value is the value returned, by copy, and must match the Return Type\n    operand of the OpTypeFunction type of the OpFunction body this return\n    instruction is in.\n\n    This instruction must be the last instruction in a block.\n\n    #### Example:\n\n    ```mlir\n    spirv.ReturnValue %0 : f32\n    ```",
    "inputs": [
      { "name": "value", "type": "SPIRV_Type" }
    ],
    "assemblyFormat": "$value attr-dict `:` type($value)"
  },
  {
    "name": "spirv.SConvert",
    "summary": "Convert signed width.  This is either a truncate or a sign extend.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    Signed Value must be a scalar or vector of integer type.  It must have\n    the same number of components as Result Type.  The component width\n    cannot equal the component width in Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.SConvertOp %0 : i32 to i64\n    %3 = spirv.SConvertOp %2 : vector<3xi32> to vector<3xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.SDiv",
    "summary": "Signed-integer division of Operand 1 divided by Operand 2.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same number of components as Result\n    Type. They must have the same component width as Result Type.\n\n    Results are computed per component.  The resulting value is undefined\n    if Operand 2 is 0.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.SDiv %0, %1 : i32\n    %5 = spirv.SDiv %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.Select",
    "summary": "Select between two objects. Before version 1.4, results are only\n    computed per component.",
    "description": "Before version 1.4, Result Type must be a pointer, scalar, or vector.\n\n    The types of Object 1 and Object 2 must be the same as Result Type.\n\n    Condition must be a scalar or vector of Boolean type.\n\n    If Condition is a scalar and true, the result is Object 1. If Condition\n    is a scalar and false, the result is Object 2.\n\n    If Condition is a vector, Result Type must be a vector with the same\n    number of components as Condition and the result is a mix of Object 1\n    and Object 2: When a component of Condition is true, the corresponding\n    component in the result is taken from Object 1, otherwise it is taken\n    from Object 2.\n\n    #### Example:\n\n    ```mlir\n    %3 = spirv.Select %0, %1, %2 : i1, f32\n    %3 = spirv.Select %0, %1, %2 : i1, vector<3xi32>\n    %3 = spirv.Select %0, %1, %2 : vector<3xi1>, vector<3xf32>\n    ```",
    "inputs": [
      { "name": "condition", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "true_value", "type": "SPIRV_SelectType" },
      { "name": "false_value", "type": "SPIRV_SelectType" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_SelectType" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($condition) `,` type($result)"
  },
  {
    "name": "spirv.SGreaterThan",
    "summary": "Signed-integer comparison if Operand 1 is greater than  Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.SGreaterThan %0, %1 : i32\n    %5 = spirv.SGreaterThan %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.SGreaterThanEqual",
    "summary": "Signed-integer comparison if Operand 1 is greater than or equal to\n    Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.SGreaterThanEqual %0, %1 : i32\n    %5 = spirv.SGreaterThanEqual %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.ShiftLeftLogical",
    "summary": "Shift the bits in Base left by the number of bits specified in Shift.\n    The least-significant bits are zero filled.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of each Base and Shift must be a scalar or vector of integer\n    type. Base and Shift must have the same number of components.  The\n    number of components and bit width of the type of Base must be the same\n    as in Result Type.\n\n    Shift is treated as unsigned. The result is undefined if Shift is\n    greater than or equal to the bit width of the components of Base.\n\n    The number of components and bit width of Result Type must match those\n    Base type. All types must be integer types.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.ShiftLeftLogical %0, %1 : i32, i16\n    %5 = spirv.ShiftLeftLogical %3, %4 : vector<3xi32>, vector<3xi16>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($operand1) `,` type($operand2)"
  },
  {
    "name": "spirv.ShiftRightArithmetic",
    "summary": "Shift the bits in Base right by the number of bits specified in Shift.\n    The most-significant bits are filled with the sign bit from Base.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of each Base and Shift must be a scalar or vector of integer\n    type. Base and Shift must have the same number of components.  The\n    number of components and bit width of the type of Base must be the same\n    as in Result Type.\n\n    Shift is treated as unsigned. The result is undefined if Shift is\n    greater than or equal to the bit width of the components of Base.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.ShiftRightArithmetic %0, %1 : i32, i16\n    %5 = spirv.ShiftRightArithmetic %3, %4 : vector<3xi32>, vector<3xi16>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($operand1) `,` type($operand2)"
  },
  {
    "name": "spirv.ShiftRightLogical",
    "summary": "Shift the bits in Base right by the number of bits specified in Shift.\n    The most-significant bits are zero filled.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of each Base and Shift must be a scalar or vector of integer\n    type. Base and Shift must have the same number of components.  The\n    number of components and bit width of the type of Base must be the same\n    as in Result Type.\n\n    Shift is consumed as an unsigned integer. The result is undefined if\n    Shift is greater than or equal to the bit width of the components of\n    Base.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.ShiftRightLogical %0, %1 : i32, i16\n    %5 = spirv.ShiftRightLogical %3, %4 : vector<3xi32>, vector<3xi16>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($operand1) `,` type($operand2)"
  },
  {
    "name": "spirv.SLessThan",
    "summary": "Signed-integer comparison if Operand 1 is less than Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.SLessThan %0, %1 : i32\n    %5 = spirv.SLessThan %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.SLessThanEqual",
    "summary": "Signed-integer comparison if Operand 1 is less than or equal to Operand\n    2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.SLessThanEqual %0, %1 : i32\n    %5 = spirv.SLessThanEqual %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.SMod",
    "summary": "Signed remainder operation for the remainder whose sign matches the sign\n    of Operand 2.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same number of components as Result\n    Type. They must have the same component width as Result Type.\n\n    Results are computed per component.  The resulting value is undefined\n    if Operand 2 is 0.  Otherwise, the result is the remainder r of Operand\n    1 divided by Operand 2 where if r ≠ 0, the sign of r is the same as the\n    sign of Operand 2.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.SMod %0, %1 : i32\n    %5 = spirv.SMod %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.SMulExtended",
    "summary": "Result is the full value of the signed integer multiplication of Operand\n    1 and Operand 2.",
    "description": "Result Type must be from OpTypeStruct.  The struct must have two\n    members, and the two members must be the same type.  The member type\n    must be a scalar or vector of integer type.\n\n    Operand 1 and Operand 2 must have the same type as the members of Result\n    Type. These are consumed as signed integers.\n\n    Results are computed per component.\n\n    Member 0 of the result gets the low-order bits of the multiplication.\n\n    Member 1 of the result gets the high-order bits of the multiplication.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.SMulExtended %0, %1 : !spirv.struct<(i32, i32)>\n    %2 = spirv.SMulExtended %0, %1 : !spirv.struct<(vector<2xi32>, vector<2xi32>)>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyStruct" }
    ]
  },
  {
    "name": "spirv.SNegate",
    "summary": "Signed-integer subtract of Operand from zero.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    Operand's type  must be a scalar or vector of integer type.  It must\n    have the same number of components as Result Type.  The component width\n    must equal the component width in Result Type.\n\n     Results are computed per component.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.SNegate %0 : i32\n    %3 = spirv.SNegate %2 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.SpecConstant",
    "summary": "Declare a new integer-type or floating-point-type scalar specialization\n    constant.",
    "description": "This op declares a SPIR-V scalar specialization constant. SPIR-V has\n    multiple constant instructions covering different scalar types:\n\n    * `OpSpecConstantTrue` and `OpSpecConstantFalse` for boolean constants\n    * `OpSpecConstant` for scalar constants\n\n    Similar as `spirv.Constant`, this op represents all of the above cases.\n    `OpSpecConstantComposite` and `OpSpecConstantOp` are modelled with\n    separate ops.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    spv-spec-constant-op ::= `spirv.SpecConstant` symbol-ref-id\n                             `spec_id(` integer `)`\n                             `=` attribute-value (`:` spirv-type)?\n    ```\n\n    where `spec_id` specifies the SPIR-V SpecId decoration associated with\n    the op.\n\n    #### Example:\n\n    ```mlir\n    spirv.SpecConstant @spec_const1 = true\n    spirv.SpecConstant @spec_const2 spec_id(5) = 42 : i32\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "StrAttr" },
      { "name": "default_value", "type": "TypedAttrInterface" }
    ]
  },
  {
    "name": "spirv.SpecConstantComposite",
    "summary": "Declare a new composite specialization constant.",
    "description": "This op declares a SPIR-V composite specialization constant. This covers\n    the `OpSpecConstantComposite` SPIR-V instruction. Scalar constants are\n    covered by `spirv.SpecConstant`.\n\n    A constituent of a spec constant composite can be:\n    - A symbol referring of another spec constant.\n    - The SSA ID of a non-specialization constant (i.e. defined through\n      `spirv.SpecConstant`).\n    - The SSA ID of a `spirv.Undef`.\n\n    ```\n    spv-spec-constant-composite-op ::= `spirv.SpecConstantComposite` symbol-ref-id ` (`\n                                       symbol-ref-id (`, ` symbol-ref-id)*\n                                       `) :` composite-type\n    ```\n\n     where `composite-type` is some non-scalar type that can be represented in the `spv`\n     dialect: `spirv.struct`, `spirv.array`, or `vector`.\n\n     #### Example:\n\n     ```mlir\n     spirv.SpecConstant @sc1 = 1   : i32\n     spirv.SpecConstant @sc2 = 2.5 : f32\n     spirv.SpecConstant @sc3 = 3.5 : f32\n     spirv.SpecConstantComposite @scc (@sc1, @sc2, @sc3) : !spirv.struct<i32, f32, f32>\n     ```\n\n    TODO Add support for constituents that are:\n    - regular constants.\n    - undef.\n    - spec constant composite.",
    "attributes": [
      { "name": "type", "type": "TypeAttr" },
      { "name": "sym_name", "type": "StrAttr" },
      { "name": "constituents", "type": "SymbolRefArrayAttr" }
    ]
  },
  {
    "name": "spirv.SpecConstantOperation",
    "summary": "Declare a new specialization constant that results from doing an operation.",
    "description": "This op declares a SPIR-V specialization constant that results from\n    doing an operation on other constants (specialization or otherwise).\n\n    In the `spv` dialect, this op is modelled as follows:\n\n    ```\n    spv-spec-constant-operation-op ::= `spirv.SpecConstantOperation` `wraps`\n                                         generic-spirv-op `:` function-type\n    ```\n\n    In particular, an `spirv.SpecConstantOperation` contains exactly one\n    region. In turn, that region, contains exactly 2 instructions:\n    - One of SPIR-V's instructions that are allowed within an\n    OpSpecConstantOp.\n    - An `spirv.mlir.yield` instruction as the terminator.\n\n    The following SPIR-V instructions are valid:\n    - OpSConvert,\n    - OpUConvert,\n    - OpFConvert,\n    - OpSNegate,\n    - OpNot,\n    - OpIAdd,\n    - OpISub,\n    - OpIMul,\n    - OpUDiv,\n    - OpSDiv,\n    - OpUMod,\n    - OpSRem,\n    - OpSMod\n    - OpShiftRightLogical,\n    - OpShiftRightArithmetic,\n    - OpShiftLeftLogical\n    - OpBitwiseOr,\n    - OpBitwiseXor,\n    - OpBitwiseAnd\n    - OpVectorShuffle,\n    - OpCompositeExtract,\n    - OpCompositeInsert\n    - OpLogicalOr,\n    - OpLogicalAnd,\n    - OpLogicalNot,\n    - OpLogicalEqual,\n    - OpLogicalNotEqual\n    - OpSelect\n    - OpIEqual,\n    - OpINotEqual\n    - OpULessThan,\n    - OpSLessThan\n    - OpUGreaterThan,\n    - OpSGreaterThan\n    - OpULessThanEqual,\n    - OpSLessThanEqual\n    - OpUGreaterThanEqual,\n    - OpSGreaterThanEqual\n\n    TODO Add capability-specific ops when supported.\n\n    #### Example:\n    ```mlir\n    %0 = spirv.Constant 1: i32\n    %1 = spirv.Constant 1: i32\n\n    %2 = spirv.SpecConstantOperation wraps \"spirv.IAdd\"(%0, %1) : (i32, i32) -> i32\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ]
  },
  {
    "name": "spirv.SRem",
    "summary": "Signed remainder operation for the remainder whose sign matches the sign\n    of Operand 1.",
    "description": "Result Type must be a scalar or vector of integer type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same number of components as Result\n    Type. They must have the same component width as Result Type.\n\n    Results are computed per component.  The resulting value is undefined\n    if Operand 2 is 0.  Otherwise, the result is the remainder r of Operand\n    1 divided by Operand 2 where if r ≠ 0, the sign of r is the same as the\n    sign of Operand 1.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.SRem %0, %1 : i32\n    %5 = spirv.SRem %2, %3 : vector<4xi32>\n\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.UConvert",
    "summary": "Convert unsigned width. This is either a truncate or a zero extend.",
    "description": "Result Type must be a scalar or vector of integer type, whose Signedness\n    operand is 0.\n\n    Unsigned Value must be a scalar or vector of integer type.  It must have\n    the same number of components as Result Type.  The component width\n    cannot equal the component width in Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %1 = spirv.UConvertOp %0 : i32 to i64\n    %3 = spirv.UConvertOp %2 : vector<3xi32> to vector<3xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "spirv.UDiv",
    "summary": "Unsigned-integer division of Operand 1 divided by Operand 2.",
    "description": "Result Type must be a scalar or vector of integer type, whose Signedness\n    operand is 0.\n\n    The types of Operand 1 and Operand 2 both must be the same as Result\n    Type.\n\n    Results are computed per component.  The resulting value is undefined\n    if Operand 2 is 0.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.UDiv %0, %1 : i32\n    %5 = spirv.UDiv %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOrCoopMatrixOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.UGreaterThan",
    "summary": "Unsigned-integer comparison if Operand 1 is greater than  Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.UGreaterThan %0, %1 : i32\n    %5 = spirv.UGreaterThan %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.UGreaterThanEqual",
    "summary": "Unsigned-integer comparison if Operand 1 is greater than or equal to\n    Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.UGreaterThanEqual %0, %1 : i32\n    %5 = spirv.UGreaterThanEqual %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.ULessThan",
    "summary": "Unsigned-integer comparison if Operand 1 is less than Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.ULessThan %0, %1 : i32\n    %5 = spirv.ULessThan %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.ULessThanEqual",
    "summary": "Unsigned-integer comparison if Operand 1 is less than or equal to\n    Operand 2.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    The type of Operand 1 and Operand 2  must be a scalar or vector of\n    integer type.  They must have the same component width, and they must\n    have the same number of components as Result Type.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.ULessThanEqual %0, %1 : i32\n    %5 = spirv.ULessThanEqual %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.UMod",
    "summary": "Unsigned modulo operation of Operand 1 modulo Operand 2.",
    "description": "Result Type must be a scalar or vector of integer type, whose Signedness\n    operand is 0.\n\n    The types of Operand 1 and Operand 2 both must be the same as Result\n    Type.\n\n    Results are computed per component.  The resulting value is undefined\n    if Operand 2 is 0.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.UMod %0, %1 : i32\n    %5 = spirv.UMod %2, %3 : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "spirv.UMulExtended",
    "summary": "Result is the full value of the unsigned integer multiplication of\n    Operand 1 and Operand 2.",
    "description": "Result Type must be from OpTypeStruct.  The struct must have two\n    members, and the two members must be the same type.  The member type\n    must be a scalar or vector of integer type, whose Signedness operand is\n    0.\n\n    Operand 1 and Operand 2 must have the same type as the members of Result\n    Type. These are consumed as unsigned integers.\n\n    Results are computed per component.\n\n    Member 0 of the result gets the low-order bits of the multiplication.\n\n    Member 1 of the result gets the high-order bits of the multiplication.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %2 = spirv.UMulExtended %0, %1 : !spirv.struct<(i32, i32)>\n    %2 = spirv.UMulExtended %0, %1 : !spirv.struct<(vector<2xi32>, vector<2xi32>)>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_AnyStruct" }
    ]
  },
  {
    "name": "spirv.Unordered",
    "summary": "Result is true if either x or y is an IEEE NaN, otherwise result is\n    false.",
    "description": "Result Type must be a scalar or vector of Boolean type.\n\n    x must be a scalar or vector of floating-point type.  It must have the\n    same number of components as Result Type.\n\n    y must have the same type as x.\n\n    Results are computed per component.\n\n    #### Example:\n\n    ```mlir\n    %4 = spirv.Unordered %0, %1 : f32\n    %5 = spirv.Unordered %2, %3 : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "operand1", "type": "SPIRV_ScalarOrVectorOf" },
      { "name": "operand2", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_ScalarOrVectorOf" }
    ],
    "assemblyFormat": "$operand1 `,` $operand2 `:` type($operand1) attr-dict"
  },
  {
    "name": "spirv.Unreachable",
    "summary": "Behavior is undefined if this instruction is executed.",
    "description": "This instruction must be the last instruction in a block.",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "spirv.VectorExtractDynamic",
    "summary": "Extract a single, dynamically selected, component of a vector.",
    "description": "Result Type must be a scalar type.\n\n    Vector must have a type OpTypeVector whose Component Type is Result\n    Type.\n\n    Index must be a scalar integer. It is interpreted as a 0-based index of\n    which component of Vector to extract.\n\n    Behavior is undefined if Index's value is less than zero or greater than\n    or equal to the number of components in Vector.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```\n    %2 = spirv.VectorExtractDynamic %0[%1] : vector<8xf32>, i32\n    ```",
    "inputs": [
      { "name": "vector", "type": "SPIRV_Vector" },
      { "name": "index", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Scalar" }
    ],
    "assemblyFormat": "$vector `[` $index `]` attr-dict `:` type($vector) `,` type($index)"
  },
  {
    "name": "spirv.VectorInsertDynamic",
    "summary": "Make a copy of a vector, with a single, variably selected, component\n    modified.",
    "description": "Result Type must be an OpTypeVector.\n\n    Vector must have the same type as Result Type and is the vector that the\n    non-written components are copied from.\n\n    Component is the value supplied for the component selected by Index. It\n    must have the same type as the type of components in Result Type.\n\n    Index must be a scalar integer. It is interpreted as a 0-based index of\n    which component to modify.\n\n    Behavior is undefined if Index's value is less than zero or greater than\n    or equal to the number of components in Vector.\n\n    #### Example:\n\n    ```mlir\n    %scalar = ... : f32\n    %2 = spirv.VectorInsertDynamic %scalar %0[%1] : f32, vector<8xf32>, i32\n    ```",
    "inputs": [
      { "name": "vector", "type": "SPIRV_Vector" },
      { "name": "component", "type": "SPIRV_Scalar" },
      { "name": "index", "type": "SPIRV_Integer" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Vector" }
    ],
    "assemblyFormat": "$component `,` $vector `[` $index `]` attr-dict `:` type($vector) `,` type($index)"
  },
  {
    "name": "spirv.VectorShuffle",
    "summary": "Select arbitrary components from two vectors to make a new vector.",
    "description": "Result Type must be an OpTypeVector. The number of components in Result\n    Type must be the same as the number of Component operands.\n\n    Vector 1 and Vector 2 must both have vector types, with the same\n    Component Type as Result Type. They do not have to have the same number\n    of components as Result Type or with each other. They are logically\n    concatenated, forming a single vector with Vector 1's components\n    appearing before Vector 2's. The components of this logical vector are\n    logically numbered with a single consecutive set of numbers from 0 to N\n    - 1, where N is the total number of components.\n\n    Components are these logical numbers (see above), selecting which of the\n    logically numbered components form the result. Each component is an\n    unsigned 32-bit integer.  They can select the components in any order\n    and can repeat components. The first component of the result is selected\n    by the first Component operand,  the second component of the result is\n    selected by the second Component operand, etc. A Component literal may\n    also be FFFFFFFF, which means the corresponding result component has no\n    source and is undefined. All Component literals must either be FFFFFFFF\n    or in [0, N - 1] (inclusive).\n\n    Note: A vector “swizzle” can be done by using the vector for both Vector\n    operands, or using an OpUndef for one of the Vector operands.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.VectorShuffle [1: i32, 3: i32, 5: i32] %vector1, %vector2 :\n      vector<4xf32>, vector<2xf32> -> vector<3xf32>\n    ```",
    "inputs": [
      { "name": "vector1", "type": "SPIRV_Vector" },
      { "name": "vector2", "type": "SPIRV_Vector" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Vector" }
    ],
    "attributes": [
      { "name": "components", "type": "I32ArrayAttr" }
    ],
    "assemblyFormat": "attr-dict $components $vector1 `,` $vector2 `:`\n      type($vector1) `,` type($vector2) `->` type($result)"
  },
  {
    "name": "spirv.VectorTimesScalar",
    "summary": "Scale a floating-point vector.",
    "description": "Result Type must be a vector of floating-point type.\n\n     The type of Vector must be the same as Result Type. Each component of\n    Vector is multiplied by Scalar.\n\n    Scalar must have the same type as the Component Type in Result Type.\n\n    <!-- End of AutoGen section -->\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.VectorTimesScalar %vector, %scalar : vector<4xf32>\n    ```",
    "inputs": [
      { "name": "vector", "type": "VectorOfLengthAndType" },
      { "name": "scalar", "type": "SPIRV_Float" }
    ],
    "outputs": [
      { "name": "result", "type": "VectorOfLengthAndType" }
    ],
    "assemblyFormat": "operands attr-dict `:` `(` type(operands) `)` `->` type($result)"
  },
  {
    "name": "spirv.vendorName#.#mnemonic",
    "summary": "See extension SPV_KHR_shader_ballot",
    "description": "Computes a bitfield value combining the Predicate value from all invocations\n    in the current Subgroup that execute the same dynamic instance of this\n    instruction. The bit is set to one if the corresponding invocation is active\n    and the predicate is evaluated to true; otherwise, it is set to zero.\n\n    Predicate must be a Boolean type.\n\n    Result Type must be a 4 component vector of 32 bit integer types.\n\n    Result is a set of bitfields where the first invocation is represented in bit\n    0 of the first vector component and the last (up to SubgroupSize) is the\n    higher bit number of the last bitmask needed to represent all bits of the\n    subgroup invocations.\n\n    <!-- End of AutoGen section -->\n\n    ```\n    subgroup-ballot-op ::= ssa-id `=` `spirv.KHR.SubgroupBallot`\n                                ssa-use `:` `vector` `<` 4 `x` `i32` `>`\n    ```\n\n    #### Example:\n\n    ```mlir\n    %0 = spirv.KHR.SubgroupBallot %predicate : vector<4xi32>\n    ```",
    "inputs": [
      { "name": "predicate", "type": "SPIRV_Bool" }
    ],
    "outputs": [
      { "name": "result", "type": "SPIRV_Int32Vec4" }
    ],
    "assemblyFormat": "$predicate attr-dict `:` type($result)"
  },
  {
    "name": "stablehlo.abs",
    "summary": "Abs operation",
    "description": "Performs element-wise abs operation on `operand` tensor and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#abs\n\n    Example:\n    ```mlir\n    %result = stablehlo.abs %operand : tensor<3xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.add",
    "summary": "Add operation",
    "description": "Performs element-wise addition of two tensors `lhs` and `rhs` and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#add\n\n    Example:\n    ```mlir\n    %result = stablehlo.add %lhs, %rhs : tensor<2x2xi32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.after_all",
    "summary": "AfterAll operation",
    "description": "Ensures that the operations producing the `inputs` are executed before any\n    operations that depend on `result`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#after_all\n\n    Example:\n    ```mlir\n    %result = stablehlo.after_all %input0, %input1 : !stablehlo.token\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Token" }
    ],
    "assemblyFormat": "$inputs attr-dict\n      `:` custom<VariadicSameOperandsAndResultType>(ref($inputs), type($inputs), type($result))"
  },
  {
    "name": "stablehlo.all_gather",
    "summary": "AllGather operation",
    "description": "Within each process group in the process grid, concatenates the values of the\n    `operand` tensor from each process along `all_gather_dim` and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#all_gather\n\n    Example:\n    ```mlir\n    %result:2 = \"stablehlo.all_gather\"(%operand0, %operand1) {\n      all_gather_dim = 1 : i64,\n      replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>,\n      channel_handle = #stablehlo.channel_handle<handle = 0, type = 0>\n    } : (tensor<2x2xi64>, tensor<2x2xi64>) -> (tensor<2x4xi64>, tensor<2x4xi64>)\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "all_gather_dim", "type": "ConfinedAttr" },
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" },
      { "name": "use_global_device_ids", "type": "UnitAttr" }
    ]
  },
  {
    "name": "stablehlo.all_reduce",
    "summary": "AllReduce operation",
    "description": "Within each process group in the process grid, applies a reduction function\n    `computation` to the values of the `operand` tensor from each process and\n    produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#all_reduce\n\n    Example:\n    ```mlir\n    %result:2 = \"stablehlo.all_reduce\"(%operand0, %operand0) ({\n      ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):\n      %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<i64>, tensor<i64>) -> tensor<i64>\n      \"stablehlo.return\"(%0) : (tensor<i64>) -> ()\n    }) {\n      replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>,\n      channel_handle = #stablehlo.channel_handle<handle = 0, type = 0>\n    } : (tensor<4xi64>, tensor<4xi64>) -> (tensor<4xi64>, tensor<4xi64>)\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" },
      { "name": "use_global_device_ids", "type": "UnitAttr" }
    ]
  },
  {
    "name": "stablehlo.all_to_all",
    "summary": "AllToAll operation",
    "description": "Within each process group in the process grid, splits the values of the\n    `operand` tensor along `split_dimension` into parts, scatters the split parts\n    between the processes, concatenates the scattered parts along `concat_dimension`\n    and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#all_to_all\n\n    Example:\n    ```mlir\n    %result:2 = \"stablehlo.all_to_all\"(%operand1, %operand2) {\n      split_dimension = 1 : i64,\n      concat_dimension = 0 : i64,\n      split_count = 2 : i64,\n      replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>\n    } : (tensor<2x4xi64>, tensor<2x4xi64>) -> (tensor<4x2xi64>, tensor<4x2xi64>)\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "split_dimension", "type": "ConfinedAttr" },
      { "name": "concat_dimension", "type": "ConfinedAttr" },
      { "name": "split_count", "type": "ConfinedAttr" },
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.and",
    "summary": "And operation",
    "description": "Performs element-wise AND of two tensors `lhs` and `rhs` and produces a\n    `result` tensor\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#and\n\n    Example:\n    ```mlir\n    %result = stablehlo.and %lhs, %rhs : tensor<2x2xi32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_PredOrIntTensor" },
      { "name": "rhs", "type": "HLO_PredOrIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.atan2",
    "summary": "Atan2 operation",
    "description": "Performs element-wise atan2 operation on `lhs` and `rhs` tensor and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#atan2\n\n    Example:\n    ```mlir\n    %result = stablehlo.atan2 %lhs, %rhs : tensor<3xf64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.batch_norm_grad",
    "summary": "BatchNormGrad operation",
    "description": "Computes gradients of several inputs of BatchNormTrainingOp backpropagating\n    from `grad_output`, and produces `grad_operand`, `grad_scale` and\n    `grad_offset` tensors.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#batch_norm_grad\n\n    Example:\n    ```mlir\n    %grad_operand, %grad_scale, %grad_offset =\n    \"stablehlo.batch_norm_grad\"(%operand, %scale, %mean, %variance, %grad_output) {\n      epsilon = 0.0 : f32,\n      feature_index = 2 : i64\n    } : (tensor<2x2x2xf64>, tensor<2xf64>, tensor<2xf64>, tensor<2xf64>,\n         tensor<2x2x2xf64>) -> (tensor<2x2x2xf64>, tensor<2xf64>, tensor<2xf64>)\n    ```",
    "inputs": [
      { "name": "operand", "type": "RankedTensorOf" },
      { "name": "scale", "type": "DTensorOf" },
      { "name": "mean", "type": "DTensorOf" },
      { "name": "variance", "type": "DTensorOf" },
      { "name": "grad_output", "type": "RankedTensorOf" }
    ],
    "outputs": [
      { "name": "grad_operand", "type": "RankedTensorOf" },
      { "name": "grad_scale", "type": "DTensorOf" },
      { "name": "grad_offset", "type": "DTensorOf" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "feature_index", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "stablehlo.batch_norm_inference",
    "summary": "BatchNormInference operation",
    "description": "Normalizes the `operand` tensor across all dimensions except for the\n    `feature_index` dimension and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#batch_norm_inference\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.batch_norm_inference\"(%operand, %scale, %offset, %mean, %variance) {\n      epsilon = 0.0 : f32,\n      feature_index = 2 : i64\n    } : (tensor<2x2x2xf64>, tensor<2xf64>, tensor<2xf64>, tensor<2xf64>, tensor<2xf64>) -> tensor<2x2x2xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "RankedTensorOf" },
      { "name": "scale", "type": "DTensorOf" },
      { "name": "offset", "type": "DTensorOf" },
      { "name": "mean", "type": "DTensorOf" },
      { "name": "variance", "type": "DTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "RankedTensorOf" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "feature_index", "type": "ConfinedAttr" }
    ],
    "category": "Normalization"
  },
  {
    "name": "stablehlo.batch_norm_training",
    "summary": "BatchNormTraining operation",
    "description": "Computes mean and variance across batch and spatial dimensions and\n    normalizes the `operand` tensor, for each feature in the `feature_index`\n    dimension and produces `output`, `batch_mean` and `batch_var` tensors.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#batch_norm_training\n\n    Example:\n    ```mlir\n    %output, %batch_mean, %batch_var = \"stablehlo.batch_norm_training\"(%operand, %scale, %offset) {\n      epsilon = 0.0 : f32,\n      feature_index = 2 : i64\n    } : (tensor<2x2x2xf64>, tensor<2xf64>, tensor<2xf64>) ->\n        (tensor<2x2x2xf64>, tensor<2xf64>, tensor<2xf64>)\n    ```",
    "inputs": [
      { "name": "operand", "type": "RankedTensorOf" },
      { "name": "scale", "type": "DTensorOf" },
      { "name": "offset", "type": "DTensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "RankedTensorOf" },
      { "name": "batch_mean", "type": "DTensorOf" },
      { "name": "batch_var", "type": "DTensorOf" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "feature_index", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "stablehlo.bitcast_convert",
    "summary": "BitcastConvert operation",
    "description": "Performs a bitcast operation on `operand` tensor and produces a `result`\n    tensor where the bits of the entire `operand` tensor are reinterpreted using\n    the type of the `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#bitcast_convert\n\n    Example:\n    ```mlir\n    %result = stablehlo.bitcast_convert %operand : (tensor<f64>) -> tensor<4xf16>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrPerAxisQuantizedTensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.broadcast",
    "summary": "Broadcast operation",
    "description": "This operation is on its way out of StableHLO, so it is not included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/3.\n\n    Informally, this operation does the same thing as XLA's Broadcast:\n    https://www.tensorflow.org/xla/operation_semantics#broadcast\n\n    Example:\n    ```mlir\n    %result = stablehlo.broadcast %operand, sizes = [1, 2] : (tensor<3xi32>) -> tensor<1x2x3xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "broadcast_sizes", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand `,` `sizes` `=` $broadcast_sizes\n      attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.broadcast_in_dim",
    "summary": "BroadcastInDim operation",
    "description": "Expands the dimensions and/or rank of an input tensor by duplicating the\n    data in the `operand` tensor and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#broadcast_in_dim\n\n    Example:\n    ```mlir\n    %result = stablehlo.broadcast_in_dim %operand, dims = [2, 1] : (tensor<1x3xi32>) -> tensor<2x3x2xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrPerAxisQuantizedTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand `,` `dims` `=` $broadcast_dimensions\n      attr-dict `:` functional-type(operands, results)",
    "category": "Shape"
  },
  {
    "name": "stablehlo.case",
    "summary": "Case operation",
    "description": "Produces the output from executing exactly one `function` from `branches`\n    depending on the value of `index`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#case\n\n    Example:\n    ```mlir\n    %result0, %result1 = \"stablehlo.case\"(%index) ({\n      stablehlo.return %result_branch0, %result_branch0 : tensor<2xi64>, tensor<2xi64>\n    }, {\n      stablehlo.return %result_branch1, %result_branch1 : tensor<2xi64>, tensor<2xi64>\n    }) : (tensor<i32>) -> (tensor<2xi64>, tensor<2xi64>)\n    ```",
    "inputs": [
      { "name": "index", "type": "I32RankedTensor" }
    ]
  },
  {
    "name": "stablehlo.cbrt",
    "summary": "Cbrt operation",
    "description": "Performs element-wise cubic root operation on `operand` tensor and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#cbrt\n\n    Example:\n    ```mlir\n    %result = stablehlo.cbrt %operand : tensor<4xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.ceil",
    "summary": "Ceil operation",
    "description": "Performs element-wise ceil of `operand` tensor and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#ceil\n\n    Example:\n    ```mlir\n    %result = stablehlo.ceil %operand : tensor<5xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.cholesky",
    "summary": "Cholesky operation",
    "description": "Computes the Cholesky decomposition of a batch of matrices.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#cholesky\n\n    Example:\n    ```mlir\n    %result = stablehlo.cholesky %a, lower = true : tensor<3x3xf64>\n    ```",
    "inputs": [
      { "name": "a", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "lower", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$a (`,` `lower` `=` $lower^)? attr-dict `:` custom<SameOperandsAndResultType>(type($a), type($result))"
  },
  {
    "name": "stablehlo.clamp",
    "summary": "Clamp operation",
    "description": "Clamps every element of the `operand` tensor between a minimum and maximum\n    value and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#clamp\n\n    Example:\n    ```mlir\n    %result = stablehlo.clamp %min, %operand, %max : tensor<3xi32>\n    ```",
    "inputs": [
      { "name": "min", "type": "HLO_Tensor" },
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "max", "type": "HLO_Tensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "assemblyFormat": "$min `,` $operand `,` $max attr-dict\n      `:` custom<SameOperandsAndResultType>(type($min), type($operand), type($max), type($result))"
  },
  {
    "name": "stablehlo.collective_broadcast",
    "summary": "CollectiveBroadcast operation",
    "description": "Within each process group in the process grid, send the value of the\n    `operand` tensor from the source process to the target processes and produce a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#collective_broadcast\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.collective_broadcast\"(%operand) {\n      replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>,\n      channel_handle = #stablehlo.channel_handle<handle = 0, type = 0>\n    } : (tensor<1x2xi64>) -> tensor<1x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.collective_permute",
    "summary": "CollectivePermute operation",
    "description": "Within each process group in the process grid, sends the value of the\n    `operand` tensor from the source process to the target process and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#collective_permute\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.collective_permute\"(%operand) {\n      source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>,\n      channel_handle = #stablehlo.channel_handle<handle = 0, type = 0>\n    } : (tensor<2x2xi64>) -> tensor<2x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "source_target_pairs", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.compare",
    "summary": "Compare operation",
    "description": "Performs element-wise comparison of `lhs` and `rhs` tensors according to\n    `comparison_direction` and `compare_type`, and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#compare\n\n    Example:\n    ```mlir\n    %result = stablehlo.compare LT, %lhs, %rhs, FLOAT : (tensor<2xf32>, tensor<2xf32>) -> tensor<2xi1>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "comparison_direction", "type": "StableHLO_ComparisonDirectionAttr" },
      { "name": "compare_type", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$comparison_direction `,` $lhs `,` $rhs (`,` $compare_type^)?\n      attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.complex",
    "summary": "Complex operation",
    "description": "Performs element-wise conversion to a complex value from a pair of real and\n    imaginary values, `lhs` and `rhs`, and produces a `result` tensor.\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#complex\n    Example:\n    ```mlir\n    %result = stablehlo.complex %lhs, %rhs : tensor<2xcomplex<f64>>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_Fp32Or64Tensor" },
      { "name": "rhs", "type": "HLO_Fp32Or64Tensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_ComplexTensor" }
    ],
    "assemblyFormat": "operands attr-dict\n      `:` custom<ComplexOpType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.composite",
    "summary": "Composite operation",
    "description": "Encapsulates an operation made up (composed) of other StableHLO operations,\n    taking `inputs` and `composite_attributes` and producing `results`. The\n    semantics of the op are implemented by the `decomposition` attribute. The\n    `composite` op can be replaced with its decomposition without changing program\n    semantics. In cases where inlining the decomposition does not provide the same\n    op semantics, prefer using `custom_call`.\n\n    The `version` field (defaults to `0`) is used to denote when a composite's\n    semantics change.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#composite\n\n    Example:\n    ```mlir\n    %results = stablehlo.composite \"my.op\" %input0, %input1 {\n      composite_attributes = {\n        my_attribute = \"my_value\"\n      },\n      decomposition = @my_op,\n      version = 1 : i32\n    } : (tensor<f32>, tensor<f32>) -> tensor<f32>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" },
      { "name": "composite_attributes", "type": "DefaultValuedOptionalAttr" },
      { "name": "decomposition", "type": "FlatSymbolRefAttr" },
      { "name": "version", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$name $inputs attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.concatenate",
    "summary": "Concatenate operation",
    "description": "Concatenates a variadic number of tensors in `inputs` along `dimension`\n    dimension in the same order as the given arguments and produces a `result`\n    tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#concatenate\n\n    Example:\n    ```mlir\n    %result = stablehlo.concatenate %input0, %input1, dim = 0 : (tensor<3x2xi64>, tensor<1x2xi64>) -> tensor<4x2xi64>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "custom<VariadicOperandWithAttribute>($inputs) `dim` `=` $dimension attr-dict `:` functional-type(operands, results)",
    "category": "Tensor"
  },
  {
    "name": "stablehlo.constant",
    "summary": "Constant operation",
    "description": "Produces an `output` tensor from a constant `value`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#constant\n\n    Example:\n    ```mlir\n    %output = stablehlo.constant dense<[[0.0, 1.0], [2.0, 3.0]]> : tensor<2x2xf32>\n    ```",
    "outputs": [
      { "name": "output", "type": "HLO_StaticShapeTensorOrPerAxisQuantizedTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "ElementsAttr" }
    ]
  },
  {
    "name": "stablehlo.convert",
    "summary": "Convert operation",
    "description": "Performs an element-wise conversion from one element type to another on\n    `operand` tensor and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#convert\n\n    Example:\n    ```mlir\n    %result = stablehlo.convert %operand : (tensor<3xi64>) -> tensor<3xcomplex<f64>>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.convolution",
    "summary": "Convolution operation",
    "description": "Computes dot products between windows of `lhs` and slices of `rhs` and\n    produces `result`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#convolution\n\n    Example:\n    ```mlir\n    %result = stablehlo.convolution(%lhs, %rhs)\n      dim_numbers = [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f],\n      window = {\n        stride = [4, 4],\n        pad = [[0, 0], [0, 0]],\n        lhs_dilate = [2, 2],\n        rhs_dilate = [1, 1],\n        reverse = [0, 0]\n      } {\n        feature_group_count = 1 : i64,\n        batch_group_count = 1 : i64,\n        precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]\n      } :\n    (tensor<1x4x4x1xi64>, tensor<3x3x1x1xi64>) -> tensor<1x2x2x1xi64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_TensorOrPerAxisQuantizedTensor" },
      { "name": "dimension_numbers", "type": "StableHLO_ConvDimensionNumbers" }
    ],
    "attributes": [
      { "name": "window_strides", "type": "OptionalAttr" },
      { "name": "padding", "type": "OptionalAttr" },
      { "name": "lhs_dilation", "type": "OptionalAttr" },
      { "name": "rhs_dilation", "type": "OptionalAttr" },
      { "name": "window_reversal", "type": "OptionalAttr" },
      { "name": "feature_group_count", "type": "ConfinedAttr" },
      { "name": "batch_group_count", "type": "ConfinedAttr" },
      { "name": "precision_config", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`(`operands`)`\n       `dim_numbers` `=` custom<ConvolutionDimensions>($dimension_numbers) `,`\n       `window` `=` `{` custom<WindowAttributes>($window_strides, $padding,\n                                                 $lhs_dilation, $rhs_dilation,\n                                                 $window_reversal) `}`\n       attr-dict `:` functional-type(operands, results)",
    "category": "Layer"
  },
  {
    "name": "stablehlo.cosine",
    "summary": "Cosine operation",
    "description": "Performs element-wise cosine operation on `operand` tensor and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#cosine\n\n    Example:\n    ```mlir\n    %result = stablehlo.cosine %operand : tensor<2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.count_leading_zeros",
    "summary": "Clz operation",
    "description": "Performs element-wise count of the number of leading zero bits in the\n    `operand` tensor and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#count_leading_zeros\n\n    Example:\n    ```mlir\n    %result = stablehlo.count_leading_zeros %operand : tensor<2x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.create_token",
    "summary": "CreateToken operation",
    "description": "This operation is on its way out of StableHLO, so it is not included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/3.\n\n    Informally, this operation does the same thing as AfterAllOp with 0 inputs:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#after_all\n\n    Example:\n    ```mlir\n    %output = stablehlo.create_token : !stablehlo.token\n    ```",
    "outputs": [
      { "name": "output", "type": "HLO_Token" }
    ],
    "assemblyFormat": "attr-dict `:` type(results)"
  },
  {
    "name": "stablehlo.cross-replica-sum",
    "summary": "CrossReplicaSum operation",
    "description": "This operation is on its way out of StableHLO, so it is not included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/3.\n\n    Informally, this operation does the same thing as AllReduceOp with\n    `channel_id = 0`, `use_global_device_ids = false` and `computation`\n    implementing addition:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#all_reduce\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.cross-replica-sum\"(%operand) {\n      replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>\n    } : (tensor<4xf32>) -> tensor<4xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "replica_groups", "type": "I64ElementsAttr" }
    ]
  },
  {
    "name": "stablehlo.custom_call",
    "summary": "CustomCall operation",
    "description": "Encapsulates an implementation-defined operation `call_target_name` that\n    takes `inputs` and `called_computations` and produces `results`.\n\n    Depending on the API version there are two ways to pass extra bits of static\n    information to the external function:\n    1. Use `API_VERSION_TYPED_FFI` which allows passing a dictionary attribute.\n    2. Use a previous API version with a StringAttr to encode backend config.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#custom_call\n\n    Example:\n    ```mlir\n    %results = stablehlo.custom_call @foo(%input0) {\n      backend_config = {bar = 42 : i32},\n      api_version = 4 : i32,\n      called_computations = [@foo]\n    } : (tensor<f64>) -> tensor<f64>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "call_target_name", "type": "StrAttr" },
      { "name": "has_side_effect", "type": "DefaultValuedOptionalAttr" },
      { "name": "backend_config", "type": "OptionalAttr" },
      { "name": "api_version", "type": "DefaultValuedOptionalAttr" },
      { "name": "called_computations", "type": "DefaultValuedOptionalAttr" },
      { "name": "operand_layouts", "type": "OptionalAttr" },
      { "name": "result_layouts", "type": "OptionalAttr" },
      { "name": "output_operand_aliases", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "custom<CustomCallTarget>($call_target_name) `(` $inputs `)`\n      attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.divide",
    "summary": "Div operation",
    "description": "Performs element-wise division of dividend `lhs` and divisor `rhs` tensors\n    and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#divide\n\n    Example:\n    ```mlir\n    %result = stablehlo.divide %lhs, %rhs : tensor<4xf32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.dot",
    "summary": "Dot operation",
    "description": "This operation is on its way out of StableHLO, so it is not included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/3.\n\n    Informally, this operation does the same thing as XLA's Dot:\n    https://www.tensorflow.org/xla/operation_semantics#dot\n\n    Example:\n    ```mlir\n    %0 = stablehlo.dot %arg0, %arg1 : (tensor<1x2xi32>, tensor<2x1xi32>) -> tensor<1x1xi32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_TensorOrPerAxisQuantizedTensor" }
    ],
    "attributes": [
      { "name": "precision_config", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs `` custom<PrecisionConfig>($precision_config) attr-dict\n      `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.dot_general",
    "summary": "DotGeneral operation",
    "description": "Computes dot products between slices of `lhs` and slices of `rhs` and\n    produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dot_general\n\n    Example:\n    ```mlir\n    %result = stablehlo.dot_general %lhs, %rhs,\n      batching_dims = [0] x [0],\n      contracting_dims = [2] x [1],\n      precision = [DEFAULT, DEFAULT],\n      algorithm = <lhs_precision_type = tf32, rhs_precision_type = tf32, accumulation_type = f32, lhs_component_count = 1, rhs_component_count = 1, num_primitive_operations = 1, allow_imprecise_accumulation = false>\n      : (tensor<2x2x2xi64>, tensor<2x2x2xi64>) -> tensor<2x2x2xi64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_TensorOrPerAxisQuantizedTensor" },
      { "name": "dot_dimension_numbers", "type": "StableHLO_DotDimensionNumbers" }
    ],
    "attributes": [
      { "name": "precision_config", "type": "OptionalAttr" },
      { "name": "algorithm", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs `,` custom<DotDimensionNumbers>($dot_dimension_numbers) ``\n    custom<PrecisionConfigAndAlgorithm>($precision_config, $algorithm) attr-dict\n      `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.dynamic_broadcast_in_dim",
    "summary": "DynamicBroadcastInDim operation",
    "description": "This operation is functionally identical to\n    [broadcast_in_dim](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#broadcast_in_dim)\n    op, but the result shape is specified dynamically via `output_dimensions`.\n\n    It also accepts optional attributes to express static knowledge about the\n    expanding behavior of dimensions. If not specified, all dimensions are\n    assumed to be possibly expanding. The sets of dimensions that are known to\n    be expanding and the set of dimensions that are known to be non-expanding\n    must be disjoint and they must be a subset of the operand's dimensions.\n\n    See: https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dynamic_broadcast_in_dim\n\n    Example:\n    ```mlir\n    %operand = stablehlo.constant dense<[[1, 2, 3]]> : tensor<1x3xi64>\n    %output_dimensions = stablehlo.constant dense<[2, 3, 2]> : tensor<3xi64>\n    %result = \"stablehlo.dynamic_broadcast_in_dim\"(%operand, %output_dimensions) {\n      broadcast_dimensions = array<i64: 2, 1>,\n      known_expanding_dimensions = array<i64: 0>,\n      known_nonexpanding_dimensions = array<i64: 1>\n    } : (tensor<1x3xi64>, tensor<3xi64>) -> tensor<2x3x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrPerAxisQuantizedTensor" },
      { "name": "output_dimensions", "type": "HLO_StaticDimensionTensor" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "DenseI64ArrayAttr" },
      { "name": "known_expanding_dimensions", "type": "OptionalAttr" },
      { "name": "known_nonexpanding_dimensions", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$operand `,` $output_dimensions `,` `dims` `=` $broadcast_dimensions\n      attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.dynamic_conv",
    "summary": "DynamicConv operation",
    "description": "This operation is functionally identical to\n    [convolution](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#convolution)\n    op, but the padding is specified dynamically via `padding`.\n\n    Example:\n    ```mlir\n    %padding = stablehlo.constant dense<2> : tensor<2x2xi64>\n    %result = \"stablehlo.dynamic_conv\"(%lhs, %rhs, %padding) {\n      window_strides = array<i64: 4, 4>,\n      lhs_dilation = array<i64: 2, 2>,\n      rhs_dilation = array<i64: 1, 1>,\n      window_reversal = array<i1: false, false>,\n      dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,\n      batch_group_count = 1 : i64,\n      feature_group_count = 1 : i64,\n      precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>]\n    } : (tensor<1x4x4x1xi64>, tensor<3x3x1x1xi64>, tensor<2x2xi64>) -> tensor<1x2x2x1xi64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" },
      { "name": "padding", "type": "HLO_Static2DIntTensor" },
      { "name": "dimension_numbers", "type": "StableHLO_ConvDimensionNumbers" }
    ],
    "attributes": [
      { "name": "window_strides", "type": "OptionalAttr" },
      { "name": "lhs_dilation", "type": "OptionalAttr" },
      { "name": "rhs_dilation", "type": "OptionalAttr" },
      { "name": "window_reversal", "type": "OptionalAttr" },
      { "name": "feature_group_count", "type": "ConfinedAttr" },
      { "name": "batch_group_count", "type": "ConfinedAttr" },
      { "name": "precision_config", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.dynamic_gather",
    "summary": "DynamicGather operation",
    "description": "This operation is functionally identical to\n    [gather](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#gather)\n    op, with the `slice_sizes` specified dynamically as an operand.\n\n    Example:\n    ```mlir\n    %slice_sizes = stablehlo.constant dense<[1, 2, 2]> : tensor<3xi64>\n    %result = \"stablehlo.dynamic_gather\"(%operand, %start_indices, %slice_sizes) {\n      dimension_numbers = #stablehlo.gather<\n        offset_dims = [2, 3],\n        collapsed_slice_dims = [0],\n        start_index_map = [0, 2],\n        index_vector_dim = 2>,\n      indices_are_sorted = false\n    } : (tensor<3x4x2xi64>, tensor<2x3x2xi64>, tensor<3xi64>) -> tensor<2x3x2x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "HLO_IntTensor" },
      { "name": "slice_sizes", "type": "HLO_Static1DIntTensor" },
      { "name": "dimension_numbers", "type": "StableHLO_GatherDimensionNumbers" }
    ],
    "attributes": [
      { "name": "indices_are_sorted", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.dynamic_iota",
    "summary": "DynamicIota operation",
    "description": "This operation is functionally identical to\n    [iota](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#iota)\n    op, but the result shape is specified dynamically via `output_shape`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dynamic_iota\n\n    Example:\n    ```mlir\n    %output_shape = stablehlo.constant dense<[4, 5]> : tensor<2xi64>\n    %0 = stablehlo.dynamic_iota %output_shape, dim = 0 : (tensor<2xi64>) -> tensor<4x5xi64>\n    ```",
    "inputs": [
      { "name": "output_shape", "type": "HLO_StaticDimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "iota_dimension", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$output_shape `,` `dim` `=` $iota_dimension attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.dynamic_pad",
    "summary": "DynamicPad operation",
    "description": "This operation is functionally identical to\n    [pad](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#pad)\n    https://github.com/openxla/stablehlo/pull/2306#discussion_r1595669709\n    op, but with `edge_padding_low`, `edge_padding_high` and `interior_padding`\n    specified dynamically as values.\n\n    See: https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dynamic_pad\n\n    Example:\n    ```mlir\n    %edge_padding_low = stablehlo.constant dense<[0, 1]> : tensor<2xi32>\n    %edge_padding_high = stablehlo.constant dense<[2, 1]> : tensor<2xi32>\n    %interior_padding = stablehlo.constant dense<[1, 2]> : tensor<2xi32>\n    %result = stablehlo.dynamic_pad %operand, %padding_value,\n                %edge_padding_low, %edge_padding_high, %interior_padding\n                : (tensor<2x3xi64>, tensor<i64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<5x9xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "padding_value", "type": "HLO_ScalarTensor" },
      { "name": "edge_padding_low", "type": "HLO_StaticDimensionTensor" },
      { "name": "edge_padding_high", "type": "HLO_StaticDimensionTensor" },
      { "name": "interior_padding", "type": "HLO_StaticDimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.dynamic_reshape",
    "summary": "DynamicReshape operation",
    "description": "This operation is functionally identical to\n    [reshape](https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reshape)\n    op, but the result shape is specified dynamically via `output_shape`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dynamic_reshape\n\n    Example:\n    ```mlir\n    %output_shape = stablehlo.constant dense<[3, 2]> : tensor<2xi64>\n    %result = stablehlo.dynamic_reshape %operand, %output_shape : (tensor<2x3xi64>, tensor<2xi64>) -> tensor<3x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrPerAxisQuantizedTensor" },
      { "name": "output_shape", "type": "HLO_StaticDimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_TensorOrPerAxisQuantizedTensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Shape"
  },
  {
    "name": "stablehlo.dynamic_slice",
    "summary": "DynamicSlice operation",
    "description": "Extracts a slice from the `operand` using dynamically-computed starting\n    indices and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dynamic_slice\n\n    Example:\n    ```mlir\n    %result = stablehlo.dynamic_slice %operand, %start_indices0, %start_indices1, sizes = [2, 2]\n      : (tensor<4x4xi32>, tensor<i64>, tensor<i64>) -> tensor<2x2xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "slice_sizes", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand `,` custom<VariadicOperandWithAttribute>($start_indices)\n      `sizes` `=` $slice_sizes attr-dict `:` functional-type(operands, results)",
    "category": "Tensor"
  },
  {
    "name": "stablehlo.dynamic_update_slice",
    "summary": "DynamicUpdateSlice operation",
    "description": "Produces a `result` tensor which is equal to the `operand` tensor except\n    that the slice starting at `start_indices` is updated with the values in\n    `update`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#dynamic_update_slice\n\n    Example:\n    ```mlir\n    %result = stablehlo.dynamic_update_slice %operand, %update, %start_indices0, %start_indices1\n      : (tensor<4x4xi32>, tensor<2x2xi32>, tensor<i64>, tensor<i64>) -> tensor<4x4xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "update", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.einsum",
    "summary": "Einsum operation",
    "description": "This operation is on its way out of StableHLO, so it is not included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/3.\n\n    Informally, this operation does the same thing as TF's einsum:\n    https://www.tensorflow.org/api_docs/python/tf/einsum\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.einsum\"(%lhs, %rhs) {\n      einsum_config = \"ab,bc->ac\"\n    } : (tensor<4x16xf32>, tensor<16x4xf32>) -> tensor<4x4xf32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_Tensor" },
      { "name": "rhs", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "einsum_config", "type": "StrAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs `,` `config` `=` $einsum_config attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.exponential",
    "summary": "Exp operation",
    "description": "Performs element-wise exponential operation on `operand` tensor and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#exponential\n\n    Example:\n    ```mlir\n    %result = stablehlo.exponential %operand : tensor<2x2xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.exponential_minus_one",
    "summary": "Expm1 operation",
    "description": "Performs element-wise exponential minus one operation on `operand` tensor\n    and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#exponential_minus_one\n\n    Example:\n    ```mlir\n    %result = stablehlo.exponential_minus_one %operand : tensor<2xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.fft",
    "summary": "Fft operation",
    "description": "Performs the forward and inverse Fourier transforms for real and complex\n    inputs/outputs.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#fft\n\n    Example:\n    ```mlir\n    %result = stablehlo.fft %operand, type = FFT, length = [4] : (tensor<4xcomplex<f32>>) -> tensor<4xcomplex<f32>>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpOrComplexTensor" }
    ],
    "attributes": [
      { "name": "fft_type", "type": "StableHLO_FftTypeAttr" },
      { "name": "fft_length", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand `,` `type` `=` $fft_type `,` `length` `=` $fft_length\n      attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.floor",
    "summary": "Floor operation",
    "description": "Performs element-wise floor of `operand` tensor and produces a `result`\n    tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#floor\n\n    Example:\n    ```mlir\n    %result = stablehlo.floor %operand : tensor<2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.gather",
    "summary": "Gather operation",
    "description": "Gathers slices from `operand` tensor from offsets specified in\n    `start_indices` and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#gather\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.gather\"(%operand, %start_indices) {\n      dimension_numbers = #stablehlo.gather<\n        offset_dims = [3, 4],\n        collapsed_slice_dims = [1],\n        operand_batching_dims = [0],\n        start_indices_batching_dims = [1],\n        start_index_map = [2, 1],\n        index_vector_dim = 3>,\n      slice_sizes = array<i64: 1, 1, 2, 2>,\n      indices_are_sorted = false\n    } : (tensor<2x3x4x2xi64>, tensor<2x2x3x2xi64>) -> tensor<2x2x3x2x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "HLO_IntTensor" },
      { "name": "dimension_numbers", "type": "StableHLO_GatherDimensionNumbers" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "slice_sizes", "type": "DenseI64ArrayAttr" },
      { "name": "indices_are_sorted", "type": "DefaultValuedOptionalAttr" }
    ],
    "category": "Tensor"
  },
  {
    "name": "stablehlo.get_dimension_size",
    "summary": "GetDimensionSize operation",
    "description": "Produces the size of the given `dimension` of the `operand`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#get_dimension_size\n\n    Example:\n    ```mlir\n    %result = stablehlo.get_dimension_size %operand, dim = 1 : (tensor<2x3xi64>) -> tensor<i32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrPerAxisQuantizedTensor" }
    ],
    "attributes": [
      { "name": "dimension", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$operand `,` `dim` `=` $dimension attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.get_tuple_element",
    "summary": "GetTupleElement operation",
    "description": "Extracts element at `index` position of the `operand` tuple and produces a\n    `result`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#get_tuple_element\n\n    Example:\n    ```mlir\n    %result = stablehlo.get_tuple_element %operand[0] : (tuple<tensor<2xf64>, tuple<tensor<i64>>>) -> tensor<2xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tuple" }
    ],
    "attributes": [
      { "name": "index", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$operand `[` $index `]` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.if",
    "summary": "If operation",
    "description": "Produces the output from executing exactly one branch from `true_branch` or\n    `false_branch` depending on the value of `pred`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#if\n\n    Example:\n    %result = \"stablehlo.if\"(%pred) ({\n      \"stablehlo.return\"(%result_true_branch) : (tensor<i32>) -> ()\n    }, {\n      \"stablehlo.return\"(%result_false_branch) : (tensor<i32>) -> ()\n    }) : (tensor<i1>) -> tensor<i32>",
    "inputs": [
      { "name": "pred", "type": "HLO_PredTensor" }
    ]
  },
  {
    "name": "stablehlo.imag",
    "summary": "Imag operation",
    "description": "Extracts the imaginary part, element-wise, from the `operand` and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#imag\n\n    Example:\n    ```mlir\n    %result = stablehlo.imag %operand : (tensor<2xcomplex<f32>>) -> tensor<2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.infeed",
    "summary": "Infeed operation",
    "description": "Reads data from the infeed and produces `results`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#infeed\n\n    Example:\n    ```mlir\n    %results0:2 = \"stablehlo.infeed\"(%token) :\n        (!stablehlo.token) -> (tensor<2x2xi64>, !stablehlo.token)\n    ```",
    "inputs": [
      { "name": "token", "type": "HLO_Token" }
    ],
    "attributes": [
      { "name": "infeed_config", "type": "DefaultValuedStrAttr" },
      { "name": "layout", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.iota",
    "summary": "Iota operation",
    "description": "Fills an `output` tensor with values in increasing order starting from zero\n    along the `iota_dimension` dimension.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#iota\n\n    Example:\n    ```mlir\n    %output = stablehlo.iota dim = 0 : tensor<4x5xi32>\n    ```",
    "outputs": [
      { "name": "output", "type": "HLO_StaticShapeIntFpComplexOrQuantizedTensor" }
    ],
    "attributes": [
      { "name": "iota_dimension", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "`dim` `=` $iota_dimension attr-dict `:` type($output)"
  },
  {
    "name": "stablehlo.is_finite",
    "summary": "IsFinite operation",
    "description": "Performs element-wise check whether the value in `x` is finite (i.e. is\n    neither +Inf, -Inf, nor NaN) and produces a `y` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#is_finite\n\n    Example:\n    ```mlir\n    %y = stablehlo.is_finite %x : (tensor<7xf64>) -> tensor<7xi1>\n    ```",
    "inputs": [
      { "name": "x", "type": "HLO_FpOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "HLO_PredTensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.log",
    "summary": "Log operation",
    "description": "Performs element-wise logarithm operation on `operand` tensor and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#log\n\n    Example:\n    ```mlir\n    %result = stablehlo.log %operand : tensor<2x2xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.log_plus_one",
    "summary": "Log1p operation",
    "description": "Performs element-wise logarithm plus one operation on `operand` tensor and\n    produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#log_plus_one\n\n    Example:\n    ```mlir\n    %result = stablehlo.log_plus_one %operand : tensor<5xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.logistic",
    "summary": "Logistic operation",
    "description": "Performs element-wise logistic operation on `operand` tensor and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#logistic\n\n    Example:\n    ```mlir\n    %result = stablehlo.logistic %operand : tensor<2x2xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.map",
    "summary": "Map operation",
    "description": "Applies a map function `computation` to `inputs` along the `dimensions` and\n    produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#map\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.map\"(%input0, %input1) ({\n      ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):\n        %0 = stablehlo.multiply %arg0, %arg1 : tensor<i64>\n        stablehlo.return %0 : tensor<i64>\n    }) {\n      dimensions = array<i64: 0, 1>\n    } : (tensor<2x2xi64>, tensor<2x2xi64>) -> tensor<2x2xi64>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimensions", "type": "DenseI64ArrayAttr" }
    ]
  },
  {
    "name": "stablehlo.maximum",
    "summary": "Max operation",
    "description": "Performs element-wise max operation on tensors `lhs` and `rhs` and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#maximum\n\n    Example:\n    ```mlir\n    %result = stablehlo.maximum %lhs, %rhs : tensor<4xf32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.minimum",
    "summary": "Min operation",
    "description": "Performs element-wise min operation on tensors `lhs` and `rhs` and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#minimum\n\n    Example:\n    ```mlir\n    %result = stablehlo.minimum %lhs, %rhs : tensor<4xf32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.multiply",
    "summary": "Mul operation",
    "description": "Performs element-wise product of two tensors `lhs` and `rhs` and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#multiply\n\n    Example:\n    ```mlir\n    %result = stablehlo.multiply %lhs, %rhs : tensor<2xi32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.negate",
    "summary": "Neg operation",
    "description": "Performs element-wise negation of `operand` tensor and produces a `result`\n    tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#negate\n\n    Example:\n    ```mlir\n    %result = stablehlo.negate %operand : tensor<2x3xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.not",
    "summary": "Not operation",
    "description": "Performs element-wise NOT of tensor `operand` of type integer and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#not\n\n    Example:\n    ```mlir\n    %result = stablehlo.not %operand : tensor<5x3x1xi1>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.optimization_barrier",
    "summary": "OptimizationBarrier operation",
    "description": "Ensures that the operations that produce the `operand` are executed before any\n    operations that depend on the `result` and prevents compiler transformations\n    from moving operations across the barrier. Other than that, the operation is\n    an identity, i.e. `result` = `operand`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#optimization_barrier\n\n    Example:\n    ```mlir\n    %result0, %result1 = stablehlo.optimization_barrier %operand0, %operand1 : tensor<f32>, tensor<f32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operand^ `:` custom<PairwiseOpType>(type($operand), type($result))):(`(` `)`)?"
  },
  {
    "name": "stablehlo.or",
    "summary": "Or operation",
    "description": "Performs element-wise OR of two tensors `lhs` and `rhs` and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#or\n\n    Example:\n    ```mlir\n    %result = stablehlo.or %lhs, %rhs : tensor<2xi1>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_PredOrIntTensor" },
      { "name": "rhs", "type": "HLO_PredOrIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.outfeed",
    "summary": "Outfeed operation",
    "description": "Writes `inputs` to the outfeed and produces a `result` token.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#outfeed\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.outfeed\"(%input0, %token) :\n        (tensor<2x2x2xi64>, !stablehlo.token) -> !stablehlo.token\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "token", "type": "HLO_Token" }
    ],
    "attributes": [
      { "name": "outfeed_config", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "stablehlo.pad",
    "summary": "Pad operation",
    "description": "Expands `operand` by padding around the tensor as well as between the\n    elements of the tensor with the given `padding_value`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#pad\n\n    Example:\n    ```mlir\n    %0 = stablehlo.pad %arg0, %arg1, low = [0, 1], high = [2, 1], interior = [1, 2]\n      : (tensor<2x3xi32>, tensor<i32>) -> tensor<5x9xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "padding_value", "type": "HLO_ScalarTensor" }
    ],
    "attributes": [
      { "name": "edge_padding_low", "type": "DenseI64ArrayAttr" },
      { "name": "edge_padding_high", "type": "DenseI64ArrayAttr" },
      { "name": "interior_padding", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand `,` $padding_value `,`\n      `low` `=` $edge_padding_low `,`\n      `high` `=` $edge_padding_high `,`\n      `interior` `=` $interior_padding\n      attr-dict `:` functional-type(operands, results)",
    "category": "Transform"
  },
  {
    "name": "stablehlo.partition_id",
    "summary": "PartitionId operation",
    "description": "Produces `partition_id` of the current process.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#partition_id\n\n    Example:\n    ```mlir\n    %result = stablehlo.partition_id : tensor<ui32>\n    ```",
    "assemblyFormat": "attr-dict `:` type(results)"
  },
  {
    "name": "stablehlo.popcnt",
    "summary": "PopulationCount operation",
    "description": "Performs element-wise count of the number of bits set in the `operand`\n    tensor and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#popcnt\n\n    Example:\n    ```mlir\n    %result = stablehlo.popcnt %operand : tensor<4xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.power",
    "summary": "Power operation",
    "description": "Performs element-wise exponentiation of `lhs` tensor by `rhs` tensor and\n    produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#power\n\n    Example:\n    ```mlir\n    %result = stablehlo.power %lhs, %rhs : tensor<6xf64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.real",
    "summary": "Real operation",
    "description": "Extracts the real part, element-wise, from the `operand` and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#real\n\n    Example:\n    ```mlir\n    %result = stablehlo.real %operand : (tensor<2xcomplex<f32>>) -> tensor<2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.real_dynamic_slice",
    "summary": "RealDynamicSlice operation",
    "description": "This operation is a work in progress, so it is not yet included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/8.\n\n    Informally, this operation does the same thing as SliceOp except\n    that `start_indices`, `limit_indices` and `strides` are specified dynamically:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#slice\n\n    Example:\n    ```mlir\n    %result = stablehlo.real_dynamic_slice %operand,\n                %start_indices, %limit_indices, %strides\n           : (tensor<256x?xf32>, tensor<2xindex>, tensor<2xindex>, tensor<2xindex>) -> tensor<256x?xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "start_indices", "type": "HLO_DimensionTensor" },
      { "name": "limit_indices", "type": "HLO_DimensionTensor" },
      { "name": "strides", "type": "HLO_DimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.recv",
    "summary": "Recv operation",
    "description": "Receives data from a channel with `channel_id` and produces `results`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#recv\n\n    Example:\n    ```mlir\n    %results:2 = \"stablehlo.recv\"(%token) {\n      channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>,\n      is_host_transfer = false,\n      source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>\n    } : (!stablehlo.token) -> (tensor<2x2xi64>, !stablehlo.token)\n    ```",
    "inputs": [
      { "name": "token", "type": "HLO_Token" },
      { "name": "channel_handle", "type": "StableHLO_ChannelHandle" }
    ],
    "attributes": [
      { "name": "is_host_transfer", "type": "DefaultValuedOptionalAttr" },
      { "name": "source_target_pairs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.reduce",
    "summary": "Reduce operation",
    "description": "Applies a reduction function `body` to `inputs` and `init_values` along the\n    `dimensions` and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reduce\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.reduce\"(%input, %init_value) ({\n      ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):\n        %0 = stablehlo.add %arg0, %arg1 : tensor<i64>\n        stablehlo.return %0 : tensor<i64>\n    }) {\n      dimensions = array<i64: 1>\n    } : (tensor<1x6xi64>, tensor<i64>) -> tensor<1xi64>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "init_values", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimensions", "type": "DenseI64ArrayAttr" }
    ]
  },
  {
    "name": "stablehlo.reduce_precision",
    "summary": "ReducePrecision operation",
    "description": "Performs element-wise conversion of `operand` to another floating-point type\n    that uses `exponent_bits` and `mantissa_bits` and back to the original\n    floating-point type and produces an `output` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reduce_precision\n\n    Example:\n    ```mlir\n    %output = stablehlo.reduce_precision %operand, format = e5m10 : tensor<6xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "HLO_FpOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "exponent_bits", "type": "ConfinedAttr" },
      { "name": "mantissa_bits", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$operand `,` `format` `=` custom<ExponentMantissa>($exponent_bits, $mantissa_bits)\n      attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($output))"
  },
  {
    "name": "stablehlo.reduce_scatter",
    "summary": "ReduceScatter operation",
    "description": "Within each process group in the process grid, performs reduction, using\n     `computations`, over the values of the `operand` tensor from each process,\n     splits the reduction result along `scatter_dimension` into parts, and\n     scatters the split parts between the processes to produce the `result`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reduce_scatter\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.reduce_scatter\"(%operand) ({\n      ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):\n      %0 = stablehlo.add %arg0, %arg1 : tensor<i64>\n      stablehlo.return %0 : tensor<i64>\n    }) {\n      scatter_dimension = 1 : i64,\n      replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>,\n      channel_handle = #stablehlo.channel_handle<handle = 0, type = 0>\n    } : (tensor<2x4xi64>) -> tensor<2x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "scatter_dimension", "type": "ConfinedAttr" },
      { "name": "replica_groups", "type": "I64ElementsAttr" },
      { "name": "channel_handle", "type": "OptionalAttr" },
      { "name": "use_global_device_ids", "type": "UnitAttr" }
    ]
  },
  {
    "name": "stablehlo.reduce_window",
    "summary": "ReduceWindow operation",
    "description": "Applies a reduction function `body` to windows of `inputs` and `init_values`\n    and produces `results`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reduce_window\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.reduce_window\"(%input, %init_value) ({\n      ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):\n        %0 = stablehlo.add %arg0, %arg1 : tensor<i64>\n        stablehlo.return %0 : tensor<i64>\n    }) {\n      window_dimensions = array<i64: 2, 1>,\n      window_strides = array<i64: 4, 1>,\n      base_dilations = array<i64: 2, 1>,\n      window_dilations = array<i64: 3, 1>,\n      padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>\n    } : (tensor<3x2xi64>, tensor<i64>) -> tensor<2x2xi64>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "init_values", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "window_dimensions", "type": "DenseI64ArrayAttr" },
      { "name": "window_strides", "type": "OptionalAttr" },
      { "name": "base_dilations", "type": "OptionalAttr" },
      { "name": "window_dilations", "type": "OptionalAttr" },
      { "name": "padding", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.remainder",
    "summary": "Rem operation",
    "description": "Performs element-wise remainder of dividend `lhs` and divisor `rhs` tensors\n    and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#remainder\n\n    Example:\n    ```mlir\n    %result = stablehlo.remainder %lhs, %rhs : tensor<4xi64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.replica_id",
    "summary": "ReplicaId operation",
    "description": "Produces `replica_id` of the current process.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#replica_id\n\n    Example:\n    ```mlir\n    %result = stablehlo.replica_id : tensor<ui32>\n    ```",
    "assemblyFormat": "attr-dict `:` type(results)"
  },
  {
    "name": "stablehlo.reshape",
    "summary": "Reshape operation",
    "description": "Performs reshape of `operand` tensor to a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reshape\n\n    Example:\n    ```mlir\n    %result = stablehlo.reshape %operand : (tensor<2xf32>) -> tensor<1x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrPerAxisQuantizedTensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Shape"
  },
  {
    "name": "stablehlo.return",
    "summary": "This operation is a work in progress, so it is not yet included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/425.\n\n    Informally, this operation serves as a terminator for regions defined by\n    the StableHLO ops. Non-StableHLO ops, e.g. `func.func`, have their own\n    terminators, e.g. `func.return`.\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.reduce\"(%input, %init_value) ({\n      ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):\n        %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>\n        \"stablehlo.return\"(%0) : (tensor<i32>) -> ()\n    }) {\n      dimensions = array<i64: 1>\n    } : (tensor<1x6xi32>, tensor<i32>) -> tensor<1xi32>\n    ```",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$results attr-dict (`:` type($results)^)?"
  },
  {
    "name": "stablehlo.reverse",
    "summary": "Reverse operation",
    "description": "Reverses the order of elements in the `operand` along the specified\n    `dimensions` and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reverse\n\n    Example:\n    ```mlir\n    %result = stablehlo.reverse %operand, dims = [1] : tensor<3x2xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "dimensions", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand `,` `dims` `=` $dimensions\n      attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))",
    "category": "Transform"
  },
  {
    "name": "stablehlo.rng",
    "summary": "Rng operation",
    "description": "Generates random numbers using the `rng_distribution` algorithm and produces\n    a `result` tensor of a given shape `shape`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#rng\n\n    Example:\n    ```mlir\n    %result = stablehlo.rng %a, %b, %shape, distribution = NORMAL : (tensor<i32>, tensor<i32>, tensor<2xi64>) -> tensor<3x3xi32>\n    ```",
    "inputs": [
      { "name": "a", "type": "DTensorOf" },
      { "name": "b", "type": "DTensorOf" },
      { "name": "shape", "type": "HLO_StaticDimensionTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_PredIntOrFpTensor" }
    ],
    "attributes": [
      { "name": "rng_distribution", "type": "StableHLO_RngDistributionAttr" }
    ],
    "assemblyFormat": "$a `,` $b `,` $shape `,` `distribution` `=` $rng_distribution\n      attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.rng_bit_generator",
    "summary": "RngBitGenerator operation",
    "description": "Returns an `output` filled with uniform random data and an updated output\n    state `output_state` given an initial state `initial_state` using the\n    pseudorandom number generator algorithm `rng_algorithm`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#rng_bit_generator\n\n    Example:\n    ```mlir\n    %output_state, %output = stablehlo.rng_bit_generator %initial_state, algorithm = THREE_FRY : (tensor<2xui64>) -> (tensor<2xui64>, tensor<2x2xui64>)\n    ```",
    "inputs": [
      { "name": "initial_state", "type": "HLO_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "output_state", "type": "HLO_IntOrFpTensor" },
      { "name": "output", "type": "HLO_StaticShapeIntOrFpTensor" }
    ],
    "attributes": [
      { "name": "rng_algorithm", "type": "StableHLO_RngAlgorithmAttr" }
    ],
    "assemblyFormat": "$initial_state `,` `algorithm` `=` $rng_algorithm attr-dict\n      `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.round_nearest_afz",
    "summary": "Round operation",
    "description": "Performs element-wise rounding towards the nearest integer, breaking ties\n    away from zero, on the `operand` tensor and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#round_nearest_afz\n\n    Example:\n    ```mlir\n    %result = stablehlo.round_nearest_afz %operand : tensor<5xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.round_nearest_even",
    "summary": "RoundNearestEven operation",
    "description": "Performs element-wise rounding towards the nearest integer, breaking ties\n    towards the even integer, on the `operand` tensor and produces a `result`\n    tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#round_nearest_even\n\n    Example:\n    ```mlir\n    %result = stablehlo.round_nearest_even %operand : tensor<5xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.rsqrt",
    "summary": "Rsqrt operation",
    "description": "Performs element-wise reciprocal square root operation on `operand` tensor\n    and produces a `result` tensor, implementing the `rSqrt` operation from the\n    IEEE-754 specification.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#rsqrt\n\n    Example:\n    ```mlir\n    %result = stablehlo.rsqrt %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.scatter",
    "summary": "Scatter operation",
    "description": "Produces `results` tensors which are equal to `inputs` tensors except that\n    several slices specified by `scatter_indices` are updated with the values\n    `updates` using `update_computation`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#scatter\n\n   Example:\n   ```mlir\n   %result = \"stablehlo.scatter\"(%input, %scatter_indices, %update) ({\n     ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):\n       %0 = stablehlo.add %arg0, %arg1 : tensor<i64>\n       stablehlo.return %0 : tensor<i64>\n   }) {\n     scatter_dimension_numbers = #stablehlo.scatter<\n       update_window_dims = [3, 4],\n       inserted_window_dims = [1],\n       input_batching_dims = [0],\n       scatter_indices_batching_dims = [1],\n       scatter_dims_to_operand_dims = [2, 1],\n       index_vector_dim = 3>,\n     indices_are_sorted = false,\n     unique_indices = false\n   } : (tensor<2x3x4x2xi64>, tensor<2x2x3x2xi64>, tensor<2x2x3x2x2xi64>) -> tensor<2x3x4x2xi64>\n   ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "scatter_indices", "type": "RankedTensorOf" },
      { "name": "updates", "type": "Variadic" },
      { "name": "scatter_dimension_numbers", "type": "StableHLO_ScatterDimensionNumbers" }
    ],
    "attributes": [
      { "name": "indices_are_sorted", "type": "DefaultValuedOptionalAttr" },
      { "name": "unique_indices", "type": "DefaultValuedOptionalAttr" }
    ],
    "category": "Tensor"
  },
  {
    "name": "stablehlo.select",
    "summary": "Select operation",
    "description": "Produces a `result` tensor where each element is selected from `on_true` or\n    `on_false` tensor based on the value of the corresponding element of `pred`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#select\n\n    Example:\n    ```mlir\n    %result = stablehlo.select %pred, %on_true, %on_false : tensor<2x2xi1>, tensor<2x2xi32>\n    ```",
    "inputs": [
      { "name": "pred", "type": "HLO_PredTensor" },
      { "name": "on_true", "type": "HLO_Tensor" },
      { "name": "on_false", "type": "HLO_Tensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:`\n      custom<SelectOpType>(type($pred), type($on_true), type($on_false), type($result))"
  },
  {
    "name": "stablehlo.select_and_scatter",
    "summary": "SelectAndScatter operation",
    "description": "Scatters the values from the `source` tensor using `scatter` based on the\n    outcome of `reduce_window` of the `input` tensor using `select` and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#select_and_scatter\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.select_and_scatter\"(%operand, %source, %init_value) ({\n      ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):\n        %0 = stablehlo.compare GE, %arg0, %arg1 : (tensor<i64>, tensor<i64>) -> tensor<i1>\n        stablehlo.return %0 : tensor<i1>\n    }, {\n      ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):\n        %0 = stablehlo.add %arg0, %arg1 : tensor<i64>\n        stablehlo.return %0 : tensor<i64>\n    }) {\n      window_dimensions = array<i64: [3, 1]>,\n      window_strides = array<i64: [2, 1]>,\n      padding = dense<[[0, 1], [0, 0]]> : tensor<2x2xi64>\n    } : (tensor<4x2xi64>, tensor<2x2xi64>, tensor<i64>) -> tensor<4x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "source", "type": "HLO_Tensor" },
      { "name": "init_value", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "window_dimensions", "type": "OptionalAttr" },
      { "name": "window_strides", "type": "OptionalAttr" },
      { "name": "padding", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.send",
    "summary": "Send operation",
    "description": "Sends `inputs` to a channel `channel_id` and produces a `result` token.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#send\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.send\"(%operand, %token) {\n      channel_handle = #stablehlo.channel_handle<handle = 0, type = 1>,\n      is_host_transfer = false,\n      source_target_pairs = dense<[[0, 1], [1, 2]]> : tensor<2x2xi64>\n    } : (tensor<2x2xi64>, !stablehlo.token) -> !stablehlo.token\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "token", "type": "HLO_Token" },
      { "name": "channel_handle", "type": "StableHLO_ChannelHandle" }
    ],
    "attributes": [
      { "name": "is_host_transfer", "type": "DefaultValuedOptionalAttr" },
      { "name": "source_target_pairs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.set_dimension_size",
    "summary": "SetDimensionSize operation",
    "description": "This operation is a work in progress, so it is not yet included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/8.\n\n    Informally, this operation does the same thing as XLA's SetDimensionSize:\n    https://www.tensorflow.org/xla/operation_semantics#setdimensionsize\n\n    Example:\n    ```mlir\n    %0 = stablehlo.set_dimension_size %arg0, %arg1, dim = 1 : (tensor<4x2xf32>, tensor<i32>) -> tensor<4x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "size", "type": "I32RankedTensor" }
    ],
    "attributes": [
      { "name": "dimension", "type": "ConfinedAttr" }
    ],
    "assemblyFormat": "$operand `,` $size  `,` `dim` `=` $dimension attr-dict\n      `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.shift_left",
    "summary": "ShiftLeft operation",
    "description": "Performs element-wise left-shift operation on the `lhs` tensor by `rhs`\n    number of bits and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#shift_left\n\n    Example:\n    ```mlir\n    %result = stablehlo.shift_left %lhs, %rhs : tensor<3xi64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.shift_right_arithmetic",
    "summary": "ShiftRightArithmetic operation",
    "description": "Performs element-wise arithmetic right-shift operation on the `lhs` tensor\n    by `rhs` number of bits and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#shift_right_arithmetic\n\n    Example:\n    ```mlir\n    %result = stablehlo.shift_right_arithmetic %lhs, %rhs : tensor<3xi64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.shift_right_logical",
    "summary": "ShiftRightLogical operation",
    "description": "Performs element-wise logical right-shift operation on the `lhs` tensor by\n    `rhs` number of bits and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#shift_right_logical\n\n    Example:\n    ```mlir\n    %result = stablehlo.shift_right_logical %lhs, %rhs : tensor<3xi64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.sign",
    "summary": "Sign operation",
    "description": "Returns the sign of the `operand` element-wise and produces a `result`\n    tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#sign\n\n    Example:\n    ```mlir\n    %result = stablehlo.sign %operand : tensor<5xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.sine",
    "summary": "Sine operation",
    "description": "Performs element-wise sine operation on `operand` tensor and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#sine\n\n    Example:\n    ```mlir\n    %result = stablehlo.sine %operand : tensor<2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.slice",
    "summary": "Slice operation",
    "description": "Extracts a slice from the `operand` using statically-computed starting\n    indices and produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#slice\n\n    Example:\n    ```mlir\n    %result = stablehlo.slice %operand [1:3, 4:8:2]\n       : (tensor<3x8xi64>) -> tensor<2x2xi64>\n\n    // Same in generic form: the `1:3` above is mapped to the first entry in\n    // `start_indices` and `limit_indices`, while `strides` is implicitly 1.\n    // The `4:8:2` above is parsed into the second entry of `start_indices`,\n    // `limit_indices` and `strides` respectively.\n    %result = \"stablehlo.slice\" (%operand) {\n      start_indices = array<i64: 1, 4>,\n      limit_indices = array<i64: 3, 8>,\n      strides = array<i64: 1, 2>\n    } : (tensor<3x8xi64>) -> tensor<2x2xi64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "start_indices", "type": "DenseI64ArrayAttr" },
      { "name": "limit_indices", "type": "DenseI64ArrayAttr" },
      { "name": "strides", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand custom<SliceRanges>($start_indices, $limit_indices, $strides)\n      attr-dict `:` functional-type(operands, results)",
    "category": "Tensor"
  },
  {
    "name": "stablehlo.sort",
    "summary": "Sort operation",
    "description": "Sorts a variadic number of tensors in `inputs` together, according to a\n    custom `comparator`, along the given `dimension` and produces a variadic\n    number of tensors as `results`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#sort\n\n    Example:\n    ```mlir\n    %result0, %result1 = \"stablehlo.sort\"(%input0, %input1) ({\n      ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>, %arg2: tensor<i64>, %arg3: tensor<i64>):\n        %predicate = stablehlo.compare GT, %arg0, %arg1 : (tensor<i64>, tensor<i64>) -> tensor<i1>\n        stablehlo.return %predicate : tensor<i1>\n    }) {\n      dimension = 0 : i64,\n      is_stable = true\n    } : (tensor<2x3xi64>, tensor<2x3xi64>) -> (tensor<2x3xi64>, tensor<2x3xi64>)",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_stable", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "stablehlo.sqrt",
    "summary": "Sqrt operation",
    "description": "Performs element-wise square root operation on `operand` tensor and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#sqrt\n\n    Example:\n    ```mlir\n    %result = stablehlo.sqrt %operand : tensor<2x2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.subtract",
    "summary": "Subtract operation",
    "description": "Performs element-wise subtraction of two tensors `lhs` and `rhs` and\n    produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#subtract\n\n    Example:\n    ```mlir\n    %result = stablehlo.subtract %lhs, %rhs : tensor<2xi32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "OperandType" },
      { "name": "rhs", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stablehlo.tan",
    "summary": "Tan operation",
    "description": "Performs element-wise tangent operation on `operand` tensor and\n    produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#tan\n\n    Example:\n    ```mlir\n    %result = stablehlo.tan %operand : tensor<2x2xf64>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.tanh",
    "summary": "Tanh operation",
    "description": "Performs element-wise hyperbolic tangent operation on `operand` tensor and\n    produces a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#tanh\n\n    Example:\n    ```mlir\n    %result = stablehlo.tanh %operand : tensor<2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_FpComplexOrQuantizedIntTensor" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))",
    "category": "Activation"
  },
  {
    "name": "stablehlo.torch_index_select",
    "summary": "TorchIndexSelect operation",
    "description": "This operation is on its way out of StableHLO, so it is not included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/3.\n\n    Informally, this operation does the same thing as PyTorch's index_select,\n    augmented with support for batch dimensions:\n    https://pytorch.org/docs/stable/generated/torch.index_select.html.\n\n    The `batch_dims` attribute specifies the number of major batch dimensions\n    (0 or more) that act like a multidimensional loop over both the operand and\n    the index.\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.torch_index_select\"(%operand, %index) {\n      dim = 2 : i64,\n      batch_dims = 1 : i64\n    } : (tensor<8x128x3072x64xf32>, tensor<8x16x1024xi32>) -> tensor<8x128x16x1024x64xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" },
      { "name": "index", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "dim", "type": "I64Attr" },
      { "name": "batch_dims", "type": "I64Attr" }
    ]
  },
  {
    "name": "stablehlo.transpose",
    "summary": "Transpose operation",
    "description": "Permutes the dimensions of `operand` tensor using `permutation` and produces\n    a `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#transpose\n\n    Example:\n    ```mlir\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<1x2x3xi32>) -> tensor<3x2x1xi32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_TensorOrPerAxisQuantizedTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_TensorOrPerAxisQuantizedTensor" }
    ],
    "attributes": [
      { "name": "permutation", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$operand `,` `dims` `=` $permutation\n      attr-dict `:` functional-type(operands, results)",
    "category": "Transform"
  },
  {
    "name": "stablehlo.triangular_solve",
    "summary": "TriangularSolve operation",
    "description": "Solves batches of systems of linear equations with lower or upper triangular\n    coefficient matrices.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#triangular_solve\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.triangular_solve\"(%a, %b) {\n      left_side = true,\n      lower = true,\n      unit_diagonal = false,\n      transpose_a = #stablehlo<transpose NO_TRANSPOSE>\n    } : (tensor<3x3xf32>, tensor<3x3xf32>) -> tensor<3x3xf32>\n    ```",
    "inputs": [
      { "name": "a", "type": "HLO_FpOrComplexTensor" },
      { "name": "b", "type": "HLO_FpOrComplexTensor" }
    ],
    "attributes": [
      { "name": "left_side", "type": "BoolAttr" },
      { "name": "lower", "type": "BoolAttr" },
      { "name": "unit_diagonal", "type": "BoolAttr" },
      { "name": "transpose_a", "type": "StableHLO_TransposeAttr" }
    ]
  },
  {
    "name": "stablehlo.tuple",
    "summary": "Tuple operation",
    "description": "Produces a `result` tuple from values `val`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#tuple\n\n    Example:\n    ```mlir\n    %result = stablehlo.tuple %val0, %val1 : tuple<tensor<2xf64>, tuple<tensor<i64>>>\n    ```",
    "inputs": [
      { "name": "val", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "HLO_Tuple" }
    ],
    "assemblyFormat": "$val attr-dict `:` custom<TupleOpType>(type($val), type($result))"
  },
  {
    "name": "stablehlo.unary_einsum",
    "summary": "UnaryEinsum operation",
    "description": "This operation is on its way out of StableHLO, so it is not included in\n    the StableHLO specification: https://github.com/openxla/stablehlo/issues/3.\n\n    Informally, this operation does the same thing as TF's einsum:\n    https://www.tensorflow.org/api_docs/python/tf/einsum\n\n    Example:\n    ```mlir\n    %result = \"stablehlo.unary_einsum\"(%operand) {\n      einsum_config = \"ab->a\"\n    } : (tensor<4x16xf32>) -> tensor<4xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "HLO_Tensor" }
    ],
    "attributes": [
      { "name": "einsum_config", "type": "StrAttr" }
    ],
    "assemblyFormat": "$operand `,` `config` `=` $einsum_config attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stablehlo.uniform_dequantize",
    "summary": "UniformDequantize operation",
    "description": "Performs element-wise conversion of quantized tensor `operand` to a\n    floating-point tensor `result` according to the quantization parameters\n    defined by the `operand` type.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#uniform_dequantize\n\n    Example:\n    ```mlir\n    %result = stablehlo.uniform_dequantize %operand : (tensor<2x!quant.uniform<i8:f32:0, {0.1:-30,0.5:-20}>>) -> tensor<2xf32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.uniform_quantize",
    "summary": "UniformQuantize operation",
    "description": "Performs element-wise conversion of floating-point tensor or quantized\n    tensor `operand` to a quantized tensor `result` according to the\n    quantization parameters defined by the `result` type.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#uniform_quantize\n\n    Example:\n    ```mlir\n    %result = stablehlo.uniform_quantize %operand : (tensor<2xf32>) -> tensor<2x!quant.uniform<i8:f32:0, {0.1:-30,0.5:-20}>>\n    ```",
    "inputs": [
      { "name": "operand", "type": "OperandType" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))"
  },
  {
    "name": "stablehlo.while",
    "summary": "While operation",
    "description": "Produces the output from executing `body` function 0 or more times while the\n    `cond` function outputs `true`.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#while\n\n    Example:\n    ```mlir\n    %results0, %results1 = stablehlo.while(%arg0 = %init_i, %arg1 = %init_sum) : tensor<i64>, tensor<i64>\n    cond {\n      %cond = stablehlo.compare LT, %arg0, %ten : (tensor<i64>, tensor<i64>) -> tensor<i1>\n      stablehlo.return %cond : tensor<i1>\n    } do {\n      %new_sum = stablehlo.add %arg1, %one : tensor<i64>\n      %new_i = stablehlo.add %arg0, %one : tensor<i64>\n      stablehlo.return %new_i, %new_sum : tensor<i64>, tensor<i64>\n    }\n    ```",
    "inputs": [
      { "name": "operand", "type": "Variadic" }
    ]
  },
  {
    "name": "stablehlo.xor",
    "summary": "Xor operation",
    "description": "Performs element-wise XOR of two tensors `lhs` and `rhs` and produces a\n    `result` tensor.\n\n    See:\n    https://github.com/openxla/stablehlo/blob/main/docs/spec.md#xor\n\n    Example:\n    ```mlir\n    %result = stablehlo.xor %lhs, %rhs : tensor<2xi32>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "HLO_PredOrIntTensor" },
      { "name": "rhs", "type": "HLO_PredOrIntTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "ResultType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict\n      `:` custom<SameOperandsAndResultType>(type($lhs), type($rhs), type($result))"
  },
  {
    "name": "stdx.acos",
    "summary": "arccosine",
    "inputs": [
      { "name": "value", "type": "AnyType" }
    ],
    "assemblyFormat": "`(` $value `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stdx.acosh",
    "summary": "hyperbolic arccosine",
    "inputs": [
      { "name": "value", "type": "AnyType" }
    ],
    "assemblyFormat": "`(` $value `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stdx.asin",
    "summary": "arcsine",
    "inputs": [
      { "name": "value", "type": "AnyType" }
    ],
    "assemblyFormat": "`(` $value `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stdx.asinh",
    "summary": "hyperbolic arcsine",
    "inputs": [
      { "name": "value", "type": "AnyType" }
    ],
    "assemblyFormat": "`(` $value `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stdx.atanh",
    "summary": "hyperbolic arctangent",
    "inputs": [
      { "name": "value", "type": "AnyType" }
    ],
    "assemblyFormat": "`(` $value `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stdx.closure",
    "attributes": [
      { "name": "type", "type": "TypeAttr" }
    ]
  },
  {
    "name": "stdx.cosh",
    "summary": "hyperbolic cosine",
    "inputs": [
      { "name": "value", "type": "AnyType" }
    ],
    "assemblyFormat": "`(` $value `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stdx.pack",
    "summary": "Pack multiple values into a tuple",
    "inputs": [
      { "name": "in", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "out", "type": "AnyTuple" }
    ],
    "assemblyFormat": "`(` $in `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stdx.relu",
    "summary": "ReLU operation",
    "inputs": [
      { "name": "value", "type": "AnyType" }
    ],
    "assemblyFormat": "`(` $value `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stdx.reshape",
    "summary": "memref reshape operation",
    "inputs": [
      { "name": "tensor", "type": "AnyMemRef" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyMemRef" }
    ],
    "assemblyFormat": "`(` $tensor `)` attr-dict `:` functional-type($tensor, results)"
  },
  {
    "name": "stdx.round",
    "summary": "round to nearest integer",
    "inputs": [
      { "name": "value", "type": "AnyType" }
    ],
    "assemblyFormat": "`(` $value `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stdx.sinh",
    "summary": "hyperbolic sine",
    "inputs": [
      { "name": "value", "type": "AnyType" }
    ],
    "assemblyFormat": "`(` $value `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stdx.subgroup_block_read_intel",
    "summary": "See intel_subgroups extension.",
    "inputs": [
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyVectorOrScalar" }
    ],
    "assemblyFormat": "$memref `[` $indices `]` attr-dict `:` type($memref)`,` type($result)"
  },
  {
    "name": "stdx.subgroup_block_write_intel",
    "summary": "See intel_subgroups extension.",
    "inputs": [
      { "name": "value", "type": "AnyVectorOrScalar" },
      { "name": "memref", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "assemblyFormat": "$value `,` $memref `[` $indices `]` attr-dict `:` type($value) `,` type($memref)"
  },
  {
    "name": "stdx.subgroup_broadcast",
    "summary": "broadcast to all elements in subgroup",
    "inputs": [
      { "name": "value", "type": "AnyStdScalar" },
      { "name": "localid", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyStdScalar" }
    ],
    "assemblyFormat": "`(` $value `,` $localid `)` attr-dict `:` type($value)"
  },
  {
    "name": "stdx.tan",
    "summary": "tangent",
    "inputs": [
      { "name": "value", "type": "AnyType" }
    ],
    "assemblyFormat": "`(` $value `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stdx.unpack",
    "summary": "Unpack multiple values from a tuple",
    "inputs": [
      { "name": "in", "type": "AnyTuple" }
    ],
    "outputs": [
      { "name": "out", "type": "Variadic" }
    ],
    "assemblyFormat": "`(` $in `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "stdx.yield",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "stream.async.alloca",
    "summary": "Allocates a transient value with undefined contents.",
    "description": "Allocates a transient value (one that is short-lived and local to the\n    current computation) with undefined contents. Consumers of the allocated\n    result must assume nothing of the contents and use `discard` access.",
    "inputs": [
      { "name": "storage_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    attr-dict `:` type($result) `{` $storage_size `}`"
  },
  {
    "name": "stream.async.barrier",
    "summary": "Indicates a value that must have a specific affinity.",
    "description": "Prevents fusion and scheduling of a value across an affinity boundary.\n    May introduce copy-on-write behavior if the operand value is used as well as\n    the result and users should try to keep the operand to a single use by this\n    op.",
    "inputs": [
      { "name": "source", "type": "AnyTypeOf" },
      { "name": "size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $source `:` type($source)\n    `` `{` $size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.async.call",
    "summary": "Calls a streamable external host function.",
    "description": "Calls a function taking/returning resource values with stream semantics.\n    Asynchronous calls must have no side-effects.\n\n    Note that returned resources must have their sizes declared prior to the\n    call as this is what allows the call to be made on the stream. If external\n    host logic is required to compute the size (avoid at all costs!) a separate\n    func.call can be used outside of the stream to do so. If sizes are\n    unknownable until the operation is performed it should be made as a normal\n    asynchronous host call with 'coarse-fences' instead.",
    "inputs": [
      { "name": "resource_operands", "type": "Variadic" },
      { "name": "resource_operand_sizes", "type": "Variadic" },
      { "name": "resource_operand_offsets", "type": "Variadic" },
      { "name": "resource_operand_ends", "type": "Variadic" },
      { "name": "resource_operand_lengths", "type": "Variadic" },
      { "name": "result_sizes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "callee", "type": "FlatSymbolRefAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $callee ``\n    custom<DispatchOperands>($resource_operands,\n                             $resource_operand_offsets,\n                             $resource_operand_ends,\n                             $resource_operand_lengths) attr-dict `:`\n    custom<ShapedFunctionType>(ref($resource_operands),\n                               type($resource_operands), $resource_operand_sizes,\n                               type($results), $result_sizes,\n                               $tied_operands)"
  },
  {
    "name": "stream.async.clone",
    "summary": "Clones the contents of a value.",
    "description": "Clones the contents of a value at a snapshot in time. Future changes to the\n    cloned value will not effect the result. Acts as a copy-on-write operation.",
    "inputs": [
      { "name": "source", "type": "Stream_AnyStreamResource" },
      { "name": "source_size", "type": "Stream_Size" },
      { "name": "result_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $source `:`\n    type($source) `` `{` $source_size `}` `->`\n    type($result) `` `{` $result_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.async.collective",
    "summary": "Performs a collective operation.",
    "description": "TODO: document different usage. For now this should be considered a\n    prototype and that modeling of collective operations may change in the\n    future to better ensure in-place operations (where send/recv is a subset of\n    recv/send). We may have dedicated operations for the send and recv verbs as\n    they have sequencing implications - or we could add optional sequencing to\n    this base op.",
    "inputs": [
      { "name": "target", "type": "Stream_AnyStreamResource" },
      { "name": "target_size", "type": "Stream_Size" },
      { "name": "target_offset", "type": "Stream_Offset" },
      { "name": "target_end", "type": "Stream_Offset" },
      { "name": "target_length", "type": "Stream_Size" },
      { "name": "source", "type": "Stream_AnyStreamResource" },
      { "name": "source_size", "type": "Stream_Size" },
      { "name": "source_offset", "type": "Stream_Offset" },
      { "name": "source_end", "type": "Stream_Offset" },
      { "name": "source_length", "type": "Stream_Size" },
      { "name": "element_count", "type": "Stream_Size" },
      { "name": "channel", "type": "Stream_Channel" },
      { "name": "param", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "op", "type": "Stream_CollectiveAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`` $op `` `[` $element_count `]`\n    (`on` `(` $affinity^ `)`)?\n    `channel` `(` $channel `)`\n    custom<CollectiveParam>(ref($op), $param) ``\n    $source `[` $source_offset `to` $source_end `for` $source_length `]` `,`\n    $target `[` $target_offset `to` $target_end `for` $target_length `]` `:`\n    type($source) `` `{` $source_size `}` `->`\n    custom<ShapedTiedResult>(type($target), $target_size)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.async.concurrent",
    "summary": "Executes all ops concurrently.",
    "description": "Represents a wave of work scheduled concurrently (each op executing at the\n    same time). All resource inputs must be captured explicitly. All results are\n    only ready once all nested ops complete execution.\n\n    Waves can be nested to create a DAG. For example, take the following graph:\n    ```\n                      |\n            v---------+---------v\n    +-------|-------+   +-------|-------+\n    |    v--+--v    |   |    v--+--v    |\n    | +----+ +----+ |   | +----+ +----+ |\n    | | %a | | %b | |   | | %c | | %d | |\n    | +----+ +----+ |   | +----+ +----+ |\n    |    +--v--+    |   |    +--v--+    |\n    +-------|-------+   +-------|-------+\n            +---------v---------+\n                      |\n    ```\n\n    Represented with nested waves:\n    ```mlir\n      %0 = stream.async.concurrent with(%arg) -> ... {\n        %1 = stream.async.concurrent with(%arg as %arg0) -> ... {\n          %a = ...\n          %b = ...\n          stream.yield %a, %b\n        }\n        %2 = stream.async.concurrent with(%arg as %arg1) -> ... {\n          %c = ...\n          %d = ...\n          stream.yield %c, %d\n        }\n        stream.yield %1, %2\n      }\n    ```",
    "inputs": [
      { "name": "resource_operands", "type": "Variadic" },
      { "name": "resource_operand_sizes", "type": "Variadic" },
      { "name": "result_sizes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "tied_operands", "type": "OptionalAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    `with` ``\n    custom<ResourceRegion>($resource_operands,\n                           type($resource_operands), $resource_operand_sizes,\n                           type($results), $result_sizes,\n                           $tied_operands, $body)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.async.constant",
    "summary": "Defines a constant resource.",
    "description": "Returns a new resource with the given constant value.",
    "inputs": [
      { "name": "result_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "value", "type": "AnyAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    `:`\n    type($result) `` `{` $result_size `}`\n    `=`\n    $value\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.async.copy",
    "summary": "Copies a subview of a stream resource to another.",
    "description": "Copies a subview of a resource into a subview of another.\n    As with memcpy this does not support overlapping updates into the same\n    resource. Unlike `stream.async.update` copy sources cannot be allocated\n    in-place.\n\n    Equivalent to a stream.async.slice + stream.async.update.",
    "inputs": [
      { "name": "target", "type": "Stream_AnyStreamResource" },
      { "name": "target_size", "type": "Stream_Size" },
      { "name": "target_offset", "type": "Stream_Offset" },
      { "name": "target_end", "type": "Stream_Offset" },
      { "name": "source", "type": "Stream_AnyStreamResource" },
      { "name": "source_size", "type": "Stream_Size" },
      { "name": "source_offset", "type": "Stream_Offset" },
      { "name": "source_end", "type": "Stream_Offset" },
      { "name": "length", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $source `[` $source_offset `to` $source_end `]` `,`\n    $target `[` $target_offset `to` $target_end `]` `,`\n    $length `:`\n    type($source) `` `{` $source_size `}` `->`\n    custom<ShapedTiedResult>(type($target), $target_size)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.async.dispatch",
    "summary": "Dispatches a parallelized grid of work.",
    "description": "Calls the specified entry point function once for each element in the\n    specified workgroup count. Each workgroup has access to the same operands\n    and results and is able to load/store at will.",
    "inputs": [
      { "name": "workload", "type": "Variadic" },
      { "name": "resource_operands", "type": "Variadic" },
      { "name": "resource_operand_sizes", "type": "Variadic" },
      { "name": "resource_operand_offsets", "type": "Variadic" },
      { "name": "resource_operand_ends", "type": "Variadic" },
      { "name": "resource_operand_lengths", "type": "Variadic" },
      { "name": "result_sizes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "entry_points", "type": "SymbolRefArrayAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    custom<DispatchEntryPoints>($entry_points)\n    (`[` $workload^ `]`)? ``\n    custom<DispatchOperands>($resource_operands,\n                             $resource_operand_offsets,\n                             $resource_operand_ends,\n                             $resource_operand_lengths) attr-dict `:`\n    custom<ShapedFunctionType>(ref($resource_operands),\n                               type($resource_operands), $resource_operand_sizes,\n                               type($results), $result_sizes,\n                               $tied_operands)"
  },
  {
    "name": "stream.async.execute",
    "summary": "Executes a dependency-aware sequence of streamable ops.",
    "description": "Evaluates the operations within the region by dependency order while obeying\n    ties when present. Nested ops execute serially in block order and nested\n    `stream.async.concurrent` ops can be used to run multiple ops concurrently\n    within the stream. All resource inputs must be captured explicitly. All\n    results are only ready once all nested ops complete execution and the\n    returned timepoint is reached. Zero or more timepoints may be provided to\n    block execution until they are all reached; zero timepoints indicates that\n    execution may begin immediately.",
    "inputs": [
      { "name": "resource_operands", "type": "Variadic" },
      { "name": "resource_operand_sizes", "type": "Variadic" },
      { "name": "result_sizes", "type": "Variadic" },
      { "name": "await_timepoint", "type": "Optional" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" },
      { "name": "result_timepoint", "type": "Stream_Timepoint" }
    ],
    "attributes": [
      { "name": "tied_operands", "type": "OptionalAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    (`await` `(` $await_timepoint^ `)` `=` `` `>`)?\n    `with` ``\n    custom<ResourceRegion>($resource_operands,\n                           type($resource_operands), $resource_operand_sizes,\n                           type($results), $result_sizes,\n                           $tied_operands, $body)\n    `=` `` `>` type($result_timepoint)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.async.fill",
    "summary": "Fills a subview of a stream resource with a value.",
    "description": "Splats a value into a subview of the given stream resource and returns the\n    resource with the update applied.\n\n    Equivalent to a stream.async.splat + stream.async.update.",
    "inputs": [
      { "name": "target", "type": "Stream_AnyStreamResource" },
      { "name": "target_size", "type": "Stream_Size" },
      { "name": "target_offset", "type": "Stream_Offset" },
      { "name": "target_end", "type": "Stream_Offset" },
      { "name": "target_length", "type": "Stream_Size" },
      { "name": "value", "type": "Stream_FillPatternType" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $value `,`\n    $target `[` $target_offset `to` $target_end `for` $target_length `]` `:`\n    type($value) `->`\n    custom<ShapedTiedResult>(type($target), $target_size)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.async.func",
    "summary": "Streamable function declaration.",
    "description": "Declares a function that can be called as an asynchronous streaming\n    operation via `stream.async.call`. Today only external functions are\n    allowed.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "tied_operands", "type": "OptionalAttr" },
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    $sym_name\n    ``\n    custom<ShapedFunctionSignature>($function_type,\n                                    $tied_operands,\n                                    $arg_attrs,\n                                    $res_attrs)\n    attr-dict-with-keyword\n    ($body^)?"
  },
  {
    "name": "stream.async.load",
    "summary": "Loads a value from a resource.",
    "description": "Returns the element at the given location from within the resource.",
    "inputs": [
      { "name": "source", "type": "Stream_StagingResource" },
      { "name": "source_size", "type": "Stream_Size" },
      { "name": "source_offset", "type": "Stream_Offset" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$source `[` $source_offset `]` `:`\n    type($source) `` `{` $source_size `}`\n    `->`\n    type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.async.slice",
    "summary": "Slices out a cloned subview of a value.",
    "description": "Slices a subrange of a stream resource based on a byte range. Acts as a\n    copy-on-write operation.",
    "inputs": [
      { "name": "source", "type": "Stream_AnyStreamResource" },
      { "name": "source_size", "type": "Stream_Size" },
      { "name": "source_offset", "type": "Stream_Offset" },
      { "name": "source_end", "type": "Stream_Offset" },
      { "name": "result_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $source `[` $source_offset `to` $source_end `]` `:`\n    type($source) `` `{` $source_size `}` `->`\n    type($result) `` `{` $result_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.async.splat",
    "summary": "Splats a value into a resource.",
    "description": "Returns a new resource with the given primitive value splatted out to fill\n    the entire contents.",
    "inputs": [
      { "name": "value", "type": "Stream_FillPatternType" },
      { "name": "result_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $value `:` type($value) `->` type($result) `` `{` $result_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.async.store",
    "summary": "Stores a value into a resource.",
    "description": "Returns a resource with the element at the given offset set to the given\n    value.",
    "inputs": [
      { "name": "target", "type": "Stream_StagingResource" },
      { "name": "target_size", "type": "Stream_Size" },
      { "name": "target_offset", "type": "Stream_Offset" },
      { "name": "value", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_StagingResource" }
    ],
    "assemblyFormat": "$value `,`\n    $target `[` $target_offset `]` `:`\n    type($value)\n    `->`\n    custom<ShapedTiedResult>(type($target), $target_size)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.async.transfer",
    "summary": "Transfers a resource from one location/state to another.",
    "description": "Transfers a resource between different states (such as a `staging` lifetime\n    to a `local` lifetime) or different affinities. This is roughly equivalent\n    to a cast but may have special semantics when later lowered to one or more\n    devices with discrete memory spaces or pools.",
    "inputs": [
      { "name": "source", "type": "AnyTypeOf" },
      { "name": "source_size", "type": "Stream_Size" },
      { "name": "result_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "source_affinity", "type": "OptionalAttr" },
      { "name": "target_affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$source `:` type($source)\n    `` `{` $source_size `}`\n    (`from` `(` $source_affinity^ `)`)?\n    `->`\n    (`to` `(` $target_affinity^ `)`)?\n    type($result) `` `{` $result_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.async.update",
    "summary": "Updates a slice of a subview of a resource in-place.",
    "description": "Copies a value into a resource based on a byte range. The returned value\n    is the entire updated target value. Updates can be turned into placement\n    allocations and avoid copies.",
    "inputs": [
      { "name": "target", "type": "Stream_AnyStreamResource" },
      { "name": "target_size", "type": "Stream_Size" },
      { "name": "target_offset", "type": "Stream_Offset" },
      { "name": "target_end", "type": "Stream_Offset" },
      { "name": "update", "type": "Stream_AnyStreamResource" },
      { "name": "update_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $update `,`\n    $target `[` $target_offset `to` $target_end `]` `:`\n    type($update) `` `{` $update_size `}` `->`\n    custom<ShapedTiedResult>(type($target), $target_size)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.binding.subspan",
    "summary": "Returns an alias to a subspan of interface binding data.",
    "description": "Returns a subview to a tensor or memref-like type from a binding. The same\n    binding may have multiple subviews at different byte offsets.",
    "inputs": [
      { "name": "binding", "type": "Stream_AnyBinding" },
      { "name": "byte_offset", "type": "Stream_Offset" },
      { "name": "dynamic_dims", "type": "Stream_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "$binding `` `[` $byte_offset `]`\n    attr-dict `:` type($binding) `->` type($result) (`{` $dynamic_dims^ `}`)?"
  },
  {
    "name": "stream.channel.count",
    "summary": "Returns the total number of participants in the group.",
    "description": "Returns the total participant count in the collective communicator group.",
    "inputs": [
      { "name": "channel", "type": "Stream_Channel" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$channel `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.channel.create",
    "summary": "Creates a new channel for collective communication.",
    "description": "Returns a new channel with the given rank associated with the specified\n    affinity. Collective operations using this channel must only be submitted on\n    compatible affinities.\n\n    The group and ID are optional and may be null. The rank and count can be\n    omitted to indicate a default inherited from the environment or device\n    configuration at runtime.",
    "inputs": [
      { "name": "id", "type": "Optional" },
      { "name": "rank", "type": "Optional" },
      { "name": "count", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_Channel" }
    ],
    "attributes": [
      { "name": "group", "type": "OptionalAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    (`id` `(` $id^ `)`)?\n    (`group` `(` $group^ `)`)?\n    (`rank` `(` $rank^ `)`)?\n    (`count` `(` $count^ `)`)?\n    `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.channel.rank",
    "summary": "Returns the rank of the local participant in the group.",
    "description": "Returns the rank the channel represents as a participant in a collective\n    group in `[0, count)`.",
    "inputs": [
      { "name": "channel", "type": "Stream_Channel" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "$channel `:` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.channel.split",
    "summary": "Splits a collective communication channel.",
    "description": "Partitions the group associated with the given channel into disjoint\n    subgroups for each unique value of color. Each new subgroup contains all\n    participants of the same color and within each subgroup the key argument\n    is used to define the rank order. When multiple participants in a group\n    use the same key the tie will be broken using their rank in the parent\n    group. A color of -1 indicates that the rank does not participate in any\n    subgroup and will return a null channel.",
    "inputs": [
      { "name": "channel", "type": "Stream_Channel" },
      { "name": "color", "type": "Index" },
      { "name": "key", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_Channel" }
    ],
    "assemblyFormat": "$channel `,` $color `,` $key\n    `:` type($channel) `->` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.cmd.call",
    "summary": "Calls a streamable external host function.",
    "description": "Calls a function operating on resource values with stream semantics.\n    Asynchronous calls must have no side-effects.",
    "inputs": [
      { "name": "resource_operands", "type": "Variadic" },
      { "name": "resource_operand_sizes", "type": "Variadic" },
      { "name": "resource_operand_offsets", "type": "Variadic" },
      { "name": "resource_operand_lengths", "type": "Variadic" },
      { "name": "result_sizes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "callee", "type": "FlatSymbolRefAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "resource_operand_accesses", "type": "Stream_ResourceAccessArrayAttr" }
    ],
    "assemblyFormat": "$callee ``\n    custom<CmdCallOperands>($resource_operands,\n                            $resource_operand_offsets,\n                            $resource_operand_lengths,\n                            $resource_operand_accesses) attr-dict `:`\n    custom<ShapedFunctionType>(ref($resource_operands),\n                               type($resource_operands),\n                               $resource_operand_sizes,\n                               type($results),\n                               $result_sizes,\n                               $tied_operands)"
  },
  {
    "name": "stream.cmd.collective",
    "summary": "Dispatches a collective operation.",
    "description": "Dispatches a collective operation specified against the device. If grouped\n    with other collectives in a `stream.cmd.concurrent` region the collective\n    operations may fuse and execute more efficiently.",
    "inputs": [
      { "name": "channel", "type": "Stream_Channel" },
      { "name": "element_count", "type": "Stream_Size" },
      { "name": "param", "type": "Optional" },
      { "name": "resources", "type": "Variadic" },
      { "name": "resource_sizes", "type": "Variadic" },
      { "name": "resource_offsets", "type": "Variadic" },
      { "name": "resource_lengths", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "op", "type": "Stream_CollectiveAttr" },
      { "name": "resource_accesses", "type": "Stream_ResourceAccessArrayAttr" }
    ],
    "assemblyFormat": "`` $op `` `[` $element_count `]`\n    `channel` `(` $channel `)`\n    (`param` `(` $param^ `:` type($param) `)`)? `{`\n    custom<DispatchResources>($resources, type($resources), $resource_sizes,\n                              $resource_offsets, $resource_lengths,\n                              $resource_accesses)\n    `\\n` `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.cmd.concurrent",
    "summary": "Executes all ops concurrently.",
    "description": "Represents a wave of work scheduled concurrently (each op executing at the\n    same time).\n\n    Waves can be nested to create a DAG. For example, take the following graph:\n    ```\n                      |\n            v---------+---------v\n    +-------|-------+   +-------|-------+\n    |    v--+--v    |   |    v--+--v    |\n    | +----+ +----+ |   | +----+ +----+ |\n    | | @a | | @b | |   | | @c | | @d | |\n    | +----+ +----+ |   | +----+ +----+ |\n    |    +--v--+    |   |    +--v--+    |\n    +-------|-------+   +-------|-------+\n            +---------v---------+\n                      |\n    ```\n\n    Represented with nested waves:\n    ```mlir\n      stream.cmd.concurrent {\n        stream.cmd.concurrent {\n          stream.cmd.dispatch @a\n          stream.cmd.dispatch @b\n        }\n        stream.cmd.concurrent {\n          stream.cmd.dispatch @c\n          stream.cmd.dispatch @d\n        }\n      }\n    ```",
    "assemblyFormat": "$body\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.cmd.copy",
    "summary": "Copies a subview of a stream resource to another.",
    "description": "Copies a subview of a resource into a subview of another.\n    As with memcpy this does not support overlapping updates into the same\n    resource.",
    "inputs": [
      { "name": "source", "type": "Stream_AnyResource" },
      { "name": "source_size", "type": "Stream_Size" },
      { "name": "source_offset", "type": "Stream_Offset" },
      { "name": "target", "type": "Stream_AnyResource" },
      { "name": "target_size", "type": "Stream_Size" },
      { "name": "target_offset", "type": "Stream_Offset" },
      { "name": "length", "type": "Stream_Size" }
    ],
    "assemblyFormat": "$source `[` $source_offset `]` `,`\n    $target `[` $target_offset `]` `,`\n    $length `:`\n    type($source) `` `{` $source_size `}` `->`\n    type($target) `` `{` $target_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.cmd.discard",
    "summary": "Discards a subview of a resource.",
    "description": "Discards a subview of a resource, indicating that after this command the\n    specified contents are no longer needed. This can be used to trim memory\n    or invalidate caches.",
    "inputs": [
      { "name": "target", "type": "Stream_AnyResource" },
      { "name": "target_size", "type": "Stream_Size" },
      { "name": "target_offset", "type": "Stream_Offset" },
      { "name": "target_length", "type": "Stream_Size" }
    ],
    "assemblyFormat": "$target `[` $target_offset `for` $target_length `]` `:`\n    type($target) `` `{` $target_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.cmd.dispatch",
    "summary": "Dispatches a parallelized grid of work.",
    "description": "Calls the specified entry point function once for each element in the\n    specified workgroup count. Each workgroup has access to the same operands\n    and results and is able to load/store at will.",
    "inputs": [
      { "name": "workload", "type": "Variadic" },
      { "name": "uniform_operands", "type": "Variadic" },
      { "name": "resources", "type": "Variadic" },
      { "name": "resource_sizes", "type": "Variadic" },
      { "name": "resource_offsets", "type": "Variadic" },
      { "name": "resource_lengths", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "entry_points", "type": "SymbolRefArrayAttr" },
      { "name": "resource_accesses", "type": "Stream_ResourceAccessArrayAttr" }
    ],
    "assemblyFormat": "custom<DispatchEntryPoints>($entry_points)\n    (`[` $workload^ `]`)? ``\n    (`(` $uniform_operands^ `:` type($uniform_operands) `)`)? `{`\n    custom<DispatchResources>($resources, type($resources), $resource_sizes,\n                              $resource_offsets, $resource_lengths,\n                              $resource_accesses)\n    `\\n` `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.cmd.execute",
    "summary": "Executes a dependency-aware sequence of streamable ops.",
    "description": "Evaluates the operations within the region by dependency order while obeying\n    ties when present. Nested ops execute serially in block order and nested\n    `stream.cmd.concurrent` ops can be used to run multiple ops concurrently\n    within the stream. All resource inputs must be captured explicitly. All\n    results are only ready once all nested ops complete execution and the\n    returned timepoint is reached. Zero or more timepoints may be provided to\n    block execution until they are all reached; zero timepoints indicates that\n    execution may begin immediately.",
    "inputs": [
      { "name": "resource_operands", "type": "Variadic" },
      { "name": "resource_operand_sizes", "type": "Variadic" },
      { "name": "await_timepoint", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result_timepoint", "type": "Stream_Timepoint" }
    ],
    "attributes": [
      { "name": "once", "type": "UnitAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`once` $once^)?\n    (`on` `(` $affinity^ `)`)?\n    (`await` `(` $await_timepoint^ `)` `=` `` `>`)?\n    `with` ``\n    custom<ExplicitResourceRegion>($resource_operands,\n                                   type($resource_operands), $resource_operand_sizes,\n                                   $body)\n    `=` `` `>` type($result_timepoint)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.cmd.fill",
    "summary": "Fills a subview of a stream resource with a value.",
    "description": "Splats a value into a subview of the given stream resource and returns the\n    resource with the update applied.",
    "inputs": [
      { "name": "target", "type": "Stream_AnyStreamResource" },
      { "name": "target_size", "type": "Stream_Size" },
      { "name": "target_offset", "type": "Stream_Offset" },
      { "name": "target_length", "type": "Stream_Size" },
      { "name": "value", "type": "Stream_NativeFillPatternType" }
    ],
    "assemblyFormat": "$value `,`\n    $target `[` $target_offset `for` $target_length `]` `:`\n    type($value) `->`\n    type($target) `` `{` $target_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.cmd.flush",
    "summary": "Flushes a subview of a resource.",
    "description": "Transfers a resource to an external target. The resource memory is made\n    available to the target and can be made visible there using\n    `stream.cmd.invalidate`.",
    "inputs": [
      { "name": "target", "type": "Stream_AnyResource" },
      { "name": "target_size", "type": "Stream_Size" },
      { "name": "target_offset", "type": "Stream_Offset" },
      { "name": "target_length", "type": "Stream_Size" }
    ],
    "attributes": [
      { "name": "source_affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`to` `(` $source_affinity^ `)`)?\n    $target `[` $target_offset `for` $target_length `]` `:`\n    type($target) `` `{` $target_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.cmd.func",
    "summary": "Streamable function declaration.",
    "description": "Declares a function that can be called as an asynchronous streaming\n    operation via `stream.cmd.call`. Today only external functions are\n    allowed.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    $sym_name ``\n    custom<DispatchFunctionSignature>($function_type,\n                                      $arg_attrs,\n                                      $res_attrs)\n    attr-dict-with-keyword\n    ($body^)?"
  },
  {
    "name": "stream.cmd.invalidate",
    "summary": "Invalidates a subview of a resource.",
    "description": "Transfers a resource from an external source into the current target. The\n    resource memory is assumed to have been made available at the source via\n    `stream.cmd.flush`.",
    "inputs": [
      { "name": "target", "type": "Stream_AnyResource" },
      { "name": "target_size", "type": "Stream_Size" },
      { "name": "target_offset", "type": "Stream_Offset" },
      { "name": "target_length", "type": "Stream_Size" }
    ],
    "attributes": [
      { "name": "source_affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`from` `(` $source_affinity^ `)`)?\n    $target `[` $target_offset `for` $target_length `]` `:`\n    type($target) `` `{` $target_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.cmd.serial",
    "summary": "Executes all ops serially (in-order).",
    "description": "Represents a sequence of work scheduled serially (each op executing one\n    after the other).\n\n    Regions can be nested to create a DAG. For example, take the following graph:\n    ```\n                      |\n            v---------+-----v\n    +-------|-------+   +---|----+\n    |    v--+--v    |   |   v    |\n    | +----+ +----+ |   | +----+ |\n    | | @a | | @b | |   | | @c | |\n    | +----+ +----+ |   | +----+ |\n    |    |     |    |   |   |    |\n    |    |     |    |   | +-v--+ |\n    |    |     |    |   | | @d | |\n    |    |     |    |   | +----+ |\n    |    +--v--+    |   |   |    |\n    +-------|-------+   +---|----+\n            +---------v-----+\n                      |\n    ```\n\n    Represented with nested regions:\n    ```mlir\n      stream.cmd.concurrent {\n        stream.cmd.concurrent {\n          stream.cmd.dispatch @a\n          stream.cmd.dispatch @b\n        }\n        stream.cmd.serial {\n          stream.cmd.dispatch @c\n          stream.cmd.dispatch @d\n        }\n      }\n    ```",
    "assemblyFormat": "$body\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.context.resolve",
    "summary": "Resolves low-level context resources based on type.",
    "description": "WIP; allows for accessing the implementation details of lower-level dialects\n    such as the HAL. This will likely be reworked in the future to either\n    live inside other dialects, use some op interface instead of having a\n    dedicated op here, or remove the op entirely and make resolution happen\n    explicitly.\n\n    Examples:\n    ```\n    // Returns a HAL device.\n    = stream.context.resolve on(#something) : !hal.device\n    // Returns a HAL device and (optional) queue affinity.\n    = stream.context.resolve on(#something) : !hal.device, i64\n    // Returns a HAL allocator and (optional) queue affinity.\n    = stream.context.resolve on(#something) : !hal.allocator, i64\n    ```",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    attr-dict `:` type($results)"
  },
  {
    "name": "stream.dispatch.workgroup.count",
    "summary": "Returns the total workgroup count of the grid.",
    "description": "The total number of workgroups along each dimension in the dispatch grid.\n\n    Represented as a 3D grid classically written as XYZ.\n    Corresponds to the `NumWorkgroups` SPIR-V built-in and the `gridDim` CUDA\n    built-in variable.\n\n    ```mlir\n    %x = stream.dispatch.workgroup.count[0] : index\n    %y = stream.dispatch.workgroup.count[1] : index\n    %z = stream.dispatch.workgroup.count[2] : index\n    ```",
    "outputs": [
      { "name": "result", "type": "Stream_Dim" }
    ],
    "attributes": [
      { "name": "dimension", "type": "IndexAttr" }
    ],
    "assemblyFormat": "`[` $dimension `]` attr-dict `:` type($result)"
  },
  {
    "name": "stream.dispatch.workgroup.id",
    "summary": "Returns the index of the current workgroup in the grid.",
    "description": "The global workgroup ID of the current workgroup in the range of\n    `[0, stream.dispatch.workgroup.count)` along each dimension.\n\n    Represented as a 3D grid classically written as XYZ.\n    Corresponds to the `WorkgroupId` SPIR-V built-in and the `blockIdx` CUDA\n    built-in variable.\n\n    ```mlir\n    %x = stream.dispatch.workgroup.id[0] : index\n    %y = stream.dispatch.workgroup.id[1] : index\n    %z = stream.dispatch.workgroup.id[2] : index\n    ```",
    "outputs": [
      { "name": "result", "type": "Stream_Dim" }
    ],
    "attributes": [
      { "name": "dimension", "type": "IndexAttr" }
    ],
    "assemblyFormat": "`[` $dimension `]` attr-dict `:` type($result)"
  },
  {
    "name": "stream.dispatch.workgroup.size",
    "summary": "Returns the size of each workgroup in invocations.",
    "description": "The number of local invocations within the current workgroup along each\n    dimension. Depending on backend this may map to the SIMT thread count or\n    inner loop nest parameters.\n\n    Workgroup sizes are not determined at the stream dialect level as they are\n    dependent on the target backend determined when lowering into the HAL. It's\n    still possible to use the symbolic workgroup size inside of dispatch\n    executables as a placeholder for the resolved value once in the HAL.\n\n    Represented as a 3D grid classically written as XYZ.\n    Corresponds to the `WorkgroupSize` SPIR-V built-in and the `blockDim` CUDA\n    built-in variable.\n\n    ```mlir\n    %x = stream.dispatch.workgroup.size[0] : index\n    %y = stream.dispatch.workgroup.size[1] : index\n    %z = stream.dispatch.workgroup.size[2] : index\n    ```",
    "outputs": [
      { "name": "result", "type": "Stream_Dim" }
    ],
    "attributes": [
      { "name": "dimension", "type": "IndexAttr" }
    ],
    "assemblyFormat": "`[` $dimension `]` attr-dict `:` type($result)"
  },
  {
    "name": "stream.executable",
    "summary": "Generic executable module.",
    "description": "An executable module containing one or more public functions. The contents\n    of the functions are safe to dispatch and can be lowered further to\n    target-specific backend IR representations.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    $sym_name\n    attr-dict-with-keyword\n    regions"
  },
  {
    "name": "stream.executable.end",
    "summary": "Terminator pseudo-op for the executable op.",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "stream.executable.export",
    "summary": "Defines an executable entry point for dispatch operations.",
    "description": "Specifies an exported function with an externally-visible alias. Multiple\n    exports can reference the same internal function.\n\n    Each entry point can have a unique workgroup count calculation region.\n    This region takes the workload parameters passed to each flow.dispatch and\n    produces an XYZ workgroup count for the 3D grid dispatch.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_ref", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    custom<SymbolAlias>($sym_name, $function_ref)\n    custom<WorkgroupCountRegion>($workgroup_count)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.file.constant",
    "summary": "Creates a file backed by the provided constant host memory.",
    "description": "Synchronously wraps a host heap buffer into a stream-accessible file handle.\n    Changing the source buffer after definition has undefined behavior.",
    "inputs": [
      { "name": "source", "type": "Util_BufferType" },
      { "name": "source_size", "type": "Util_Size" },
      { "name": "source_offset", "type": "Stream_Offset" },
      { "name": "source_length", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_File" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $source `[` $source_offset `for` $source_length `]` `:`\n    type($source) `` `{` $source_size `}`\n    `->`\n    type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.file.read",
    "summary": "Reads a segment of a file into a resource.",
    "description": "Asynchronously reads a segment of a file into a resource.\n\n    Some implementations can stream directly from the source file into\n    device-local memory and file ops should be preferred to manually staging\n    memory through host buffers.",
    "inputs": [
      { "name": "source", "type": "Stream_File" },
      { "name": "source_offset", "type": "I64" },
      { "name": "target", "type": "Stream_AnyStreamResource" },
      { "name": "target_size", "type": "Stream_Size" },
      { "name": "target_offset", "type": "Stream_Offset" },
      { "name": "length", "type": "Stream_Size" },
      { "name": "await_timepoint", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result_timepoint", "type": "Stream_Timepoint" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    (`await` `(` $await_timepoint^ `)` `=` `` `>`):(`:`)?\n    $source `[` $source_offset `]` `,`\n    $target `[` $target_offset `]` `,`\n    $length `:`\n    type($source) `->`\n    type($target) `` `{` $target_size `}`\n    `=` `` `>`\n    type($result_timepoint)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.file.write",
    "summary": "Writes a segment of a file from a resource.",
    "description": "Asynchronously writes a segment of a resource into a file.\n    The file range must be valid within the file as this operation cannot\n    grow the underlying file storage.\n\n    Some implementations can stream directly from device-local memory into the\n    target file and file ops should be preferred to manually staging memory\n    through host buffers.",
    "inputs": [
      { "name": "source", "type": "Stream_AnyStreamResource" },
      { "name": "source_size", "type": "Stream_Size" },
      { "name": "source_offset", "type": "Stream_Offset" },
      { "name": "target", "type": "Stream_File" },
      { "name": "target_offset", "type": "I64" },
      { "name": "length", "type": "Stream_Size" },
      { "name": "await_timepoint", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result_timepoint", "type": "Stream_Timepoint" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    (`await` `(` $await_timepoint^ `)` `=` `` `>`):(`:`)?\n    $source `[` $source_offset `]` `,`\n    $target `[` $target_offset `]` `,`\n    $length `:`\n    type($source) `` `{` $source_size `}` `->`\n    type($target)\n    `=` `` `>`\n    type($result_timepoint)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.parameter.gather",
    "summary": "Gathers multiple resources from a parameter scope.",
    "description": "Asynchronously gathers one or more resources into a single target stream\n    resource. This is equivalent to one `stream.parameter.read` per parameter\n    but allows implementations that can batch operations to do so without\n    additional timeline overhead.",
    "inputs": [
      { "name": "source_offsets", "type": "Variadic" },
      { "name": "target", "type": "Stream_AnyStreamResource" },
      { "name": "target_size", "type": "Stream_Size" },
      { "name": "target_offsets", "type": "Variadic" },
      { "name": "target_lengths", "type": "Variadic" },
      { "name": "await_timepoint", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result_timepoint", "type": "Stream_Timepoint" }
    ],
    "attributes": [
      { "name": "source_scope", "type": "OptionalAttr" },
      { "name": "source_keys", "type": "StrArrayAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    (`await` `(` $await_timepoint^ `)` `=` `` `>`)?\n    `{`\n    custom<ParameterGatherOperations>(\n        $source_scope, $source_keys, $source_offsets,\n        $target, type($target), $target_size, $target_offsets, $target_lengths)\n    `}`\n    `=` `` `>`\n    type($result_timepoint)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.parameter.load",
    "summary": "Reads one or more resources from a parameter scope.",
    "description": "Asynchronously reads one or more resources from an external parameter\n    provider and returns the resulting stream resources. Depending on the\n    resource type this may alias existing cached storage or be directly mapped\n    to the parameter origin or result in a copy as if `stream.resource.alloca`\n    and `stream.parameter.read` had been used per parameter.",
    "inputs": [
      { "name": "source_offsets", "type": "Variadic" },
      { "name": "result_sizes", "type": "Variadic" },
      { "name": "await_timepoint", "type": "Optional" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" },
      { "name": "result_timepoint", "type": "Stream_Timepoint" }
    ],
    "attributes": [
      { "name": "source_scope", "type": "OptionalAttr" },
      { "name": "source_keys", "type": "StrArrayAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    (`await` `(` $await_timepoint^ `)` `=` `` `>`)?\n    `{`\n    custom<ParameterLoadOperations>(\n        $source_scope, $source_keys, $source_offsets,\n        type($results), $result_sizes)\n    `}`\n    `=` `` `>`\n    type($result_timepoint)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.parameter.read",
    "summary": "Reads a resource from a parameter scope.",
    "description": "Asynchronously reads a resource from an external parameter provider into the\n    provided target resource range.",
    "inputs": [
      { "name": "source_offset", "type": "I64" },
      { "name": "target", "type": "Stream_AnyStreamResource" },
      { "name": "target_size", "type": "Stream_Size" },
      { "name": "target_offset", "type": "Stream_Offset" },
      { "name": "target_length", "type": "Stream_Size" },
      { "name": "await_timepoint", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result_timepoint", "type": "Stream_Timepoint" }
    ],
    "attributes": [
      { "name": "source_scope", "type": "OptionalAttr" },
      { "name": "source_key", "type": "StrAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    (`await` `(` $await_timepoint^ `)` `=` `` `>`)?\n    custom<ParameterReference>($source_scope, $source_key)\n    `` `[` $source_offset `]` `->`\n    $target `[` $target_offset `for` $target_length `]` `:`\n    type($target) `` `{` $target_size `}`\n    `=` `` `>`\n    type($result_timepoint)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.parameter.scatter",
    "summary": "Scatters multiple resources to a parameter scope.",
    "description": "Asynchronously scatters one or more resources from a single source resource\n    into one or more parameters. This is equivalent to one\n    `stream.parameter.write` per parameter but allows implementations that can\n    batch operations to do so without additional overhead.",
    "inputs": [
      { "name": "source", "type": "Stream_AnyStreamResource" },
      { "name": "source_size", "type": "Stream_Size" },
      { "name": "source_offsets", "type": "Variadic" },
      { "name": "source_lengths", "type": "Variadic" },
      { "name": "target_offsets", "type": "Variadic" },
      { "name": "await_timepoint", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result_timepoint", "type": "Stream_Timepoint" }
    ],
    "attributes": [
      { "name": "target_scope", "type": "OptionalAttr" },
      { "name": "target_keys", "type": "StrArrayAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    (`await` `(` $await_timepoint^ `)` `=` `` `>`)?\n    `{`\n    custom<ParameterScatterOperations>(\n        $source, type($source), $source_size, $source_offsets, $source_lengths,\n        $target_scope, $target_keys, $target_offsets)\n    `}`\n    `=` `` `>`\n    type($result_timepoint)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.parameter.write",
    "summary": "Writes a resource to a parameter scope.",
    "description": "Asynchronously writes a resource to an external parameter provider from\n    the provided source resource range.",
    "inputs": [
      { "name": "source", "type": "Stream_AnyStreamResource" },
      { "name": "source_size", "type": "Stream_Size" },
      { "name": "source_offset", "type": "Stream_Offset" },
      { "name": "source_length", "type": "Stream_Size" },
      { "name": "target_offset", "type": "I64" },
      { "name": "await_timepoint", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result_timepoint", "type": "Stream_Timepoint" }
    ],
    "attributes": [
      { "name": "target_scope", "type": "OptionalAttr" },
      { "name": "target_key", "type": "StrAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    (`await` `(` $await_timepoint^ `)` `=` `` `>`)?\n    $source `[` $source_offset `for` $source_length `]` `:`\n    type($source) `` `{` $source_size `}` `->`\n    custom<ParameterReference>($target_scope, $target_key)\n    `` `[` $target_offset `]`\n    `=` `` `>`\n    type($result_timepoint)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.resource.alloc",
    "summary": "Allocates a persistent resource.",
    "description": "Allocates a persistent value (one that is long-lived and possibly external\n    to the program) with undefined contents. Consumers of the allocated\n    result must assume nothing of the contents and use `discard` access.\n\n    Uninitialized allocations will have undefined contents and must only be used\n    when all bytes are discarded prior to any reads. Runtimes decide what\n    \"undefined contents\" means and here it only indicates that execution will be\n    correct even if the memory starts with non-zero values.\n\n    If multiple values are allocated from the same operation it implies that\n    they have matching lifetimes. When lowering to execution environments the\n    separate allocations may be fused into one or more slab allocations in order\n    to reduce overheads. How many allocations can be fused is based on the size\n    of the individual resources and the target constraints (how large any single\n    buffer may be, etc).",
    "inputs": [
      { "name": "storage_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyResource" }
    ],
    "attributes": [
      { "name": "uninitialized", "type": "UnitAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`uninitialized` $uninitialized^)?\n    (`on` `(` $affinity^ `)`)?\n    attr-dict `:`\n    type($result) `{` $storage_size `}`"
  },
  {
    "name": "stream.resource.alloca",
    "summary": "Allocates a transient value with undefined contents.",
    "description": "Allocates a transient value (one that is short-lived and local to the\n    current computation) with undefined contents. Consumers of the allocated\n    result must assume nothing of the contents and use `discard` access.\n\n    The resource returned is not valid for use until the timepoint is reached;\n    execution using this resource must await on the timepoint.\n\n    If the lifetime of the allocation is unknowable (analysis fails, etc) the\n    `indeterminate_lifetime` will be set indicating that the resource should not\n    be deallocated asynchronously and only after all references have been\n    released.",
    "inputs": [
      { "name": "storage_size", "type": "Stream_Size" },
      { "name": "await_timepoint", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyResource" },
      { "name": "result_timepoint", "type": "Stream_Timepoint" }
    ],
    "attributes": [
      { "name": "indeterminate_lifetime", "type": "UnitAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`uninitialized`\n    (`indeterminate` $indeterminate_lifetime^)?\n    (`on` `(` $affinity^ `)`)?\n    (`await` `(` $await_timepoint^ `)` `=` `` `>`):(`:`)?\n    attr-dict\n    type($result) `{` $storage_size `}`\n    `=` `` `>`\n    type($result_timepoint)"
  },
  {
    "name": "stream.resource.constants",
    "summary": "Asynchronously uploads or maps constant values.",
    "description": "Represents an upload of constant resources that may be packed, suballocated,\n    and mapped depending on the final lowering target.\n\n    In runtime environments where memory is shared between host and device this\n    turns into a mapping operation that avoids additional memory allocation and\n    copies. When memory cannot be shared an asynchronous stream will be created\n    to allocate and copy all of the constant values.\n\n    Though this op returns a unique resource for each constant value it's\n    expected that almost all end up aliasing into the same storage. The exact\n    packing and number of storage resources that are needed are not known until\n    lowering to a particular backend, though, so they are separate here for\n    proper usage tracking.\n\n    Both constant and variable resources can be produced; a constant is\n    immutable while a variable will be treated as a constant-value initializer\n    for a mutable resource. By modeling these together it's not required that\n    variable initializers first be allocated, copied to the target, and then\n    copied into the variable storage if the target is capable of doing a direct\n    upload or mapping.",
    "inputs": [
      { "name": "result_sizes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" },
      { "name": "result_timepoint", "type": "Stream_Timepoint" }
    ],
    "attributes": [
      { "name": "values", "type": "TypedArrayAttrBase" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    attr-dict `:`\n    custom<ConstantValueList>(type($results),\n                              $result_sizes,\n                              $values)\n    `\\n` ` ` ` ` `=` `` `>` type($result_timepoint)"
  },
  {
    "name": "stream.resource.dealloca",
    "summary": "Frees a transient value when available.",
    "description": "Deallocates a transient value (one that is short-lived and local to the\n    current computation) previously allocated using `stream.resource.alloca`.\n\n    The resource is considered live and valid until the provided timepoint is\n    reached and the memory is only made available for future requests after\n    the result timepoint is reached.\n\n    If `prefer_origin` is set then the deallocation will be scheduled against\n    the original placement affinity of the allocation at runtime. In cases where\n    the original placement is unavailable the affinity of the deallocation op\n    will be used to perform a barrier operation.",
    "inputs": [
      { "name": "operand", "type": "Stream_AnyResource" },
      { "name": "operand_size", "type": "Stream_Size" },
      { "name": "await_timepoint", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result_timepoint", "type": "Stream_Timepoint" }
    ],
    "attributes": [
      { "name": "prefer_origin", "type": "UnitAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`origin` $prefer_origin^)?\n    (`on` `(` $affinity^ `)`)?\n    (`await` `(` $await_timepoint^ `)` `=` `` `>`)?\n    $operand `:` type($operand) `{` $operand_size `}`\n    `=` `` `>` type($result_timepoint)\n    attr-dict"
  },
  {
    "name": "stream.resource.is_terminal",
    "summary": "Returns true if the resource has no other claims to ownership.",
    "description": "Resources may have multiple conceptual owners and are generally only safe to\n    deallocate if the code performing the deallocation is the last owner. This\n    op can be used to see if the current owner is the last one claiming\n    ownership and can deallocate or reuse the resource for other purposes.",
    "inputs": [
      { "name": "operand", "type": "Stream_AnyResource" },
      { "name": "operand_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "I1" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `{` $operand_size `}`"
  },
  {
    "name": "stream.resource.load",
    "summary": "Loads a value from a staging resource.",
    "description": "Returns the element(s) at the given offset in the staging resource.\n    The operation will complete synchronously against the resource though it may\n    introduce a yield point if the staging resource needs to be transferred.",
    "inputs": [
      { "name": "source", "type": "Stream_StagingResource" },
      { "name": "source_size", "type": "Stream_Size" },
      { "name": "source_offset", "type": "Stream_Offset" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$source `[` $source_offset `]` `:`\n    type($source) `` `{` $source_size `}`\n    `->`\n    type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.resource.pack",
    "summary": "Packs variable-sized slices into a single slab.",
    "description": "Performs a greedy packing of one or more sized slices with specified\n    lifetimes and returns their relative offsets in an aliased linear space.\n\n    Slices are `[start, end] = %slice_byte_size`, where the start and end values\n    define an inclusive lifetime range and the size is the total number of bytes\n    required to be live for that range.\n\n    ```mlir\n    // Computes the total length required for the packed values and the offsets\n    // of the 3 slices requested relative to the base of the packed memory:\n    %total_length, %offset_0, %offset_1, %offset_2 =\n        stream.resource.pack\n            // Each slice gets one result offset:\n            slices({\n              // 3 slices where A and B overlap and will get unique offsets\n              // while B and C do not overlap and are allowed to alias.\n              [0, 10] = %size_0,  // A => %offset_0\n              [3,  8] = %size_1,  // B => %offset_1\n              [9, 10] = %size_2,  // C => %offset_2\n              ...\n            }) : index\n    ```\n\n    The lifetime start and end points (inclusive) are only used for relative\n    comparisons and may originate with any meaning (op order in block, epoch,\n    phase of the moon, etc). The packing algorithm uses the intervals to\n    determine slice liveness and when aliasing is safe.\n\n    The size of each slice may either be a constant or runtime-computed dynamic\n    value. Constant slices can achieve more dense packing than the dynamic\n    values and CSE/canonicalization should be applied to ensure that as many of\n    the dynamic values are equivalent if possible.\n\n    The total length required to pack all slices is returned and can be used to\n    acquire storage. The individual slice offsets are 0-based and as such if are\n    directly used as buffer offsets may need additional offsetting. This can\n    either be applied via the optional `offset` operand or slicing of the\n    underlying allocation buffer.",
    "inputs": [
      { "name": "offset", "type": "Optional" },
      { "name": "dynamic_slice_sizes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "total_length", "type": "Stream_Size" },
      { "name": "packed_offsets", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "lifetime_intervals", "type": "Stream_IndexArrayAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    (`offset` `(` $offset^ `)`)?\n    `slices` `(` `{`\n    custom<PackSliceRanges>($lifetime_intervals,\n                            $dynamic_slice_sizes,\n                            type($packed_offsets))\n    `}` `)`\n    `:` type($total_length)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.resource.release",
    "summary": "Releases an ownership claim on the resource.",
    "description": "A resource is allowed to be deallocated or reused once the last owner\n    releases their ownership. Returns a boolean indicating whether the owner\n    releasing their claim was the last owner (equivalent to having used\n    `stream.resource.is_terminal`).",
    "inputs": [
      { "name": "operand", "type": "Stream_AnyResource" },
      { "name": "operand_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "I1" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `{` $operand_size `}`"
  },
  {
    "name": "stream.resource.retain",
    "summary": "Retains ownership of the resource.",
    "description": "Retains ownership of the resource for the parent code until a corresponding\n    `stream.resource.release` is used to release the ownership claim. A\n    particular resource may have multiple owners at any given time indicating\n    that multiple parts of the program are sharing responsibility for\n    deallocating the resource.\n\n    This is primarily only relevant for resources allocated asynchronously and\n    that need `stream.resource.dealloca` operations scheduled on timelines.\n    Synchronously allocated resources have their lifetimes tied directly to\n    their reference-type and do not need additional ownership management.",
    "inputs": [
      { "name": "operand", "type": "Stream_AnyResource" },
      { "name": "operand_size", "type": "Stream_Size" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `{` $operand_size `}`"
  },
  {
    "name": "stream.resource.size",
    "summary": "Returns the size of the resource storage in bytes.",
    "description": "Returns a possibly runtime-dynamic byte size of the resource backing\n    storage. This may differ from the logical storage size of a value based on\n    the alignment requirements of the target as well as encoding of higher level\n    values such as sparse tensor formats.",
    "inputs": [
      { "name": "operand", "type": "Stream_AnyResource" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_Size" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$operand\n    attr-dict `:` type($operand)"
  },
  {
    "name": "stream.resource.store",
    "summary": "Stores a value into a staging resource.",
    "description": "The operation will complete synchronously against the resource though it may\n    introduce a yield point if the staging resource needs to be acquired.",
    "inputs": [
      { "name": "target", "type": "Stream_StagingResource" },
      { "name": "target_size", "type": "Stream_Size" },
      { "name": "target_offset", "type": "Stream_Offset" },
      { "name": "value", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$value `,`\n    $target `[` $target_offset `]` `:`\n    type($value)\n    `->`\n    type($target) `{` $target_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.resource.subview",
    "summary": "Slices out a subview of a value.",
    "description": "Aliases a byte subrange of a resource.",
    "inputs": [
      { "name": "source", "type": "Stream_AnyResource" },
      { "name": "source_size", "type": "Stream_Size" },
      { "name": "source_offset", "type": "Stream_Offset" },
      { "name": "result_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyResource" }
    ],
    "assemblyFormat": "$source `[` $source_offset `]` `:`\n    type($source) `` `{` $source_size `}` `->`\n    type($result) `` `{` $result_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.resource.try_map",
    "summary": "Maps read-only memory into a resource.",
    "description": "Synchronously maps a host heap buffer into a stream-accessible resource\n    with the requested lifetime. If the given source cannot be mapped the\n    `did_map` result will be 0 and users must find another route into memory\n    (such as file I/O). The resulting resource is not coherent with the source\n    and behavior is undefined if the underlying contents change.",
    "inputs": [
      { "name": "source", "type": "Util_BufferType" },
      { "name": "source_offset", "type": "Stream_Offset" },
      { "name": "result_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "did_map", "type": "I1" },
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $source `[` $source_offset `]` `:`\n    type($source)\n    `->`\n    type($did_map) `,` type($result) `` `{` $result_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.return",
    "summary": "Returns results from a region.",
    "description": "The values returned are copied by-value.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict\n    $operands `:` type($operands)"
  },
  {
    "name": "stream.tensor.clone",
    "summary": "Clones the contents of a value.",
    "description": "Clones the contents of a value at a snapshot in time. Future changes to the\n    cloned value will not effect the result. Acts as a copy-on-write operation.",
    "inputs": [
      { "name": "source", "type": "Stream_AnyStreamResource" },
      { "name": "source_encoding_dims", "type": "Stream_ShapeDynamicDims" },
      { "name": "source_size", "type": "Stream_Size" },
      { "name": "result_encoding_dims", "type": "Stream_ShapeDynamicDims" },
      { "name": "result_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "source_encoding", "type": "TypeAttr" },
      { "name": "result_encoding", "type": "TypeAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $source `:`\n    $source_encoding (`` `{` $source_encoding_dims^ `}`)?\n    `in`\n    type($source) `` `{` $source_size `}`\n    `->`\n    $result_encoding (`` `{` $result_encoding_dims^ `}`)?\n    `in`\n    type($result) `` `{` $result_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.tensor.constant",
    "summary": "Defines a constant tensor value.",
    "description": "Returns a typed resource initialized to the given constant value.",
    "inputs": [
      { "name": "result_encoding_dims", "type": "Stream_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "value", "type": "TypedAttrInterface" },
      { "name": "result_encoding", "type": "TypeAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    `:`\n    $result_encoding (`` `{` $result_encoding_dims^ `}`)?\n    `in`\n    type($result)\n    `=`\n    $value\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.tensor.dispatch",
    "summary": "Dispatches a parallelized grid of work.",
    "description": "Calls the specified entry point function once for each element in the\n    specified workgroup count. Each workgroup has access to the same operands\n    and results and is able to load/store at will.",
    "inputs": [
      { "name": "workload", "type": "Variadic" },
      { "name": "mixed_operands", "type": "Variadic" },
      { "name": "operand_sizes", "type": "Variadic" },
      { "name": "operand_encoding_dims", "type": "Stream_ShapeDynamicDims" },
      { "name": "result_sizes", "type": "Variadic" },
      { "name": "result_encoding_dims", "type": "Stream_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "entry_points", "type": "SymbolRefArrayAttr" },
      { "name": "operand_encodings", "type": "TypeArrayAttr" },
      { "name": "result_encodings", "type": "TypeArrayAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    custom<DispatchEntryPoints>($entry_points)\n    (`[` $workload^ `]`)? ``\n    `(` $mixed_operands `)`\n    attr-dict `:`\n    custom<EncodedShapedFunctionType>(\n        ref($mixed_operands),\n        type($mixed_operands), $operand_sizes,\n        $operand_encodings, $operand_encoding_dims,\n        type($results), $result_sizes,\n        $result_encodings, $result_encoding_dims,\n        $tied_operands)"
  },
  {
    "name": "stream.tensor.empty",
    "summary": "Defines an empty tensor value.",
    "description": "Returns a typed resource initialized with no contents. This still carries\n    shape metadata and may encode to a non-empty resource such as in cases\n    where the empty representation still has data (e.g. sparse tensors).\n    Subsequent writes must populate any ranges of the tensor that are later\n    read.",
    "inputs": [
      { "name": "result_encoding_dims", "type": "Stream_ShapeDynamicDims" },
      { "name": "result_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "result_encoding", "type": "TypeAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    `:`\n    $result_encoding (`` `{` $result_encoding_dims^ `}`)?\n    `in`\n    type($result) `` `{` $result_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.tensor.encode",
    "summary": "Encodes the contents of a value.",
    "description": "Elones the contents of a value at a snapshot in time. Future changes to the\n    identity encoding will not effect the result. Acts as a copy-on-write\n    operation. Otherwise, an executable for encoding the value is generated\n    during lowering.",
    "inputs": [
      { "name": "source", "type": "Stream_AnyStreamResource" },
      { "name": "source_encoding_dims", "type": "Stream_ShapeDynamicDims" },
      { "name": "source_size", "type": "Stream_Size" },
      { "name": "result_encoding_dims", "type": "Stream_ShapeDynamicDims" },
      { "name": "result_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "source_encoding", "type": "TypeAttr" },
      { "name": "result_encoding", "type": "TypeAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $source `:`\n    $source_encoding (`` `{` $source_encoding_dims^ `}`)?\n    `in`\n    type($source) `` `{` $source_size `}`\n    `->`\n    $result_encoding (`` `{` $result_encoding_dims^ `}`)?\n    `in`\n    type($result) `` `{` $result_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.tensor.export",
    "summary": "Conversion placeholder for stream->other type conversion.",
    "description": "Defines a conversion to a higher-level dialect type such as `tensor` that\n    is resolved during lowering into the stream dialect. This can be used to\n    interoperate between levels of the stack that require specifying stream\n    types and those that prior to lowering do not handle them.",
    "inputs": [
      { "name": "source", "type": "AnyTypeOf" },
      { "name": "source_encoding_dims", "type": "Stream_ShapeDynamicDims" },
      { "name": "source_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "source_encoding", "type": "TypeAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $source `:`\n    $source_encoding (`` `{` $source_encoding_dims^ `}`)?\n    `in`\n    type($source) `` `{` $source_size `}`\n    `->`\n    type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.tensor.fill",
    "summary": "Fills a subview of a stream resource with a value.",
    "description": "Splats a value into a subview of the given stream resource and returns the\n    resource with the update applied.\n\n    Equivalent to a stream.tensor.splat + stream.tensor.update.",
    "inputs": [
      { "name": "target", "type": "Stream_AnyStreamResource" },
      { "name": "target_encoding_dims", "type": "Stream_ShapeDynamicDims" },
      { "name": "target_size", "type": "Stream_Size" },
      { "name": "start_indices", "type": "Variadic" },
      { "name": "lengths", "type": "Variadic" },
      { "name": "value", "type": "Stream_PrimitiveType" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "target_encoding", "type": "TypeAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $value `,` $target (`[` $start_indices `for` $lengths^ `]`)? `:`\n    type($value)\n    `->`\n    $target_encoding (`` `{` $target_encoding_dims^ `}`)?\n    `in`\n    custom<ShapedTiedResult>(type($target), $target_size)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.tensor.import",
    "summary": "Conversion placeholder for other->stream type conversion.",
    "description": "Defines a conversion from a higher-level dialect type such as `tensor` that\n    is resolved during lowering into the stream dialect. This can be used to\n    interoperate between levels of the stack that require specifying stream\n    types and those that prior to lowering do not handle them.\n\n    `consume` can be used to indicate a transfer of ownership. Though the\n    imported value may still have external references when consumed a resource\n    will be conceptually released from its existing owner and retained by the\n    importer atomically.",
    "inputs": [
      { "name": "source", "type": "AnyType" },
      { "name": "result_encoding_dims", "type": "Stream_ShapeDynamicDims" },
      { "name": "result_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "result_encoding", "type": "TypeAttr" },
      { "name": "consume", "type": "UnitAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    (`consume` $consume^)?\n    $source `:`\n    type($source)\n    `->`\n    $result_encoding (`` `{` $result_encoding_dims^ `}`)?\n    `in`\n    type($result) `{` $result_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.tensor.load",
    "summary": "Loads a value from a tensor element.",
    "description": "Returns the element at the given location from within the tensor.",
    "inputs": [
      { "name": "source", "type": "Stream_StagingResource" },
      { "name": "source_encoding_dims", "type": "Stream_ShapeDynamicDims" },
      { "name": "source_size", "type": "Stream_Size" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "source_encoding", "type": "TypeAttr" }
    ],
    "assemblyFormat": "$source (`[` $indices^ `]`)? `:`\n    $source_encoding (`` `{` $source_encoding_dims^ `}`)?\n    `in`\n    type($source) `` `{` $source_size `}`\n    `->`\n    type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.tensor.sizeof",
    "summary": "Calculates the storage size of a given high-level type.",
    "description": "Target-dependent storage size calculation using a high-level annotated type.\n    While within the stream dialect the storage size of a value is left as a\n    placeholder using this op. The requisite target-specific parameters for\n    expanding the size calculation are only available after affinities have been\n    assigned.",
    "inputs": [
      { "name": "encoding_dims", "type": "Stream_ShapeDynamicDims" }
    ],
    "outputs": [
      { "name": "storage_size", "type": "Stream_Size" }
    ],
    "attributes": [
      { "name": "encoding", "type": "TypeAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $encoding (`{` $encoding_dims^ `}`)?\n    attr-dict `:` type($storage_size)"
  },
  {
    "name": "stream.tensor.slice",
    "summary": "Slices out a cloned subview of a value.",
    "description": "Slices a subrange of a stream resource based on a tensor encoding. Acts as a\n    copy-on-write operation.",
    "inputs": [
      { "name": "source", "type": "Stream_AnyStreamResource" },
      { "name": "source_encoding_dims", "type": "Stream_ShapeDynamicDims" },
      { "name": "source_size", "type": "Stream_Size" },
      { "name": "start_indices", "type": "Variadic" },
      { "name": "lengths", "type": "Variadic" },
      { "name": "result_encoding_dims", "type": "Stream_ShapeDynamicDims" },
      { "name": "result_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "source_encoding", "type": "TypeAttr" },
      { "name": "result_encoding", "type": "TypeAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $source `[` $start_indices `for` $lengths `]` `:`\n    $source_encoding (`` `{` $source_encoding_dims^ `}`)?\n    `in`\n    type($source) `` `{` $source_size `}`\n    `->`\n    $result_encoding (`` `{` $result_encoding_dims^ `}`)?\n    `in`\n    type($result) `` `{` $result_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.tensor.splat",
    "summary": "Splats a value into a shaped tensor.",
    "description": "Returns a typed resource initialized to the given primitive value.",
    "inputs": [
      { "name": "value", "type": "Stream_PrimitiveType" },
      { "name": "result_encoding_dims", "type": "Stream_ShapeDynamicDims" },
      { "name": "result_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "result_encoding", "type": "TypeAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $value\n    `:` type($value)\n    `->`\n    $result_encoding (`` `{` $result_encoding_dims^ `}`)?\n    `in`\n    type($result) `` `{` $result_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.tensor.store",
    "summary": "Stores a value into a tensor element.",
    "description": "Returns a tensor with the element at the given index set to the given value.",
    "inputs": [
      { "name": "target", "type": "Stream_StagingResource" },
      { "name": "target_encoding_dims", "type": "Stream_ShapeDynamicDims" },
      { "name": "target_size", "type": "Stream_Size" },
      { "name": "indices", "type": "Variadic" },
      { "name": "value", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_StagingResource" }
    ],
    "attributes": [
      { "name": "target_encoding", "type": "TypeAttr" }
    ],
    "assemblyFormat": "$value `,`\n    $target (`[` $indices^ `]`)? `:`\n    type($value)\n    `->`\n    $target_encoding (`` `{` $target_encoding_dims^ `}`)?\n    `in`\n    custom<ShapedTiedResult>(type($target), $target_size)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.tensor.trace",
    "summary": "Traces one or more tensor values at runtime.",
    "description": "Traces out to a runtime trace sink (console, log file, etc) the given\n    tensors. The key is arbitrary and can be used for identifying the set of\n    values being traced.",
    "inputs": [
      { "name": "resources", "type": "Variadic" },
      { "name": "resource_sizes", "type": "Variadic" },
      { "name": "resource_encoding_dims", "type": "Stream_ShapeDynamicDims" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" },
      { "name": "resource_encodings", "type": "TypeArrayAttr" }
    ],
    "assemblyFormat": "$key `=` `[`\n    custom<EncodedResourceOperands>(\n        $resources, type($resources), $resource_sizes,\n        $resource_encodings, $resource_encoding_dims)\n    `]` attr-dict-with-keyword"
  },
  {
    "name": "stream.tensor.update",
    "summary": "Updates a slice of a subview of a resource in-place.",
    "description": "Copies a value into a resource based on tensor encodings. The returned value\n    is the entire updated target value.",
    "inputs": [
      { "name": "target", "type": "Stream_AnyStreamResource" },
      { "name": "target_encoding_dims", "type": "Stream_ShapeDynamicDims" },
      { "name": "target_size", "type": "Stream_Size" },
      { "name": "start_indices", "type": "Variadic" },
      { "name": "update", "type": "Stream_AnyStreamResource" },
      { "name": "update_encoding_dims", "type": "Stream_ShapeDynamicDims" },
      { "name": "update_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Stream_AnyStreamResource" }
    ],
    "attributes": [
      { "name": "target_encoding", "type": "TypeAttr" },
      { "name": "update_encoding", "type": "TypeAttr" },
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $update `,` $target `[` $start_indices `]` `:`\n    $update_encoding (`` `{` $update_encoding_dims^ `}`)?\n    `in`\n    type($update) `` `{` $update_size `}`\n    `->`\n    $target_encoding (`` `{` $target_encoding_dims^ `}`)?\n    `in`\n    custom<ShapedTiedResult>(type($target), $target_size)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.timepoint.await",
    "summary": "Awaits a timepoint before returning a set of resources.",
    "description": "After asynchronous execution scheduling resources may exist in different\n    states at different points in the execution timeline. This op enables\n    resolving the version of a resource after a particular point in the\n    timeline. As timepoints transitively chain the timepoint must only cover the\n    resource availability but not be limited to its original production\n    timepoint.",
    "inputs": [
      { "name": "resource_operands", "type": "Variadic" },
      { "name": "resource_operand_sizes", "type": "Variadic" },
      { "name": "await_timepoint", "type": "Stream_Timepoint" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "sync", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`sync` $sync^)?\n    $await_timepoint `=` `` `>`\n    $resource_operands `:`\n    custom<ShapedTypeList>(type($resource_operands),\n                              type($results), $resource_operand_sizes)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.timepoint.barrier",
    "summary": "Returns a timepoint indicating when a resource is available.",
    "description": "After asynchronous execution scheduling resources may exist in different\n    states at different points in the execution timeline. This op enables\n    identifying when the version of a resource after a particular point in the\n    timeline is available. As timepoints transitively chain the timepoint must\n    only cover the resource availability but not be limited to its original\n    production timepoint.",
    "inputs": [
      { "name": "resource", "type": "AnyTypeOf" },
      { "name": "resource_size", "type": "Stream_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" },
      { "name": "result_timepoint", "type": "Stream_Timepoint" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $resource `:` type($resource) `` `{` $resource_size `}`\n    `=` `` `>`\n    type($result_timepoint)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.timepoint.chain_external",
    "summary": "Exports a timepoint to an external dialect type.",
    "description": "Defines a conversion to an external dialect type such as `hal.fence`\n    that is resolved during lowering into the stream dialect. This can be used\n    to interoperate between levels of the stack that require specifying stream\n    types and those that prior to lowering do not handle them.",
    "inputs": [
      { "name": "await_timepoint", "type": "Stream_Timepoint" },
      { "name": "external_values", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $await_timepoint\n    `=` `` `>`\n    `(` $external_values `:` type($external_values) `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.timepoint.export",
    "summary": "Exports a timepoint to an external dialect type.",
    "description": "Defines a conversion to an external dialect type such as `hal.fence`\n    that is resolved during lowering into the stream dialect. This can be used\n    to interoperate between levels of the stack that require specifying stream\n    types and those that prior to lowering do not handle them.",
    "inputs": [
      { "name": "await_timepoint", "type": "Stream_Timepoint" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $await_timepoint\n    `=` `` `>`\n    `(` type($results) `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.timepoint.immediate",
    "summary": "Results an immediately-available timepoint.",
    "description": "Timepoints indicate a point in the execution timeline and this op can be\n    used to get a placeholder representing the start of the timeline. Any waits\n    on the returned timepoint will resolve immediately. This generally folds\n    away but can be useful if needing to initialize globals or branch args.",
    "outputs": [
      { "name": "result_timepoint", "type": "Stream_Timepoint" }
    ],
    "assemblyFormat": "attr-dict\n    `=` `` `>` type($result_timepoint)"
  },
  {
    "name": "stream.timepoint.import",
    "summary": "Imports a timepoint from an external dialect type.",
    "description": "Defines a conversion from an external dialect type such as `hal.semaphore`\n    that is resolved during lowering into the stream dialect. This can be used\n    to interoperate between levels of the stack that require specifying stream\n    types and those that prior to lowering do not handle them.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result_timepoint", "type": "Stream_Timepoint" }
    ],
    "attributes": [
      { "name": "affinity", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "(`on` `(` $affinity^ `)`)?\n    $operands `:` `(` type($operands) `)`\n    `=` `` `>`\n    type($result_timepoint)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.timepoint.join",
    "summary": "Joins one or more timepoints into the max of all of them.",
    "description": "Returns a timepoint that indicates that all of the input timepoints have\n    been reached.",
    "inputs": [
      { "name": "await_timepoints", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result_timepoint", "type": "Stream_Timepoint" }
    ],
    "assemblyFormat": "`max` `(` $await_timepoints `)` `=` `` `>` type($result_timepoint)\n    attr-dict-with-keyword"
  },
  {
    "name": "stream.yield",
    "summary": "Yields stream values from an execution region.",
    "description": "The values returned represent the asynchronous value at the point in time\n    the SSA value is defined (or tied).",
    "inputs": [
      { "name": "resource_operands", "type": "Variadic" },
      { "name": "resource_operand_sizes", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict\n    ($resource_operands^ `:`\n        custom<ShapedTypeList>(type($resource_operands),\n                                  $resource_operand_sizes))?"
  },
  {
    "name": "tensor.bitcast",
    "summary": "tensor bitcast operation",
    "description": "Bitcast a tensor from one type to another type of equivalent element width.\n    If both are ranked, then the rank should be the same and static dimensions\n    should match.\n\n    Example:\n\n    ```mlir\n    // Bitcast from unsigned to signed or signless integer.\n    %2 = tensor.bitcast %1 : tensor<4xui32> to tensor<4xi32>\n    ```",
    "inputs": [
      { "name": "source", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "dest", "type": "TensorOf" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($dest)"
  },
  {
    "name": "tensor.cast",
    "summary": "tensor cast operation",
    "description": "Convert a tensor from one type to an equivalent type without changing any\n    data elements. The source and destination types must both be tensor types\n    with the same element type. If both are ranked, then the rank should be the\n    same and static dimensions should match. The operation is invalid if\n    converting to a mismatching constant dimension.\n\n    Example:\n\n    ```mlir\n    // Convert from unknown rank to rank 2 with unknown dimension sizes.\n    %2 = tensor.cast %1 : tensor<*xf32> to tensor<?x?xf32>\n\n    // Convert to a type with more known dimensions.\n    %3 = tensor.cast %2 : tensor<?x?xf32> to tensor<4x?xf32>\n\n    // Discard static dimension and rank information.\n    %4 = tensor.cast %3 : tensor<4x?xf32> to tensor<?x?xf32>\n    %5 = tensor.cast %4 : tensor<?x?xf32> to tensor<*xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "dest", "type": "AnyTensor" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($dest)"
  },
  {
    "name": "tensor.collapse_shape",
    "summary": "operation to produce a tensor with a smaller rank",
    "description": "The `tensor.collapse_shape` op produces a new tensor of lower (or equal)\n    rank whose dimension sizes are a reassociation of the original `src` dimensions.\n\n    A reassociation is defined as a continuous grouping of dimensions and is\n    represented by an array of DenseI64ArrayAttr attribute. The reassociation\n    maps are applied to the operand shape to obtain the result shape.\n\n\n    Example:\n\n    ```mlir\n    // Dimension collapse (i, j) -> i' and k -> k'\n    %b = tensor.collapse_shape %a [[0, 1], [2]]\n        : tensor<?x?x?xf32> into tensor<?x?xf32>\n    ```",
    "inputs": [
      { "name": "src", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "reassociation", "type": "IndexListArrayAttr" }
    ],
    "assemblyFormat": "$src $reassociation attr-dict `:` type($src) `into` type($result)"
  },
  {
    "name": "tensor.concat",
    "summary": "tensor concatenation operation",
    "description": "The \"concat\" operation constructs a tensor out of a variadic list of input\n    tensors, concatenated along a static dimension number. All inputs and the\n    result type must share the same rank.\n\n    `dim` specifies the dimension along which to concatenate. The size of the\n    concatenated dimension in the result must be equal to the sum of the sizes\n    of the inputs along that dimension. All other dimensions in both the inputs\n    and result must be the same size.\n\n    Example:\n\n    ```mlir\n    %0 = tensor.concat dim(0) %0, %1, %2 :\n        (tensor<3x6xf32>, tensor<3x6xf32>, tensor<1x6xf32) -> tensor<7x6xf32>\n\n    // Dynamic + dynamic -> static\n    %0 = tensor.concat dim(1) %0, %1, %2 :\n        (tensor<3x?xf32>, tensor<3x2xf32>, tensor<3x?xf32) -> tensor<3x10xf32>\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "dim", "type": "I64Attr" }
    ],
    "assemblyFormat": "`dim` `(` $dim `)` $inputs attr-dict\n    `:` functional-type(operands, results)"
  },
  {
    "name": "tensor.dim",
    "summary": "dimension index operation",
    "description": "The `tensor.dim` operation takes a tensor and a dimension operand of type\n    `index`. It returns the size of the requested dimension of the given\n    tensor. If the dimension index is out of bounds, the behavior is undefined.\n\n    The specified tensor type is that of the first operand.\n\n    Example:\n\n    ```mlir\n    // Always returns 4, can be constant folded:\n    %c0 = arith.constant 0 : index\n    %x = tensor.dim %A, %c0 : tensor<4x?xf32>\n\n    // Return the dynamic dimension of %A.\n    %c1 = arith.constant 1 : index\n    %y = tensor.dim %A, %c1 : tensor<4x?xf32>\n\n    // Equivalent generic form:\n    %x = \"tensor.dim\"(%A, %c0) : (tensor<4x?xf32>, index) -> index\n    %y = \"tensor.dim\"(%A, %c1) : (tensor<4x?xf32>, index) -> index\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyNon0RankedOrUnrankedTensor" },
      { "name": "index", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "attr-dict $source `,` $index `:` type($source)"
  },
  {
    "name": "tensor.empty",
    "summary": "empty tensor operation",
    "description": "`tensor.empty` is an operation that defines a tensor of a particular shape.\n    The shape could be dynamic or static. The contents of the tensor are\n    unspecified and the only purpose of the op result is to materialize the\n    specified shape in IR and make it available to other transformations.\n\n    `tensor.empty` is useful in transformations that expect destination style\n    ops. I.e., ops that implement `DestinationStyleOpInterface`. Ops that are\n    not in destination style can be made compatible with such transformations\n    with a `tensor.empty` destination.\n\n    Note: This op can be lowered to a `bufferization.alloc_tensor`, at which\n    point it turns into an explicit buffer allocation.",
    "inputs": [
      { "name": "dynamicSizes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "`(`$dynamicSizes`)` attr-dict `:` type($result)"
  },
  {
    "name": "tensor.expand_shape",
    "summary": "operation to produce a tensor with a higher rank",
    "description": "The `tensor.expand_shape` op produces a tensor of higher (or equal)\n    rank than the operand `src` whose dimension sizes are a reassociation of\n    `src`.\n\n    A reassociation is defined as a continuous grouping of dimensions and is\n    represented with an array of DenseI64ArrayAttr attribute.  The reassociation\n    maps applied to the result tensor with the higher rank must result in the\n    operand tensor with the smaller rank.\n\n    The representation for the output shape supports a partially-static\n    specification via attributes specified through the `static_output_shape`\n    argument.  A special sentinel value `ShapedType::kDynamic` encodes that the\n    corresponding entry has a dynamic value.  There must be exactly as many SSA\n    inputs in `output_shape` as there are `ShapedType::kDynamic` entries in\n    `static_output_shape`.\n\n    Example:\n\n    ```mlir\n    // Dimension expansion i -> (i', j') and (k) -> (k')\n    %b = tensor.expand_shape %a [[0, 1], [2]] output_shape [%sz0, %sz1, 32]\n        : tensor<?x32xf32> into tensor<?x?x32xf32>\n    ```",
    "inputs": [
      { "name": "src", "type": "AnyTensor" },
      { "name": "output_shape", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "reassociation", "type": "IndexListArrayAttr" },
      { "name": "static_output_shape", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$src $reassociation `output_shape`\n    custom<DynamicIndexList>($output_shape, $static_output_shape) attr-dict `:`\n    type($src) `into` type($result)"
  },
  {
    "name": "tensor.extract",
    "summary": "element extraction operation",
    "description": "The `tensor.extract` op reads a ranked tensor and returns one element as\n    specified by the given indices. The result of the op is a value with the\n    same type as the elements of the tensor. The arity of indices must match\n    the rank of the accessed value. All indices should all be of `index` type.\n\n    Example:\n\n    ```mlir\n    %4 = tensor.extract %t[%1, %2] : tensor<4x4xi32>\n    %5 = tensor.extract %rt[%1, %2] : tensor<?x?xi32>\n    ```",
    "inputs": [
      { "name": "tensor", "type": "AnyRankedTensor" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "$tensor `[` $indices `]` attr-dict `:` type($tensor)"
  },
  {
    "name": "tensor.extract_slice",
    "summary": "extract slice operation",
    "description": "The \"extract_slice\" operation extract a tensor from another tensor as\n    specified by the operation's offsets, sizes and strides arguments.\n\n    The extract_slice operation supports the following arguments:\n\n    * source: the \"base\" tensor from which to extract a slice.\n    * offsets: tensor-rank number of offsets into the \"base\" tensor from which\n               to extract the slice.\n    * sizes: tensor-rank number of sizes which specify the sizes of the result\n             tensor type.\n    * strides: tensor-rank number of strides specifying subsampling in each\n               dimension.\n\n    The representation based on offsets, sizes and strides support a\n    partially-static specification via attributes specified through the\n    `static_offsets`, `static_sizes` and `static_strides` arguments. A special\n    sentinel value ShapedType::kDynamic encodes that the corresponding entry has\n    a dynamic value.\n\n    After buffer allocation, the \"extract_slice\" op is expected to lower into a\n    memref.subview op.\n\n    An extract_slice operation may additionally reduce the rank of the resulting\n    tensor by removing dimensions that are statically known to be of size 1.\n    This rank-reduction behavior is not required by the op semantics: this\n    flexibility allows to progressively drop unit dimensions while lowering\n    between different flavors of ops on that operate on tensors.\n\n    #### Verification vs Inference in the rank-reduced case\n\n    Note that there may be multiple ways to infer a resulting rank-reduced type.\n      e.g. 1x6x1 could potentially rank-reduce to either 1x6 or 6x1 2-D shapes.\n\n    To disambiguate, the inference helpers `inferCanonicalRankReducedResultType`\n    only drop the first unit dimensions, in order:\n      e.g. 1x6x1 rank-reduced to 2-D will infer the 6x1 2-D shape, but not 1x6.\n\n    Verification however has access to result type and does not need to infer.\n    The verifier calls `isRankReducedType(getSource(), getResult())` to\n    determine whether the result type is rank-reduced from the source type.\n    This computes a so-called rank-reduction mask, consisting of dropped unit\n    dims, to map the rank-reduced type to the source type by dropping ones:\n      e.g. 1x6 is a rank-reduced version of 1x6x1 by mask {2}\n           6x1 is a rank-reduced version of 1x6x1 by mask {0}\n           1x2x1x4 is a rank-reduced version of 1x1x2x1x1x4x1 by mask {1, 4, 6}\n             (remaining common 1 dimensions are matched eagerly)\n\n    Example:\n\n    ```mlir\n    // Rank-reducing extract_slice.\n    %1 = tensor.extract_slice %0[0, 0, 0][1, 16, 4][1, 1, 1] :\n      tensor<8x16x4xf32> to tensor<16x4xf32>\n    %3 = tensor.extract_slice %2[%o0, 4, %o2][1, %sz1, 1][1, %st1, 1] :\n      tensor<8x16x4xf32> to tensor<1x?xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedTensor" },
      { "name": "offsets", "type": "Variadic" },
      { "name": "sizes", "type": "Variadic" },
      { "name": "strides", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "static_offsets", "type": "DenseI64ArrayAttr" },
      { "name": "static_sizes", "type": "DenseI64ArrayAttr" },
      { "name": "static_strides", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$source ``\n    custom<DynamicIndexList>($offsets, $static_offsets)\n    custom<DynamicIndexList>($sizes, $static_sizes)\n    custom<DynamicIndexList>($strides, $static_strides)\n    attr-dict `:` type($source) `to` type($result)"
  },
  {
    "name": "tensor.from_elements",
    "summary": "tensor from elements operation.",
    "description": "Create a N-D tensor from a range of same-type arguments. The number of\n    provided `elements` should equal to the number of the elements in the\n    result type. The `elements` correspond to a flattened tensor.\n\n    Example:\n\n    ```mlir\n    tensor.from_elements %a, %b, %c, %d, %e, %f :  tensor<2x3xindex>\n    ```\n\n    will result in a tensor\n\n    [[%a, %b, %c]\n     [%d, %e, %f]]",
    "inputs": [
      { "name": "elements", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyStaticShapeTensor" }
    ],
    "assemblyFormat": "$elements attr-dict `:` type($result)"
  },
  {
    "name": "tensor.gather",
    "summary": "gather a subset of a tensor at specified indices",
    "description": "The `gather` operation extracts a subset of the elements from a `source`\n    tensor at the given indices.\n\n    In its most general form, the tensor of indices specifies all the coordinates\n    of every element to extract (i.e. COO format, without the payload).\n    The indices are expected to be confined to coordinate values that fit the\n    range of the `source` tensor, otherwise the behavior is undefined.\n\n    The leading dimensions of the index tensor give the result tensor its leading\n    dimensions. The trailing dimensions of the result tensor are obtained from\n    the source tensor by omitting the dimensions specified in `gather_dims`\n    (rank-reducing semantics) or setting them to `1` (rank-preserving semantics)\n    (see examples).\n    The trailing dimension of the index tensor contains the coordinates and is\n    expected to have its size equal to the number of dimensions being gathered.\n    This convention allows an idiomatic specification and lowering of \"gathering\n    multiple N-D slices from the source tensor\".\n\n    Note: in the examples below, we separate out the indexing part of the tensor\n    type by a whitespace for readability purposes.\n\n    Example:\n\n    ```mlir\n        // For each 1x2 triple of coordinates in %indices, extract the\n        // element (i.e. 0-D subset) at the coordinates triple in %source.\n        //\n        %out = tensor.gather %source[%indices] gather_dims([0, 1, 2]) :\n          (tensor<4x4x4xf32>, tensor<1x2x 3xindex>) -> tensor<1x2x 1x1x1xf32>\n\n        // Note: result type may be further rank-reduced to tensor<1x2x f32>.\n    ```\n\n    A slice variant is provided to allow specifying whole slices of the source\n    tensor.\n\n    Example:\n\n    ```mlir\n        // For each 5x6 singleton of coordinates in %indices, extract the 2-D\n        // slice %source[*, %indices[...]:%indices[...] + 1, *] with the indices\n        // corresponding to the `gather_dims` attribute specified by %indices.\n        //\n        %out = tensor.gather %source[%indices] gather_dims([1]) :\n          (tensor<3x4x5xf32>, tensor<6x7x 1xindex>) -> tensor<6x7x 3x1x5xf32>\n\n        // Note: result type may be further rank-reduced to tensor<6x7x 3x5xf32>.\n    ```\n\n    The dimensions specified in the gather_dims attribute are ones for which the\n    result tensor has size `1`.\n    I.e. if the source type is `axbxcxd` and the coordinates are [1, 3], then\n    the shape suffix is `ax1xcx1`.\n    Gather also allows rank-reducing semantics where the shape `ax1xcx1` can be\n    further simplified to `axc`.\n\n    The elemental type of the indices tensor can be any integer type.\n    In the absence of target-specific or problem specific information the default\n    type one should use is `index`.\n\n    This operation does not support unranked tensors.\n\n    An optional `unique` unit attribute may be specified to indicate that the\n    coordinates in `indices` are statically guaranteed to be unique at runtime.\n    Incorrectly setting the `unique` attribute when the coordinates are not truly\n    unique is undefined behavior.\n\n    Only full slices are meant to be supported by this op, if one desires\n    partial slices (e.g. strided windows) one should compose this op with other\n    tensor ops (e.g. tensor.extract_slice). This is to avoid a slippery slope of\n    complexity that would make the op unusable in practice.\n\n    At the tensor-level, the index tensor is specified in an AoS form (i.e.\n    coordinate tuple is the most minor). It is the responsibility of further\n    lowerings and bufferization to implement various concrete layouts.\n\n    Note: As currently specified, the operation must lower to an abstraction that\n    performs copies to the output tensor. This is because the buffer type system\n    is currently not rich enough to allow multiple non-contiguous views in the\n    same type. This is visible more clearly in a notional buffer version of the\n    op:\n\n    ```mlir\n        // memref<?x4x1xf32> is a contiguous buffer of ?x4x1 elements.\n        // gather from random source slices must copy to the contiguous output.\n        %out = memref.gather %source[%indices] gather_dims([1]) :\n          (memref<4x4xf32>, memref<?x 1xindex>) -> memref<?x 4x1xf32>\n\n        // Nested buffer support would allow gather to directly index into the\n        // source buffer (i.e. represent a jagged view into the source).\n        %out = memref.gather %source[%indices] gather_dims([1]) :\n          (memref<4x4xf32>, memref<?x 1xindex>) -> memref<? x memref<4x1xf32>>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedTensor" },
      { "name": "indices", "type": "RankedTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "gather_dims", "type": "DenseI64ArrayAttr" },
      { "name": "unique", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$source `[` $indices `]`\n      `gather_dims` `(` $gather_dims `)`\n      (`unique` $unique^)?\n      attr-dict\n    `:` functional-type(operands, results)"
  },
  {
    "name": "tensor.generate",
    "summary": "Creates a dynamically sized tensor from elements",
    "description": "This operation creates a dynamically sized tensor with elements of any type.\n    It expects one index operand per dynamic extent of the result tensor.\n\n    The body region defines the tensor's elements. It takes index operands as\n    its region arguments that span the index space. The element at the given\n    position is yielded with the `yield` operation (see `YieldOp`). There is\n    no defined ordering to the invocations of the body. It is conceptually\n    a \"parallel map\" operation.\n\n    Example:\n\n    ```mlir\n      %tnsr = tensor.generate %m, %n {\n      ^bb0(%i : index, %j : index, %k : index):\n        ...\n        yield %elem : f32\n      } : tensor<?x3x?f32>\n    ```",
    "inputs": [
      { "name": "dynamicExtents", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "$dynamicExtents $body attr-dict `:` type($result)"
  },
  {
    "name": "tensor.insert",
    "summary": "element insertion operation",
    "description": "The `tensor.insert` op inserts a scalar into a ranked tensor `dest` as\n    specified by the operation's indices.\n\n    It returns a copy of `dest` with the indexed position updated to the value\n    of `scalar`.\n\n    The arity of `indices `must match the rank of the tensor `dest`. All\n    indices should be of `index` type.\n\n    Example:\n\n    ```mlir\n    %4 = tensor.insert %t into %dest[%1, %2] : tensor<4x4xi32>\n    %5 = tensor.insert %rt into %dest[%1, %2] : tensor<?x?xi32>\n    ```",
    "inputs": [
      { "name": "scalar", "type": "AnyType" },
      { "name": "dest", "type": "AnyRankedTensor" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "$scalar `into` $dest `[` $indices `]` attr-dict `:` type($dest)"
  },
  {
    "name": "tensor.insert_slice",
    "summary": "insert_slice operation",
    "description": "The \"insert_slice\" operation insert a tensor `source` into another\n    tensor `dest` as specified by the operation's offsets, sizes and strides\n    arguments.\n\n    It returns a copy of `dest` with the proper slice updated with the value\n    of `source`.\n\n    The insert_slice operation supports the following arguments:\n\n    * source: the tensor that is inserted.\n    * dest: the tensor into which the source tensor is inserted.\n    * offsets: tensor-rank number of offsets into the `dest` tensor into which\n               the slice is inserted.\n    * sizes: tensor-rank number of sizes which specify the sizes of the source\n             tensor type.\n    * strides: tensor-rank number of strides that specify subsampling in each\n               dimension.\n\n    The representation based on offsets, sizes and strides support a\n    partially-static specification via attributes specified through the\n    `static_offsets`, `static_sizes` and `static_strides` arguments. A special\n    sentinel value ShapedType::kDynamic encodes that the corresponding entry has\n    a dynamic value.\n\n    After buffer allocation, the \"insert_slice\" op is expected to lower into a\n    memref.subview op.\n\n    An insert_slice operation may additionally specify insertion into a tensor\n    of higher rank than the source tensor, along dimensions that are statically\n    known to be of size 1.\n    This rank-altering behavior is not required by the op semantics: this\n    flexibility allows to progressively drop unit dimensions while lowering\n    between different flavors of ops on that operate on tensors.\n    The rank-altering behavior of tensor.insert_slice matches the rank-reducing\n    behavior of tensor.extract_slice.\n\n    #### Verification in the rank-reduced case\n\n    The same verification discussion and mechanisms apply as for ExtractSliceOp.\n    Unlike ExtractSliceOp however, there is no need for a specific inference.\n\n    Example:\n\n    ```mlir\n    // Rank-altering insert_slice.\n    %1 = tensor.insert_slice %t into %0[0, 0, 0][1, 16, 4][1, 1, 1] :\n      tensor<16x4xf32> into tensor<8x16x4xf32>\n    %3 = tensor.insert_slice %tt into %2[%o0, 4, %o2][1, %sz1, 1][1, %st1, 1] :\n      tensor<1x?xf32> into tensor<8x16x4xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedTensor" },
      { "name": "dest", "type": "AnyRankedTensor" },
      { "name": "offsets", "type": "Variadic" },
      { "name": "sizes", "type": "Variadic" },
      { "name": "strides", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "static_offsets", "type": "DenseI64ArrayAttr" },
      { "name": "static_sizes", "type": "DenseI64ArrayAttr" },
      { "name": "static_strides", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$source `into` $dest ``\n    custom<DynamicIndexList>($offsets, $static_offsets)\n    custom<DynamicIndexList>($sizes, $static_sizes)\n    custom<DynamicIndexList>($strides, $static_strides)\n    attr-dict `:` type($source) `into` type($dest)"
  },
  {
    "name": "tensor.pad",
    "summary": "tensor pad operation",
    "description": "`tensor.pad` is an operation that pads the `source` tensor\n    with given `low` and `high` padding config.\n\n    The PadOp operation supports the following arguments:\n\n    * source: the \"base\" tensor on which to pad.\n    * low: A list contains the padding along the start of each\n           dimension, i.e., how many padded values are prepended\n           to the beginning of the tensor in each dimension.\n    * high: A list contains the padding along the end of each\n            dimension, i.e., how many padded values are appended\n            to the end of the tensor in each dimension.\n    * nofold: indicates that the operation should not be folded when source and\n              result types are equal.\n\n    The result tensor dimensions are `low[i]` + `dim[i]` + `high[i]` for each\n    dimension `i`. The number of elements of `low` and `high` must match the\n    rank of the input tensor. They can be either a constant or a dynamic value.\n\n    The region of the `tensor.pad` operation returns the value to use\n    for the padding. The arguments of the region represent the index\n    of the source being accessed. There should be as many arguments as\n    the rank of the `source` tensor. The value `yield`-ed by the\n    region is used as the value of the view at the given position.\n\n    If `nofold` is set, the padding operation will not be folded away even\n    if the source type and the padded type have the same static shape. This can\n    be used, e.g., for packing or promotion to faster memory.\n\n    Example 1: add 3 zeros to the beginning and 5 zeros to the end of a 1D\n    tensor.\n\n    ```mlir\n      %arg0 = ... : tensor<10xi32>\n      %c0_i32 = arith.constant 0 : i32\n      %padded = tensor.pad %arg0 low[3] high[5] {\n      ^bb0(%arg1: index):\n        tensor.yield %c0_i32 : i32\n      } : tensor<10xi32> to tensor<18xi32>\n    ```\n\n    Example 2: add 1 value to the beginning of dimension 0, 2 values to the end\n    of dimension 0, 2 values to the start of dimension 1, and 3 values to the\n    end of dimension 1.\n\n    ```mlir\n      %pad_value = ... : f32\n      %0 = tensor.pad %0 low[1, 2] high[2, 3] {\n      ^bb0(%arg0 : index, %arg1 : index):\n        tensor.yield %pad_value : f32\n      } : tensor<?x?xf32> to tensor<?x?xf32>\n    ```\n\n    Example 3:\n\n    ```mlir\n      %pad_value = ... : f32\n      %0 = tensor.pad %arg0 low[2, %arg1, 3, 3] high[3, 3, %arg1, 2] {\n      ^bb0(%arg2: index, %arg3: index, %arg4: index, %arg5: index):\n          tensor.yield %pad_value : f32\n      } : tensor<1x2x2x?xf32> to tensor<6x?x?x?xf32>\n    ```\n\n    Example 4:\n\n    ```mlir\n      %pad_value = ... : f32\n      %0 = tensor.pad %arg0 low[0, 0] high[%ub0, %ub1] {\n      ^bb0(%arg1: index, %arg2: index):\n        tensor.yield %pad_value : f32\n      } : tensor<2x3xf32> to tensor<?x?xf32>\n    ```\n\n    Example 5: Force a padded value to be always exist with `nofold`, even\n    though the padding config specifies that no new elements will be added to\n    the tensor.\n\n    ```mlir\n      %pad_value = ... : f32\n      %0 = tensor.pad %arg0 nofold low[0, 0] high[0, 0] {\n      ^bb0(%arg1: index, %arg2: index):\n        tensor.yield %pad_value : f32\n      } : tensor<2x3xf32> to tensor<2x3xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedTensor" },
      { "name": "low", "type": "Variadic" },
      { "name": "high", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "static_low", "type": "DenseI64ArrayAttr" },
      { "name": "static_high", "type": "DenseI64ArrayAttr" },
      { "name": "nofold", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$source\n    (`nofold` $nofold^)?\n    `low` `` custom<DynamicIndexList>($low, $static_low)\n    `high` `` custom<DynamicIndexList>($high, $static_high)\n    $region attr-dict `:` type($source) `to` type($result)"
  },
  {
    "name": "tensor.parallel_insert_slice",
    "summary": "Specify the tensor slice update of a single thread of a parent\n    InParallelOpInterface op.",
    "description": "The `parallel_insert_slice` yields a subset tensor value to its parent\n    InParallelOpInterface. These subset tensor values are aggregated to\n    in some unspecified order into a full tensor value returned by the parent\n    parallel iterating op.\n    The `parallel_insert_slice` is one such op allowed in the\n    InParallelOpInterface op.\n\n    Conflicting writes result in undefined semantics, in that the indices written\n    to by multiple parallel updates might contain data from any of the updates,\n    or even a malformed bit pattern.\n\n    If an index is updated exactly once, the value contained at that index\n    in the resulting tensor will be equal to the value at a corresponding index\n    of a slice that was used for the updated. If an index is not updated at all,\n    its value will be equal to the one in the original tensor.\n\n    This op does not create a new value, which allows maintaining a clean\n    separation between the subset and full tensor.\n\n    Note that we cannot mark this operation as pure (Pures), even\n    though it has no side effects, because it will get DCEd during\n    canonicalization.\n\n    The parallel_insert_slice operation supports the following arguments:\n\n    * source: the tensor that is inserted.\n    * dest: the tensor into which the source tensor is inserted.\n    * offsets: tensor-rank number of offsets into the `dest` tensor into which\n               the slice is inserted.\n    * sizes: tensor-rank number of sizes which specify the sizes of the source\n             tensor type.\n    * strides: tensor-rank number of strides that specify subsampling in each\n               dimension.\n\n    The representation based on offsets, sizes and strides support a\n    partially-static specification via attributes specified through the\n    `static_offsets`, `static_sizes` and `static_strides` arguments. A special\n    sentinel value ShapedType::kDynamic encodes that the corresponding entry has\n    a dynamic value.\n\n    After buffer allocation, the \"parallel_insert_slice\" op is expected to lower\n    into a memref.subview op.\n\n    A parallel_insert_slice operation may additionally specify insertion into a\n    tensor of higher rank than the source tensor, along dimensions that are\n    statically known to be of size 1.\n    This rank-altering behavior is not required by the op semantics: this\n    flexibility allows to progressively drop unit dimensions while lowering\n    between different flavors of ops on that operate on tensors.\n    The rank-altering behavior of tensor.parallel_insert_slice matches the\n    rank-reducing behavior of tensor.insert_slice and tensor.extract_slice.\n\n    #### Verification in the rank-reduced case\n\n    The same verification discussion and mechanisms apply as for ExtractSliceOp.\n    Unlike ExtractSliceOp however, there is no need for a specific inference.",
    "inputs": [
      { "name": "source", "type": "AnyRankedTensor" },
      { "name": "dest", "type": "AnyRankedTensor" },
      { "name": "offsets", "type": "Variadic" },
      { "name": "sizes", "type": "Variadic" },
      { "name": "strides", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "static_offsets", "type": "DenseI64ArrayAttr" },
      { "name": "static_sizes", "type": "DenseI64ArrayAttr" },
      { "name": "static_strides", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$source `into` $dest ``\n    custom<DynamicIndexList>($offsets, $static_offsets)\n    custom<DynamicIndexList>($sizes, $static_sizes)\n    custom<DynamicIndexList>($strides, $static_strides)\n    attr-dict `:` type($source) `into` type($dest)"
  },
  {
    "name": "tensor.rank",
    "summary": "rank operation",
    "description": "The `tensor.rank` operation takes a tensor operand and returns its rank.\n\n    Example:\n\n    ```mlir\n    %0 = tensor.rank %arg0 : tensor<*xf32>\n    %1 = tensor.rank %arg1 : tensor<?x?xf32>\n    ```",
    "inputs": [
      { "name": "tensor", "type": "AnyTensor" }
    ],
    "assemblyFormat": "$tensor attr-dict `:` type($tensor)"
  },
  {
    "name": "tensor.reshape",
    "summary": "tensor reshape operation",
    "description": "The `reshape` operation converts a tensor from one type to an equivalent\n    type with a provided shape. The source and destination types are compatible\n    if both have the same element type, same number of elements. The following\n    combinations are possible:\n\n    a. Source type is ranked or unranked. Shape argument has static size.\n    Result type is ranked.\n\n    ```mlir\n    // Reshape statically-shaped tensor.\n    %dst = tensor.reshape %src(%shape)\n             : (tensor<4x1xf32>, tensor<1xi32>) -> tensor<4xf32>\n    %dst0 = tensor.reshape %src(%shape0)\n             : (tensor<4x1xf32>, tensor<2xi32>) -> tensor<2x2xf32>\n    // Flatten unranked tensor.\n    %dst = tensor.reshape %src(%shape)\n             : (tensor<*xf32>, tensor<1xi32>) -> tensor<?xf32>\n    ```\n\n    b. Source type is ranked or unranked. Shape argument has dynamic size.\n    Result type is unranked.\n\n    ```mlir\n    // Reshape dynamically-shaped 1D tensor.\n    %dst = tensor.reshape %src(%shape)\n             : (tensor<?xf32>, tensor<?xi32>) -> tensor<*xf32>\n    // Reshape unranked tensor.\n    %dst = tensor.reshape %src(%shape)\n             : (tensor<*xf32>, tensor<?xi32>) -> tensor<*xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyTensor" },
      { "name": "shape", "type": "TensorRankOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTensor" }
    ],
    "assemblyFormat": "$source `(` $shape `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tensor.scatter",
    "summary": "scatter a tensor into a destination tensor at specified indices",
    "description": "The `scatter` operation inserts a `source` tensor into a `dest` tensor at\n    the given indices.\n\n    In its most general form, the tensor of indices specifies all the coordinates\n    of every element to insert (i.e. COO format, without the payload).\n    The indices are expected to be confined to coordinate values that fit the\n    range of the `dest` tensor, otherwise the behavior is undefined.\n\n    The leading dimensions of the index tensor must match that of the dest\n    tensor. The trailing dimensions of the dest tensor must match those of the\n    source tensor by omitting the dimensions specified in scatter_dims\n    (rank-reducing semantics) or setting them to `1` (rank-preserving semantics)\n    (see examples).\n    This convention allows an idiomatic specification and lowering of\n    \"scattering multiple N-D slices into the dest tensor\".\n    The result type must match the type of the dest tensor.\n\n    Note: in the examples below, we separate out the indexing part of the tensor\n    type by a whitespace for readability purposes.\n\n    Example:\n\n    ```mlir\n        // For each 1x2 triple of coordinates in %indices, insert the\n        // element (i.e. 0-D subset) at the coordinates triple in %dest.\n        //\n        %out = tensor.scatter %source into %dest[%indices]\n            scatter_dims([0, 1, 2]) unique :\n          (tensor<1x2x 1x1x1xf32>, tensor<4x4x4xf32>, tensor<1x2x 3xindex>)\n            -> tensor<4x4x4xf32>\n\n        // Note: source type may be further rank-reduced to tensor<1x2x f32>.\n    ```\n\n    A slice variant is provided to allow specifying insertion of whole tensor\n    slices into the `dest` tensor.\n\n    Example:\n\n    ```mlir\n        // For each 3 singleton of coordinates in %indices, insert the 2-D\n        // slice into %dest[*, %indices[...]:%indices[...] + 1, *] with the\n        // indices corresponding to the scatter_dims attribute specified by\n        // %indices.\n        //\n        %out = tensor.scatter %source into %dest[%indices] scatter_dims([1]) unique :\n          (tensor<3x 4x1x6xf32>, tensor<4x5x6xf32>, tensor<3x 1xindex>)\n            -> tensor<4x5x6xf32>\n    ```\n\n    The dimensions specified in the scatter_dims attribute are ones for which the\n    source tensor has size `1`.\n    I.e. if the dest type is `axbxcxd` and the coordinates are [1, 3], then\n    the source type suffix is `ax1xcx1`.\n    Scatter also allows rank-reducing semantics where the shape `ax1xcx1` can be\n    further simplified to `axc`.\n\n    The elemental type of the indices tensor can be any integer type.\n    In the absence of target-specific or problem specific information the default\n    type one should use is `index`.\n\n    This operation does not support unranked tensors.\n\n    A `unique` unit attribute must be be specified to indicate that the\n    coordinates are statically guaranteed to be unique at runtime. If coordinates\n    are not truly unique at runtime, the behavior is undefined.\n\n    Only full slices are meant to be supported by this op, if one desires\n    partial slices (e.g. strided windows) one should compose this op with other\n    tensor ops (e.g. tensor.insert_slice). This is to avoid a slippery slope of\n    complexity that would make the op unusable in practice.\n\n    At the tensor-level, the index tensor is specified in an AoS form (i.e.\n    coordinate tuple is the most minor). It is the responsibility of further\n    lowerings and bufferization to implement various concrete layouts.\n\n    Note: As currently specified, the operation must lower to an abstraction that\n    performs copies to the output tensor. This is because the buffer type system\n    is currently not rich enough to allow multiple non-contiguous views in the\n    same type. This is visible more clearly in a notional buffer version of the\n    op:\n\n    ```mlir\n        // memref<?x 4xf32> is a contiguous buffer of ?x4 elements, scatter into\n        // random dest slices must copy to the contiguous dest.\n        //\n        some_side_effecting_op_writing_into %source, ...: memref<3x 4xf32>\n        memref.scatter %source into %dest[%indices] scatter_dims([1]) unique :\n          (memref<3x 4xf32>, memref<?x 4xf32>, memref<?x 1xindex>)\n\n        // Nested buffer support in the producing op would allow writing directly\n        // into the dest buffer.\n        %v = some_nested_buffer_view_op %dest[%indices] scatter_dims([1]) unique :\n          memref<? x memref<4xf32>>\n        some_side_effecting_op_writing_into %v, ...: memref<? x memref<4xf32>>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyRankedTensor" },
      { "name": "dest", "type": "AnyRankedTensor" },
      { "name": "indices", "type": "RankedTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "scatter_dims", "type": "DenseI64ArrayAttr" },
      { "name": "unique", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$source `into` $dest `[` $indices `]`\n      `scatter_dims` `(` $scatter_dims `)`\n      (`unique` $unique^)?\n      attr-dict\n    `:` functional-type(operands, results)"
  },
  {
    "name": "tensor.splat",
    "summary": "tensor splat or broadcast operation",
    "description": "Broadcast the operand to all elements of the result tensor.\n\n    An additional argument of type `index` must be provided for each dynamic\n    dimension present in the result type.\n\n    Example for a statically shaped tensor:\n\n    ```mlir\n    %s = arith.constant 1.0 : f32\n    %t = tensor.splat %s : tensor<8x16xf32>\n    ```\n\n    Example for a tensor containing dynamic dimensions:\n\n    ```mlir\n    // Broadcasts %s to a 3D dynamically shaped tensor, with %m and %n binding\n    // to dimensions 0 and 2 of the resulting tensor, respectively.\n    %m = arith.constant 10 : index\n    %n = arith.constant 30 : index\n    %t = tensor.splat %s[%m, %n] : tensor<?x20x?xf32>\n    ```",
    "inputs": [
      { "name": "input", "type": "AnyType" },
      { "name": "dynamicSizes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "aggregate", "type": "AnyRankedTensor" }
    ],
    "assemblyFormat": "$input (`[` $dynamicSizes^ `]`)? attr-dict `:` type($aggregate)"
  },
  {
    "name": "tensor.yield",
    "summary": "Yield a value from a region",
    "description": "This operation is used to yield a single value from a within a region. It\n     is used to create dynamically sized tensors\n     (see `tensor.generate` and `tensor.pad` ops).",
    "inputs": [
      { "name": "value", "type": "AnyType" }
    ],
    "assemblyFormat": "$value attr-dict `:` type($value)"
  },
  {
    "name": "tf_device.cluster",
    "summary": "The `tf_device.cluster` op wraps containing operations in a region.",
    "description": "This op can be used to group operations, and captures all needed live-in values.\n\nOptional policy attribute allows to tag clusters with a policy name that was\nused to form the cluster.",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "policy", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tf_device.cluster_func",
    "summary": "The `tf_device.cluster_func` launches a function containing the body of a\ncluster.",
    "description": "This op is used for outlining a cluster.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "func", "type": "FlatSymbolRefAttr" }
    ]
  },
  {
    "name": "tf_device.launch",
    "summary": "The `tf_device.launch` op launches containing operations on target device.",
    "description": "This op captures all needed live-in values.",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "device", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf_device.launch_func",
    "summary": "The `tf_device.launch_func` launches a function on target device.",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "device", "type": "StrAttr" },
      { "name": "func", "type": "FlatSymbolRefAttr" }
    ]
  },
  {
    "name": "tf_device.parallel_execute",
    "description": "ParallelExecute op concurrently executes variadic number of regions. Regions\n    must represent separate sets of instructions to execute concurrently. In\n    order to represent concurrently executed regions with dependencies, multiple\n    ParallelExecute ops can be used instead. As so, regions within\n    ParallelExecute op must not have control/data dependencies.\n\n    While explicit dependencies between regions are disallowed, ParallelExecute\n    op does not prevent implicit communication between regions (e.g.\n    communication via send/recvs). In this case, users of ParallelExecute op\n    must provide correct control dependencies between regions to guarantee\n    correctness. Regions in ParallelExecute may include Resource ops.\n\n    In the case where different regions include ops access the same resource,\n    the users of the ParallelExecute op must provide mechanism (via send/recvs\n    or via control dependencies) to guarantee correct ordering. Sequential\n    ordering of ops within a region is guaranteed. Also, sequential ordering of\n    ops before/after ParallelExecute ops are guaranteed. That is, execution of\n    regions inside ParallelExecute op is blocked until all inputs to all regions\n    are materialized and ops following ParallelExecute op are blocked until all\n    regions are executed.",
    "outputs": [
      { "name": "execute_outputs", "type": "Variadic" }
    ]
  },
  {
    "name": "tf_device.receive",
    "summary": "Rceive a value from a host.",
    "description": "Receive a value from the given host with the given rendezvous key.",
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" },
      { "name": "src_host", "type": "StrAttr" }
    ],
    "assemblyFormat": "$key $src_host attr-dict `:` type($result)"
  },
  {
    "name": "tf_device.remote_run",
    "summary": "The `tf_device.remote_run` op launches the containing operations on a specific\nhost.",
    "description": "This op captures all needed live-in values.",
    "inputs": [
      { "name": "callee_args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "host", "type": "StrAttr" },
      { "name": "callee", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$host $callee `(` $callee_args `)` attr-dict `:` functional-type ( $callee_args , $results )"
  },
  {
    "name": "tf_device.replicate",
    "summary": "Wraps an N-way replicated computation.",
    "description": "The region held by this operation represents a computation that is replicated\nacross multiple devices. The number of replications is based on the `n`\nattribute. Explicit devices can be populated in the `devices` attribute, and it\nmust be a mapping of device alias to list of explicit or aliased device names\nfrom the outer scope. The device name map specifies devices on which replicated\nops inside tf_device.replicate will be executed.\n\nA tf_device.parallel_execute inside the tf_device.replicate op region may be\nused to represent computations across a larger set of devices. In that case, the\ndevice alias can be used to specify device assignment and replication of each\nconcurrent execution (i.e. region) defined by tf_device.parallel_execute op.\nThe size of each value list in the device name map must match `n`. Within a\nreplica, the execution semantics follow standard sequential behavior. Ops in the\ntf_device.replicate wrapped with a tf_device.launch will have its device set to\nthe associated replicated device from `devices` if the tf_device.launch refers\nto an aliased device name. Otherwise the device already set in tf_device.launch\nis used instead.\n\nOperands are replicated inputs and packed inputs.\n\nreplicated_inputs: each group of `n` inputs corresponds to an input for a single\nindividual replica and is mapped to a single region argument. Inside one group\nthe operands are matching in order the `devices` attribute. Each replicated\ninput must have compatible shapes and types.\npacked_inputs: each input corresponds to an input broadcasted across all\nreplicas and is mapped to a single region argument.\n\nOperands not replicated can be implicitly captured by ops in the region. Results\nare replicated each from the regions terminator.\n\nFor example:\n```\n%0 = \"tf.opA\"() : () -> tensor<i32>\n%1 = \"tf.opB\"() : () -> tensor<i32>\n%2 = \"tf.opC\"() : () -> tensor<f32>\n%3 = \"tf.opD\"() : () -> tensor<f32>\n%4 = \"tf.opE\"() : () -> tensor<!tf_type.resource>\n%5 = \"tf.opF\"() : () -> tensor<!tf_type.resource>\n%6 = \"tf.opG\"() : () -> tensor<!tf_type.string>\n%7 = \"tf.opH\"() : () -> tensor<!tf_type.string>\n%8 = \"tf.opI\"() : () -> tensor<!tf_type.variant>\n%9 = \"tf.opJ\"() : () -> tensor<i1>\n%output:8 = tf_device.replicate([%0, %1] as %input_0: tensor<i32>,\n                                [%2, %3] as %input_1: tensor<f32>,\n                                [%4, %5] as %input_2: tensor<!tf_type.resource>,\n                                [%6, %7] as %input_3: tensor<!tf_type.string>,\n                                %8 as %input_4: tensor<!tf_type.variant>)\n                {n = 2 : i32,\n                 devices = {DEVICE_ALIAS_0 = [\"/DEVICE:0\", \"/DEVICE:1\"],\n                            DEVICE_ALIAS_1 = [\"/DEVICE:2\", \"/DEVICE:3\"]}} {\n  // Inside the region, %0, %2, %4, and %6 corresponds to\n  // \"/DEVICE:0\"/\"/DEVICE:2\" and %1, %3, %5, and %7 corresponds to\n  // \"/DEVICE:1\"/\"/DEVICE:3\", depending on which device alias is used.\n  %k = \"tf_device.launch\"() ( {\n    %9 = \"tf.opK\"(%input_0, %input_4, %9) :\n      (tensor<i32>, tensor<!tf_type.variant>, tensor<i1>) -> tensor<i32>\n    tf_device.return %9 : tensor<i32>\n  }) {device = \"DEVICE_ALIAS_0\"} : () -> tensor<i32>\n  %l = \"tf_device.launch\"() ( {\n    %10 = \"tf.opL\"(%input_1, %input_4, %9) :\n      (tensor<f32>, tensor<!tf_type.variant>, tensor<i1>) -> tensor<f32>\n    tf_device.return %10 : tensor<f32>\n  }) {device = \"DEVICE_ALIAS_1\"} : () -> tensor<f32>\n  %m = \"tf_device.launch\"() ( {\n    %11 = \"tf.opM\"(%input_2, %input_4, %9) :\n      (tensor<!tf_type.resource>, tensor<!tf_type.variant>, tensor<i1>)\n        -> tensor<!tf_type.resource>\n    tf_device.return %11 : tensor<!tf_type.resource>\n  }) {device = \"/DEVICE:4\"} : () -> tensor<f32>\n  %n = \"tf.opN\"(%input_3, %input_4, %9) :\n    (tensor<!tf_type.string>, tensor<!tf_type.variant>, tensor<i1>)\n      -> tensor<!tf_type.string>\n  tf_device.return %k, %l, %m, %n :\n    tensor<i32>, tensor<f32>, tensor<!tf_type.resource>, tensor<!tf_type.string>\n}\n// %output#0 corresponds to %k returned from \"/DEVICE:0\"\n// %output#1 corresponds to %k returned from \"/DEVICE:1\"\n// %output#2 corresponds to %l returned from \"/DEVICE:2\"\n// %output#3 corresponds to %l returned from \"/DEVICE:3\"\n// %output#4, %output#5 corresponds to %m and will be returned from \"/DEVICE:4\"\n// %output#6, %output#7 corresponds to %n and will have no device set\n```",
    "inputs": [
      { "name": "replicated_inputs", "type": "Variadic" },
      { "name": "packed_inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "replicated_outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "n", "type": "ConfinedAttr" },
      { "name": "devices", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tf_device.return",
    "summary": "The `tf_device.return` operation terminates and returns values from a\n`tf_device` dialect operation.",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($results^ `:` type($results))?"
  },
  {
    "name": "tf_device.send",
    "summary": "Send a value to a host.",
    "description": "Send the value to the given host with the given rendezvous key.",
    "inputs": [
      { "name": "value", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" },
      { "name": "dst_host", "type": "StrAttr" }
    ],
    "assemblyFormat": "$value $key $dst_host attr-dict `:` type($value)"
  },
  {
    "name": "tf_executor._SwitchN",
    "summary": "The \"tf_executor._SwitchN\" operation takes two inputs, `data` and `index`\n    and an integer attribute `num_outs` indicating the number of outputs. The\n    `data` input is copied to output indicated by the `index` input. The other\n    outputs are marked as dead. If one of the inputs or a control token is\n    dead, then all of the outputs are marked as dead as well.",
    "description": "This is defined in TensorFlow as:\n\n    REGISTER_OP(\"_SwitchN\")\n        .Input(\"data: T\")\n        .Input(\"output_index: int32\")\n        .Output(\"outputs: num_outs * T\")\n        .Attr(\"num_outs: int >= 1\")\n        .Attr(\"T: type\")\n        .SetShapeFn(SwitchNShape);\n\n    For example:\n      %2:6 = tf_executor.SwitchN %0, %1 of 5 : tensor<??xf32>\n\n    Note: One additional result corresponds to the control output.",
    "inputs": [
      { "name": "data", "type": "AnyType" },
      { "name": "index", "type": "TensorOf" },
      { "name": "controlInputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" },
      { "name": "control", "type": "TfeControlType" }
    ],
    "attributes": [
      { "name": "num_outs", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf_executor.ControlTrigger",
    "summary": "The `tf_executor.ControlTrigger` operation is similar to a no-op except that\n    it always produces a valid output even when inputs are dead.",
    "description": "Its primary use so far is in the scheduling of recvs, where we add\n    ControlTrigger nodes and use them to trigger recvs. We allow ControlTrigger\n    nodes to be enabled by dead nodes.",
    "inputs": [
      { "name": "controlInputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "control", "type": "TfeControlType" }
    ],
    "assemblyFormat": "$controlInputs attr-dict"
  },
  {
    "name": "tf_executor.Enter",
    "summary": "The \"tf_executor.Enter\" operation forwards its input to Tensorflow while\n    loop.",
    "description": "More details can be found in Tensorflow Control Flow white paper:\n    https://storage.googleapis.com/download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf\n\n    Each tensor needs its own tf_executor.Enter to be made available inside a\n    while loop.\n\n    This is defined in Tensorflow as:\n\n    REGISTER_OP(\"Enter\")\n       .Input(\"data: T\")\n       .Output(\"output: T\")\n       .Attr(\"T: type\")\n       .Attr(\"frame_name: string\")\n       .Attr(\"is_constant: bool = false\")\n       .Attr(\"parallel_iterations: int = 10\")\n\n    For example:\n       %res:2 = tf_executor.Enter %arg0 frame \"some/frame\" parallel_iterations 42 constant : tensor<*xf32>\n\n    Note: Additional result corresponds to the control output.",
    "inputs": [
      { "name": "data", "type": "AnyType" },
      { "name": "controlInputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyType" },
      { "name": "control", "type": "TfeControlType" }
    ],
    "attributes": [
      { "name": "frame_name", "type": "StrAttr" },
      { "name": "is_constant", "type": "DefaultValuedOptionalAttr" },
      { "name": "parallel_iterations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf_executor.Exit",
    "summary": "The \"tf_executor.Exit\" operation forwards a value from an while loop to its\n    consumer outside of loop. Each returned tensor needs its own\n    tf_executor.Exit.",
    "description": "More details can be found in Tensorflow Control Flow white paper:\n    https://storage.googleapis.com/download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf\n\n    This is defined in Tensorflow as:\n\n    REGISTER_OP(\"Exit\")\n       .Input(\"data: T\")\n       .Output(\"output: T\")\n       .Attr(\"T: type\")\n\n    For example:\n     %1:2 = tf_executor.Exit %0#0 : tensor<*xi32> {T: \"tfdtype$DT_INT32\"}\n\n    Note: Additional result corresponds to the control output.",
    "inputs": [
      { "name": "data", "type": "AnyType" },
      { "name": "controlInputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyType" },
      { "name": "control", "type": "TfeControlType" }
    ]
  },
  {
    "name": "tf_executor.fetch",
    "summary": "The `tf_executor.fetch` operation terminates the graph and returns values;",
    "description": "The non-control operands of the fetch operation are returned outside of the\n    graph and must match the return type of the graph.",
    "inputs": [
      { "name": "fetches", "type": "Variadic" }
    ],
    "assemblyFormat": "($fetches^ `:` type($fetches))? attr-dict"
  },
  {
    "name": "tf_executor.graph",
    "summary": "The `tf_executor.graph` operation contains a region with a\n    single block that lists the operations in a TensorFlow graph.",
    "description": "The operations are topologically sorted in-order (no cycles are allowed in\n    the values). The execution model for operations in this block follows the\n    TensorFlow executor semantics:\n      1. Operations that don’t have any transitive dependencies through the\n         def/use chains may be executed in parallel\n         (`tf_executor.NextIteration.Source` is the exception).\n      2. SSA values in this block can be implicitly dead. This means that every\n         SSA value defined in a `tf_executor.graph` can be considered implicitly\n         wrapped in a conceptual `dead_or<T>` structure, and includes a runtime\n         flag indicating if the value is dead or present.\n      3. Operations may have special case handling of dead values.\n\n    The `tf_executor.graph` op only allows specific `tf_executor` dialect\n    operations in its body: the `tf_executor.graph` verifier will reject any\n    unknown operation. In order to execute standard `tf` dialect operations\n    (like `tf.Add`) they must be wrapped in the `tf_executor.island` operation.\n\n    The `tf_executor.graph` operation does not accept any operands, inputs are\n    implicitly captured by the region, representing the feeds to the graph.\n\n    The region attached to `tf_executor.graph` is terminated by a\n    `tf_executor.fetch` operation. The operands of the terminator correspond to\n    the result values (or fetches) of the `tf_executor.graph` operation. The\n    behavior is undefined if any of the operands of the `tf_executor.fetch` is\n    dead.",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "tf_executor.island",
    "summary": "The `tf_executor.island` operation is a wrapper for operations in other\n    dialects to be nested in a `tf_executor.graph`.",
    "description": "The `tf_executor.graph` operation does not allow `tf` dialect operations to\n    be immediately nested underneath it. The `tf_executor.island` is introduced\n    as a wrapper for `tf` dialect operations: this results in a more consistent\n    representation which makes analysis and transformation simpler.\n    The `tf_executor.island` operation has a single region with a single block\n    attached (only functional control flow is allowed). The block is terminated\n    by a `tf_executor.yield` operation. The operands of the terminator\n    correspond to the result values of the `tf_executor.island` operation. An\n    extra result of type `!tf_executor.control` is always produced by every\n    `tf_executor.island`.\n    Within an island, execution semantics follow standard sequential behavior as\n    expected by TF2 and by compiler analyses and transformations, and values\n    can’t be dead. Other nested `tf_executor.graph` operations can be present in\n    the region to re-enable the TensorFlow executor for a subsection of the\n    code.\n     - Initially the functional control flow operations are calling functions\n       involving graphs, if `tf_executor.graph` weren’t allowed in an island,\n       these operations would need to have an equivalent in the `tf_executor`\n       dialect to be modelled in a graph.\n     - Nesting also allows forming islands without involving inter-procedural\n       analyses: any function call may involve a callee with a graph.\n    The `tf_executor.island` region allows implicit capture. If any value\n    captured by a `tf_executor.island` is dead, the whole region does not\n    execute and every produced value is marked as dead as well.\n    An arbitrary number of `tf_executor.control` operands are accepted by a\n    `tf_executor.island` operation.\n    If any operand or implicitly captured value are dead, the region is not\n    executed and dead values are immediately returned for every result.",
    "inputs": [
      { "name": "controlInputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" },
      { "name": "control", "type": "TfeControlType" }
    ]
  },
  {
    "name": "tf_executor.LoopCond",
    "summary": "The \"tf_executor.LoopCond\" operation forwards a boolean value as loop\n    condition of Tensorflow while loops.",
    "description": "More details can be found in Tensorflow Control Flow white paper:\n    https://storage.googleapis.com/download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf\n\n    This is defined in Tensorflow as:\n\n    REGISTER_OP(\"LoopCond\")\n       .Input(\"input: bool\")\n       .Output(\"output: bool\")\n\n    For example:\n      %5:2 = tf_executor.LoopCond %4#0 {name: \"while/LoopCond\"}\n\n    Note: Additional result corresponds to the control output.",
    "inputs": [
      { "name": "input", "type": "TensorOf" },
      { "name": "controlInputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" },
      { "name": "control", "type": "TfeControlType" }
    ]
  },
  {
    "name": "tf_executor.Merge",
    "summary": "The \"tf_executor.Merge\" operation takes a list of input operands and returns\n    a value of the operand type along with the index of the first match encountered.",
    "description": "More details can be found in Tensorflow Control Flow white paper:\n    https://storage.googleapis.com/download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf\n\n    This is defined in TensorFlow as:\n\n    REGISTER_OP(\"Merge\")\n       .Input(\"inputs: N * T\")\n       .Output(\"output: T\")\n       .Output(\"value_index: int32\")\n\n    For example:\n      %2 = tf_executor.Merge %0, %1, %2, %3 : tensor<*xf32>\n\n    Note: Additional result corresponds to the control output.",
    "inputs": [
      { "name": "inputs_and_control", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" },
      { "name": "value_index", "type": "TensorOf" },
      { "name": "control", "type": "TfeControlType" }
    ]
  },
  {
    "name": "tf_executor.NextIteration.Sink",
    "summary": "The \"tf_executor.NextIteration.Sink\" is paired with a\n    \"tf_executor.NextIteration.source\" to represent NextIteration op in\n    Tensorflow.",
    "description": "Tensorflow NextIteration operation forwards its input to the next iteration\n    of a while loop. Each loop variable needs its own NextIteration op.\n\n    More details can be found in Tensorflow Control Flow white paper:\n    https://storage.googleapis.com/download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf\n\n    In the TF executor dialect, the NextIteration op is broken into\n    tf_executor.NextIteration.sink and tf_executor.NextIteration.source because\n    NextIteration is a back-edge in Tensorflow graph, which would form a data\n    flow cycle if expressed naively in a basic block.\n    tf_executor.NextIteration.source takes no input but returns results while\n    tf_executor.NextIteration.sink takes input but doesn't return anything. When\n    optimizing these ops, they are paired by token and considered as a single\n    op.\n\n    This is defined in Tensorflow as:\n\n    REGISTER_OP(\"NextIteration\")\n       .Input(\"data: T\")\n       .Output(\"output: T\")\n       .Attr(\"T: type\")\n\n    For example:\n      %value, %token, %ctl = tf_executor.NextIteration.Source : tensor<*xi32>\n      tf_executor.NextIteration.sink [%token] (%value) : tensor<*xi32>\n\n    Note: Additional result corresponds to the control output.",
    "inputs": [
      { "name": "token", "type": "TfeTokenType" },
      { "name": "input", "type": "AnyType" },
      { "name": "controlInputs", "type": "Variadic" }
    ],
    "assemblyFormat": "`[` $token `]` $input (`,` $controlInputs^)? `:` type($input) attr-dict"
  },
  {
    "name": "tf_executor.NextIteration.Source",
    "summary": "The \"tf_executor.NextIteration.Source\" is paired with a\n    \"tf_executor.NextIteration.sink\" to represent NextIteration op in\n    Tensorflow.",
    "description": "Tensorflow NextIteration operation forwards its input to the next iteration\n    of a while loop. Each loop variable needs its own NextIteration op.\n\n    More details can be found in Tensorflow Control Flow white paper:\n    https://storage.googleapis.com/download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf\n\n    In the TF executor dialect, the NextIteration op is broken into\n    tf_executor.NextIteration.sink and tf_executor.NextIteration.source because\n    NextIteration is a back-edge in Tensorflow graph, which would form a data\n    flow cycle if expressed naively in a basic block.\n    tf_executor.NextIteration.source takes no input but returns results while\n    tf_executor.NextIteration.sink takes input but doesn't return anything. When\n    optimizing these ops, they are paired by token and considered as a single\n    op.\n\n    This is defined in Tensorflow as:\n\n    REGISTER_OP(\"NextIteration\")\n       .Input(\"data: T\")\n       .Output(\"output: T\")\n       .Attr(\"T: type\")\n\n    For example:\n      %value, %token, %ctl = tf_executor.NextIteration.Source : tensor<*xi32>\n      tf_executor.NextIteration.sink [%token] (%value) : tensor<*xi32>\n\n    Note: Additional result corresponds to the control output.",
    "outputs": [
      { "name": "output", "type": "AnyType" },
      { "name": "token", "type": "TfeTokenType" },
      { "name": "control", "type": "TfeControlType" }
    ],
    "assemblyFormat": "`:` type($output) attr-dict"
  },
  {
    "name": "tf_executor.Switch",
    "summary": "The \"tf_executor.Switch\" operation takes a data operand and a boolean\n    predicate condition, and returns two values matching the type of the data\n    predicate.",
    "description": "More details can be found in Tensorflow Control Flow white paper:\n    https://storage.googleapis.com/download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf\n\n    This is defined in TensorFlow as:\n\n    REGISTER_OP(\"Switch\")\n       .Input(\"data: T\")\n       .Input(\"pred: bool\")\n       .Output(\"output_false: T\")\n       .Output(\"output_true: T\")\n\n    For example:\n      %2 = tf_executor.Switch %0, %1 : tensor<*xf32>\n\n    Note: Additional result corresponds to the control output.",
    "inputs": [
      { "name": "data", "type": "AnyType" },
      { "name": "predicate", "type": "TensorOf" },
      { "name": "controlInputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "falseOutput", "type": "AnyType" },
      { "name": "trueOutput", "type": "AnyType" },
      { "name": "control", "type": "TfeControlType" }
    ]
  },
  {
    "name": "tf_executor.yield",
    "summary": "The `tf_executor.yield` operation terminates and returns values for the\n    `tf_executor.island` operation.",
    "inputs": [
      { "name": "fetches", "type": "Variadic" }
    ],
    "assemblyFormat": "($fetches^ `:` type($fetches))? attr-dict"
  },
  {
    "name": "tf_framework.alloc",
    "summary": "allocation of tensors that uses TF Framework",
    "description": "Allocation of tensors during kernel execution in the Compute method.\n\n    This should be used to allocate any temporary or output memref. If\n    `output_index` and `input_indices` are given, attempts to forward one of\n    the input tensors to the output by calling `OpKernelContext::forward_input`.\n\n    If the attributes are missing or the forwarding fails, calls\n    `Allocator::AllocateRaw` in tensorflow/core/framework/allocator.h.",
    "inputs": [
      { "name": "ctx", "type": "TFFramework_OpKernelContextType" },
      { "name": "dyn_sizes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Res" }
    ],
    "attributes": [
      { "name": "input_indices", "type": "OptionalAttr" },
      { "name": "output_index", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`(` $ctx (`,` $dyn_sizes^ )? `)` attr-dict `:` type($result)"
  },
  {
    "name": "tf_framework.assert",
    "summary": "Assert operation with message attribute and error code",
    "description": "Assert operation that propagates the error message to TF Framework.",
    "inputs": [
      { "name": "ctx", "type": "TFFramework_OpKernelContextType" },
      { "name": "arg", "type": "I1" }
    ],
    "attributes": [
      { "name": "error_code", "type": "TFFramework_ErrorCodeAttr" },
      { "name": "msg", "type": "StrAttr" }
    ],
    "assemblyFormat": "$ctx `,` $arg `,` $error_code `,` $msg attr-dict"
  },
  {
    "name": "tf_framework.dealloc",
    "summary": "deallocation of tensors that uses TF Framework",
    "description": "Deallocation of tensors during kernel execution in the Compute method.\n\n    This should be used to deallocate any temporary memref that was allocated\n    with `tf_framework.alloc`.\n    Corresponds to `Allocator::DeallocateRaw` in\n    tensorflow/core/framework/allocator.h.",
    "inputs": [
      { "name": "ctx", "type": "TFFramework_OpKernelContextType" },
      { "name": "memref", "type": "Arg" }
    ],
    "assemblyFormat": "`(` $ctx `,` $memref `)` attr-dict `:` type($memref)"
  },
  {
    "name": "tf_framework.is_valid_memref",
    "summary": "Op to check if the memref is valid.",
    "description": "The op checks if the allocation was successful so that the underlying ptr\n    in the descriptor is not equal to NULL. Also, we check if the number of\n    elements is not zero to support empty shapes inputs correctly.\n\n    is_valid_memref(memref) = ptr(memref) != NULL || num_elements(memref) == 0",
    "inputs": [
      { "name": "arg", "type": "AnyMemRef" }
    ],
    "outputs": [
      { "name": "result", "type": "I1" }
    ],
    "assemblyFormat": "`(` $arg `)` attr-dict `:` type($arg) `->` type($result)"
  },
  {
    "name": "tf_framework.jit_compile",
    "summary": "Invokes JIT compilation through the TF framework",
    "description": "The op takes an optional TF context, so that it can be added at a later\n    stage in the compilation pipeline. The op's body corresponds to a function\n    body and the corresponding `tf_framework.jit_compile_yield` represents the\n    result value(s).",
    "inputs": [
      { "name": "ctx", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "TFFramework_JITCallableType" }
    ],
    "assemblyFormat": "$ctx $body attr-dict"
  },
  {
    "name": "tf_framework.jit_compile_from_str",
    "summary": "Invokes JIT compilation through the TF framework",
    "description": "This operation is similar to `tf_framework.jit_compile`. Instead of a body,\n    the to-be-compiled function is represented as a string attribute. The string\n    shall be the serialized form of a module with a single function named\n    `main`.",
    "inputs": [
      { "name": "ctx", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "TFFramework_JITCallableType" }
    ],
    "attributes": [
      { "name": "code", "type": "StrAttr" },
      { "name": "tileSizes", "type": "I64ArrayAttr" },
      { "name": "unrollFactors", "type": "I64ArrayAttr" },
      { "name": "enableFtz", "type": "BoolAttr" },
      { "name": "index64Bit", "type": "BoolAttr" },
      { "name": "cpuCodegen", "type": "BoolAttr" }
    ],
    "assemblyFormat": "($ctx^ `,`)? $code attr-dict"
  },
  {
    "name": "tf_framework.jit_compile_yield",
    "summary": "Yields the results in a `tf_framework.jit_compile` op's body",
    "description": "See `tf_framework.jit_compile`.",
    "inputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$result attr-dict `:` type($result)"
  },
  {
    "name": "tf_framework.jit_execute",
    "summary": "Executes a JIT-compiled function through the TF framework",
    "description": "The op takes an optional TF context, so that it can be added at a later\n    stage in the compilation pipeline. The callable must be a JIT-compiled\n    function that is the result of either `tf_framework.jit_compile` or\n    `tf_framework.jit_compile_from_str`. The remaining operands must be tensor\n    or memref arguments and will be forwarded to the callable function. The\n    result types must match those of the callable function. Otherwise, the\n    execution behavior is undefined.",
    "inputs": [
      { "name": "ctx", "type": "Optional" },
      { "name": "callable", "type": "TFFramework_JITCallableType" },
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Res" }
    ],
    "assemblyFormat": "(`ctx` `(` $ctx^ `)`)? $callable `(` $inputs `)` attr-dict\n        `:` type($inputs) `->` type($result)"
  },
  {
    "name": "tf_framework.null_context",
    "summary": "Creates a fake TF context that will be lowered to nullptr",
    "description": "Needed for testing",
    "outputs": [
      { "name": "result", "type": "TFFramework_OpKernelContextType" }
    ],
    "assemblyFormat": "attr-dict `:` type($result)"
  },
  {
    "name": "tf_framework.null_memref",
    "summary": "Op to construct unranked memref with 0-rank",
    "description": "The op is needed to construct a throw-away result after error reporting\n    happened. It constructs a ranked memref descriptor with 0-rank and\n    {NULL, NULL, 0} underlying ranked descriptor of type memref<elem_type>.\n    In ranked case it constructs a ranked memref descriptor depending on the\n    result type, but sets both allocated and aligned pointers to NULL.",
    "outputs": [
      { "name": "result", "type": "AnyRankedOrUnrankedMemRef" }
    ],
    "assemblyFormat": "attr-dict `:` type($result)"
  },
  {
    "name": "tf_framework.report_error",
    "summary": "Operation that propagates error message to TF Framework",
    "description": "Error reporting operation that corresponds to\n    `OpKernelContext::CtxFailureWithWarning`.",
    "inputs": [
      { "name": "ctx", "type": "TFFramework_OpKernelContextType" }
    ],
    "attributes": [
      { "name": "error_code", "type": "TFFramework_ErrorCodeAttr" },
      { "name": "msg", "type": "StrAttr" }
    ],
    "assemblyFormat": "$ctx `,` $error_code `,` $msg attr-dict"
  },
  {
    "name": "tf_saved_model.asset",
    "summary": "Represents an asset in saved model.",
    "description": "Represents an asset in the saved model that points to an external file. It\n    is a scalar string tensor and it is passed as an argument to the session\n    initializer functions.\n\n    The `sym_name` represents the symbol table name used for internal IR\n    references.\n\n    The `filename` attribute contains the file path to the asset file and it is\n    relative to saved model directory.",
    "attributes": [
      { "name": "sym_name", "type": "StrAttr" },
      { "name": "filename", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf_saved_model.global_tensor",
    "summary": "Represents a global tensor value.",
    "description": "Represents a tensor that is not bound to the lifetime of any particular\n    function. Such tensors can be marked as mutable via the `is_mutable`\n    attribute.\n\n    These tensors are bound to the arguments of func ops via the\n    `tf_saved_model.bound_input` argument attr.\n\n    The `sym_name` represents the symbol table name used for internal IR\n    references. The externally visible names, if any, are represented via\n    a `tf_saved_model.exported_names` attribute.\n\n    The `value` attribute contains the tensor's value (or initial value, in the\n    case it is mutable).\n\n    The `type` attribute contains the tensor's type, which for the case of\n    mutable tensors might be more general than just the fixed static shape of\n    the `value` attribute. For example, a global tensor might be unranked such\n    as `tensor<*xf32>`, or a more complex shape such as `tensor<4x?x27xf32>`.\n    The shape of `value` must be compatible with the shape of `type` in the\n    sense of `tf.TensorShape` compatibility. And the element types must match.",
    "attributes": [
      { "name": "sym_name", "type": "StrAttr" },
      { "name": "value", "type": "OptionalAttr" },
      { "name": "type", "type": "TypeAttr" },
      { "name": "is_mutable", "type": "UnitAttr" }
    ]
  },
  {
    "name": "tf_saved_model.session_initializer",
    "summary": "Initializes TensorFlow session state.",
    "description": "The session initializer op marks one or more functions that must be called\n    by an external agent exactly once to initialize TensorFlow session state,\n    and this must happen before any other exported functions are called. There\n    must be no more than one session initializer op in a saved model.\n\n    The `initializers` represents the initialization functions. The function\n    have no output and this function should be only called once.\n\n    This is used, for example, to initialize hash tables stored in resources and\n    accessed by resource name (rather than as resource handles or bound inputs\n    which is how `global_tensor`s are referenced)",
    "attributes": [
      { "name": "initializers", "type": "SymbolRefArrayAttr" }
    ]
  },
  {
    "name": "tf._ArrayToList",
    "summary": "Converts an array of tensors to a list of tensors.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ]
  },
  {
    "name": "tf._EagerConst",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf._FusedBatchNormEx",
    "summary": "Internal FusedBatchNorm operation: reserved for internal use.",
    "description": "Do not invoke this operator directly in Python. A fusion optimization is\nexpected to create these operators.",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "scale", "type": "TF_Float32Tensor" },
      { "name": "offset", "type": "TF_Float32Tensor" },
      { "name": "mean", "type": "TF_Float32Tensor" },
      { "name": "variance", "type": "TF_Float32Tensor" },
      { "name": "side_input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" },
      { "name": "batch_mean", "type": "TF_Float32Tensor" },
      { "name": "batch_variance", "type": "TF_Float32Tensor" },
      { "name": "reserve_space_1", "type": "TF_Float32Tensor" },
      { "name": "reserve_space_2", "type": "TF_Float32Tensor" },
      { "name": "reserve_space_3", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "exponential_avg_factor", "type": "DefaultValuedOptionalAttr" },
      { "name": "activation_mode", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_training", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._FusedConv2D",
    "summary": "Performs a convolution followed by a specified series of operations.",
    "description": "The inputs to the convolution are `input` and `filter`. The series of operations\nthat follows is specified by the `fused_ops` attribute, which is a list of TF op\nnames specified as strings (e.g. \"Relu\"). They are performed in order, where the\n(first) input to each op is the output of the preceding op. The first input and\nthe output of each fused_op must be of type T.\n\nCurrently supported fused_op combinations are: [X] and [X,A], where X is one of\n{\"BiasAdd\",\"FusedBatchNorm\"} and A is one of {\"Elu\",\"Relu\",\"Relu6\"}.\n\n* The first input to op X is the Conv2D result, and the additional input(s) to X\nare specified by `args`.\n* If there is an op A specified, the output of op X is the input to op A, and op\nA produces the _FusedConv2D output. Otherwise, op X produces the _FusedConv2D\noutput.\n\n*NOTE*: Do not invoke this operator directly in Python. Grappler is expected to\ncreate these operators.",
    "inputs": [
      { "name": "input", "type": "TensorOf" },
      { "name": "filter", "type": "TensorOf" },
      { "name": "args", "type": "Variadic" },
      { "name": "host_args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "num_args", "type": "ConfinedAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "filter_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_cudnn_on_gpu", "type": "DefaultValuedOptionalAttr" },
      { "name": "fused_ops", "type": "DefaultValuedOptionalAttr" },
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "leakyrelu_alpha", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._FusedMatMul",
    "summary": "Performs a MatMul followed by a specified series of operations.",
    "description": "The inputs to the MatMul are specified by `a` and `b`. The series of operations\nthat follows is specified by the `fused_ops` attribute, which is a list of TF op\nnames specified as strings (e.g. \"Relu\"). They are performed in order, where the\n(first) input to each op is the output of the preceding op. The first input and\nthe output of each fused_op must be of type T.\n\nCurrently supported fused_op combinations are: [\"BiasAdd\"] and [\"BiasAdd\",A],\nwhere A is one of {\"Elu\",\"Relu\",\"Relu6\"}.\n\n* The first input to BiasAdd is the MatMul result, and the additional BiasAdd\ninput is specified by `args`.\n* If there is an op A specified, the output of the BiasAdd is the input to op A,\nand op A produces the _FusedConv2D output. Otherwise, the BiasAdd produces the\n_FusedConv2D output.\n\n*NOTE*: Do not invoke this operator directly in Python. Grappler is\nexpected to create these operators.",
    "inputs": [
      { "name": "a", "type": "TensorOf" },
      { "name": "b", "type": "TensorOf" },
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "product", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "transpose_a", "type": "DefaultValuedOptionalAttr" },
      { "name": "transpose_b", "type": "DefaultValuedOptionalAttr" },
      { "name": "fused_ops", "type": "DefaultValuedOptionalAttr" },
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "leakyrelu_alpha", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._HostRecv",
    "summary": "Receives the named tensor from send_device on recv_device.",
    "description": "_HostRecv produces its output on host memory whereas _Recv produces its\noutput on device memory.",
    "outputs": [
      { "name": "tensor", "type": "Res" }
    ],
    "attributes": [
      { "name": "tensor_name", "type": "StrAttr" },
      { "name": "send_device", "type": "StrAttr" },
      { "name": "send_device_incarnation", "type": "I64Attr" },
      { "name": "recv_device", "type": "StrAttr" },
      { "name": "client_terminated", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._HostSend",
    "summary": "Sends the named tensor from send_device to recv_device.",
    "description": "_HostSend requires its input on host memory whereas _Send requires its\ninput on device memory.",
    "inputs": [
      { "name": "tensor", "type": "Arg" }
    ],
    "attributes": [
      { "name": "tensor_name", "type": "StrAttr" },
      { "name": "send_device", "type": "StrAttr" },
      { "name": "send_device_incarnation", "type": "I64Attr" },
      { "name": "recv_device", "type": "StrAttr" },
      { "name": "client_terminated", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._InternalTestMustExecuteTrait_",
    "summary": "Internal op for testing only"
  },
  {
    "name": "tf._InternalTestNonResourceValueSideEffects_",
    "summary": "Internal op for testing only",
    "inputs": [
      { "name": "key", "type": "Arg" }
    ]
  },
  {
    "name": "tf._ListToArray",
    "summary": "Converts a list of tensors to an array of tensors.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ]
  },
  {
    "name": "tf._Recv",
    "summary": "Receives the named tensor from send_device on recv_device.",
    "outputs": [
      { "name": "tensor", "type": "Res" }
    ],
    "attributes": [
      { "name": "tensor_name", "type": "StrAttr" },
      { "name": "send_device", "type": "StrAttr" },
      { "name": "send_device_incarnation", "type": "I64Attr" },
      { "name": "recv_device", "type": "StrAttr" },
      { "name": "client_terminated", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._Send",
    "summary": "Sends the named tensor from send_device to recv_device.",
    "inputs": [
      { "name": "tensor", "type": "Arg" }
    ],
    "attributes": [
      { "name": "tensor_name", "type": "StrAttr" },
      { "name": "send_device", "type": "StrAttr" },
      { "name": "send_device_incarnation", "type": "I64Attr" },
      { "name": "recv_device", "type": "StrAttr" },
      { "name": "client_terminated", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._TPUCompileMlir",
    "summary": "Compiles a computations for execution on one or more TPU devices.",
    "description": "For the internal use of the distributed TPU compiler.\n\n'mlir_module' is a serialized MLIR module with a `main` function that contains\ntarget computation.\n'dynamic_shapes' contains dynamic shapes of arguments whose shapes were not\nknown statically at TPUReplication rewrite time.\n'metadata' is a serialized TPUCompileMetadataProto describing the shapes and\ntypes of the inputs to the computation, as well as a mapping onto the TPU pod\ntopology.\n'program' output is a string key that is passed to the TPUExecute op and used to\nlook up the program in the compilation cache.",
    "inputs": [
      { "name": "dynamic_shapes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "compilation_status", "type": "TF_StrTensor" },
      { "name": "program", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "mlir_module", "type": "DefaultValuedOptionalAttr" },
      { "name": "metadata", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf._TPUDeviceOrdinalPlaceholder",
    "summary": "Placeholder for a device ordinal that depends on its tf_device.replicate ancestor.",
    "description": "This op must have a tf_device.replicate ancestor. The ancestor replica_id and\nlogical_core attribute correspond to a TPU core. This op maps the TPU core to a\ndevice_ordinal, where the device ordinal is the index of the core relative to\nits host.\n\nThe replicate_to_island pass removes and flattens tf_device.replicate, so it\nconverts this op to the constant index of the core relative to its host.",
    "outputs": [
      { "name": "device_ordinal", "type": "TF_Int64Tensor" }
    ],
    "attributes": [
      { "name": "logical_core", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf._UnaryOpsComposition",
    "summary": "*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is",
    "description": "expected to create these operators.",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "op_names", "type": "StrArrayAttr" }
    ]
  },
  {
    "name": "tf._XlaCompile",
    "summary": "XLA Compile Op. For use by the XLA JIT only.",
    "description": "Compiles a TensorFlow function into an XLA LocalExecutable and returns a key\nthat _XlaRun can use to look up the LocalExecutable and execute it.",
    "inputs": [
      { "name": "constants", "type": "Variadic" },
      { "name": "args", "type": "Variadic" },
      { "name": "resources", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "key", "type": "Res" },
      { "name": "compilation_successful", "type": "Res" }
    ],
    "attributes": [
      { "name": "must_compile", "type": "BoolAttr" },
      { "name": "function", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf._XlaCompileMlirPlaceholderProgramKey",
    "summary": "Placeholder program key (compilation cache key) of a XLA `program`.",
    "description": "This op can be used when certain rewrite passes materialize ops that require a\nprogram key but the _TPUCompileMlir or _XlaCompile op has not been added yet.\nSubsequent rewrite passes must replace this op with `program` output.",
    "outputs": [
      { "name": "program", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf._XlaHostComputeMlir",
    "summary": "A pseudo-op to represent host-side computation in an XLA program.",
    "inputs": [
      { "name": "inputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "send_key", "type": "StrAttr" },
      { "name": "recv_key", "type": "StrAttr" },
      { "name": "host_mlir_module", "type": "DefaultValuedOptionalAttr" },
      { "name": "manual_sharding", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._XlaRecvAtHost",
    "summary": "A placeholder op to receive values from a running XLA computation.",
    "inputs": [
      { "name": "dynamic_key", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" },
      { "name": "device_ordinal", "type": "I64Attr" },
      { "name": "device_type", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._XlaRecvAtHostV2",
    "summary": "A placeholder op to receive values from a running XLA computation with support for a runtime device ordinal.",
    "inputs": [
      { "name": "dynamic_key", "type": "Arg" },
      { "name": "device_ordinal", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" },
      { "name": "device_type", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._XlaRun",
    "summary": "XLA Run Op. For use by the XLA JIT only.",
    "description": "Executes a TensorFlow function previously compiled into a LocalExecutable by an\n_XlaCompile op.",
    "inputs": [
      { "name": "args", "type": "Variadic" },
      { "name": "key", "type": "TF_StrTensor" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "tf._XlaSendFromHost",
    "summary": "A placeholder op to send values to a running XLA computation.",
    "inputs": [
      { "name": "inputs", "type": "Arg" },
      { "name": "dynamic_key", "type": "Arg" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" },
      { "name": "device_ordinal", "type": "I64Attr" },
      { "name": "device_type", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf._XlaSendFromHostV2",
    "summary": "A placeholder op to send values to a running XLA computation with support for a runtime device ordinal.",
    "inputs": [
      { "name": "inputs", "type": "Arg" },
      { "name": "dynamic_key", "type": "Arg" },
      { "name": "device_ordinal", "type": "Arg" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" },
      { "name": "device_type", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Abs",
    "summary": "Computes the absolute value of a tensor.",
    "description": "Given a tensor `x`, this operation returns a tensor containing the absolute\nvalue of each element in `x`. For example, if x is an input element and y is\nan output element, this operation computes \\\\(y = |x|\\\\).",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.Acos",
    "summary": "Computes acos of x element-wise.",
    "description": "Provided an input tensor, the `tf.math.acos` operation returns the inverse cosine of each element of the tensor. If `y = tf.math.cos(x)` then, `x = tf.math.acos(y)`.\n\n  Input range is `[-1, 1]` and the output has a range of `[0, pi]`.",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Acosh",
    "summary": "Computes inverse hyperbolic cosine of x element-wise.",
    "description": "Given an input tensor, the function computes inverse hyperbolic cosine of every element.\nInput range is `[1, inf]`. It returns `nan` if the input lies outside the range.\n\n```python\nx = tf.constant([-2, -0.5, 1, 1.2, 200, 10000, float(\"inf\")])\ntf.math.acosh(x) ==> [nan nan 0. 0.62236255 5.9914584 9.903487 inf]\n```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Add",
    "summary": "Returns x + y element-wise.",
    "description": "*NOTE*: `Add` supports broadcasting. `AddN` does not. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\nGiven two input tensors, the `tf.add` operation computes the sum for every element in the tensor.\n\nBoth input and output have a range `(-inf, inf)`.",
    "inputs": [
      { "name": "x", "type": "TF_NumberNotQuantizedOrStrTensor" },
      { "name": "y", "type": "TF_NumberNotQuantizedOrStrTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_NumberNotQuantizedOrStrTensor" }
    ]
  },
  {
    "name": "tf.AddN",
    "summary": "Add all input tensors element wise.",
    "description": "Inputs must be of same size and shape.\n\n  ```python\n  x = [9, 7, 10]\n  tf.math.add_n(x) ==> 26\n  ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "sum", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.AddV2",
    "summary": "Returns x + y element-wise.",
    "description": "*NOTE*: `Add` supports broadcasting. `AddN` does not. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.AdjustContrastv2",
    "summary": "Adjust the contrast of one or more images.",
    "description": "`images` is a tensor of at least 3 dimensions.  The last 3 dimensions are\ninterpreted as `[height, width, channels]`.  The other dimensions only\nrepresent a collection of images, such as `[batch, height, width, channels].`\n\nContrast is adjusted independently for each channel of each image.\n\nFor each channel, the Op first computes the mean of the image pixels in the\nchannel and then adjusts each component of each pixel to\n`(x - mean) * contrast_factor + mean`.",
    "inputs": [
      { "name": "images", "type": "Arg" },
      { "name": "contrast_factor", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.AdjustHue",
    "summary": "Adjust the hue of one or more images.",
    "description": "`images` is a tensor of at least 3 dimensions.  The last dimension is\ninterpreted as channels, and must be three.\n\nThe input image is considered in the RGB colorspace. Conceptually, the RGB\ncolors are first mapped into HSV. A delta is then applied all the hue values,\nand then remapped back to RGB colorspace.",
    "inputs": [
      { "name": "images", "type": "Arg" },
      { "name": "delta", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.AdjustSaturation",
    "summary": "Adjust the saturation of one or more images.",
    "description": "`images` is a tensor of at least 3 dimensions.  The last dimension is\ninterpreted as channels, and must be three.\n\nThe input image is considered in the RGB colorspace. Conceptually, the RGB\ncolors are first mapped into HSV. A scale is then applied all the saturation\nvalues, and then remapped back to RGB colorspace.",
    "inputs": [
      { "name": "images", "type": "Arg" },
      { "name": "scale", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.All",
    "summary": "Computes the \"logical and\" of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "reduction_indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.AllToAll",
    "summary": "An Op to exchange data across TPU replicas.",
    "description": "On each replica, the input is split into `split_count` blocks along\n`split_dimension` and send to the other replicas given group_assignment. After\nreceiving `split_count` - 1 blocks from other replicas, we concatenate the\nblocks along `concat_dimension` as the output.\n\nFor example, suppose there are 2 TPU replicas:\nreplica 0 receives input: `[[A, B]]`\nreplica 1 receives input: `[[C, D]]`\n\ngroup_assignment=`[[0, 1]]`\nconcat_dimension=0\nsplit_dimension=1\nsplit_count=2\n\nreplica 0's output: `[[A], [C]]`\nreplica 1's output: `[[B], [D]]`",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "group_assignment", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "concat_dimension", "type": "I64Attr" },
      { "name": "split_dimension", "type": "I64Attr" },
      { "name": "split_count", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.Angle",
    "summary": "Returns the argument of a complex number.",
    "description": "Given a tensor `input` of complex numbers, this operation returns a tensor of\ntype `float` that is the argument of each element in `input`. All elements in\n`input` must be complex numbers of the form \\\\(a + bj\\\\), where *a*\nis the real part and *b* is the imaginary part.\n\nThe argument returned by this operation is of the form \\\\(atan2(b, a)\\\\).\n\nFor example:\n\n```\n# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]\ntf.math.angle(input) ==> [2.0132, 1.056]\n```\n\n@compatibility(numpy)\nEquivalent to np.angle.\n@end_compatibility",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.AnonymousIterator",
    "summary": "A container for an iterator resource.",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.AnonymousIteratorV2",
    "summary": "A container for an iterator resource.",
    "outputs": [
      { "name": "handle", "type": "Res" },
      { "name": "deleter", "type": "Res" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.AnonymousIteratorV3",
    "summary": "A container for an iterator resource.",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.AnonymousMemoryCache",
    "outputs": [
      { "name": "handle", "type": "Res" },
      { "name": "deleter", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.AnonymousMultiDeviceIterator",
    "summary": "A container for a multi device iterator resource.",
    "outputs": [
      { "name": "handle", "type": "Res" },
      { "name": "deleter", "type": "Res" }
    ],
    "attributes": [
      { "name": "devices", "type": "ConfinedAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.AnonymousMultiDeviceIteratorV3",
    "summary": "A container for a multi device iterator resource.",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "devices", "type": "ConfinedAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.AnonymousRandomSeedGenerator",
    "inputs": [
      { "name": "seed", "type": "TF_Int64Tensor" },
      { "name": "seed2", "type": "TF_Int64Tensor" }
    ],
    "outputs": [
      { "name": "handle", "type": "Res" },
      { "name": "deleter", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.AnonymousSeedGenerator",
    "inputs": [
      { "name": "seed", "type": "TF_Int64Tensor" },
      { "name": "seed2", "type": "TF_Int64Tensor" },
      { "name": "reshuffle", "type": "TF_BoolTensor" }
    ],
    "outputs": [
      { "name": "handle", "type": "Res" },
      { "name": "deleter", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.Any",
    "summary": "Computes the \"logical or\" of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "reduction_indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ApproximateEqual",
    "summary": "Returns the truth value of abs(x-y) < tolerance element-wise.",
    "inputs": [
      { "name": "x", "type": "TF_NumberTensor" },
      { "name": "y", "type": "TF_NumberTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ],
    "attributes": [
      { "name": "tolerance", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ApproxTopK",
    "summary": "Returns min/max k values and their indices of the input operand in an approximate manner.",
    "description": "See https://arxiv.org/abs/2206.14286 for the algorithm details.\nThis op is only optimized on TPU currently.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "values", "type": "Res" },
      { "name": "indices", "type": "Res" }
    ],
    "attributes": [
      { "name": "k", "type": "ConfinedAttr" },
      { "name": "reduction_dimension", "type": "DefaultValuedOptionalAttr" },
      { "name": "recall_target", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_max_k", "type": "DefaultValuedOptionalAttr" },
      { "name": "reduction_input_size_override", "type": "DefaultValuedOptionalAttr" },
      { "name": "aggregate_to_topk", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ArgMax",
    "summary": "Returns the index with the largest value across dimensions of a tensor.",
    "description": "Note that in case of ties the identity of the return value is not guaranteed.\n\nUsage:\n  ```python\n  import tensorflow as tf\n  a = [1, 10, 26.9, 2.8, 166.32, 62.3]\n  b = tf.math.argmax(input = a)\n  c = tf.keras.backend.eval(b)\n  # c = 4\n  # here a[4] = 166.32 which is the largest element of a across axis 0\n  ```",
    "inputs": [
      { "name": "input", "type": "TensorOf" },
      { "name": "dimension", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.ArgMin",
    "summary": "Returns the index with the smallest value across dimensions of a tensor.",
    "description": "Note that in case of ties the identity of the return value is not guaranteed.\n\nUsage:\n  ```python\n  import tensorflow as tf\n  a = [1, 10, 26.9, 2.8, 166.32, 62.3]\n  b = tf.math.argmin(input = a)\n  c = tf.keras.backend.eval(b)\n  # c = 0\n  # here a[0] = 1 which is the smallest element of a across axis 0\n  ```",
    "inputs": [
      { "name": "input", "type": "TensorOf" },
      { "name": "dimension", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tf.Asin",
    "summary": "Computes the trignometric inverse sine of x element-wise.",
    "description": "The `tf.math.asin` operation returns the inverse of `tf.math.sin`, such that\nif `y = tf.math.sin(x)` then, `x = tf.math.asin(y)`.\n\n**Note**: The output of `tf.math.asin` will lie within the invertible range\nof sine, i.e [-pi/2, pi/2].\n\nFor example:\n\n```python\n# Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]\nx = tf.constant([1.047, 0.785])\ny = tf.math.sin(x) # [0.8659266, 0.7068252]\n\ntf.math.asin(y) # [1.047, 0.785] = x\n```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Asinh",
    "summary": "Computes inverse hyperbolic sine of x element-wise.",
    "description": "Given an input tensor, this function computes inverse hyperbolic sine\n  for every element in the tensor. Both input and output has a range of\n  `[-inf, inf]`.\n\n  ```python\n  x = tf.constant([-float(\"inf\"), -2, -0.5, 1, 1.2, 200, 10000, float(\"inf\")])\n  tf.math.asinh(x) ==> [-inf -1.4436355 -0.4812118 0.8813736 1.0159732 5.991471 9.903487 inf]\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Assert",
    "summary": "Asserts that the given condition is true.",
    "description": "If `condition` evaluates to false, print the list of tensors in `data`.\n`summarize` determines how many entries of the tensors to print.",
    "inputs": [
      { "name": "condition", "type": "Arg" },
      { "name": "data", "type": "Arg" }
    ],
    "attributes": [
      { "name": "summarize", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Assign",
    "summary": "Update 'ref' by assigning 'value' to it.",
    "description": "This operation outputs \"ref\" after the assignment is done.\nThis makes it easier to chain operations that need to use the reset value.",
    "inputs": [
      { "name": "ref", "type": "Arg" },
      { "name": "value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output_ref", "type": "Res" }
    ],
    "attributes": [
      { "name": "validate_shape", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.AssignAddVariableOp",
    "summary": "Adds a value to the current value of a variable.",
    "description": "Any ReadVariableOp with a control dependency on this op is guaranteed to\nsee the incremented value or a subsequent newer one.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "value", "type": "Arg" }
    ]
  },
  {
    "name": "tf.AssignSubVariableOp",
    "summary": "Subtracts a value from the current value of a variable.",
    "description": "Any ReadVariableOp with a control dependency on this op is guaranteed to\nsee the decremented value or a subsequent newer one.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "value", "type": "Arg" }
    ]
  },
  {
    "name": "tf.AssignVariableOp",
    "summary": "Assigns a new value to a variable.",
    "description": "Any ReadVariableOp with a control dependency on this op is guaranteed to return\nthis value or a subsequent newer value of the variable.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "value", "type": "Arg" }
    ],
    "attributes": [
      { "name": "validate_shape", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.AsString",
    "summary": "Converts each entry in the given tensor to strings.",
    "description": "Supports many numeric types and boolean.\n\nFor Unicode, see the\n[https://www.tensorflow.org/text/guide/unicode](Working with Unicode text)\ntutorial.\n\nExamples:\n\n>>> tf.strings.as_string([3, 2])\n<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>\n>>> tf.strings.as_string([3.1415926, 2.71828], precision=2).numpy()\narray([b'3.14', b'2.72'], dtype=object)",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_StrTensor" }
    ],
    "attributes": [
      { "name": "precision", "type": "DefaultValuedOptionalAttr" },
      { "name": "scientific", "type": "DefaultValuedOptionalAttr" },
      { "name": "shortest", "type": "DefaultValuedOptionalAttr" },
      { "name": "width", "type": "DefaultValuedOptionalAttr" },
      { "name": "fill", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Atan",
    "summary": "Computes the trignometric inverse tangent of x element-wise.",
    "description": "The `tf.math.atan` operation returns the inverse of `tf.math.tan`, such that\nif `y = tf.math.tan(x)` then, `x = tf.math.atan(y)`.\n\n**Note**: The output of `tf.math.atan` will lie within the invertible range\nof tan, i.e (-pi/2, pi/2).\n\nFor example:\n\n```python\n# Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]\nx = tf.constant([1.047, 0.785])\ny = tf.math.tan(x) # [1.731261, 0.99920404]\n\ntf.math.atan(y) # [1.047, 0.785] = x\n```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Atan2",
    "summary": "Computes arctangent of `y/x` element-wise, respecting signs of the arguments.",
    "description": "This is the angle \\\\( \\theta \\in [-\\pi, \\pi] \\\\) such that\n\\\\[ x = r \\cos(\\theta) \\\\]\nand\n\\\\[ y = r \\sin(\\theta) \\\\]\nwhere \\\\(r = \\sqrt{x^2 + y^2} \\\\).\n\nFor example:\n\n>>> x = [1., 1.]\n>>> y = [1., -1.]\n>>> print((tf.math.atan2(y,x) * (180 / np.pi)).numpy())\n[ 45. -45.]",
    "inputs": [
      { "name": "y", "type": "TF_FloatTensor" },
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.Atanh",
    "summary": "Computes inverse hyperbolic tangent of x element-wise.",
    "description": "Given an input tensor, this function computes inverse hyperbolic tangent\n  for every element in the tensor. Input range is `[-1,1]` and output range is\n  `[-inf, inf]`. If input is `-1`, output will be `-inf` and if the\n  input is `1`, output will be `inf`. Values outside the range will have\n  `nan` as output.\n\n  ```python\n  x = tf.constant([-float(\"inf\"), -1, -0.5, 1, 0, 0.5, 10, float(\"inf\")])\n  tf.math.atanh(x) ==> [nan -inf -0.54930615 inf  0. 0.54930615 nan nan]\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.AvgPool",
    "summary": "Performs average pooling on the input.",
    "description": "Each entry in `output` is the mean of the corresponding size `ksize`\nwindow in `value`.",
    "inputs": [
      { "name": "value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.AvgPool3D",
    "summary": "Performs 3D average pooling on the input.",
    "description": "Each entry in `output` is the mean of the corresponding size `ksize` window in\n`value`.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.AvgPool3DGrad",
    "summary": "Computes gradients of average pooling function.",
    "inputs": [
      { "name": "orig_input_shape", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.AvgPoolGrad",
    "summary": "Computes gradients of the average pooling function.",
    "inputs": [
      { "name": "orig_input_shape", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.BatchDatasetV2",
    "summary": "Creates a dataset that batches `batch_size` elements from `input_dataset`.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "batch_size", "type": "Arg" },
      { "name": "drop_remainder", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "parallel_copy", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.BatchFunction",
    "summary": "Batches all the inputs tensors to the computation done by the function.",
    "description": "So, for example, in the following code\n\n  ```python\n\n  # This input will be captured.\n  y = tf.placeholder_with_default(1.0, shape=[])\n\n  @tf.Defun(tf.float32)\n  def computation(a):\n    return tf.matmul(a, a) + y\n\n  b = gen_batch_ops.batch_function(\n          f=computation\n          in_tensors=[a],\n          captured_tensors=computation.captured_inputs,\n          Tout=[o.type for o in computation.definition.signature.output_arg],\n          num_batch_threads=1,\n          max_batch_size=10,\n          batch_timeout_micros=100000,  # 100ms\n          allowed_batch_sizes=[3, 10],\n          batching_queue=\"\")\n  ```\n\nIf more than one session.run call is simultaneously trying to compute `b`\nthe values of `a` will be gathered, non-deterministically concatenated\nalong the first axis, and only one thread will run the computation.\n\nAssumes that all arguments of the function are Tensors which will be batched\nalong their first dimension.\n\nArguments that are captured, are not batched. The session.run call which does\nthe concatenation, will use the values of the captured tensors available to it.\nTherefore, typical uses of captured tensors should involve values which remain\nunchanged across session.run calls. Inference is a good example of this.\n\nSparseTensor is not supported. The return value of the decorated function\nmust be a Tensor or a list/tuple of Tensors.",
    "inputs": [
      { "name": "in_tensors", "type": "Arg" },
      { "name": "captured_tensors", "type": "Arg" }
    ],
    "outputs": [
      { "name": "out_tensors", "type": "Res" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "num_batch_threads", "type": "I64Attr" },
      { "name": "max_batch_size", "type": "I64Attr" },
      { "name": "batch_timeout_micros", "type": "I64Attr" },
      { "name": "max_enqueued_batches", "type": "DefaultValuedOptionalAttr" },
      { "name": "allowed_batch_sizes", "type": "DefaultValuedOptionalAttr" },
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "batching_queue", "type": "DefaultValuedOptionalAttr" },
      { "name": "low_priority_max_batch_size", "type": "DefaultValuedOptionalAttr" },
      { "name": "low_priority_batch_timeout_micros", "type": "DefaultValuedOptionalAttr" },
      { "name": "low_priority_allowed_batch_sizes", "type": "DefaultValuedOptionalAttr" },
      { "name": "low_priority_max_enqueued_batches", "type": "DefaultValuedOptionalAttr" },
      { "name": "mixed_priority_policy", "type": "DefaultValuedOptionalAttr" },
      { "name": "batch_padding_policy", "type": "DefaultValuedOptionalAttr" },
      { "name": "enable_large_batch_splitting", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.BatchMatMul",
    "summary": "Multiplies slices of two tensors in batches.",
    "description": "Multiplies all slices of `Tensor` `x` and `y` (each slice can be\nviewed as an element of a batch), and arranges the individual results\nin a single output tensor of the same batch size. Each of the\nindividual slices can optionally be adjointed (to adjoint a matrix\nmeans to transpose and conjugate it) before multiplication by setting\nthe `adj_x` or `adj_y` flag to `True`, which are by default `False`.\n\nThe input tensors `x` and `y` are 2-D or higher with shape `[..., r_x, c_x]`\nand `[..., r_y, c_y]`.\n\nThe output tensor is 2-D or higher with shape `[..., r_o, c_o]`, where:\n\n    r_o = c_x if adj_x else r_x\n    c_o = r_y if adj_y else c_y\n\nIt is computed as:\n\n    output[..., :, :] = matrix(x[..., :, :]) * matrix(y[..., :, :])",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "y", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "adj_x", "type": "DefaultValuedOptionalAttr" },
      { "name": "adj_y", "type": "DefaultValuedOptionalAttr" },
      { "name": "grad_x", "type": "DefaultValuedOptionalAttr" },
      { "name": "grad_y", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.BatchMatMulV2",
    "summary": "Multiplies slices of two tensors in batches.",
    "description": "Multiplies all slices of `Tensor` `x` and `y` (each slice can be\nviewed as an element of a batch), and arranges the individual results\nin a single output tensor of the same batch size. Each of the\nindividual slices can optionally be adjointed (to adjoint a matrix\nmeans to transpose and conjugate it) before multiplication by setting\nthe `adj_x` or `adj_y` flag to `True`, which are by default `False`.\n\nThe input tensors `x` and `y` are 2-D or higher with shape `[..., r_x, c_x]`\nand `[..., r_y, c_y]`.\n\nThe output tensor is 2-D or higher with shape `[..., r_o, c_o]`, where:\n\n    r_o = c_x if adj_x else r_x\n    c_o = r_y if adj_y else c_y\n\nIt is computed as:\n\n    output[..., :, :] = matrix(x[..., :, :]) * matrix(y[..., :, :])\n\n*NOTE*: `BatchMatMulV2` supports broadcasting in the batch dimensions. More\nabout broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "y", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "adj_x", "type": "DefaultValuedOptionalAttr" },
      { "name": "adj_y", "type": "DefaultValuedOptionalAttr" },
      { "name": "grad_x", "type": "DefaultValuedOptionalAttr" },
      { "name": "grad_y", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.BatchMatMulV3",
    "summary": "Multiplies slices of two tensors in batches.",
    "description": "Multiplies all slices of `Tensor` `x` and `y` (each slice can be\nviewed as an element of a batch), and arranges the individual results\nin a single output tensor of the same batch size. Each of the\nindividual slices can optionally be adjointed (to adjoint a matrix\nmeans to transpose and conjugate it) before multiplication by setting\nthe `adj_x` or `adj_y` flag to `True`, which are by default `False`.\n\nThe input tensors `x` and `y` are 2-D or higher with shape `[..., r_x, c_x]`\nand `[..., r_y, c_y]`.\n\nThe output tensor is 2-D or higher with shape `[..., r_o, c_o]`, where:\n\n    r_o = c_x if adj_x else r_x\n    c_o = r_y if adj_y else c_y\n\nIt is computed as:\n\n    output[..., :, :] = matrix(x[..., :, :]) * matrix(y[..., :, :])\n\n*NOTE*: `BatchMatMulV3` supports broadcasting in the batch dimensions. More\nabout broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "y", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "adj_x", "type": "DefaultValuedOptionalAttr" },
      { "name": "adj_y", "type": "DefaultValuedOptionalAttr" },
      { "name": "grad_x", "type": "DefaultValuedOptionalAttr" },
      { "name": "grad_y", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.BatchNormWithGlobalNormalization",
    "summary": "Batch normalization.",
    "description": "This op is deprecated. Prefer `tf.nn.batch_normalization`.",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "m", "type": "Arg" },
      { "name": "v", "type": "Arg" },
      { "name": "beta", "type": "Arg" },
      { "name": "gamma", "type": "Arg" }
    ],
    "outputs": [
      { "name": "result", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "variance_epsilon", "type": "F32Attr" },
      { "name": "scale_after_normalization", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.BatchToSpace",
    "summary": "BatchToSpace for 4-D tensors of type T.",
    "description": "This is a legacy version of the more general BatchToSpaceND.\n\nRearranges (permutes) data from batch into blocks of spatial data, followed by\ncropping. This is the reverse transformation of SpaceToBatch. More specifically,\nthis op outputs a copy of the input tensor where values from the `batch`\ndimension are moved in spatial blocks to the `height` and `width` dimensions,\nfollowed by cropping along the `height` and `width` dimensions.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "crops", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "block_size", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.BatchToSpaceND",
    "summary": "BatchToSpace for N-D tensors of type T.",
    "description": "This operation reshapes the \"batch\" dimension 0 into `M + 1` dimensions of shape\n`block_shape + [batch]`, interleaves these blocks back into the grid defined by\nthe spatial dimensions `[1, ..., M]`, to obtain a result with the same rank as\nthe input.  The spatial dimensions of this intermediate result are then\noptionally cropped according to `crops` to produce the output.  This is the\nreverse of SpaceToBatch.  See below for a precise description.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "block_shape", "type": "Arg" },
      { "name": "crops", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.BesselI0e",
    "summary": "Computes the Bessel i0e function of `x` element-wise.",
    "description": "Exponentially scaled modified Bessel function of order 0 defined as\n`bessel_i0e(x) = exp(-abs(x)) bessel_i0(x)`.\n\nThis function is faster and numerically stabler than `bessel_i0(x)`.",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.BesselI1e",
    "summary": "Computes the Bessel i1e function of `x` element-wise.",
    "description": "Exponentially scaled modified Bessel function of order 0 defined as\n`bessel_i1e(x) = exp(-abs(x)) bessel_i1(x)`.\n\nThis function is faster and numerically stabler than `bessel_i1(x)`.",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.Betainc",
    "summary": "Compute the regularized incomplete beta integral \\\\(I_x(a, b)\\\\).",
    "description": "The regularized incomplete beta integral is defined as:\n\n\n\\\\(I_x(a, b) = \\frac{B(x; a, b)}{B(a, b)}\\\\)\n\nwhere\n\n\n\\\\(B(x; a, b) = \\int_0^x t^{a-1} (1 - t)^{b-1} dt\\\\)\n\n\nis the incomplete beta function and \\\\(B(a, b)\\\\) is the *complete*\nbeta function.",
    "inputs": [
      { "name": "a", "type": "TF_F32OrF64Tensor" },
      { "name": "b", "type": "TF_F32OrF64Tensor" },
      { "name": "x", "type": "TF_F32OrF64Tensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.BiasAdd",
    "summary": "Adds `bias` to `value`.",
    "description": "This is a special case of `tf.add` where `bias` is restricted to be 1-D.\nBroadcasting is supported, so `value` may have any number of dimensions.",
    "inputs": [
      { "name": "value", "type": "Arg" },
      { "name": "bias", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.BiasAddGrad",
    "summary": "The backward operation for \"BiasAdd\" on the \"bias\" tensor.",
    "description": "It accumulates all the values from out_backprop into the feature dimension.\nFor NHWC data format, the feature dimension is the last. For NCHW data format,\nthe feature dimension is the third-to-last.",
    "inputs": [
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.BiasAddV1",
    "summary": "Adds `bias` to `value`.",
    "description": "This is a deprecated version of BiasAdd and will be soon removed.\n\nThis is a special case of `tf.add` where `bias` is restricted to be 1-D.\nBroadcasting is supported, so `value` may have any number of dimensions.",
    "inputs": [
      { "name": "value", "type": "Arg" },
      { "name": "bias", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Bincount",
    "summary": "Counts the number of occurrences of each value in an integer array.",
    "description": "Outputs a vector with length `size` and the same dtype as `weights`. If\n`weights` are empty, then index `i` stores the number of times the value `i` is\ncounted in `arr`. If `weights` are non-empty, then index `i` stores the sum of\nthe value in `weights` at each index where the corresponding value in `arr` is\n`i`.\n\nValues in `arr` outside of the range [0, size) are ignored.",
    "inputs": [
      { "name": "arr", "type": "Arg" },
      { "name": "size", "type": "Arg" },
      { "name": "weights", "type": "Arg" }
    ],
    "outputs": [
      { "name": "bins", "type": "Res" }
    ]
  },
  {
    "name": "tf.Bitcast",
    "summary": "Bitcasts a tensor from one type to another without copying data.",
    "description": "Given a tensor `input`, this operation returns a tensor that has the same buffer\ndata as `input` with datatype `type`.\n\nIf the input datatype `T` is larger than the output datatype `type` then the\nshape changes from [...] to [..., sizeof(`T`)/sizeof(`type`)].\n\nIf `T` is smaller than `type`, the operator requires that the rightmost\ndimension be equal to sizeof(`type`)/sizeof(`T`). The shape then goes from\n[..., sizeof(`type`)/sizeof(`T`)] to [...].\n\ntf.bitcast() and tf.cast() work differently when real dtype is casted as a complex dtype\n(e.g. tf.complex64 or tf.complex128) as tf.cast() make imaginary part 0 while tf.bitcast()\ngives module error.\nFor example,\n\nExample 1:\n\n>>> a = [1., 2., 3.]\n>>> equality_bitcast = tf.bitcast(a, tf.complex128)\nTraceback (most recent call last):\n...\nInvalidArgumentError: Cannot bitcast from 1 to 18 [Op:Bitcast]\n>>> equality_cast = tf.cast(a, tf.complex128)\n>>> print(equality_cast)\ntf.Tensor([1.+0.j 2.+0.j 3.+0.j], shape=(3,), dtype=complex128)\n\nExample 2:\n\n>>> tf.bitcast(tf.constant(0xffffffff, dtype=tf.uint32), tf.uint8)\n<tf.Tensor: shape=(4,), dtype=uint8, numpy=array([255, 255, 255, 255], dtype=uint8)>\n\nExample 3:\n\n>>> x = [1., 2., 3.]\n>>> y = [0., 2., 3.]\n>>> equality= tf.equal(x,y)\n>>> equality_cast = tf.cast(equality,tf.float32)\n>>> equality_bitcast = tf.bitcast(equality_cast,tf.uint8)\n>>> print(equality)\ntf.Tensor([False True True], shape=(3,), dtype=bool)\n>>> print(equality_cast)\ntf.Tensor([0. 1. 1.], shape=(3,), dtype=float32)\n>>> print(equality_bitcast)\ntf.Tensor(\n    [[  0   0   0   0]\n     [  0   0 128  63]\n     [  0   0 128  63]], shape=(3, 4), dtype=uint8)\n\n*NOTE*: Bitcast is implemented as a low-level cast, so machines with different\nendian orderings will give different results. A copy from input buffer to output\nbuffer is made on BE machines when types are of different sizes in order to get\nthe same casting results as on LE machines.",
    "inputs": [
      { "name": "input", "type": "TF_NumberTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_NumberTensor" }
    ]
  },
  {
    "name": "tf.BitwiseAnd",
    "summary": "Elementwise computes the bitwise AND of `x` and `y`.",
    "description": "The result will have those bits set, that are set in both `x` and `y`. The\ncomputation is performed on the underlying representations of `x` and `y`.\n\nFor example:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\ndtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,\n              tf.uint8, tf.uint16, tf.uint32, tf.uint64]\n\nfor dtype in dtype_list:\n  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)\n  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)\n  exp = tf.constant([0, 0, 3, 10], dtype=tf.float32)\n\n  res = bitwise_ops.bitwise_and(lhs, rhs)\n  tf.assert_equal(tf.cast(res, tf.float32), exp) # TRUE\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntTensor" },
      { "name": "y", "type": "TF_IntTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_IntTensor" }
    ]
  },
  {
    "name": "tf.BitwiseOr",
    "summary": "Elementwise computes the bitwise OR of `x` and `y`.",
    "description": "The result will have those bits set, that are set in `x`, `y` or both. The\ncomputation is performed on the underlying representations of `x` and `y`.\n\nFor example:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\ndtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,\n              tf.uint8, tf.uint16, tf.uint32, tf.uint64]\n\nfor dtype in dtype_list:\n  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)\n  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)\n  exp = tf.constant([5, 5, 7, 15], dtype=tf.float32)\n\n  res = bitwise_ops.bitwise_or(lhs, rhs)\n  tf.assert_equal(tf.cast(res,  tf.float32), exp)  # TRUE\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntTensor" },
      { "name": "y", "type": "TF_IntTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_IntTensor" }
    ]
  },
  {
    "name": "tf.BitwiseXor",
    "summary": "Elementwise computes the bitwise XOR of `x` and `y`.",
    "description": "The result will have those bits set, that are different in `x` and `y`. The\ncomputation is performed on the underlying representations of `x` and `y`.\n\nFor example:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\ndtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,\n              tf.uint8, tf.uint16, tf.uint32, tf.uint64]\n\nfor dtype in dtype_list:\n  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)\n  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)\n  exp = tf.constant([5, 5, 4, 5],  dtype=tf.float32)\n\n  res = bitwise_ops.bitwise_xor(lhs, rhs)\n  tf.assert_equal(tf.cast(res, tf.float32), exp) # TRUE\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntTensor" },
      { "name": "y", "type": "TF_IntTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_IntTensor" }
    ]
  },
  {
    "name": "tf.BoostedTreesBucketize",
    "summary": "Bucketize each feature based on bucket boundaries.",
    "description": "An op that returns a list of float tensors, where each tensor represents the\nbucketized values for a single feature.",
    "inputs": [
      { "name": "float_values", "type": "Arg" },
      { "name": "bucket_boundaries", "type": "Arg" }
    ],
    "outputs": [
      { "name": "buckets", "type": "Res" }
    ]
  },
  {
    "name": "tf.BroadcastArgs",
    "summary": "Return the shape of s0 op s1 with broadcast.",
    "description": "Given `s0` and `s1`, tensors that represent shapes, compute `r0`, the\nbroadcasted shape. `s0`, `s1` and `r0` are all integer vectors.",
    "inputs": [
      { "name": "s0", "type": "TF_I32OrI64Tensor" },
      { "name": "s1", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "r0", "type": "TF_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tf.BroadcastGradientArgs",
    "summary": "Return the reduction indices for computing gradients of s0 op s1 with broadcast.",
    "description": "This is typically used by gradient computations for a broadcasting operation.",
    "inputs": [
      { "name": "s0", "type": "TF_I32OrI64Tensor" },
      { "name": "s1", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "r0", "type": "TF_I32OrI64Tensor" },
      { "name": "r1", "type": "TF_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tf.BroadcastTo",
    "summary": "Broadcast an array for a compatible shape.",
    "description": "Broadcasting is the process of making arrays to have compatible shapes\nfor arithmetic operations. Two shapes are compatible if for each\ndimension pair they are either equal or one of them is one.\n\nFor example:\n\n>>> x = tf.constant([[1, 2, 3]])   # Shape (1, 3,)\n>>> y = tf.broadcast_to(x, [2, 3])\n>>> print(y)\ntf.Tensor(\n    [[1 2 3]\n     [1 2 3]], shape=(2, 3), dtype=int32)\n\nIn the above example, the input Tensor with the shape of `[1, 3]`\nis broadcasted to output Tensor with shape of `[2, 3]`.\n\nWhen broadcasting, if a tensor has fewer axes than necessary its shape is\npadded on the left with ones. So this gives the same result as the previous\nexample:\n\n>>> x = tf.constant([1, 2, 3])   # Shape (3,)\n>>> y = tf.broadcast_to(x, [2, 3])\n\n\nWhen doing broadcasted operations such as multiplying a tensor\nby a scalar, broadcasting (usually) confers some time or space\nbenefit, as the broadcasted tensor is never materialized.\n\nHowever, `broadcast_to` does not carry with it any such benefits.\nThe newly-created tensor takes the full memory of the broadcasted\nshape. (In a graph context, `broadcast_to` might be fused to\nsubsequent operation and then be optimized away, however.)",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Bucketize",
    "summary": "Bucketizes 'input' based on 'boundaries'.",
    "description": "For example, if the inputs are\n    boundaries = [0, 10, 100]\n    input = [[-5, 10000]\n             [150,   10]\n             [5,    100]]\n\nthen the output will be\n    output = [[0, 3]\n              [3, 2]\n              [1, 3]]",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "boundaries", "type": "F32ArrayAttr" }
    ]
  },
  {
    "name": "tf.CacheDatasetV2",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "filename", "type": "TF_StrTensor" },
      { "name": "cache", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Case",
    "summary": "An n-way switch statement which calls a single branch function.",
    "description": "An n-way switch statement, implementing the following:\n    ```\n    switch (branch_index) {\n      case 0:\n        output = branches[0](input);\n        break;\n      case 1:\n        output = branches[1](input);\n        break;\n      ...\n      case [[nbranches-1]]:\n      default:\n        output = branches[nbranches-1](input);\n        break;\n    }\n    ```",
    "inputs": [
      { "name": "branch_index", "type": "I32Tensor" },
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "branches", "type": "ConfinedAttr" },
      { "name": "is_stateless", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.CaseRegion",
    "summary": "An n-way switch statement which calls a single branch function.",
    "description": "An n-way switch statement, implementing the following:\n    ```\n    switch (branch_index) {\n      case 0:\n        output = branches[0](input);\n        break;\n      case 1:\n        output = branches[1](input);\n        break;\n      ...\n      case [[nbranches-1]]:\n      default:\n        output = branches[nbranches-1](input);\n        break;\n    }\n    ```",
    "inputs": [
      { "name": "branch_index", "type": "I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "is_stateless", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.Cast",
    "summary": "Cast x of type SrcT to y of DstT.",
    "inputs": [
      { "name": "x", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "Truncate", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Ceil",
    "summary": "Returns element-wise smallest integer not less than x.",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.CheckNumerics",
    "summary": "Checks a tensor for NaN and Inf values.",
    "description": "When run, reports an `InvalidArgument` error if `tensor` has any values\nthat are not a number (NaN) or infinity (Inf). Otherwise, returns the input\ntensor.\n\nExample usage:\n\n``` python\na = tf.Variable(1.0)\ntf.debugging.check_numerics(a, message='')\n\nb = tf.Variable(np.nan)\ntry:\n  tf.debugging.check_numerics(b, message='Checking b')\nexcept Exception as e:\n  assert \"Checking b : Tensor had NaN values\" in e.message\n\nc = tf.Variable(np.inf)\ntry:\n  tf.debugging.check_numerics(c, message='Checking c')\nexcept Exception as e:\n  assert \"Checking c : Tensor had Inf values\" in e.message\n```",
    "inputs": [
      { "name": "tensor", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "message", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.Cholesky",
    "summary": "Computes the Cholesky decomposition of one or more square matrices.",
    "description": "The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions\nform square matrices.\n\nThe input has to be symmetric and positive definite. Only the lower-triangular\npart of the input will be used for this operation. The upper-triangular part\nwill not be read.\n\nThe output is a tensor of the same shape as the input\ncontaining the Cholesky decompositions for all input submatrices `[..., :, :]`.\n\n**Note**: The gradient computation on GPU is faster for large matrices but\nnot for large batch dimensions when the submatrices are small. In this\ncase it might be faster to use the CPU.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.ClipByValue",
    "summary": "Clips tensor values to a specified min and max.",
    "description": "Given a tensor `x`, this operation returns a tensor of the same type and\nshape as `x` with its values clipped to `clip_value_min` and `clip_value_max`.\nAny values less than `clip_value_min` are set to `clip_value_min`. Any values\ngreater than `clip_value_max` are set to `clip_value_max`.",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "clip_value_min", "type": "Arg" },
      { "name": "clip_value_max", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.CloseSummaryWriter",
    "summary": "Flushes and closes the summary writer.",
    "description": "Also removes it from the resource manager. To reopen, use another\nCreateSummaryFileWriter op.\n\nwriter: A handle to the summary writer resource.",
    "inputs": [
      { "name": "writer", "type": "Arg" }
    ]
  },
  {
    "name": "tf.CollateTPUEmbeddingMemory",
    "summary": "An op that merges the string-encoded memory config protos from all hosts.",
    "inputs": [
      { "name": "memory_configs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "merged_memory_config", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.CollectiveAllToAllV2",
    "summary": "Mutually exchanges multiple tensors of identical type and shape.",
    "description": "`is_stateless` means each op does not need control dependencies to other\ncollective ops. In this case, keys that are unique at runtime\n(e.g. `instance_key`) should be used to distinguish collective groups.",
    "inputs": [
      { "name": "input", "type": "TF_FpOrI32OrI64Tensor" },
      { "name": "group_size", "type": "TF_Int32Tensor" },
      { "name": "group_key", "type": "TF_Int32Tensor" },
      { "name": "instance_key", "type": "TF_Int32Tensor" },
      { "name": "ordering_token", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "data", "type": "TF_FpOrI32OrI64Tensor" }
    ],
    "attributes": [
      { "name": "communication_hint", "type": "DefaultValuedOptionalAttr" },
      { "name": "timeout_seconds", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_stateless", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.CollectiveAssignGroupV2",
    "summary": "Assign group keys based on group assignment.",
    "inputs": [
      { "name": "group_assignment", "type": "TF_Int32Tensor" },
      { "name": "device_index", "type": "TF_Int32Tensor" },
      { "name": "base_key", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "group_size", "type": "TF_Int32Tensor" },
      { "name": "group_key", "type": "TF_Int32Tensor" }
    ]
  },
  {
    "name": "tf.CollectiveBcastRecv",
    "summary": "Receives a tensor value broadcast from another device.",
    "outputs": [
      { "name": "data", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "group_size", "type": "I64Attr" },
      { "name": "group_key", "type": "I64Attr" },
      { "name": "instance_key", "type": "I64Attr" },
      { "name": "shape", "type": "TF_ShapeAttr" },
      { "name": "communication_hint", "type": "DefaultValuedOptionalAttr" },
      { "name": "timeout_seconds", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.CollectiveBcastSend",
    "summary": "Broadcasts a tensor value to one or more other devices.",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "data", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "group_size", "type": "I64Attr" },
      { "name": "group_key", "type": "I64Attr" },
      { "name": "instance_key", "type": "I64Attr" },
      { "name": "shape", "type": "TF_ShapeAttr" },
      { "name": "communication_hint", "type": "DefaultValuedOptionalAttr" },
      { "name": "timeout_seconds", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.CollectiveGather",
    "summary": "Mutually accumulates multiple tensors of identical type and shape.",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "data", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "group_size", "type": "I64Attr" },
      { "name": "group_key", "type": "I64Attr" },
      { "name": "instance_key", "type": "I64Attr" },
      { "name": "shape", "type": "TF_ShapeAttr" },
      { "name": "communication_hint", "type": "DefaultValuedOptionalAttr" },
      { "name": "timeout_seconds", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.CollectiveGatherV2",
    "summary": "Mutually accumulates multiple tensors of identical type and shape.",
    "description": "`is_stateless` means each op does not need control dependencies to other\ncollective ops. In this case, keys that are unique at runtime\n(e.g. `instance_key`) should be used to distinguish collective groups.",
    "inputs": [
      { "name": "input", "type": "TensorOf" },
      { "name": "group_size", "type": "TF_Int32Tensor" },
      { "name": "group_key", "type": "TF_Int32Tensor" },
      { "name": "instance_key", "type": "TF_Int32Tensor" },
      { "name": "ordering_token", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "data", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "communication_hint", "type": "DefaultValuedOptionalAttr" },
      { "name": "timeout_seconds", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_stateless", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.CollectivePermute",
    "summary": "An Op to permute tensors across replicated TPU instances.",
    "description": "Each instance supplies its own input.\n\nFor example, suppose there are 4 TPU instances: `[A, B, C, D]`. Passing\nsource_target_pairs=`[[0,1],[1,2],[2,3],[3,0]]` gets the outputs:\n`[D, A, B, C]`.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "source_target_pairs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.CollectiveReduce",
    "summary": "Mutually reduces multiple tensors of identical type and shape.",
    "inputs": [
      { "name": "input", "type": "TF_FpOrI32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "data", "type": "TF_FpOrI32OrI64Tensor" }
    ],
    "attributes": [
      { "name": "group_size", "type": "I64Attr" },
      { "name": "group_key", "type": "I64Attr" },
      { "name": "instance_key", "type": "I64Attr" },
      { "name": "merge_op", "type": "TF_AnyStrAttrOf" },
      { "name": "final_op", "type": "TF_AnyStrAttrOf" },
      { "name": "subdiv_offsets", "type": "I64ArrayAttr" },
      { "name": "wait_for", "type": "DefaultValuedOptionalAttr" },
      { "name": "communication_hint", "type": "DefaultValuedOptionalAttr" },
      { "name": "timeout_seconds", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.CollectiveReduceScatterV2",
    "summary": "Mutually reduces multiple tensors of identical type and shape and scatters the result.",
    "description": "`is_stateless` means each op does not need control dependencies to other\ncollective ops. In this case, keys that are unique at runtime\n(e.g. `instance_key`) should be used to distinguish collective groups.",
    "inputs": [
      { "name": "input", "type": "TF_FpOrI32OrI64Tensor" },
      { "name": "group_size", "type": "TF_Int32Tensor" },
      { "name": "group_key", "type": "TF_Int32Tensor" },
      { "name": "instance_key", "type": "TF_Int32Tensor" },
      { "name": "ordering_token", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "data", "type": "TF_FpOrI32OrI64Tensor" }
    ],
    "attributes": [
      { "name": "merge_op", "type": "TF_AnyStrAttrOf" },
      { "name": "final_op", "type": "TF_AnyStrAttrOf" },
      { "name": "communication_hint", "type": "DefaultValuedOptionalAttr" },
      { "name": "timeout_seconds", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_stateless", "type": "DefaultValuedOptionalAttr" },
      { "name": "max_subdivs_per_device", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.CollectiveReduceV2",
    "summary": "Mutually reduces multiple tensors of identical type and shape.",
    "description": "`is_stateless` means each op does not need control dependencies to other\ncollective ops. In this case, keys that are unique at runtime\n(e.g. `instance_key`) should be used to distinguish collective groups.",
    "inputs": [
      { "name": "input", "type": "TF_FpOrI32OrI64Tensor" },
      { "name": "group_size", "type": "TF_Int32Tensor" },
      { "name": "group_key", "type": "TF_Int32Tensor" },
      { "name": "instance_key", "type": "TF_Int32Tensor" },
      { "name": "ordering_token", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "data", "type": "TF_FpOrI32OrI64Tensor" }
    ],
    "attributes": [
      { "name": "merge_op", "type": "TF_AnyStrAttrOf" },
      { "name": "final_op", "type": "TF_AnyStrAttrOf" },
      { "name": "communication_hint", "type": "DefaultValuedOptionalAttr" },
      { "name": "timeout_seconds", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_stateless", "type": "DefaultValuedOptionalAttr" },
      { "name": "max_subdivs_per_device", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Complex",
    "summary": "Converts two real numbers to a complex number.",
    "description": "Given a tensor `real` representing the real part of a complex number, and a\ntensor `imag` representing the imaginary part of a complex number, this\noperation returns complex numbers elementwise of the form \\\\(a + bj\\\\), where\n*a* represents the `real` part and *b* represents the `imag` part.\n\nThe input tensors `real` and `imag` must have the same shape.\n\nFor example:\n\n```\n# tensor 'real' is [2.25, 3.25]\n# tensor `imag` is [4.75, 5.75]\ntf.complex(real, imag) ==> [[2.25 + 4.75j], [3.25 + 5.75j]]\n```",
    "inputs": [
      { "name": "real", "type": "TF_F32OrF64Tensor" },
      { "name": "imag", "type": "TF_F32OrF64Tensor" }
    ],
    "outputs": [
      { "name": "out", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.ComplexAbs",
    "summary": "Computes the complex absolute value of a tensor.",
    "description": "Given a tensor `x` of complex numbers, this operation returns a tensor of type\n`float` or `double` that is the absolute value of each element in `x`. All\nelements in `x` must be complex numbers of the form \\\\(a + bj\\\\). The absolute\nvalue is computed as \\\\( \\sqrt{a^2 + b^2}\\\\).\n\nFor example:\n\n>>> x = tf.complex(3.0, 4.0)\n>>> print((tf.raw_ops.ComplexAbs(x=x, Tout=tf.dtypes.float32, name=None)).numpy())\n5.0",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.Concat",
    "summary": "Concatenates tensors along one dimension.",
    "inputs": [
      { "name": "concat_dim", "type": "Arg" },
      { "name": "values", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.ConcatOffset",
    "summary": "Computes offsets of concat inputs within its output.",
    "description": "For example:\n\n>>> x = [2, 2, 7]\n>>> y = [2, 3, 7]\n>>> z = [2, 9, 7]\n>>> offsets = concat_offset(1, [x, y, z])\n>>> [[a.item() for a in list(off.numpy())] for off in offsets]\n[[0, 0, 0], [0, 2, 0], [0, 5, 0]]\n\nThis is typically used by gradient computations for a concat operation.",
    "inputs": [
      { "name": "concat_dim", "type": "Arg" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "offset", "type": "Res" }
    ]
  },
  {
    "name": "tf.ConcatV2",
    "summary": "Concatenates tensors along one dimension.",
    "inputs": [
      { "name": "values", "type": "Arg" },
      { "name": "axis", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.ConfigureAndInitializeGlobalTPU",
    "summary": "An op that initialize the TPU system in a multi-client set up.",
    "description": "Initializes global TPU system for mutli-client execution.\n\nThis op does the work of both ConfigureDistributedTpuOp and\nInitializeHostForDistributedTpuOp, and outputs the latter's result.",
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.ConfigureDistributedTPU",
    "summary": "Sets up the centralized structures for a distributed TPU system.",
    "outputs": [
      { "name": "topology", "type": "Res" }
    ],
    "attributes": [
      { "name": "embedding_config", "type": "DefaultValuedOptionalAttr" },
      { "name": "tpu_embedding_config", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_global_init", "type": "DefaultValuedOptionalAttr" },
      { "name": "enable_whole_mesh_compilations", "type": "DefaultValuedOptionalAttr" },
      { "name": "compilation_failure_closes_chips", "type": "DefaultValuedOptionalAttr" },
      { "name": "tpu_cancellation_closes_chips", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ConfigureTPUEmbedding",
    "summary": "Sets up TPUEmbedding in a distributed TPU system.",
    "attributes": [
      { "name": "config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.ConfigureTPUEmbeddingHost",
    "summary": "An op that configures the TPUEmbedding software on a host.",
    "inputs": [
      { "name": "common_config", "type": "Arg" },
      { "name": "memory_config", "type": "Arg" }
    ],
    "outputs": [
      { "name": "network_config", "type": "Res" }
    ],
    "attributes": [
      { "name": "config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.ConfigureTPUEmbeddingMemory",
    "summary": "An op that configures the TPUEmbedding software on a host.",
    "inputs": [
      { "name": "common_config", "type": "Arg" }
    ],
    "outputs": [
      { "name": "memory_config", "type": "Res" }
    ]
  },
  {
    "name": "tf.Conj",
    "summary": "Returns the complex conjugate of a complex number.",
    "description": "Given a tensor `input` of complex numbers, this operation returns a tensor of\ncomplex numbers that are the complex conjugate of each element in `input`. The\ncomplex numbers in `input` must be of the form \\\\(a + bj\\\\), where *a* is the\nreal part and *b* is the imaginary part.\n\nThe complex conjugate returned by this operation is of the form \\\\(a - bj\\\\).\n\nFor example:\n\n```\n# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]\ntf.conj(input) ==> [-2.25 - 4.75j, 3.25 - 5.75j]\n```",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.ConjugateTranspose",
    "summary": "Shuffle dimensions of x according to a permutation and conjugate the result.",
    "description": "The output `y` has the same rank as `x`. The shapes of `x` and `y` satisfy:\n  `y.shape[i] == x.shape[perm[i]] for i in [0, 1, ..., rank(x) - 1]`\n  `y[i,j,k,...,s,t,u] == conj(x[perm[i], perm[j], perm[k],...,perm[s], perm[t], perm[u]])`",
    "inputs": [
      { "name": "x", "type": "TF_Tensor" },
      { "name": "perm", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.ConnectTPUEmbeddingHosts",
    "summary": "An op that sets up communication between TPUEmbedding host software instances",
    "description": "after ConfigureTPUEmbeddingHost has been called on each host.",
    "inputs": [
      { "name": "network_configs", "type": "Arg" }
    ]
  },
  {
    "name": "tf.Const",
    "summary": "Constant tensor op",
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "value", "type": "ElementsAttr" }
    ]
  },
  {
    "name": "tf.Conv",
    "summary": "Computes a N-D convolution given (N+1+batch_dims)-D `input` and (N+2)-D `filter` tensors.",
    "description": "General function for computing a N-D convolution. It is required that\n`1 <= N <= 3`.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" },
      { "name": "batch_dims", "type": "DefaultValuedOptionalAttr" },
      { "name": "groups", "type": "DefaultValuedOptionalAttr" }
    ],
    "category": "Layer"
  },
  {
    "name": "tf.Conv2D",
    "summary": "Computes a 2-D convolution given 4-D `input` and `filter` tensors.",
    "description": "Given an input tensor of shape `[batch, in_height, in_width, in_channels]`\nand a filter / kernel tensor of shape\n`[filter_height, filter_width, in_channels, out_channels]`, this op\nperforms the following:\n\n1. Flattens the filter to a 2-D matrix with shape\n   `[filter_height * filter_width * in_channels, output_channels]`.\n2. Extracts image patches from the input tensor to form a *virtual*\n   tensor of shape `[batch, out_height, out_width,\n   filter_height * filter_width * in_channels]`.\n3. For each patch, right-multiplies the filter matrix and the image patch\n   vector.\n\nIn detail, with the default NHWC format,\n\n    output[b, i, j, k] =\n        sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q] *\n                        filter[di, dj, q, k]\n\nMust have `strides[0] = strides[3] = 1`.  For the most common case of the same\nhorizontal and vertices strides, `strides = [1, stride, stride, 1]`.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "use_cudnn_on_gpu", "type": "DefaultValuedOptionalAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv2DBackpropFilter",
    "summary": "Computes the gradients of convolution with respect to the filter.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter_sizes", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "use_cudnn_on_gpu", "type": "DefaultValuedOptionalAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv2DBackpropFilterV2",
    "summary": "Computes the gradients of convolution with respect to the filter.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "use_cudnn_on_gpu", "type": "DefaultValuedOptionalAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv2DBackpropInput",
    "summary": "Computes the gradients of convolution with respect to the input.",
    "inputs": [
      { "name": "input_sizes", "type": "Arg" },
      { "name": "filter", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "use_cudnn_on_gpu", "type": "DefaultValuedOptionalAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv2DBackpropInputV2",
    "summary": "Computes the gradients of convolution with respect to the input.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "use_cudnn_on_gpu", "type": "DefaultValuedOptionalAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv3D",
    "summary": "Computes a 3-D convolution given 5-D `input` and `filter` tensors.",
    "description": "In signal processing, cross-correlation is a measure of similarity of\ntwo waveforms as a function of a time-lag applied to one of them. This\nis also known as a sliding dot product or sliding inner-product.\n\nOur Conv3D implements a form of cross-correlation.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv3DBackpropFilter",
    "summary": "Computes the gradients of 3-D convolution with respect to the filter.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv3DBackpropFilterV2",
    "summary": "Computes the gradients of 3-D convolution with respect to the filter.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter_sizes", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv3DBackpropInput",
    "summary": "Computes the gradients of 3-D convolution with respect to the input.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Conv3DBackpropInputV2",
    "summary": "Computes the gradients of 3-D convolution with respect to the input.",
    "inputs": [
      { "name": "input_sizes", "type": "Arg" },
      { "name": "filter", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ConvertToCooTensor",
    "summary": "Op that converts tensors into coo format.",
    "description": "This op coverts the dense, sparse and ragged tensor into standard coo tensor\nformat which contains three 1D tensors.",
    "inputs": [
      { "name": "indices_or_row_splits", "type": "TF_Int32Tensor" },
      { "name": "values", "type": "TF_Int32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "row_ids", "type": "TF_Int32Tensor" },
      { "name": "col_ids", "type": "TF_Int32Tensor" },
      { "name": "gains", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "sample_count", "type": "ConfinedAttr" },
      { "name": "combiner", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.ConvertToListOfSparseCoreCooTensors",
    "summary": "An op which converts the sparse/ragged/dense tensor into a list of COO tensor for each SparseCore.",
    "inputs": [
      { "name": "indices_or_row_splits", "type": "TF_Int32Tensor" },
      { "name": "values", "type": "TF_Int32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "row_ids_list", "type": "Variadic" },
      { "name": "col_ids_list", "type": "Variadic" },
      { "name": "gains_list", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "sample_count", "type": "ConfinedAttr" },
      { "name": "row_offset", "type": "ConfinedAttr" },
      { "name": "col_offset", "type": "ConfinedAttr" },
      { "name": "col_shift", "type": "ConfinedAttr" },
      { "name": "num_sc_shards", "type": "ConfinedAttr" },
      { "name": "stacked_table_sample_count", "type": "ConfinedAttr" },
      { "name": "combiner", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.ConvertToSparseCoreCsrWrappedCooTensorOp",
    "summary": "An op which converts the sorted coo tensor into sparse core CSR wrapped COO format.",
    "inputs": [
      { "name": "sorted_row_ids_list", "type": "Variadic" },
      { "name": "sorted_col_ids_list", "type": "Variadic" },
      { "name": "sorted_gains_list", "type": "Variadic" },
      { "name": "id_counts_list", "type": "Variadic" },
      { "name": "splits", "type": "TF_Int64Tensor" }
    ],
    "outputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "row_pointers_unpadded_size", "type": "TF_Int32Tensor" },
      { "name": "ids_unpadded_size", "type": "TF_Int32Tensor" },
      { "name": "num_minibatches_per_sc", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "sample_count_per_sc", "type": "ConfinedAttr" },
      { "name": "num_replica", "type": "ConfinedAttr" },
      { "name": "max_minibatches_per_sc", "type": "ConfinedAttr" },
      { "name": "max_ids_per_chip_per_sample", "type": "ConfinedAttr" },
      { "name": "table_vocab_size", "type": "ConfinedAttr" },
      { "name": "feature_width", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" },
      { "name": "allow_id_dropping", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.Cos",
    "summary": "Computes cos of x element-wise.",
    "description": "Given an input tensor, this function computes cosine of every\n  element in the tensor. Input range is `(-inf, inf)` and\n  output range is `[-1,1]`. If input lies outside the boundary, `nan`\n  is returned.\n\n  ```python\n  x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 200, 10000, float(\"inf\")])\n  tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Cosh",
    "summary": "Computes hyperbolic cosine of x element-wise.",
    "description": "Given an input tensor, this function computes hyperbolic cosine of every\n  element in the tensor. Input range is `[-inf, inf]` and output range\n  is `[1, inf]`.\n\n  ```python\n  x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 2, 10, float(\"inf\")])\n  tf.math.cosh(x) ==> [inf 4.0515420e+03 1.1276259e+00 1.5430807e+00 1.8106556e+00 3.7621956e+00 1.1013233e+04 inf]\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.CreateSummaryDbWriter",
    "summary": "Creates summary database writer accessible by given resource handle.",
    "description": "This can be used to write tensors from the execution graph directly\nto a database. Only SQLite is supported right now. This function\nwill create the schema if it doesn't exist. Entries in the Users,\nExperiments, and Runs tables will be created automatically if they\ndon't already exist.\n\nwriter: Handle to SummaryWriter resource to overwrite.\ndb_uri: For example \"file:/tmp/foo.sqlite\".\nexperiment_name: Can't contain ASCII control characters or <>. Case\n  sensitive. If empty, then the Run will not be associated with any\n  Experiment.\nrun_name: Can't contain ASCII control characters or <>. Case sensitive.\n  If empty, then each Tag will not be associated with any Run.\nuser_name: Must be valid as both a DNS label and Linux username. If\n  empty, then the Experiment will not be associated with any User.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "db_uri", "type": "TF_StrTensor" },
      { "name": "experiment_name", "type": "TF_StrTensor" },
      { "name": "run_name", "type": "TF_StrTensor" },
      { "name": "user_name", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.CreateSummaryFileWriter",
    "summary": "Creates a summary file writer accessible by the given resource handle.",
    "description": "writer: A handle to the summary writer resource\nlogdir: Directory where the event file will be written.\nmax_queue: Size of the queue of pending events and summaries.\nflush_millis: How often, in milliseconds, to flush the pending events and\n  summaries to disk.\nfilename_suffix: Every event file's name is suffixed with this suffix.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "logdir", "type": "TF_StrTensor" },
      { "name": "max_queue", "type": "TF_Int32Tensor" },
      { "name": "flush_millis", "type": "TF_Int32Tensor" },
      { "name": "filename_suffix", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.Cross",
    "summary": "Compute the pairwise cross product.",
    "description": "`a` and `b` must be the same shape; they can either be simple 3-element vectors,\nor any shape where the innermost dimension is 3. In the latter case, each pair\nof corresponding 3-element vectors is cross-multiplied independently.",
    "inputs": [
      { "name": "a", "type": "Arg" },
      { "name": "b", "type": "Arg" }
    ],
    "outputs": [
      { "name": "product", "type": "Res" }
    ]
  },
  {
    "name": "tf.CrossReplicaSum",
    "summary": "An Op to sum inputs across replicated TPU instances.",
    "description": "Each instance supplies its own input.\n\nFor example, suppose there are 8 TPU instances: `[A, B, C, D, E, F, G, H]`.\nPassing group_assignment=`[[0,2,4,6],[1,3,5,7]]` sets `A, C, E, G` as group 0,\nand `B, D, F, H` as group 1. Thus we get the outputs:\n`[A+C+E+G, B+D+F+H, A+C+E+G, B+D+F+H, A+C+E+G, B+D+F+H, A+C+E+G, B+D+F+H]`.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "group_assignment", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Cumprod",
    "summary": "Compute the cumulative product of the tensor `x` along `axis`.",
    "description": "By default, this op performs an inclusive cumprod, which means that the first\nelement of the input is identical to the first element of the output:\n\n```python\ntf.cumprod([a, b, c])  # => [a, a * b, a * b * c]\n```\n\nBy setting the `exclusive` kwarg to `True`, an exclusive cumprod is\nperformed instead:\n\n```python\ntf.cumprod([a, b, c], exclusive=True)  # => [1, a, a * b]\n```\n\nBy setting the `reverse` kwarg to `True`, the cumprod is performed in the\nopposite direction:\n\n```python\ntf.cumprod([a, b, c], reverse=True)  # => [a * b * c, b * c, c]\n```\n\nThis is more efficient than using separate `tf.reverse` ops.\n\nThe `reverse` and `exclusive` kwargs can also be combined:\n\n```python\ntf.cumprod([a, b, c], exclusive=True, reverse=True)  # => [b * c, c, 1]\n```",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "axis", "type": "Arg" }
    ],
    "outputs": [
      { "name": "out", "type": "TF_NumberTensor" }
    ],
    "attributes": [
      { "name": "exclusive", "type": "DefaultValuedOptionalAttr" },
      { "name": "reverse", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Cumsum",
    "summary": "Compute the cumulative sum of the tensor `x` along `axis`.",
    "description": "By default, this op performs an inclusive cumsum, which means that the first\nelement of the input is identical to the first element of the output:\n\n```python\ntf.cumsum([a, b, c])  # => [a, a + b, a + b + c]\n```\n\nBy setting the `exclusive` kwarg to `True`, an exclusive cumsum is\nperformed instead:\n\n```python\ntf.cumsum([a, b, c], exclusive=True)  # => [0, a, a + b]\n```\n\nBy setting the `reverse` kwarg to `True`, the cumsum is performed in the\nopposite direction:\n\n```python\ntf.cumsum([a, b, c], reverse=True)  # => [a + b + c, b + c, c]\n```\n\nThis is more efficient than using separate `tf.reverse` ops.\n\nThe `reverse` and `exclusive` kwargs can also be combined:\n\n```python\ntf.cumsum([a, b, c], exclusive=True, reverse=True)  # => [b + c, c, 0]\n```",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "axis", "type": "Arg" }
    ],
    "outputs": [
      { "name": "out", "type": "TF_NumberTensor" }
    ],
    "attributes": [
      { "name": "exclusive", "type": "DefaultValuedOptionalAttr" },
      { "name": "reverse", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.CumulativeLogsumexp",
    "summary": "Compute the cumulative product of the tensor `x` along `axis`.",
    "description": "By default, this op performs an inclusive cumulative log-sum-exp,\nwhich means that the first\nelement of the input is identical to the first element of the output:\n```python\ntf.math.cumulative_logsumexp([a, b, c])  # => [a, log(exp(a) + exp(b)), log(exp(a) + exp(b) + exp(c))]\n```\n\nBy setting the `exclusive` kwarg to `True`, an exclusive cumulative log-sum-exp is\nperformed instead:\n```python\ntf.cumulative_logsumexp([a, b, c], exclusive=True)  # => [-inf, a, log(exp(a) * exp(b))]\n```\nNote that the neutral element of the log-sum-exp operation is `-inf`,\nhowever, for performance reasons, the minimal value representable by the\nfloating point type is used instead.\n\nBy setting the `reverse` kwarg to `True`, the cumulative log-sum-exp is performed in the\nopposite direction.",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "axis", "type": "Arg" }
    ],
    "outputs": [
      { "name": "out", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "exclusive", "type": "DefaultValuedOptionalAttr" },
      { "name": "reverse", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DataFormatDimMap",
    "summary": "Returns the dimension index in the destination data format given the one in",
    "description": "the source data format.",
    "inputs": [
      { "name": "x", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" }
    ],
    "attributes": [
      { "name": "src_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dst_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DataFormatVecPermute",
    "summary": "Permute input tensor from `src_format` to `dst_format`.",
    "description": "Given source and destination format strings of length n=4 or 5, the input\ntensor must be a vector of size n or n-2, or a 2D tensor of shape\n(n, 2) or (n-2, 2).\n\nIf the first dimension of the input tensor is n-2, it is assumed that\nnon-spatial dimensions are omitted (i.e `N`, `C`).\n\nFor example, with `src_format` of `NHWC`, `dst_format` of `NCHW`, and input:\n```\n[1, 2, 3, 4]\n```\n, the output will be:\n```\n[1, 4, 2, 3]\n```\nWith `src_format` of `NDHWC`, `dst_format` of `NCDHW`, and input:\n```\n[[1, 6], [2, 7], [3, 8], [4, 9], [5, 10]]\n```\n, the output will be:\n```\n[[1, 6], [5, 10], [2, 7], [3, 8], [4, 9]]\n```\nWith `src_format` of `NHWC`, `dst_format` of `NCHW`, and input:\n```\n[1, 2]\n```\n, the output will be:\n```\n[1, 2]\n```",
    "inputs": [
      { "name": "x", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" }
    ],
    "attributes": [
      { "name": "src_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dst_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DebugIdentity",
    "summary": "Provides an identity mapping of the non-Ref type input tensor for debugging.",
    "description": "Provides an identity mapping of the non-Ref type input tensor for debugging.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "device_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "tensor_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "debug_urls", "type": "DefaultValuedOptionalAttr" },
      { "name": "gated_grpc", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DebugIdentityV2",
    "summary": "Debug Identity V2 Op.",
    "description": "Provides an identity mapping from input to output, while writing the content of\nthe input tensor by calling DebugEventsWriter.\n\nThe semantics of the input tensor depends on tensor_debug_mode. In typical\nusage, the input tensor comes directly from the user computation only when\ngraph_debug_mode is FULL_TENSOR (see protobuf/debug_event.proto for a\nlist of all the possible values of graph_debug_mode). For the other debug modes,\nthe input tensor should be produced by an additional op or subgraph that\ncomputes summary information about one or more tensors.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "tfdbg_context_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "op_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_slot", "type": "DefaultValuedOptionalAttr" },
      { "name": "tensor_debug_mode", "type": "DefaultValuedOptionalAttr" },
      { "name": "debug_urls", "type": "DefaultValuedOptionalAttr" },
      { "name": "circular_buffer_size", "type": "DefaultValuedOptionalAttr" },
      { "name": "tfdbg_run_id", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DecodeAndCropJpeg",
    "summary": "Decode and Crop a JPEG-encoded image to a uint8 tensor.",
    "description": "The attr `channels` indicates the desired number of color channels for the\ndecoded image.\n\nAccepted values are:\n\n*   0: Use the number of channels in the JPEG-encoded image.\n*   1: output a grayscale image.\n*   3: output an RGB image.\n\nIf needed, the JPEG-encoded image is transformed to match the requested number\nof color channels.\n\nThe attr `ratio` allows downscaling the image by an integer factor during\ndecoding.  Allowed values are: 1, 2, 4, and 8.  This is much faster than\ndownscaling the image later.\n\n\nIt is equivalent to a combination of decode and crop, but much faster by only\ndecoding partial jpeg image.",
    "inputs": [
      { "name": "contents", "type": "Arg" },
      { "name": "crop_window", "type": "Arg" }
    ],
    "outputs": [
      { "name": "image", "type": "Res" }
    ],
    "attributes": [
      { "name": "channels", "type": "DefaultValuedOptionalAttr" },
      { "name": "ratio", "type": "DefaultValuedOptionalAttr" },
      { "name": "fancy_upscaling", "type": "DefaultValuedOptionalAttr" },
      { "name": "try_recover_truncated", "type": "DefaultValuedOptionalAttr" },
      { "name": "acceptable_fraction", "type": "DefaultValuedOptionalAttr" },
      { "name": "dct_method", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DecodeGif",
    "summary": "Decode the frame(s) of a GIF-encoded image to a uint8 tensor.",
    "description": "GIF images with frame or transparency compression are not supported.\nOn Linux and MacOS systems, convert animated GIFs from compressed to\nuncompressed by running:\n\n    convert $src.gif -coalesce $dst.gif\n\nThis op also supports decoding JPEGs and PNGs, though it is cleaner to use\n`tf.io.decode_image`.",
    "inputs": [
      { "name": "contents", "type": "Arg" }
    ],
    "outputs": [
      { "name": "image", "type": "Res" }
    ]
  },
  {
    "name": "tf.DecodeJpeg",
    "summary": "Decode a JPEG-encoded image to a uint8 tensor.",
    "description": "The attr `channels` indicates the desired number of color channels for the\ndecoded image.\n\nAccepted values are:\n\n*   0: Use the number of channels in the JPEG-encoded image.\n*   1: output a grayscale image.\n*   3: output an RGB image.\n\nIf needed, the JPEG-encoded image is transformed to match the requested number\nof color channels.\n\nThe attr `ratio` allows downscaling the image by an integer factor during\ndecoding.  Allowed values are: 1, 2, 4, and 8.  This is much faster than\ndownscaling the image later.\n\n\nThis op also supports decoding PNGs and non-animated GIFs since the interface is\nthe same, though it is cleaner to use `tf.io.decode_image`.",
    "inputs": [
      { "name": "contents", "type": "Arg" }
    ],
    "outputs": [
      { "name": "image", "type": "Res" }
    ],
    "attributes": [
      { "name": "channels", "type": "DefaultValuedOptionalAttr" },
      { "name": "ratio", "type": "DefaultValuedOptionalAttr" },
      { "name": "fancy_upscaling", "type": "DefaultValuedOptionalAttr" },
      { "name": "try_recover_truncated", "type": "DefaultValuedOptionalAttr" },
      { "name": "acceptable_fraction", "type": "DefaultValuedOptionalAttr" },
      { "name": "dct_method", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DecodePaddedRaw",
    "summary": "Reinterpret the bytes of a string as a vector of numbers.",
    "inputs": [
      { "name": "input_bytes", "type": "Arg" },
      { "name": "fixed_length", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "little_endian", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DecodePng",
    "summary": "Decode a PNG-encoded image to a uint8 or uint16 tensor.",
    "description": "The attr `channels` indicates the desired number of color channels for the\ndecoded image.\n\nAccepted values are:\n\n*   0: Use the number of channels in the PNG-encoded image.\n*   1: output a grayscale image.\n*   3: output an RGB image.\n*   4: output an RGBA image.\n\nIf needed, the PNG-encoded image is transformed to match the requested number\nof color channels.\n\nThis op also supports decoding JPEGs and non-animated GIFs since the interface\nis the same, though it is cleaner to use `tf.io.decode_image`.",
    "inputs": [
      { "name": "contents", "type": "Arg" }
    ],
    "outputs": [
      { "name": "image", "type": "Res" }
    ],
    "attributes": [
      { "name": "channels", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DeleteIterator",
    "summary": "A container for an iterator resource.",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "deleter", "type": "Arg" }
    ]
  },
  {
    "name": "tf.DeleteMemoryCache",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "deleter", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.DeleteMultiDeviceIterator",
    "summary": "A container for an iterator resource.",
    "inputs": [
      { "name": "multi_device_iterator", "type": "Arg" },
      { "name": "iterators", "type": "Arg" },
      { "name": "deleter", "type": "Arg" }
    ]
  },
  {
    "name": "tf.DeleteRandomSeedGenerator",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "deleter", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.DeleteSeedGenerator",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "deleter", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.DepthToSpace",
    "summary": "DepthToSpace for tensors of type T.",
    "description": "Rearranges data from depth into blocks of spatial data.\nThis is the reverse transformation of SpaceToDepth. More specifically,\nthis op outputs a copy of the input tensor where values from the `depth`\ndimension are moved in spatial blocks to the `height` and `width` dimensions.\nThe attr `block_size` indicates the input block size and how the data is moved.\n\n  * Chunks of data of size `block_size * block_size` from depth are rearranged\n    into non-overlapping blocks of size `block_size x block_size`\n  * The width of the output tensor is `input_depth * block_size`, whereas the\n    height is `input_height * block_size`.\n  * The Y, X coordinates within each block of the output image are determined\n    by the high order component of the input channel index.\n  * The depth of the input tensor must be divisible by\n    `block_size * block_size`.\n\nThe `data_format` attr specifies the layout of the input and output tensors\nwith the following options:\n  \"NHWC\": `[ batch, height, width, channels ]`\n  \"NCHW\": `[ batch, channels, height, width ]`\n  \"NCHW_VECT_C\":\n      `qint8 [ batch, channels / 4, height, width, 4 ]`\n\nIt is useful to consider the operation as transforming a 6-D Tensor.\ne.g. for data_format = NHWC,\n     Each element in the input tensor can be specified via 6 coordinates,\n     ordered by decreasing memory layout significance as:\n     n,iY,iX,bY,bX,oC  (where n=batch index, iX, iY means X or Y coordinates\n                        within the input image, bX, bY means coordinates\n                        within the output block, oC means output channels).\n     The output would be the input transposed to the following layout:\n     n,iY,bY,iX,bX,oC\n\nThis operation is useful for resizing the activations between convolutions\n(but keeping all data), e.g. instead of pooling. It is also useful for training\npurely convolutional models.\n\nFor example, given an input of shape `[1, 1, 1, 4]`, data_format = \"NHWC\" and\nblock_size = 2:\n\n```\nx = [[[[1, 2, 3, 4]]]]\n\n```\n\nThis operation will output a tensor of shape `[1, 2, 2, 1]`:\n\n```\n   [[[[1], [2]],\n     [[3], [4]]]]\n```\n\nHere, the input has a batch of 1 and each batch element has shape `[1, 1, 4]`,\nthe corresponding output will have 2x2 elements and will have a depth of\n1 channel (1 = `4 / (block_size * block_size)`).\nThe output element shape is `[2, 2, 1]`.\n\nFor an input tensor with larger depth, here of shape `[1, 1, 1, 12]`, e.g.\n\n```\nx = [[[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]]]]\n```\n\nThis operation, for block size of 2, will return the following tensor of shape\n`[1, 2, 2, 3]`\n\n```\n   [[[[1, 2, 3], [4, 5, 6]],\n     [[7, 8, 9], [10, 11, 12]]]]\n\n```\n\nSimilarly, for the following input of shape `[1 2 2 4]`, and a block size of 2:\n\n```\nx =  [[[[1, 2, 3, 4],\n       [5, 6, 7, 8]],\n      [[9, 10, 11, 12],\n       [13, 14, 15, 16]]]]\n```\n\nthe operator will return the following tensor of shape `[1 4 4 1]`:\n\n```\nx = [[[ [1],   [2],  [5],  [6]],\n      [ [3],   [4],  [7],  [8]],\n      [ [9],  [10], [13],  [14]],\n      [ [11], [12], [15],  [16]]]]\n\n```",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "block_size", "type": "ConfinedAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DepthwiseConv2dNative",
    "summary": "Computes a 2-D depthwise convolution given 4-D `input` and `filter` tensors.",
    "description": "Given an input tensor of shape `[batch, in_height, in_width, in_channels]`\nand a filter / kernel tensor of shape\n`[filter_height, filter_width, in_channels, channel_multiplier]`, containing\n`in_channels` convolutional filters of depth 1, `depthwise_conv2d` applies\na different filter to each input channel (expanding from 1 channel to\n`channel_multiplier` channels for each), then concatenates the results\ntogether. Thus, the output has `in_channels * channel_multiplier` channels.\n\n```\nfor k in 0..in_channels-1\n  for q in 0..channel_multiplier-1\n    output[b, i, j, k * channel_multiplier + q] =\n      sum_{di, dj} input[b, strides[1] * i + di, strides[2] * j + dj, k] *\n                        filter[di, dj, k, q]\n```\n\nMust have `strides[0] = strides[3] = 1`.  For the most common case of the same\nhorizontal and vertices strides, `strides = [1, stride, stride, 1]`.",
    "inputs": [
      { "name": "input", "type": "TF_FloatTensor" },
      { "name": "filter", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DepthwiseConv2dNativeBackpropFilter",
    "summary": "Computes the gradients of depthwise convolution with respect to the filter.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "filter_sizes", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DepthwiseConv2dNativeBackpropInput",
    "summary": "Computes the gradients of depthwise convolution with respect to the input.",
    "inputs": [
      { "name": "input_sizes", "type": "Arg" },
      { "name": "filter", "type": "Arg" },
      { "name": "out_backprop", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Dequantize",
    "summary": "Dequantize the 'input' tensor into a float or bfloat16 Tensor.",
    "description": "[min_range, max_range] are scalar floats that specify the range for\nthe output. The 'mode' attribute controls exactly which calculations are\nused to convert the float values to their quantized equivalents.\n\nIn 'MIN_COMBINED' mode, each value of the tensor will undergo the following:\n\n```\nif T == qint8: in[i] += (range(T) + 1)/ 2.0\nout[i] = min_range + (in[i]* (max_range - min_range) / range(T))\n```\nhere `range(T) = numeric_limits<T>::max() - numeric_limits<T>::min()`\n\n*MIN_COMBINED Mode Example*\n\nIf the input comes from a QuantizedRelu6, the output type is\nquint8 (range of 0-255) but the possible range of QuantizedRelu6 is\n0-6.  The min_range and max_range values are therefore 0.0 and 6.0.\nDequantize on quint8 will take each value, cast to float, and multiply\nby 6 / 255.\nNote that if quantizedtype is qint8, the operation will additionally add\neach value by 128 prior to casting.\n\nIf the mode is 'MIN_FIRST', then this approach is used:\n\n```c++\nnum_discrete_values = 1 << (# of bits in T)\nrange_adjust = num_discrete_values / (num_discrete_values - 1)\nrange = (range_max - range_min) * range_adjust\nrange_scale = range / num_discrete_values\nconst double offset_input = static_cast<double>(input) - lowest_quantized;\nresult = range_min + ((input - numeric_limits<T>::min()) * range_scale)\n```\n\nIf the mode is `SCALED`, dequantization is performed by multiplying each\ninput value by a scaling_factor. (Thus an input of 0 always maps to 0.0).\n\nThe scaling_factor is determined from `min_range`, `max_range`, and\n`narrow_range` in a way that is compatible with `QuantizeAndDequantize{V2|V3}`\nand `QuantizeV2`, using the following algorithm:\n\n```c++\n\n  const int min_expected_T = std::numeric_limits<T>::min() +\n    (narrow_range ? 1 : 0);\n  const int max_expected_T = std::numeric_limits<T>::max();\n  const float max_expected_T = std::numeric_limits<float>::max();\n\n  const float scale_factor =\n    (std::numeric_limits<T>::min() == 0) ? (max_range / max_expected_T)\n                                         : std::max(min_range / min_expected_T,\n                                                    max_range / max_expected_T);\n```",
    "inputs": [
      { "name": "input", "type": "TensorOf" },
      { "name": "min_range", "type": "Arg" },
      { "name": "max_range", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" },
      { "name": "axis", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DeserializeIterator",
    "summary": "Converts the given variant tensor to an iterator and stores it in the given resource.",
    "inputs": [
      { "name": "resource_handle", "type": "Arg" },
      { "name": "serialized", "type": "Arg" }
    ]
  },
  {
    "name": "tf.DeserializeSparse",
    "summary": "Deserialize `SparseTensor` objects.",
    "description": "The input `serialized_sparse` must have the shape `[?, ?, ..., ?, 3]` where\nthe last dimension stores serialized `SparseTensor` objects and the other N\ndimensions (N >= 0) correspond to a batch. The ranks of the original\n`SparseTensor` objects must all match. When the final `SparseTensor` is\ncreated, its rank is the rank of the incoming `SparseTensor` objects plus N;\nthe sparse tensors have been concatenated along new dimensions, one for each\nbatch.\n\nThe output `SparseTensor` object's shape values for the original dimensions\nare the max across the input `SparseTensor` objects' shape values for the\ncorresponding dimensions. The new dimensions match the size of the batch.\n\nThe input `SparseTensor` objects' indices are assumed ordered in\nstandard lexicographic order.  If this is not the case, after this\nstep run `SparseReorder` to restore index ordering.\n\nFor example, if the serialized input is a `[2 x 3]` matrix representing two\noriginal `SparseTensor` objects:\n\n    index = [ 0]\n            [10]\n            [20]\n    values = [1, 2, 3]\n    shape = [50]\n\nand\n\n    index = [ 2]\n            [10]\n    values = [4, 5]\n    shape = [30]\n\nthen the final deserialized `SparseTensor` will be:\n\n    index = [0  0]\n            [0 10]\n            [0 20]\n            [1  2]\n            [1 10]\n    values = [1, 2, 3, 4, 5]\n    shape = [2 50]",
    "inputs": [
      { "name": "serialized_sparse", "type": "Arg" }
    ],
    "outputs": [
      { "name": "sparse_indices", "type": "TF_Int64Tensor" },
      { "name": "sparse_values", "type": "TF_Tensor" },
      { "name": "sparse_shape", "type": "TF_Int64Tensor" }
    ]
  },
  {
    "name": "tf.DestroyResourceOp",
    "summary": "Deletes the resource specified by the handle.",
    "description": "All subsequent operations using the resource will result in a NotFound\nerror status.",
    "inputs": [
      { "name": "resource", "type": "Arg" }
    ],
    "attributes": [
      { "name": "ignore_lookup_error", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DeviceIndex",
    "summary": "Return the index of device the op runs.",
    "description": "Given a list of device names, this operation returns the index of the device\nthis op runs. The length of the list is returned in two cases:\n(1) Device does not exist in the given device list.\n(2) It is in XLA compilation.",
    "outputs": [
      { "name": "index", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "device_names", "type": "StrArrayAttr" }
    ]
  },
  {
    "name": "tf.Diag",
    "summary": "Returns a diagonal tensor with a given diagonal values.",
    "description": "Given a `diagonal`, this operation returns a tensor with the `diagonal` and\neverything else padded with zeros. The diagonal is computed as follows:\n\nAssume `diagonal` has dimensions [D1,..., Dk], then the output is a tensor of\nrank 2k with dimensions [D1,..., Dk, D1,..., Dk] where:\n\n`output[i1,..., ik, i1,..., ik] = diagonal[i1, ..., ik]` and 0 everywhere else.\n\nFor example:\n\n```\n# 'diagonal' is [1, 2, 3, 4]\ntf.diag(diagonal) ==> [[1, 0, 0, 0]\n                       [0, 2, 0, 0]\n                       [0, 0, 3, 0]\n                       [0, 0, 0, 4]]\n```",
    "inputs": [
      { "name": "diagonal", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.DiagPart",
    "summary": "Returns the diagonal part of the tensor.",
    "description": "This operation returns a tensor with the `diagonal` part\nof the `input`. The `diagonal` part is computed as follows:\n\nAssume `input` has dimensions `[D1,..., Dk, D1,..., Dk]`, then the output is a\ntensor of rank `k` with dimensions `[D1,..., Dk]` where:\n\n`diagonal[i1,..., ik] = input[i1, ..., ik, i1,..., ik]`.\n\nFor example:\n\n```\n# 'input' is [[1, 0, 0, 0]\n              [0, 2, 0, 0]\n              [0, 0, 3, 0]\n              [0, 0, 0, 4]]\n\ntf.diag_part(input) ==> [1, 2, 3, 4]\n```",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "diagonal", "type": "Res" }
    ]
  },
  {
    "name": "tf.Digamma",
    "summary": "Computes Psi, the derivative of Lgamma (the log of the absolute value of",
    "description": "`Gamma(x)`), element-wise.",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.DisableCopyOnRead",
    "summary": "Turns off the copy-on-read mode.",
    "description": "Turns off the copy-on-read mode of a resource variable. If the variable is not in copy-on-read mode, this op has no effect.",
    "inputs": [
      { "name": "resource", "type": "Arg" }
    ]
  },
  {
    "name": "tf.Div",
    "summary": "Returns x / y element-wise.",
    "description": "*NOTE*: `Div` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.DivNoNan",
    "summary": "Returns 0 if the denominator is zero.",
    "description": "*NOTE*: `DivNoNan` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" },
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.DummyMemoryCache",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ]
  },
  {
    "name": "tf.DummySeedGenerator",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ]
  },
  {
    "name": "tf.DynamicEnqueueTPUEmbeddingArbitraryTensorBatch",
    "summary": "Eases the porting of code that uses tf.nn.embedding_lookup_sparse().",
    "description": "embedding_indices[i] and aggregation_weights[i] correspond\nto the ith feature.\n\nThe tensors at corresponding positions in the three input lists (sample_indices,\nembedding_indices and aggregation_weights) must have the same shape, i.e. rank 1\nwith dim_size() equal to the total number of lookups into the table described by\nthe corresponding feature.",
    "inputs": [
      { "name": "sample_indices_or_row_splits", "type": "Arg" },
      { "name": "embedding_indices", "type": "Arg" },
      { "name": "aggregation_weights", "type": "Arg" },
      { "name": "mode_override", "type": "Arg" },
      { "name": "device_ordinal", "type": "Arg" }
    ],
    "attributes": [
      { "name": "combiners", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.DynamicPartition",
    "summary": "Partitions `data` into `num_partitions` tensors using indices from `partitions`.",
    "description": "For each index tuple `js` of size `partitions.ndim`, the slice `data[js, ...]`\nbecomes part of `outputs[partitions[js]]`.  The slices with `partitions[js] = i`\nare placed in `outputs[i]` in lexicographic order of `js`, and the first\ndimension of `outputs[i]` is the number of entries in `partitions` equal to `i`.\nIn detail,\n\n```python\n    outputs[i].shape = [sum(partitions == i)] + data.shape[partitions.ndim:]\n\n    outputs[i] = pack([data[js, ...] for js if partitions[js] == i])\n```\n\n`data.shape` must start with `partitions.shape`.\n\nFor example:\n\n```python\n    # Scalar partitions.\n    partitions = 1\n    num_partitions = 2\n    data = [10, 20]\n    outputs[0] = []  # Empty with shape [0, 2]\n    outputs[1] = [[10, 20]]\n\n    # Vector partitions.\n    partitions = [0, 0, 1, 1, 0]\n    num_partitions = 2\n    data = [10, 20, 30, 40, 50]\n    outputs[0] = [10, 20, 50]\n    outputs[1] = [30, 40]\n```\n\nSee `dynamic_stitch` for an example on how to merge partitions back.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/DynamicPartition.png\" alt>\n</div>\n\n\nRaises:\n  * `InvalidArgumentError` in following cases:\n    - If partitions is not in range `[0, num_partiions)`\n    - If `partitions.shape` does not match prefix of `data.shape` argument.",
    "inputs": [
      { "name": "data", "type": "TF_Tensor" },
      { "name": "partitions", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.DynamicStitch",
    "summary": "Interleave the values from the `data` tensors into a single tensor.",
    "description": "Builds a merged tensor such that\n\n```python\n    merged[indices[m][i, ..., j], ...] = data[m][i, ..., j, ...]\n```\n\nFor example, if each `indices[m]` is scalar or vector, we have\n\n```python\n    # Scalar indices:\n    merged[indices[m], ...] = data[m][...]\n\n    # Vector indices:\n    merged[indices[m][i], ...] = data[m][i, ...]\n```\n\nEach `data[i].shape` must start with the corresponding `indices[i].shape`,\nand the rest of `data[i].shape` must be constant w.r.t. `i`.  That is, we\nmust have `data[i].shape = indices[i].shape + constant`.  In terms of this\n`constant`, the output shape is\n\n    merged.shape = [max(indices) + 1] + constant\n\nValues are merged in order, so if an index appears in both `indices[m][i]` and\n`indices[n][j]` for `(m,i) < (n,j)` the slice `data[n][j]` will appear in the\nmerged result. If you do not need this guarantee, ParallelDynamicStitch might\nperform better on some devices.\n\nFor example:\n\n```python\n    indices[0] = 6\n    indices[1] = [4, 1]\n    indices[2] = [[5, 2], [0, 3]]\n    data[0] = [61, 62]\n    data[1] = [[41, 42], [11, 12]]\n    data[2] = [[[51, 52], [21, 22]], [[1, 2], [31, 32]]]\n    merged = [[1, 2], [11, 12], [21, 22], [31, 32], [41, 42],\n              [51, 52], [61, 62]]\n```\n\nThis method can be used to merge partitions created by `dynamic_partition`\nas illustrated on the following example:\n\n```python\n    # Apply function (increments x_i) on elements for which a certain condition\n    # apply (x_i != -1 in this example).\n    x=tf.constant([0.1, -1., 5.2, 4.3, -1., 7.4])\n    condition_mask=tf.not_equal(x,tf.constant(-1.))\n    partitioned_data = tf.dynamic_partition(\n        x, tf.cast(condition_mask, tf.int32) , 2)\n    partitioned_data[1] = partitioned_data[1] + 1.0\n    condition_indices = tf.dynamic_partition(\n        tf.range(tf.shape(x)[0]), tf.cast(condition_mask, tf.int32) , 2)\n    x = tf.dynamic_stitch(condition_indices, partitioned_data)\n    # Here x=[1.1, -1., 6.2, 5.3, -1, 8.4], the -1. values remain\n    # unchanged.\n```\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/DynamicStitch.png\" alt>\n</div>",
    "inputs": [
      { "name": "indices", "type": "Variadic" },
      { "name": "data", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "merged", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.Einsum",
    "summary": "Tensor contraction according to Einstein summation convention.",
    "description": "Implements generalized Tensor contraction and reduction. Each input Tensor must\nhave a corresponding input subscript appearing in the comma-separated left-hand\nside of the equation. The right-hand side of the equation consists of the\noutput subscript. The input subscripts and the output subscript should consist\nof zero or more named axis labels and at most one ellipsis (`...`).\n\nThe named axis labels may be any single character other than those having\nspecial meaning, namely `,.->`. The behavior of this Op is undefined if it\nreceives an ill-formatted equation; since the validation is done at\ngraph-building time, we omit format validation checks at runtime.\n\nNote: This Op is *not* intended to be called by the user; instead users should\ncall `tf.einsum` directly. It is a hidden Op used by `tf.einsum`.\n\nOperations are applied to the input(s) according to the following rules:\n\n (a) Generalized Diagonals: For input dimensions corresponding to axis labels\n     appearing more than once in the same input subscript, we take the\n     generalized (`k`-dimensional) diagonal.\n     For example, in the equation `iii->i` with input shape `[3, 3, 3]`, the\n     generalized diagonal would consist of `3` elements at indices `(0, 0, 0)`,\n     `(1, 1, 1)` and `(2, 2, 2)` to create a Tensor of shape `[3]`.\n\n (b) Reduction: Axes corresponding to labels appearing only in one input\n     subscript but not in the output subscript are summed over prior to Tensor\n     contraction.\n     For example, in the equation `ab,bc->b`, the axis labels `a` and `c` are\n     the reduction axis labels.\n\n (c) Batch Dimensions: Axes corresponding to labels appearing in each of the\n     input subscripts and also in the output subscript make up the batch\n     dimensions in Tensor contraction. Unnamed axis labels corresponding to\n     ellipsis (`...`) also correspond to batch dimensions.\n     For example, for the equation denoting batch matrix multiplication,\n     `bij,bjk->bik`, the axis label `b` corresponds to a batch dimension.\n\n (d) Contraction: In case of binary einsum, axes corresponding to labels\n     appearing in two different inputs (and not in the output) are contracted\n     against each other.\n     Considering the batch matrix multiplication equation again\n     (`bij,bjk->bik`), the contracted axis label is `j`.\n\n (e) Expand Diagonal: If the output subscripts contain repeated (explicit) axis\n     labels, the opposite operation of (a) is applied. For example, in the\n     equation `i->iii`, and input shape `[3]`, the output of shape `[3, 3, 3]`\n     are all zeros, except for the (generalized) diagonal which is populated\n     with values from the input.\n     Note: This operation is not supported by `np.einsum` or `tf.einsum`; it is\n     provided to enable computing the symbolic gradient of `tf.einsum`.\n\nThe output subscripts must contain only labels appearing in at least one of the\ninput subscripts. Furthermore, all dimensions mapping to the same axis label\nmust be equal.\n\nAny of the input and output subscripts may contain at most a single ellipsis\n(`...`). These ellipsis are mapped against dimensions not corresponding to any\nnamed axis label. If two inputs contain ellipsis, then they are broadcasted\naccording to standard NumPy broadcasting\n[rules](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n\nThe broadcasted dimensions are placed in the corresponding location of the\nellipsis in the output subscript. If the broadcasted dimensions are non-empty\nand the output subscripts do not contain ellipsis, then an InvalidArgument error\nis raised.\n\n@compatibility(numpy)\nSimilar to [`numpy.einsum`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html).\n\nComparison with `numpy.einsum`:\n\n * This Op only supports unary and binary forms of `numpy.einsum`.\n * This Op does not support implicit form. (i.e. equations without `->`).\n * This Op also supports repeated indices in the output subscript, which is not\n   supported by `numpy.einsum`.\n@end_compatibility",
    "inputs": [
      { "name": "inputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "equation", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.Elu",
    "summary": "Computes the exponential linear function.",
    "description": "The ELU function is defined as:\n\n * $ e ^ x - 1 $ if $ x < 0 $\n * $ x $ if $ x >= 0 $\n\nExamples:\n\n>>> tf.nn.elu(1.0)\n<tf.Tensor: shape=(), dtype=float32, numpy=1.0>\n>>> tf.nn.elu(0.0)\n<tf.Tensor: shape=(), dtype=float32, numpy=0.0>\n>>> tf.nn.elu(-1000.0)\n<tf.Tensor: shape=(), dtype=float32, numpy=-1.0>\n\nSee [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\n](http://arxiv.org/abs/1511.07289)",
    "inputs": [
      { "name": "features", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.EluGrad",
    "summary": "Computes gradients for the exponential linear (Elu) operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "outputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "backprops", "type": "Res" }
    ]
  },
  {
    "name": "tf.Empty",
    "summary": "Creates a tensor with the given shape.\n\nThis operation creates a tensor of `shape` and `dtype`.",
    "inputs": [
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "init", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.EmptyTensorList",
    "summary": "Creates and returns an empty tensor list.",
    "description": "All list elements must be tensors of dtype element_dtype and shape compatible\nwith element_shape.\n\nhandle: an empty tensor list.\nelement_dtype: the type of elements in the list.\nelement_shape: a shape compatible with that of elements in the list.",
    "inputs": [
      { "name": "element_shape", "type": "TF_I32OrI64Tensor" },
      { "name": "max_num_elements", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.EncodePng",
    "summary": "PNG-encode an image.",
    "description": "`image` is a 3-D uint8 or uint16 Tensor of shape `[height, width, channels]`\nwhere `channels` is:\n\n*   1: for grayscale.\n*   2: for grayscale + alpha.\n*   3: for RGB.\n*   4: for RGBA.\n\nThe ZLIB compression level, `compression`, can be -1 for the PNG-encoder\ndefault or a value from 0 to 9.  9 is the highest compression level, generating\nthe smallest output, but is slower.",
    "inputs": [
      { "name": "image", "type": "Arg" }
    ],
    "outputs": [
      { "name": "contents", "type": "Res" }
    ],
    "attributes": [
      { "name": "compression", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.EnqueueTPUEmbeddingArbitraryTensorBatch",
    "summary": "Eases the porting of code that uses tf.nn.embedding_lookup_sparse().",
    "description": "embedding_indices[i] and aggregation_weights[i] correspond\nto the ith feature.\n\nThe tensors at corresponding positions in the three input lists (sample_indices,\nembedding_indices and aggregation_weights) must have the same shape, i.e. rank 1\nwith dim_size() equal to the total number of lookups into the table described by\nthe corresponding feature.",
    "inputs": [
      { "name": "sample_indices_or_row_splits", "type": "Arg" },
      { "name": "embedding_indices", "type": "Arg" },
      { "name": "aggregation_weights", "type": "Arg" },
      { "name": "mode_override", "type": "Arg" }
    ],
    "attributes": [
      { "name": "device_ordinal", "type": "DefaultValuedOptionalAttr" },
      { "name": "combiners", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.EnqueueTPUEmbeddingBatch",
    "summary": "An op that enqueues a list of input batch tensors to TPUEmbedding.",
    "description": "An op that enqueues a list of input batch tensors to TPUEmbedding.",
    "inputs": [
      { "name": "batch", "type": "Arg" },
      { "name": "mode_override", "type": "Arg" }
    ],
    "attributes": [
      { "name": "device_ordinal", "type": "DefaultValuedOptionalAttr" },
      { "name": "combiners", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.EnqueueTPUEmbeddingIntegerBatch",
    "summary": "An op that enqueues a list of input batch tensors to TPUEmbedding.",
    "inputs": [
      { "name": "batch", "type": "Arg" },
      { "name": "mode_override", "type": "Arg" }
    ],
    "attributes": [
      { "name": "device_ordinal", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.EnqueueTPUEmbeddingRaggedTensorBatch",
    "summary": "Eases the porting of code that uses tf.nn.embedding_lookup().",
    "description": "sample_splits[i], embedding_indices[i] and aggregation_weights[i] correspond\nto the ith feature. table_ids[i] indicates which embedding table to look up ith\nfeature.\n\nThe tensors at corresponding positions in two of the input lists,\nembedding_indices and aggregation_weights, must have the same shape, i.e. rank 1\nwith dim_size() equal to the total number of lookups into the table described by\nthe corresponding feature.",
    "inputs": [
      { "name": "sample_splits", "type": "Arg" },
      { "name": "embedding_indices", "type": "Arg" },
      { "name": "aggregation_weights", "type": "Arg" },
      { "name": "mode_override", "type": "Arg" }
    ],
    "attributes": [
      { "name": "device_ordinal", "type": "DefaultValuedOptionalAttr" },
      { "name": "combiners", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_ids", "type": "I64ArrayAttr" },
      { "name": "max_sequence_lengths", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_features", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.EnqueueTPUEmbeddingSparseBatch",
    "summary": "An op that enqueues TPUEmbedding input indices from a SparseTensor.",
    "description": "This Op eases the porting of code that uses embedding_lookup_sparse(),\nalthough some Python preprocessing of the SparseTensor arguments to\nembedding_lookup_sparse() is required to produce the arguments to this Op,\nsince only a single EnqueueTPUEmbeddingSparseBatch Op is allowed per training\nstep.\n\nThe tensors at corresponding positions in the three input lists\nmust have the same shape, i.e. rank 1 with dim_size() equal to the total\nnumber of lookups into the table described by the corresponding table_id.",
    "inputs": [
      { "name": "sample_indices", "type": "Arg" },
      { "name": "embedding_indices", "type": "Arg" },
      { "name": "aggregation_weights", "type": "Arg" },
      { "name": "mode_override", "type": "Arg" }
    ],
    "attributes": [
      { "name": "device_ordinal", "type": "DefaultValuedOptionalAttr" },
      { "name": "combiners", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.EnqueueTPUEmbeddingSparseTensorBatch",
    "summary": "Eases the porting of code that uses tf.nn.embedding_lookup_sparse().",
    "description": "sample_indices[i], embedding_indices[i] and aggregation_weights[i] correspond\nto the ith feature. table_ids[i] indicates which embedding table to look up ith\nfeature.\n\nThe tensors at corresponding positions in the three input lists (sample_indices,\nembedding_indices and aggregation_weights) must have the same shape, i.e. rank 1\nwith dim_size() equal to the total number of lookups into the table described by\nthe corresponding feature.",
    "inputs": [
      { "name": "sample_indices", "type": "Arg" },
      { "name": "embedding_indices", "type": "Arg" },
      { "name": "aggregation_weights", "type": "Arg" },
      { "name": "mode_override", "type": "Arg" }
    ],
    "attributes": [
      { "name": "device_ordinal", "type": "DefaultValuedOptionalAttr" },
      { "name": "combiners", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_ids", "type": "I64ArrayAttr" },
      { "name": "max_sequence_lengths", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_features", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.EnsureShape",
    "summary": "Ensures that the tensor's shape matches the expected shape.",
    "description": "Raises an error if the input tensor's shape does not match the specified shape.\nReturns the input tensor otherwise.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "shape", "type": "TF_ShapeAttr" }
    ]
  },
  {
    "name": "tf.Equal",
    "summary": "Returns the truth value of (x == y) element-wise.",
    "description": "*NOTE*: `Equal` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n```python\nx = tf.constant([2, 4])\ny = tf.constant(2)\ntf.math.equal(x, y) ==> array([True, False])\n\nx = tf.constant([2, 4])\ny = tf.constant([2, 4])\ntf.math.equal(x, y) ==> array([True,  True])\n```",
    "inputs": [
      { "name": "x", "type": "TF_Tensor" },
      { "name": "y", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ],
    "attributes": [
      { "name": "incompatible_shape_error", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Erf",
    "summary": "Computes the [Gauss error function](https://en.wikipedia.org/wiki/Error_function) of `x` element-wise. In statistics, for non-negative values of $x$, the error function has the following interpretation: for a random variable $Y$ that is normally distributed with mean 0 and variance $1/\\sqrt{2}$, $erf(x)$ is the probability that $Y$ falls in the range $[−x, x]$.",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.Erfc",
    "summary": "Computes the complementary error function of `x` element-wise.",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.Erfinv",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.ExecuteTPUEmbeddingPartitioner",
    "summary": "An op that executes the TPUEmbedding partitioner on the central configuration",
    "description": "device and computes the HBM size (in bytes) required for TPUEmbedding operation.",
    "outputs": [
      { "name": "common_config", "type": "Res" }
    ],
    "attributes": [
      { "name": "config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.Exp",
    "summary": "Computes exponential of x element-wise.  \\\\(y = e^x\\\\).",
    "description": "This function computes the exponential of every element in the input tensor.\n  i.e. `exp(x)` or `e^(x)`, where `x` is the input tensor.\n  `e` denotes Euler's number and is approximately equal to 2.718281.\n  Output is positive for any real input.\n\n  ```python\n  x = tf.constant(2.0)\n  tf.math.exp(x) ==> 7.389056\n\n  x = tf.constant([2.0, 8.0])\n  tf.math.exp(x) ==> array([7.389056, 2980.958], dtype=float32)\n  ```\n\n  For complex numbers, the exponential value is calculated as follows:\n\n  ```\n  e^(x+iy) = e^x * e^iy = e^x * (cos y + i sin y)\n  ```\n\n  Let's consider complex number 1+1j as an example.\n  e^1 * (cos 1 + i sin 1) = 2.7182818284590 * (0.54030230586+0.8414709848j)\n\n  ```python\n  x = tf.constant(1 + 1j)\n  tf.math.exp(x) ==> 1.4686939399158851+2.2873552871788423j\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.ExpandDims",
    "summary": "Inserts a dimension of 1 into a tensor's shape.",
    "description": "Given a tensor `input`, this operation inserts a dimension of 1 at the\ndimension index `axis` of `input`'s shape. The dimension index `axis` starts at\nzero; if you specify a negative number for `axis` it is counted backward from\nthe end.\n\nThis operation is useful if you want to add a batch dimension to a single\nelement. For example, if you have a single image of shape `[height, width,\nchannels]`, you can make it a batch of 1 image with `expand_dims(image, 0)`,\nwhich will make the shape `[1, height, width, channels]`.\n\nOther examples:\n\n```\n# 't' is a tensor of shape [2]\nshape(expand_dims(t, 0)) ==> [1, 2]\nshape(expand_dims(t, 1)) ==> [2, 1]\nshape(expand_dims(t, -1)) ==> [2, 1]\n\n# 't2' is a tensor of shape [2, 3, 5]\nshape(expand_dims(t2, 0)) ==> [1, 2, 3, 5]\nshape(expand_dims(t2, 2)) ==> [2, 3, 1, 5]\nshape(expand_dims(t2, 3)) ==> [2, 3, 5, 1]\n```\n\nThis operation requires that:\n\n`-1-input.dims() <= dim <= input.dims()`\n\nThis operation is related to `squeeze()`, which removes dimensions of\nsize 1.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "dim", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Expm1",
    "summary": "Computes `exp(x) - 1` element-wise.",
    "description": "i.e. `exp(x) - 1` or `e^(x) - 1`, where `x` is the input tensor.\n  `e` denotes Euler's number and is approximately equal to 2.718281.\n\n  ```python\n  x = tf.constant(2.0)\n  tf.math.expm1(x) ==> 6.389056\n\n  x = tf.constant([2.0, 8.0])\n  tf.math.expm1(x) ==> array([6.389056, 2979.958], dtype=float32)\n\n  x = tf.constant(1 + 1j)\n  tf.math.expm1(x) ==> (0.46869393991588515+2.2873552871788423j)\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.ExtractImagePatches",
    "summary": "Extract `patches` from `images` and put them in the \"depth\" output dimension.",
    "inputs": [
      { "name": "images", "type": "Arg" }
    ],
    "outputs": [
      { "name": "patches", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksizes", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "rates", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" }
    ]
  },
  {
    "name": "tf.FakeParam",
    "summary": "This op is used as a placeholder in If branch functions. It doesn't provide a\n  valid output when run, so must either be removed (e.g. replaced with a\n  function input) or guaranteed not to be used (e.g. if mirroring an\n  intermediate output needed for the gradient computation of the other branch).",
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "shape", "type": "TF_ShapeAttr" }
    ]
  },
  {
    "name": "tf.FakeQuantWithMinMaxArgs",
    "summary": "Fake-quantize the 'inputs' tensor, type float to 'outputs' tensor of same shape and type.",
    "description": "Quantization is called fake since the output is still in floating point.\n  The API converts inputs into values within the range [min and max] and returns\n  as output.\n\nAttributes\n\n*   `[min; max]` define the clamping range for the `inputs` data.\n*   `inputs` values are quantized into the quantization range (\n`[0; 2^num_bits - 1]` when `narrow_range` is false and `[1; 2^num_bits - 1]`\nwhen it is true) and then de-quantized and output as floats in `[min; max]`\ninterval.\n*   `num_bits` is the bitwidth of the quantization; between 2 and 16, inclusive.\n\nBefore quantization, `min` and `max` values are adjusted with the following\nlogic.\nIt is suggested to have `min <= 0 <= max`. If `0` is not in the range of values,\nthe behavior can be unexpected:\n\n*   If `0 < min < max`: `min_adj = 0` and `max_adj = max - min`.\n*   If `min < max < 0`: `min_adj = min - max` and `max_adj = 0`.\n*   If `min <= 0 <= max`: `scale = (max - min) / (2^num_bits - 1) `,\n`min_adj = scale * round(min / scale)` and `max_adj = max + min_adj - min`.\n\n\nExamples\n\n```python\n\ninp = tf.constant ([10.03, -10.23, 3])\nout = tf.quantization.fake_quant_with_min_max_args(inp, min=-5, max=5,\n                                                   num_bits=16)\nprint(out)\n\n#  Output:\n#  tf.Tensor([ 4.9999237 -5.0000763  3.0000763], shape=(3,), dtype=float32)\n```\n\nRaises:\n  * InvalidArgumentError:\n    - If num_bits are outside of range [2, 16].\n    - If min >= max.\n  * ValueError: If `inputs` are of any other type than float32.",
    "inputs": [
      { "name": "inputs", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "outputs", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "min", "type": "DefaultValuedOptionalAttr" },
      { "name": "max", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FakeQuantWithMinMaxArgsGradient",
    "summary": "Compute gradients for a FakeQuantWithMinMaxArgs operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "inputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "backprops", "type": "Res" }
    ],
    "attributes": [
      { "name": "min", "type": "DefaultValuedOptionalAttr" },
      { "name": "max", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FakeQuantWithMinMaxVars",
    "summary": "Fake-quantize the 'inputs' tensor of type float via global float scalars",
    "description": "Fake-quantize the `inputs` tensor of type float via global float scalars\n`min` and `max` to `outputs` tensor of same shape as `inputs`.\n\nAttributes\n\n*   `[min; max]` define the clamping range for the `inputs` data.\n*   `inputs` values are quantized into the quantization range (\n`[0; 2^num_bits - 1]` when `narrow_range` is false and `[1; 2^num_bits - 1]`\nwhen it is true) and then de-quantized and output as floats in `[min; max]`\ninterval.\n*   `num_bits` is the bitwidth of the quantization; between 2 and 16, inclusive.\n\nBefore quantization, `min` and `max` values are adjusted with the following\nlogic.\nIt is suggested to have `min <= 0 <= max`. If `0` is not in the range of values,\nthe behavior can be unexpected:\n\n*   If `0 < min < max`: `min_adj = 0` and `max_adj = max - min`.\n*   If `min < max < 0`: `min_adj = min - max` and `max_adj = 0`.\n*   If `min <= 0 <= max`: `scale = (max - min) / (2^num_bits - 1) `,\n`min_adj = scale * round(min / scale)` and `max_adj = max + min_adj - min`.\n\nThis operation has a gradient and thus allows for training `min` and `max`\nvalues.\n\n>>> constant_input = tf.constant([[1.2, -0.3, 0.7], [2.1, 0.5, -1.0]], dtype=tf.float32)\n>>>\n>>> min_val = -0.5\n>>> max_val = 0.8\n>>> num_bits = 8\n>>> narrow_range = False #False:for the quantization range [0; 2^num_bits - 1]\n>>>\n>>> quantized_data = tf.quantization.fake_quant_with_min_max_vars(\n...   inputs=constant_input, min=min_val, max=max_val, num_bits=num_bits, narrow_range=narrow_range\n... )\n>>>\n>>> print(\"Input:\\n\", constant_input.numpy())\nInput:\n[[ 1.2 -0.3  0.7]\n[ 2.1  0.5 -1. ]]\n>>> print(\"Output:\\n\", quantized_data.numpy())\nOutput:\n[[ 0.8003921 -0.3007843  0.6984313]\n[ 0.8003921  0.4996078 -0.4996078]]",
    "inputs": [
      { "name": "inputs", "type": "TF_Float32Tensor" },
      { "name": "min", "type": "TF_Float32Tensor" },
      { "name": "max", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "outputs", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FakeQuantWithMinMaxVarsGradient",
    "summary": "Compute gradients for a FakeQuantWithMinMaxVars operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "inputs", "type": "Arg" },
      { "name": "min", "type": "TF_Float32Tensor" },
      { "name": "max", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "backprops_wrt_input", "type": "Res" },
      { "name": "backprop_wrt_min", "type": "Res" },
      { "name": "backprop_wrt_max", "type": "Res" }
    ],
    "attributes": [
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FakeQuantWithMinMaxVarsPerChannel",
    "summary": "Fake-quantize the 'inputs' tensor of type float via per-channel floats",
    "description": "Fake-quantize the `inputs` tensor of type float per-channel and one of the\nshapes: `[d]`, `[b, d]` `[b, h, w, d]` via per-channel floats `min` and `max`\nof shape `[d]` to `outputs` tensor of same shape as `inputs`.\n\nAttributes\n\n*   `[min; max]` define the clamping range for the `inputs` data.\n*   `inputs` values are quantized into the quantization range (\n`[0; 2^num_bits - 1]` when `narrow_range` is false and `[1; 2^num_bits - 1]`\nwhen it is true) and then de-quantized and output as floats in `[min; max]`\ninterval.\n*   `num_bits` is the bitwidth of the quantization; between 2 and 16, inclusive.\n\nBefore quantization, `min` and `max` values are adjusted with the following\nlogic.\nIt is suggested to have `min <= 0 <= max`. If `0` is not in the range of values,\nthe behavior can be unexpected:\n\n*   If `0 < min < max`: `min_adj = 0` and `max_adj = max - min`.\n*   If `min < max < 0`: `min_adj = min - max` and `max_adj = 0`.\n*   If `min <= 0 <= max`: `scale = (max - min) / (2^num_bits - 1) `,\n`min_adj = scale * round(min / scale)` and `max_adj = max + min_adj - min`.\n\nThis operation has a gradient and thus allows for training `min` and `max`\nvalues.",
    "inputs": [
      { "name": "inputs", "type": "TF_Float32Tensor" },
      { "name": "min", "type": "TF_Float32Tensor" },
      { "name": "max", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "outputs", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FakeQuantWithMinMaxVarsPerChannelGradient",
    "summary": "Compute gradients for a FakeQuantWithMinMaxVarsPerChannel operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "inputs", "type": "Arg" },
      { "name": "min", "type": "TF_Float32Tensor" },
      { "name": "max", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "backprops_wrt_input", "type": "Res" },
      { "name": "backprop_wrt_min", "type": "Res" },
      { "name": "backprop_wrt_max", "type": "Res" }
    ],
    "attributes": [
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FFT",
    "summary": "Fast Fourier transform.",
    "description": "Computes the 1-dimensional discrete Fourier transform over the inner-most\ndimension of `input`.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.FFT2D",
    "summary": "2D fast Fourier transform.",
    "description": "Computes the 2-dimensional discrete Fourier transform over the inner-most\n2 dimensions of `input`.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.FFT3D",
    "summary": "3D fast Fourier transform.",
    "description": "Computes the 3-dimensional discrete Fourier transform over the inner-most 3\ndimensions of `input`.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Fill",
    "summary": "Creates a tensor filled with a scalar value.",
    "description": "This operation creates a tensor of shape `dims` and fills it with `value`.\n\nFor example:\n\n```\n# Output tensor has shape [2, 3].\nfill([2, 3], 9) ==> [[9, 9, 9]\n                     [9, 9, 9]]\n```\n\n`tf.fill` differs from `tf.constant` in a few ways:\n\n*   `tf.fill` only supports scalar contents, whereas `tf.constant` supports\n    Tensor values.\n*   `tf.fill` creates an Op in the computation graph that constructs the actual\n    Tensor value at runtime. This is in contrast to `tf.constant` which embeds\n    the entire Tensor into the graph with a `Const` node.\n*   Because `tf.fill` evaluates at graph runtime, it supports dynamic shapes\n    based on other runtime Tensors, unlike `tf.constant`.",
    "inputs": [
      { "name": "dims", "type": "Arg" },
      { "name": "value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.FinalizeDataset",
    "summary": "Creates a dataset by applying `tf.data.Options` to `input_dataset`.",
    "inputs": [
      { "name": "input_dataset", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "has_captured_ref", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.FinalizeTPUEmbedding",
    "summary": "An op that finalizes the TPUEmbedding configuration.",
    "inputs": [
      { "name": "common_config", "type": "Arg" },
      { "name": "memory_config", "type": "Arg" }
    ]
  },
  {
    "name": "tf.FlatMapDataset",
    "summary": "Creates a dataset that applies `f` to the outputs of `input_dataset`.",
    "description": "Unlike MapDataset, the `f` in FlatMapDataset is expected to return a\nDataset variant, and FlatMapDataset will flatten successive results\ninto a single Dataset.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "other_arguments", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Floor",
    "summary": "Returns element-wise largest integer not greater than x.",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.FloorDiv",
    "summary": "Returns x // y element-wise.",
    "description": "*NOTE*: `FloorDiv` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.FloorMod",
    "summary": "Returns element-wise remainder of division.",
    "description": "This follows Python semantics in that the\nresult here is consistent with a flooring divide. E.g.\n`floor(x / y) * y + floormod(x, y) = x`, regardless of the signs of x and y.\n\n*NOTE*: `FloorMod` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_IntOrFpTensor" },
      { "name": "y", "type": "TF_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_IntOrFpTensor" }
    ]
  },
  {
    "name": "tf.FlushSummaryWriter",
    "summary": "Flushes the writer's unwritten events.",
    "description": "writer: A handle to the summary writer resource.",
    "inputs": [
      { "name": "writer", "type": "Arg" }
    ]
  },
  {
    "name": "tf.FusedBatchNorm",
    "summary": "Batch normalization.",
    "description": "Note that the size of 4D Tensors are defined by either \"NHWC\" or \"NCHW\".\nThe size of 1D Tensors matches the dimension C of the 4D Tensors.",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "scale", "type": "Arg" },
      { "name": "offset", "type": "Arg" },
      { "name": "mean", "type": "Arg" },
      { "name": "variance", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" },
      { "name": "batch_mean", "type": "Res" },
      { "name": "batch_variance", "type": "Res" },
      { "name": "reserve_space_1", "type": "Res" },
      { "name": "reserve_space_2", "type": "Res" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "exponential_avg_factor", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_training", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FusedBatchNormGrad",
    "summary": "Gradient for batch normalization.",
    "description": "Note that the size of 4D Tensors are defined by either \"NHWC\" or \"NCHW\".\nThe size of 1D Tensors matches the dimension C of the 4D Tensors.",
    "inputs": [
      { "name": "y_backprop", "type": "Arg" },
      { "name": "x", "type": "Arg" },
      { "name": "scale", "type": "Arg" },
      { "name": "reserve_space_1", "type": "Arg" },
      { "name": "reserve_space_2", "type": "Arg" }
    ],
    "outputs": [
      { "name": "x_backprop", "type": "Res" },
      { "name": "scale_backprop", "type": "Res" },
      { "name": "offset_backprop", "type": "Res" },
      { "name": "reserve_space_3", "type": "Res" },
      { "name": "reserve_space_4", "type": "Res" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_training", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FusedBatchNormGradV2",
    "summary": "Gradient for batch normalization.",
    "description": "Note that the size of 4D Tensors are defined by either \"NHWC\" or \"NCHW\".\nThe size of 1D Tensors matches the dimension C of the 4D Tensors.",
    "inputs": [
      { "name": "y_backprop", "type": "Arg" },
      { "name": "x", "type": "Arg" },
      { "name": "scale", "type": "Arg" },
      { "name": "reserve_space_1", "type": "Arg" },
      { "name": "reserve_space_2", "type": "Arg" }
    ],
    "outputs": [
      { "name": "x_backprop", "type": "Res" },
      { "name": "scale_backprop", "type": "Res" },
      { "name": "offset_backprop", "type": "Res" },
      { "name": "reserve_space_3", "type": "Res" },
      { "name": "reserve_space_4", "type": "Res" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_training", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FusedBatchNormGradV3",
    "summary": "Gradient for batch normalization.",
    "description": "Note that the size of 4D Tensors are defined by either \"NHWC\" or \"NCHW\".\nThe size of 1D Tensors matches the dimension C of the 4D Tensors.",
    "inputs": [
      { "name": "y_backprop", "type": "Arg" },
      { "name": "x", "type": "Arg" },
      { "name": "scale", "type": "Arg" },
      { "name": "reserve_space_1", "type": "Arg" },
      { "name": "reserve_space_2", "type": "Arg" },
      { "name": "reserve_space_3", "type": "Arg" }
    ],
    "outputs": [
      { "name": "x_backprop", "type": "Res" },
      { "name": "scale_backprop", "type": "Res" },
      { "name": "offset_backprop", "type": "Res" },
      { "name": "reserve_space_4", "type": "Res" },
      { "name": "reserve_space_5", "type": "Res" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_training", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FusedBatchNormV2",
    "summary": "Batch normalization.",
    "description": "Note that the size of 4D Tensors are defined by either \"NHWC\" or \"NCHW\".\nThe size of 1D Tensors matches the dimension C of the 4D Tensors.",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "scale", "type": "Arg" },
      { "name": "offset", "type": "Arg" },
      { "name": "mean", "type": "Arg" },
      { "name": "variance", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" },
      { "name": "batch_mean", "type": "Res" },
      { "name": "batch_variance", "type": "Res" },
      { "name": "reserve_space_1", "type": "Res" },
      { "name": "reserve_space_2", "type": "Res" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "exponential_avg_factor", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_training", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FusedBatchNormV3",
    "summary": "Batch normalization.",
    "description": "Note that the size of 4D Tensors are defined by either \"NHWC\" or \"NCHW\".\nThe size of 1D Tensors matches the dimension C of the 4D Tensors.",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "scale", "type": "Arg" },
      { "name": "offset", "type": "Arg" },
      { "name": "mean", "type": "Arg" },
      { "name": "variance", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" },
      { "name": "batch_mean", "type": "Res" },
      { "name": "batch_variance", "type": "Res" },
      { "name": "reserve_space_1", "type": "Res" },
      { "name": "reserve_space_2", "type": "Res" },
      { "name": "reserve_space_3", "type": "Res" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedOptionalAttr" },
      { "name": "exponential_avg_factor", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_training", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.FusedConv2DBiasActivation",
    "summary": "Computes a fused kernel which implements: 2-D convolution, adds side input,",
    "description": "with separate scaling on convolution and side inputs, then adds bias and\n    applies the RELU activation function to the result. Supports both float and\n    qint8 data formats. In the case of qint8, the output is clipped to [0..127].\n\n    conv_input: A tensor with format as specified by `data_format` (see below).\n    filter: A tensor with format depending on `data_format` as follows:\n        \"NHWC\", \"NCHW\":\n             `float [ filter_height, filter_width, in_channels, out_channels ]`\n        \"NCHW_VECT_C\":\n             `qint8 [ out_channels, in_channels, filter_height, filter_width ]`\n    bias: 1-D float tensor with size matching the `out_channels` dimension of\n        `filter`.\n        Note: this tensor is still float, even if other inputs are qint8.\n    side_input: A tensor with format as specified by `data_format` (see below).\n        This tensor will be ignored and can be [] if side_input_scale == 0.\n        Otherwise, the size of each dimension must match the `output` tensor.\n    conv_input_scale: scalar float value to be multiplied by `conv_input`.\n        (conceptually.. in reality it is applied after convolution).\n        For the CPU version, this can also be a 1-D Tensor of per output-channel\n        scales.\n    side_input_scale: scalar float value to be multiplied by `side_input`.\n    output: A tensor with format as specified by `data_format` (see below).\n        The dimension sizes are determined automatically based on other inputs\n        and attributes.\n    T: The element data type of `conv_input`, `side_input` and `output` tensors.\n        Note: must match with the `data_format`.\n    Tbias: The element data type of `bias`.\n    strides: 1-D tensor of length 4.  The stride of the sliding window for each\n        dimension of `input`. The dimension order is determined by the value of\n        `data_format`, see below for details.\n        Note: the stride for batch and channel dimensions must be 1.\n    padding: The type of padding algorithm to use.\n    data_format: A string specifying the data format of `conv_input`,\n        `side_input` and `output` tensors with the following options:\n        \"NHWC\": `float [ batch, height, width, channels ]`\n        \"NCHW\": `float [ batch, channels, height, width ]`\n        \"NCHW_VECT_C\":\n            `qint8 [ batch, channels / 4, height, width, channels % 4 ]`\n        Note: for \"NCHW_VECT_C\", `channels` must be a multiple of 4.\n    filter_format: A string specifying the data format of `filter`,\n        \"HWIO\": `float [ kernel_height, kernel_width, input_channels,\n                         output_channels ]`\n        \"OIHW_VECT_I\":\n            `qint8 [ output_channels, input_channels / 4,\n                     kernel_height, kernel_width, input_channels % 4 ]`\n    activation_mode: The activation applied to the output.\n        Must be \"Relu\" or \"None\".\n    dilations: 1-D tensor of length 4.  The dilation factor for each dimension\n        of `input`. If set to k > 1, there will be k-1 skipped cells between\n        each filter element on that dimension. The dimension order is determined\n        by the value of `data_format`, see above for details. Dilations in the\n        batch and depth dimensions must be 1.",
    "inputs": [
      { "name": "conv_input", "type": "TensorOf" },
      { "name": "filter", "type": "TensorOf" },
      { "name": "bias", "type": "TensorOf" },
      { "name": "side_input", "type": "TensorOf" },
      { "name": "conv_input_scale", "type": "TF_Float32Tensor" },
      { "name": "side_input_scale", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "filter_format", "type": "DefaultValuedOptionalAttr" },
      { "name": "activation_mode", "type": "DefaultValuedOptionalAttr" },
      { "name": "dilations", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Gather",
    "summary": "Gather slices from `params` according to `indices`.",
    "description": "`indices` must be an integer tensor of any dimension (usually 0-D or 1-D).\nProduces an output tensor with shape `indices.shape + params.shape[1:]` where:\n\n```python\n    # Scalar indices\n    output[:, ..., :] = params[indices, :, ... :]\n\n    # Vector indices\n    output[i, :, ..., :] = params[indices[i], :, ... :]\n\n    # Higher rank indices\n    output[i, ..., j, :, ... :] = params[indices[i, ..., j], :, ..., :]\n```\n\nIf `indices` is a permutation and `len(indices) == params.shape[0]` then\nthis operation will permute `params` accordingly.\n\n`validate_indices`: DEPRECATED. If this operation is assigned to CPU, values in\n`indices` are always validated to be within range. If assigned to GPU,\nout-of-bound indices result in safe but unspecified behavior, which may include\nraising an error.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/Gather.png\" alt>\n</div>",
    "inputs": [
      { "name": "params", "type": "TF_Tensor" },
      { "name": "indices", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "validate_indices", "type": "DefaultValuedOptionalAttr" }
    ],
    "category": "Tensor"
  },
  {
    "name": "tf.GatherNd",
    "summary": "Gather slices from `params` into a Tensor with shape specified by `indices`.",
    "description": "`indices` is a K-dimensional integer tensor, best thought of as a\n(K-1)-dimensional tensor of indices into `params`, where each element defines a\nslice of `params`:\n\n    output[\\\\(i_0, ..., i_{K-2}\\\\)] = params[indices[\\\\(i_0, ..., i_{K-2}\\\\)]]\n\nWhereas in `tf.gather` `indices` defines slices into the `axis`\ndimension of `params`, in `tf.gather_nd`, `indices` defines slices into the\nfirst `N` dimensions of `params`, where `N = indices.shape[-1]`.\n\nThe last dimension of `indices` can be at most the rank of\n`params`:\n\n    indices.shape[-1] <= params.rank\n\nThe last dimension of `indices` corresponds to elements\n(if `indices.shape[-1] == params.rank`) or slices\n(if `indices.shape[-1] < params.rank`) along dimension `indices.shape[-1]`\nof `params`.  The output tensor has shape\n\n    indices.shape[:-1] + params.shape[indices.shape[-1]:]\n\nIf `indices` contains any out-of-bound indices, depending on\n`bad_indices_policy`, the op will either return an error or ignore the\nout-of-bound indices. `bad_indices_policy` can be one of the following values:\n1. \"\" or \"DEFAULT\": raises on CPU and ignore on GPU. This is because\n   historically on CPU and GPU we handle errors in different ways, and for\n   backward compatibility we keep the default behavior.\n2. \"ERROR\": raises error; GPU does not support this value.\n3. \"IGNORE\": ignore error and set the corresponding output to 0;\n   supported on both CPU and GPU.\n\nSome examples below.\n\nSimple indexing into a matrix:\n\n```python\n    indices = [[0, 0], [1, 1]]\n    params = [['a', 'b'], ['c', 'd']]\n    output = ['a', 'd']\n```\n\nSlice indexing into a matrix:\n\n```python\n    indices = [[1], [0]]\n    params = [['a', 'b'], ['c', 'd']]\n    output = [['c', 'd'], ['a', 'b']]\n```\n\nIndexing into a 3-tensor:\n\n```python\n    indices = [[1]]\n    params = [[['a0', 'b0'], ['c0', 'd0']],\n              [['a1', 'b1'], ['c1', 'd1']]]\n    output = [[['a1', 'b1'], ['c1', 'd1']]]\n\n\n    indices = [[0, 1], [1, 0]]\n    params = [[['a0', 'b0'], ['c0', 'd0']],\n              [['a1', 'b1'], ['c1', 'd1']]]\n    output = [['c0', 'd0'], ['a1', 'b1']]\n\n\n    indices = [[0, 0, 1], [1, 0, 1]]\n    params = [[['a0', 'b0'], ['c0', 'd0']],\n              [['a1', 'b1'], ['c1', 'd1']]]\n    output = ['b0', 'b1']\n```\n\nBatched indexing into a matrix:\n\n```python\n    indices = [[[0, 0]], [[0, 1]]]\n    params = [['a', 'b'], ['c', 'd']]\n    output = [['a'], ['b']]\n```\n\nBatched slice indexing into a matrix:\n\n```python\n    indices = [[[1]], [[0]]]\n    params = [['a', 'b'], ['c', 'd']]\n    output = [[['c', 'd']], [['a', 'b']]]\n```\n\nBatched indexing into a 3-tensor:\n\n```python\n    indices = [[[1]], [[0]]]\n    params = [[['a0', 'b0'], ['c0', 'd0']],\n              [['a1', 'b1'], ['c1', 'd1']]]\n    output = [[[['a1', 'b1'], ['c1', 'd1']]],\n              [[['a0', 'b0'], ['c0', 'd0']]]]\n\n    indices = [[[0, 1], [1, 0]], [[0, 0], [1, 1]]]\n    params = [[['a0', 'b0'], ['c0', 'd0']],\n              [['a1', 'b1'], ['c1', 'd1']]]\n    output = [[['c0', 'd0'], ['a1', 'b1']],\n              [['a0', 'b0'], ['c1', 'd1']]]\n\n\n    indices = [[[0, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 1, 0]]]\n    params = [[['a0', 'b0'], ['c0', 'd0']],\n              [['a1', 'b1'], ['c1', 'd1']]]\n    output = [['b0', 'b1'], ['d0', 'c1']]\n```\n\nSee also `tf.gather` and `tf.batch_gather`.",
    "inputs": [
      { "name": "params", "type": "Arg" },
      { "name": "indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.GatherV2",
    "summary": "Gather slices from `params` axis `axis` according to `indices`.",
    "description": "`indices` must be an integer tensor of any dimension (usually 0-D or 1-D).\nProduces an output tensor with shape `params.shape[:axis] +\nindices.shape[batch_dims:] + params.shape[axis + 1:]` where:\n\n```python\n    # Scalar indices (output is rank(params) - 1).\n    output[a_0, ..., a_n, b_0, ..., b_n] =\n      params[a_0, ..., a_n, indices, b_0, ..., b_n]\n\n    # Vector indices (output is rank(params)).\n    output[a_0, ..., a_n, i, b_0, ..., b_n] =\n      params[a_0, ..., a_n, indices[i], b_0, ..., b_n]\n\n    # Higher rank indices (output is rank(params) + rank(indices) - 1).\n    output[a_0, ..., a_n, i, ..., j, b_0, ... b_n] =\n      params[a_0, ..., a_n, indices[i, ..., j], b_0, ..., b_n]\n```\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/Gather.png\" alt>\n</div>\n\nNote that on CPU, if an out of bound index is found, an error is returned.\nOn GPU, if an out of bound index is found, a 0 is stored in the\ncorresponding output value.\n\nNote that on TPU, if any dimension of `params` is of size 0 then the output will\nbe the expected shape filled with zeros. On CPU and GPU an error will be\nreturned.\n\nSee also `tf.batch_gather` and `tf.gather_nd`.",
    "inputs": [
      { "name": "params", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "axis", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "batch_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.GeneratorDataset",
    "summary": "Creates a dataset that invokes a function to generate elements.",
    "inputs": [
      { "name": "init_func_other_args", "type": "Variadic" },
      { "name": "next_func_other_args", "type": "Variadic" },
      { "name": "finalize_func_other_args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "init_func", "type": "SymbolRefAttr" },
      { "name": "next_func", "type": "SymbolRefAttr" },
      { "name": "finalize_func", "type": "SymbolRefAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.GeneratorDatasetRegion",
    "summary": "Regional version of GeneratorDataset",
    "description": "Creates a dataset that invokes its 'next' region to generate elements. Conceptually,\nwithin MLIR, we treat this op as if it fills a buffer with all the results right away,\nand those results are then passed (through the variant tensor result) to\nMakeIterator / IteratorGetNext. Note that the actual TF implementation differs: It\ngenerates the next element just in time, during IteratorGetNext.\n\ninit_extra_args: Additional arguments to pass to 'init'.\nnext_extra_args: Additional arguments to pass to 'next'. (Passed after the\n                 normal arguments which are from the return values of 'init'.)\nfinalize_extra_args: Additional arguments to pass to 'finalize'. (Passed after\n                 the normal arguments which are from the return values of 'init'.)",
    "inputs": [
      { "name": "init_func_other_args", "type": "Variadic" },
      { "name": "next_func_other_args", "type": "Variadic" },
      { "name": "finalize_func_other_args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.GetMinibatchesInCsrWithPhysicalReplica",
    "inputs": [
      { "name": "program_key", "type": "TF_StrTensor" },
      { "name": "row_ids", "type": "TF_Int32Tensor" },
      { "name": "col_ids", "type": "TF_Int32Tensor" },
      { "name": "gains", "type": "TF_Float32Tensor" },
      { "name": "splits", "type": "TF_Int64Tensor" },
      { "name": "id_counts", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "row_pointers_unpadded_size", "type": "TF_Int32Tensor" },
      { "name": "ids_unpadded_size", "type": "TF_Int32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "sample_count", "type": "ConfinedAttr" },
      { "name": "num_replica", "type": "ConfinedAttr" },
      { "name": "max_minibatches_per_sc", "type": "ConfinedAttr" },
      { "name": "max_ids_per_chip_per_sample", "type": "ConfinedAttr" },
      { "name": "table_vocab_size", "type": "ConfinedAttr" },
      { "name": "feature_width", "type": "ConfinedAttr" },
      { "name": "num_sc_per_chip", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" },
      { "name": "mini_batch_in_csr", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.GetMinibatchSplitsWithPhysicalReplica",
    "inputs": [
      { "name": "program_key", "type": "TF_StrTensor" },
      { "name": "row_ids", "type": "TF_Int32Tensor" },
      { "name": "col_ids", "type": "TF_Int32Tensor" },
      { "name": "gains", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "sorted_row_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_col_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "splits", "type": "TF_Int64Tensor" },
      { "name": "id_counts", "type": "TF_Int32Tensor" },
      { "name": "max_ids", "type": "TF_Int32Tensor" },
      { "name": "max_uniques", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "sample_count", "type": "ConfinedAttr" },
      { "name": "num_replica", "type": "ConfinedAttr" },
      { "name": "table_vocab_size", "type": "ConfinedAttr" },
      { "name": "feature_width", "type": "ConfinedAttr" },
      { "name": "num_sc_per_chip", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" },
      { "name": "mini_batch_splits", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.GetStatsFromListOfSparseCoreCooTensors",
    "summary": "An op which computes the max_ids/uniques for a given table.",
    "inputs": [
      { "name": "row_ids_list", "type": "Variadic" },
      { "name": "col_ids_list", "type": "Variadic" },
      { "name": "gains_list", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "max_ids_per_sparse_core", "type": "TF_Int32Tensor" },
      { "name": "max_unique_ids_per_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "sample_count_list", "type": "I64ArrayAttr" },
      { "name": "col_offset_list", "type": "I64ArrayAttr" },
      { "name": "num_replica", "type": "ConfinedAttr" },
      { "name": "table_vocab_size", "type": "ConfinedAttr" },
      { "name": "feature_width", "type": "ConfinedAttr" },
      { "name": "num_sc_per_chip", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.GlobalIterId",
    "summary": "Op that gets the global step id.",
    "description": "This op gets the step id for each loop iteration.",
    "outputs": [
      { "name": "iter_id", "type": "TF_Int64Tensor" }
    ]
  },
  {
    "name": "tf.Greater",
    "summary": "Returns the truth value of (x > y) element-wise.",
    "description": "*NOTE*: `Greater` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\nExample:\n\n```python\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 2, 5])\ntf.math.greater(x, y) ==> [False, True, True]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.greater(x, y) ==> [False, False, True]\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntOrFpTensor" },
      { "name": "y", "type": "TF_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.GreaterEqual",
    "summary": "Returns the truth value of (x >= y) element-wise.",
    "description": "*NOTE*: `GreaterEqual` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\nExample:\n\n```python\nx = tf.constant([5, 4, 6, 7])\ny = tf.constant([5, 2, 5, 10])\ntf.math.greater_equal(x, y) ==> [True, True, True, False]\n\nx = tf.constant([5, 4, 6, 7])\ny = tf.constant([5])\ntf.math.greater_equal(x, y) ==> [True, False, True, True]\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntOrFpTensor" },
      { "name": "y", "type": "TF_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.HashTable",
    "summary": "Creates a non-initialized hash table.",
    "description": "This op creates a hash table, specifying the type of its keys and values.\nBefore using the table you will have to initialize it.  After initialization the\ntable will be immutable.",
    "outputs": [
      { "name": "table_handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_node_name_sharing", "type": "DefaultValuedOptionalAttr" },
      { "name": "key_dtype", "type": "TypeAttr" },
      { "name": "value_dtype", "type": "TypeAttr" }
    ]
  },
  {
    "name": "tf.HashTableV2",
    "summary": "Creates a non-initialized hash table.",
    "description": "This op creates a hash table, specifying the type of its keys and values.\nBefore using the table you will have to initialize it.  After initialization the\ntable will be immutable.",
    "outputs": [
      { "name": "table_handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_node_name_sharing", "type": "DefaultValuedOptionalAttr" },
      { "name": "key_dtype", "type": "TypeAttr" },
      { "name": "value_dtype", "type": "TypeAttr" }
    ]
  },
  {
    "name": "tf.HSVToRGB",
    "summary": "Convert one or more images from HSV to RGB.",
    "description": "Outputs a tensor of the same shape as the `images` tensor, containing the RGB\nvalue of the pixels. The output is only well defined if the value in `images`\nare in `[0,1]`.\n\nSee `rgb_to_hsv` for a description of the HSV encoding.",
    "inputs": [
      { "name": "images", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Identity",
    "summary": "Return a tensor with the same shape and contents as the input tensor or value.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.IdentityN",
    "summary": "Returns a list of tensors with the same shapes and contents as the input",
    "description": "tensors.\n\nThis op can be used to override the gradient for complicated functions. For\nexample, suppose y = f(x) and we wish to apply a custom function g for backprop\nsuch that dx = g(dy). In Python,\n\n```python\nwith tf.get_default_graph().gradient_override_map(\n    {'IdentityN': 'OverrideGradientWithG'}):\n  y, _ = identity_n([f(x), x])\n\n@tf.RegisterGradient('OverrideGradientWithG')\ndef ApplyG(op, dy, _):\n  return [None, g(dy)]  # Do not backprop to f(x).\n```",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.If",
    "summary": "output = cond ? then_branch(input) : else_branch(input)",
    "description": "output = cond ? then_branch(input) : else_branch(input)\n\ncond: A Tensor. If the tensor is a scalar of non-boolean type, the\n    scalar is converted to a boolean according to the\n    following rule: if the scalar is a numerical value, non-zero means\n    True and zero means False; if the scalar is a string, non-empty\n    means True and empty means False. If the tensor is not a scalar,\n    being empty means False and being non-empty means True.\ninput: A list of input tensors.\nthen_branch: A function that takes 'inputs' and returns a list of\n    tensors, whose types are the same as what else_branch returns.\nelse_branch: A function that takes 'inputs' and returns a list of\n    tensors.  whose types are the same as what then_branch returns.",
    "inputs": [
      { "name": "cond", "type": "TF_Tensor" },
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "then_branch", "type": "FlatSymbolRefAttr" },
      { "name": "else_branch", "type": "FlatSymbolRefAttr" },
      { "name": "is_stateless", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.IFFT",
    "summary": "Inverse fast Fourier transform.",
    "description": "Computes the inverse 1-dimensional discrete Fourier transform over the\ninner-most dimension of `input`.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.IFFT2D",
    "summary": "Inverse 2D fast Fourier transform.",
    "description": "Computes the inverse 2-dimensional discrete Fourier transform over the\ninner-most 2 dimensions of `input`.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.IFFT3D",
    "summary": "Inverse 3D fast Fourier transform.",
    "description": "Computes the inverse 3-dimensional discrete Fourier transform over the\ninner-most 3 dimensions of `input`.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.IfRegion",
    "summary": "output = cond ? then_branch output : else_branch output",
    "description": "\"output = cond ? then_branch output : else_branch output\"\n\ncond: A Tensor. If the tensor is a scalar of non-boolean type, the\n    scalar is converted to a boolean according to the\n    following rule: if the scalar is a numerical value, non-zero means\n    True and zero means False; if the scalar is a string, non-empty\n    means True and empty means False. If the tensor is not a scalar,\n    being empty means False and being non-empty means True.\nthen_branch: A region that computes the outputs of the op if cond = true.\n    It returns a list of tensors using tf.yield (as the terminator). The\n    types of these returned tensors is same as that of the else_branch\nelse_branch: A region that computes the outputs of the op if cond = false.\n    It returns a list of tensors using tf.yield (as the terminator). The\n    types of these returned tensors is same as that of the then_branch",
    "inputs": [
      { "name": "cond", "type": "DTensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "is_stateless", "type": "BoolAttr" },
      { "name": "_then_func_name", "type": "OptionalAttr" },
      { "name": "_else_func_name", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tf.Igamma",
    "summary": "Compute the lower regularized incomplete Gamma function `P(a, x)`.",
    "description": "The lower regularized incomplete Gamma function is defined as:\n\n\n\\\\(P(a, x) = gamma(a, x) / Gamma(a) = 1 - Q(a, x)\\\\)\n\nwhere\n\n\\\\(gamma(a, x) = \\int_{0}^{x} t^{a-1} exp(-t) dt\\\\)\n\nis the lower incomplete Gamma function.\n\nNote, above `Q(a, x)` (`Igammac`) is the upper regularized complete\nGamma function.",
    "inputs": [
      { "name": "a", "type": "TF_FloatTensor" },
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.Igammac",
    "summary": "Compute the upper regularized incomplete Gamma function `Q(a, x)`.",
    "description": "The upper regularized incomplete Gamma function is defined as:\n\n\\\\(Q(a, x) = Gamma(a, x) / Gamma(a) = 1 - P(a, x)\\\\)\n\nwhere\n\n\\\\(Gamma(a, x) = \\int_{x}^{\\infty} t^{a-1} exp(-t) dt\\\\)\n\nis the upper incomplete Gamma function.\n\nNote, above `P(a, x)` (`Igamma`) is the lower regularized complete\nGamma function.",
    "inputs": [
      { "name": "a", "type": "TF_FloatTensor" },
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.IgammaGradA",
    "summary": "Computes the gradient of `igamma(a, x)` wrt `a`.",
    "inputs": [
      { "name": "a", "type": "TF_F32OrF64Tensor" },
      { "name": "x", "type": "TF_F32OrF64Tensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.Imag",
    "summary": "Returns the imaginary part of a complex number.",
    "description": "Given a tensor `input` of complex numbers, this operation returns a tensor of\ntype `float` that is the imaginary part of each element in `input`. All\nelements in `input` must be complex numbers of the form \\\\(a + bj\\\\), where *a*\nis the real part and *b* is the imaginary part returned by this operation.\n\nFor example:\n\n```\n# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]\ntf.imag(input) ==> [4.75, 5.75]\n```",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.ImportEvent",
    "summary": "Outputs a `tf.Event` protocol buffer.",
    "description": "When CreateSummaryDbWriter is being used, this op can be useful for\nimporting data from event logs.\n\nwriter: A handle to a summary writer.\nevent: A string containing a binary-encoded tf.Event proto.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "event", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.InfeedDequeue",
    "summary": "A placeholder op for a value that will be fed into the computation.",
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "shape", "type": "TF_ShapeAttr" }
    ]
  },
  {
    "name": "tf.InfeedDequeueTuple",
    "summary": "Fetches multiple values from infeed as an XLA tuple.",
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "_XlaSharding", "type": "OptionalAttr" },
      { "name": "layouts", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tf.InfeedEnqueueTuple",
    "summary": "Feeds multiple Tensor values into the computation as an XLA tuple.",
    "inputs": [
      { "name": "inputs", "type": "Arg" }
    ],
    "attributes": [
      { "name": "dtypes", "type": "ConfinedAttr" },
      { "name": "shapes", "type": "TF_ShapeAttrArray" },
      { "name": "layouts", "type": "DefaultValuedOptionalAttr" },
      { "name": "device_ordinal", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.InitializeTable",
    "summary": "Table initializer that takes two tensors for keys and values respectively.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "keys", "type": "Arg" },
      { "name": "values", "type": "Arg" }
    ]
  },
  {
    "name": "tf.InitializeTableFromDataset",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "dataset", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.InitializeTableFromTextFile",
    "summary": "Initializes a table from a text file.",
    "description": "It inserts one key-value pair into the table for each line of the file.\nThe key and value is extracted from the whole line content, elements from the\nsplit line based on `delimiter` or the line number (starting from zero).\nWhere to extract the key and value from a line is specified by `key_index` and\n`value_index`.\n\n- A value of -1 means use the line number(starting from zero), expects `int64`.\n- A value of -2 means use the whole line content, expects `string`.\n- A value >= 0 means use the index (starting at zero) of the split line based\n  on `delimiter`.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "filename", "type": "Arg" }
    ],
    "attributes": [
      { "name": "key_index", "type": "ConfinedAttr" },
      { "name": "value_index", "type": "ConfinedAttr" },
      { "name": "vocab_size", "type": "ConfinedAttr" },
      { "name": "delimiter", "type": "DefaultValuedOptionalAttr" },
      { "name": "offset", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.InitializeTableFromTextFileV2",
    "summary": "Initializes a table from a text file.",
    "description": "It inserts one key-value pair into the table for each line of the file.\nThe key and value is extracted from the whole line content, elements from the\nsplit line based on `delimiter` or the line number (starting from zero).\nWhere to extract the key and value from a line is specified by `key_index` and\n`value_index`.\n\n- A value of -1 means use the line number(starting from zero), expects `int64`.\n- A value of -2 means use the whole line content, expects `string`.\n- A value >= 0 means use the index (starting at zero) of the split line based\n  on `delimiter`.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "filename", "type": "Arg" }
    ],
    "attributes": [
      { "name": "key_index", "type": "ConfinedAttr" },
      { "name": "value_index", "type": "ConfinedAttr" },
      { "name": "vocab_size", "type": "ConfinedAttr" },
      { "name": "delimiter", "type": "DefaultValuedOptionalAttr" },
      { "name": "offset", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.InitializeTableV2",
    "summary": "Table initializer that takes two tensors for keys and values respectively.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "keys", "type": "Arg" },
      { "name": "values", "type": "Arg" }
    ]
  },
  {
    "name": "tf.InplaceAdd",
    "summary": "Adds v into specified rows of x.",
    "description": "Computes y = x; y[i, :] += v; return y.",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "i", "type": "Arg" },
      { "name": "v", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" }
    ]
  },
  {
    "name": "tf.InplaceUpdate",
    "summary": "Updates specified rows 'i' with values 'v'.",
    "description": "Computes `x[i, :] = v; return x`.\n\nOriginally this function is mutative however for compilation we make this\noperation create / operate on a copy of `x`.",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "i", "type": "Arg" },
      { "name": "v", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" }
    ]
  },
  {
    "name": "tf.InTopKV2",
    "summary": "Says whether the targets are in the top `K` predictions.",
    "description": "This outputs a `batch_size` bool array, an entry `out[i]` is `true` if the\nprediction for the target class is among the top `k` predictions among\nall predictions for example `i`. Note that the behavior of `InTopK` differs\nfrom the `TopK` op in its handling of ties; if multiple classes have the\nsame prediction value and straddle the top-`k` boundary, all of those\nclasses are considered to be in the top `k`.\n\nMore formally, let\n\n  \\\\(predictions_i\\\\) be the predictions for all classes for example `i`,\n  \\\\(targets_i\\\\) be the target class for example `i`,\n  \\\\(out_i\\\\) be the output for example `i`,\n\n$$out_i = predictions_{i, targets_i} \\in TopKIncludingTies(predictions_i)$$",
    "inputs": [
      { "name": "predictions", "type": "Arg" },
      { "name": "targets", "type": "Arg" },
      { "name": "k", "type": "Arg" }
    ],
    "outputs": [
      { "name": "precision", "type": "Res" }
    ]
  },
  {
    "name": "tf.Inv",
    "summary": "Computes the reciprocal of x element-wise.",
    "description": "I.e., \\\\(y = 1 / x\\\\).",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.Invert",
    "summary": "Invert (flip) each bit of supported types; for example, type `uint8` value 01010101 becomes 10101010.",
    "description": "Flip each bit of supported types.  For example, type `int8` (decimal 2) binary 00000010 becomes (decimal -3) binary 11111101.\nThis operation is performed on each element of the tensor argument `x`.\n\nExample:\n```python\nimport tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\n\n# flip 2 (00000010) to -3 (11111101)\ntf.assert_equal(-3, bitwise_ops.invert(2))\n\ndtype_list = [dtypes.int8, dtypes.int16, dtypes.int32, dtypes.int64,\n              dtypes.uint8, dtypes.uint16, dtypes.uint32, dtypes.uint64]\n\ninputs = [0, 5, 3, 14]\nfor dtype in dtype_list:\n  # Because of issues with negative numbers, let's test this indirectly.\n  # 1. invert(a) and a = 0\n  # 2. invert(a) or a = invert(0)\n  input_tensor = tf.constant([0, 5, 3, 14], dtype=dtype)\n  not_a_and_a, not_a_or_a, not_0 = [bitwise_ops.bitwise_and(\n                                      input_tensor, bitwise_ops.invert(input_tensor)),\n                                    bitwise_ops.bitwise_or(\n                                      input_tensor, bitwise_ops.invert(input_tensor)),\n                                    bitwise_ops.invert(\n                                      tf.constant(0, dtype=dtype))]\n\n  expected = tf.constant([0, 0, 0, 0], dtype=tf.float32)\n  tf.assert_equal(tf.cast(not_a_and_a, tf.float32), expected)\n\n  expected = tf.cast([not_0] * 4, tf.float32)\n  tf.assert_equal(tf.cast(not_a_or_a, tf.float32), expected)\n\n  # For unsigned dtypes let's also check the result directly.\n  if dtype.is_unsigned:\n    inverted = bitwise_ops.invert(input_tensor)\n    expected = tf.constant([dtype.max - x for x in inputs], dtype=tf.float32)\n    tf.assert_equal(tf.cast(inverted, tf.float32), tf.cast(expected, tf.float32))\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_IntTensor" }
    ]
  },
  {
    "name": "tf.InvertPermutation",
    "summary": "Computes the inverse permutation of a tensor.",
    "description": "This operation computes the inverse of an index permutation. It takes a 1-D\ninteger tensor `x`, which represents the indices of a zero-based array, and\nswaps each value with its index position. In other words, for an output tensor\n`y` and an input tensor `x`, this operation computes the following:\n\n`y[x[i]] = i for i in [0, 1, ..., len(x) - 1]`\n\nThe values must include 0. There can be no duplicate values or negative values.\n\nFor example:\n\n```\n# tensor `x` is [3, 4, 0, 2, 1]\ninvert_permutation(x) ==> [2, 4, 3, 0, 1]\n```",
    "inputs": [
      { "name": "x", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" }
    ]
  },
  {
    "name": "tf.IRFFT",
    "summary": "Inverse real-valued fast Fourier transform.",
    "description": "Computes the inverse 1-dimensional discrete Fourier transform of a real-valued\nsignal over the inner-most dimension of `input`.\n\nThe inner-most dimension of `input` is assumed to be the result of `RFFT`: the\n`fft_length / 2 + 1` unique components of the DFT of a real-valued signal. If\n`fft_length` is not provided, it is computed from the size of the inner-most\ndimension of `input` (`fft_length = 2 * (inner - 1)`). If the FFT length used to\ncompute `input` is odd, it should be provided since it cannot be inferred\nproperly.\n\nAlong the axis `IRFFT` is computed on, if `fft_length / 2 + 1` is smaller\nthan the corresponding dimension of `input`, the dimension is cropped. If it is\nlarger, the dimension is padded with zeros.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "fft_length", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.IRFFT2D",
    "summary": "Inverse 2D real-valued fast Fourier transform.",
    "description": "Computes the inverse 2-dimensional discrete Fourier transform of a real-valued\nsignal over the inner-most 2 dimensions of `input`.\n\nThe inner-most 2 dimensions of `input` are assumed to be the result of `RFFT2D`:\nThe inner-most dimension contains the `fft_length / 2 + 1` unique components of\nthe DFT of a real-valued signal. If `fft_length` is not provided, it is computed\nfrom the size of the inner-most 2 dimensions of `input`. If the FFT length used\nto compute `input` is odd, it should be provided since it cannot be inferred\nproperly.\n\nAlong each axis `IRFFT2D` is computed on, if `fft_length` (or\n`fft_length / 2 + 1` for the inner-most dimension) is smaller than the\ncorresponding dimension of `input`, the dimension is cropped. If it is larger,\nthe dimension is padded with zeros.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "fft_length", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.IRFFT3D",
    "summary": "Inverse 3D real-valued fast Fourier transform.",
    "description": "Computes the inverse 3-dimensional discrete Fourier transform of a real-valued\nsignal over the inner-most 3 dimensions of `input`.\n\nThe inner-most 3 dimensions of `input` are assumed to be the result of `RFFT3D`:\nThe inner-most dimension contains the `fft_length / 2 + 1` unique components of\nthe DFT of a real-valued signal. If `fft_length` is not provided, it is computed\nfrom the size of the inner-most 3 dimensions of `input`. If the FFT length used\nto compute `input` is odd, it should be provided since it cannot be inferred\nproperly.\n\nAlong each axis `IRFFT3D` is computed on, if `fft_length` (or\n`fft_length / 2 + 1` for the inner-most dimension) is smaller than the\ncorresponding dimension of `input`, the dimension is cropped. If it is larger,\nthe dimension is padded with zeros.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "fft_length", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.IsFinite",
    "summary": "Returns which elements of x are finite.",
    "description": "@compatibility(numpy)\nEquivalent to np.isfinite\n@end_compatibility\n\nExample:\n\n```python\nx = tf.constant([5.0, 4.8, 6.8, np.inf, np.nan])\ntf.math.is_finite(x) ==> [True, True, True, False, False]\n```",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.IsInf",
    "summary": "Returns which elements of x are Inf.",
    "description": "@compatibility(numpy)\nEquivalent to np.isinf\n@end_compatibility\n\nExample:\n\n```python\nx = tf.constant([5.0, np.inf, 6.8, np.inf])\ntf.math.is_inf(x) ==> [False, True, False, True]\n```",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.IsNan",
    "summary": "Returns which elements of x are NaN.",
    "description": "@compatibility(numpy)\nEquivalent to np.isnan\n@end_compatibility\n\nExample:\n\n```python\nx = tf.constant([5.0, np.nan, 6.8, np.nan, np.inf])\ntf.math.is_nan(x) ==> [False, True, False, True, False]\n```",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.Iterator",
    "summary": "A container for an iterator resource.",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "shared_name", "type": "StrAttr" },
      { "name": "container", "type": "StrAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.IteratorFromStringHandle",
    "summary": "Converts the given string representing a handle to an iterator to a resource.",
    "inputs": [
      { "name": "string_handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "resource_handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "output_types", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_shapes", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.IteratorFromStringHandleV2",
    "inputs": [
      { "name": "string_handle", "type": "TF_StrTensor" }
    ],
    "outputs": [
      { "name": "resource_handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "output_types", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_shapes", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.IteratorGetNext",
    "summary": "Gets the next output from the given iterator .",
    "inputs": [
      { "name": "iterator", "type": "Arg" }
    ],
    "outputs": [
      { "name": "components", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.IteratorGetNextAsOptional",
    "summary": "Gets the next output from the given iterator as an Optional variant.",
    "inputs": [
      { "name": "iterator", "type": "Arg" }
    ],
    "outputs": [
      { "name": "optional", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.IteratorGetNextSync",
    "summary": "Gets the next output from the given iterator.",
    "description": "This operation is a synchronous version IteratorGetNext. It should only be used\nin situations where the iterator does not block the calling thread, or where\nthe calling thread is not a member of the thread pool used to execute parallel\noperations (e.g. in eager mode).",
    "inputs": [
      { "name": "iterator", "type": "Arg" }
    ],
    "outputs": [
      { "name": "components", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.IteratorToStringHandle",
    "summary": "Converts the given `resource_handle` representing an iterator to a string.",
    "inputs": [
      { "name": "resource_handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "string_handle", "type": "Res" }
    ]
  },
  {
    "name": "tf.IteratorV2",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "shared_name", "type": "StrAttr" },
      { "name": "container", "type": "StrAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.KthOrderStatistic",
    "summary": "Computes the Kth order statistic of a data set. The current",
    "description": "implementation uses a binary search requiring exactly 32 passes over\nthe input data. The running time is linear with respect to input\nsize. The median-of-medians algorithm is probably faster, but is\ndifficult to implement efficiently in XLA. The implementation imposes\na total ordering on floats. The ordering is consistent with the usual\npartial order.  Positive NaNs are greater than positive\ninfinity. Negative NaNs are less than negative infinity. NaNs with\ndistinct payloads are treated as distinct. Subnormal numbers are\npreserved (not flushed to zero). Positive infinity is greater than all\nnumbers. Negative infinity is less than all numbers. Positive is\ngreater than negative zero. There are less than k values greater than\nthe kth order statistic. There are at least k values greater than or\nequal to the Kth order statistic. The semantics are not the same as\ntop_k_unique.",
    "inputs": [
      { "name": "input", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "k", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.L2Loss",
    "summary": "L2 Loss.",
    "description": "Computes half the L2 norm of a tensor without the `sqrt`:\n\n    output = sum(t ** 2) / 2",
    "inputs": [
      { "name": "x", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.LeakyRelu",
    "summary": "Computes rectified linear: `max(features, features * alpha)`.",
    "inputs": [
      { "name": "features", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LeakyReluGrad",
    "summary": "Computes rectified linear gradients for a LeakyRelu operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "features", "type": "Arg" }
    ],
    "outputs": [
      { "name": "backprops", "type": "Res" }
    ],
    "attributes": [
      { "name": "alpha", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LeftShift",
    "summary": "Elementwise computes the bitwise left-shift of `x` and `y`.",
    "description": "If `y` is negative, or greater than or equal to the width of `x` in bits the\nresult is implementation defined.\n\nExample:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\nimport numpy as np\ndtype_list = [tf.int8, tf.int16, tf.int32, tf.int64]\n\nfor dtype in dtype_list:\n  lhs = tf.constant([-1, -5, -3, -14], dtype=dtype)\n  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)\n\n  left_shift_result = bitwise_ops.left_shift(lhs, rhs)\n\n  print(left_shift_result)\n\n# This will print:\n# tf.Tensor([ -32   -5 -128    0], shape=(4,), dtype=int8)\n# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int16)\n# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int32)\n# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int64)\n\nlhs = np.array([-2, 64, 101, 32], dtype=np.int8)\nrhs = np.array([-1, -5, -3, -14], dtype=np.int8)\nbitwise_ops.left_shift(lhs, rhs)\n# <tf.Tensor: shape=(4,), dtype=int8, numpy=array([ -2,  64, 101,  32], dtype=int8)>\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntTensor" },
      { "name": "y", "type": "TF_IntTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_IntTensor" }
    ]
  },
  {
    "name": "tf.LegacyCall",
    "summary": "returns `f(inputs)`, where `f` is a function.",
    "description": "The LegacyCall operation represents a direct call to a function that is\n    within the same symbol scope as the call and is mapped to a GraphDef node\n    with the function name as the op name. Unlike a PartitionedCall which\n    represents asynchronously executing a function across multiple devices, a\n    LegacyCall ignores specification for ops in the attached function and\n    instead executes it on the device assigned to this op.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "f", "type": "FlatSymbolRefAttr" },
      { "name": "_disable_call_shape_inference", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Less",
    "summary": "Returns the truth value of (x < y) element-wise.",
    "description": "*NOTE*: `Less` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\nExample:\n\n```python\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.less(x, y) ==> [False, True, False]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 6, 7])\ntf.math.less(x, y) ==> [False, True, True]\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntOrFpTensor" },
      { "name": "y", "type": "TF_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.LessEqual",
    "summary": "Returns the truth value of (x <= y) element-wise.",
    "description": "*NOTE*: `LessEqual` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\nExample:\n\n```python\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.less_equal(x, y) ==> [True, True, False]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 6, 6])\ntf.math.less_equal(x, y) ==> [True, True, True]\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntOrFpTensor" },
      { "name": "y", "type": "TF_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.Lgamma",
    "summary": "Computes the log of the absolute value of `Gamma(x)` element-wise.",
    "description": "For positive numbers, this function computes log((input - 1)!) for every element in the tensor.\n  `lgamma(5) = log((5-1)!) = log(4!) = log(24) = 3.1780539`\n\nExample:\n\n```python\nx = tf.constant([0, 0.5, 1, 4.5, -4, -5.6])\ntf.math.lgamma(x) ==> [inf, 0.5723649, 0., 2.4537368, inf, -4.6477685]\n```",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.LinSpace",
    "summary": "Generates values in an interval.",
    "description": "A sequence of `num` evenly-spaced values are generated beginning at `start`.\nIf `num > 1`, the values in the sequence increase by\n`(stop - start) / (num - 1)`, so that the last one is exactly `stop`.\n\nFor example:\n\n```\ntf.linspace(10.0, 12.0, 3, name=\"linspace\") => [ 10.0  11.0  12.0]\n```",
    "inputs": [
      { "name": "start", "type": "Arg" },
      { "name": "stop", "type": "Arg" },
      { "name": "num", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.ListDiff",
    "summary": "Computes the difference between two lists of numbers or strings.",
    "description": "Given a list `x` and a list `y`, this operation returns a list `out` that\nrepresents all values that are in `x` but not in `y`. The returned list `out`\nis sorted in the same order that the numbers appear in `x` (duplicates are\npreserved). This operation also returns a list `idx` that represents the\nposition of each `out` element in `x`. In other words:\n\n`out[i] = x[idx[i]] for i in [0, 1, ..., len(out) - 1]`\n\nFor example, given this input:\n\n```\nx = [1, 2, 3, 4, 5, 6]\ny = [1, 3, 5]\n```\n\nThis operation would return:\n\n```\nout ==> [2, 4, 6]\nidx ==> [1, 3, 5]\n```",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "y", "type": "Arg" }
    ],
    "outputs": [
      { "name": "out", "type": "Res" },
      { "name": "idx", "type": "Res" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingAdadeltaParameters",
    "summary": "Load Adadelta embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "accumulators", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingAdadeltaParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "accumulators", "type": "TF_Float32Tensor" },
      { "name": "updates", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingAdagradParameters",
    "summary": "Load Adagrad embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "accumulators", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingAdagradParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "accumulators", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingADAMParameters",
    "summary": "Load ADAM embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "momenta", "type": "Arg" },
      { "name": "velocities", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingADAMParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "velocities", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingCenteredRMSPropParameters",
    "summary": "Load centered RMSProp embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "ms", "type": "Arg" },
      { "name": "mom", "type": "Arg" },
      { "name": "mg", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingFTRLParameters",
    "summary": "Load FTRL embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "accumulators", "type": "Arg" },
      { "name": "linears", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingFTRLParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "accumulators", "type": "TF_Float32Tensor" },
      { "name": "linears", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingMDLAdagradLightParameters",
    "summary": "Load MDL Adagrad Light embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "accumulators", "type": "Arg" },
      { "name": "weights", "type": "Arg" },
      { "name": "benefits", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingMomentumParameters",
    "summary": "Load Momentum embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "momenta", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingMomentumParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingProximalAdagradParameters",
    "summary": "Load proximal Adagrad embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "accumulators", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingProximalAdagradParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "accumulators", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingProximalYogiParameters",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "v", "type": "TF_Float32Tensor" },
      { "name": "m", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingProximalYogiParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "v", "type": "TF_Float32Tensor" },
      { "name": "m", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingRMSPropParameters",
    "summary": "Load RMSProp embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" },
      { "name": "ms", "type": "Arg" },
      { "name": "mom", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingRMSPropParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "ms", "type": "TF_Float32Tensor" },
      { "name": "mom", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingStochasticGradientDescentParameters",
    "summary": "Load SGD embedding parameters.",
    "description": "An op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.",
    "inputs": [
      { "name": "parameters", "type": "Arg" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LoadTPUEmbeddingStochasticGradientDescentParametersGradAccumDebug",
    "inputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Log",
    "summary": "Computes natural logarithm of x element-wise.",
    "description": "I.e., \\\\(y = \\log_e x\\\\).\n\nExample:\n\n```python\nx = tf.constant([0, 0.5, 1, 5])\ntf.math.log(x) ==> [-inf, -0.6931472,  0. ,  1.609438]\n```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Log1p",
    "summary": "Computes natural logarithm of (1 + x) element-wise.",
    "description": "I.e., \\\\(y = \\log_e (1 + x)\\\\).\n\nExample:\n\n```python\nx = tf.constant([0, 0.5, 1, 5])\ntf.math.log1p(x) ==> [0., 0.4054651, 0.6931472, 1.7917595]\n```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.LogicalAnd",
    "summary": "Returns the truth value of x AND y element-wise.",
    "description": "*NOTE*: `LogicalAnd` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_BoolTensor" },
      { "name": "y", "type": "TF_BoolTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.LogicalNot",
    "summary": "Returns the truth value of `NOT x` element-wise.",
    "inputs": [
      { "name": "x", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" }
    ]
  },
  {
    "name": "tf.LogicalOr",
    "summary": "Returns the truth value of x OR y element-wise.",
    "description": "*NOTE*: `LogicalOr` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_BoolTensor" },
      { "name": "y", "type": "TF_BoolTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.LogSoftmax",
    "summary": "Computes log softmax activations.",
    "description": "For each batch `i` and class `j` we have\n\n    logsoftmax[i, j] = logits[i, j] - log(sum(exp(logits[i])))",
    "inputs": [
      { "name": "logits", "type": "Arg" }
    ],
    "outputs": [
      { "name": "logsoftmax", "type": "Res" }
    ]
  },
  {
    "name": "tf.LookupTableExportV2",
    "summary": "Outputs all keys and values in the table.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "keys", "type": "Res" },
      { "name": "values", "type": "Res" }
    ]
  },
  {
    "name": "tf.LookupTableFind",
    "summary": "Looks up keys in a table, outputs the corresponding values.",
    "description": "The tensor `keys` must of the same type as the keys of the table.\nThe output `values` is of the type of the table values.\n\nThe scalar `default_value` is the value output for keys not present in the\ntable. It must also be of the same type as the table values.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "keys", "type": "Arg" },
      { "name": "default_value", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "values", "type": "Res" }
    ]
  },
  {
    "name": "tf.LookupTableFindV2",
    "summary": "Looks up keys in a table, outputs the corresponding values.",
    "description": "The tensor `keys` must of the same type as the keys of the table.\nThe output `values` is of the type of the table values.\n\nThe scalar `default_value` is the value output for keys not present in the\ntable. It must also be of the same type as the table values.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "keys", "type": "Arg" },
      { "name": "default_value", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "values", "type": "Res" }
    ]
  },
  {
    "name": "tf.LookupTableImportV2",
    "summary": "Replaces the contents of the table with the specified keys and values.",
    "description": "The tensor `keys` must be of the same type as the keys of the table.\nThe tensor `values` must be of the type of the table values.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "keys", "type": "Arg" },
      { "name": "values", "type": "Arg" }
    ]
  },
  {
    "name": "tf.LookupTableInsertV2",
    "summary": "Updates the table to associates keys with values.",
    "description": "The tensor `keys` must be of the same type as the keys of the table.\nThe tensor `values` must be of the type of the table values.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "keys", "type": "Arg" },
      { "name": "values", "type": "Arg" }
    ]
  },
  {
    "name": "tf.LookupTableRemoveV2",
    "summary": "Removes keys and its associated values from a table.",
    "description": "The tensor `keys` must of the same type as the keys of the table. Keys not\nalready in the table are silently ignored.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" },
      { "name": "keys", "type": "Arg" }
    ]
  },
  {
    "name": "tf.LookupTableSize",
    "summary": "Computes the number of elements in the given table.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "size", "type": "Res" }
    ]
  },
  {
    "name": "tf.LookupTableSizeV2",
    "summary": "Computes the number of elements in the given table.",
    "inputs": [
      { "name": "table_handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "size", "type": "Res" }
    ]
  },
  {
    "name": "tf.LowerBound",
    "summary": "Applies lower_bound(sorted_search_values, values) along each row.",
    "description": "Each set of rows with the same index in (sorted_inputs, values) is treated\nindependently.  The resulting row is the equivalent of calling\n`np.searchsorted(sorted_inputs, values, side='left')`.\n\nThe result is not a global index to the entire\n`Tensor`, but rather just the index in the last dimension.\n\nA 2-D example:\n  sorted_sequence = [[0, 3, 9, 9, 10],\n                     [1, 2, 3, 4, 5]]\n  values = [[2, 4, 9],\n            [0, 2, 6]]\n\n  result = LowerBound(sorted_sequence, values)\n\n  result == [[1, 2, 2],\n             [0, 1, 5]]",
    "inputs": [
      { "name": "sorted_inputs", "type": "Arg" },
      { "name": "values", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.LRN",
    "summary": "Local Response Normalization.",
    "description": "The 4-D `input` tensor is treated as a 3-D array of 1-D vectors (along the last\ndimension), and each vector is normalized independently.  Within a given vector,\neach component is divided by the weighted, squared sum of inputs within\n`depth_radius`.  In detail,\n\n    sqr_sum[a, b, c, d] =\n        sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)\n    output = input / (bias + alpha * sqr_sum) ** beta\n\nFor details, see [Krizhevsky et al., ImageNet classification with deep\nconvolutional neural networks (NIPS 2012)](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks).",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "depth_radius", "type": "DefaultValuedOptionalAttr" },
      { "name": "bias", "type": "DefaultValuedOptionalAttr" },
      { "name": "alpha", "type": "DefaultValuedOptionalAttr" },
      { "name": "beta", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.LRNGrad",
    "summary": "Gradients for Local Response Normalization.",
    "inputs": [
      { "name": "input_grads", "type": "Arg" },
      { "name": "input_image", "type": "Arg" },
      { "name": "output_image", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "depth_radius", "type": "DefaultValuedOptionalAttr" },
      { "name": "bias", "type": "DefaultValuedOptionalAttr" },
      { "name": "alpha", "type": "DefaultValuedOptionalAttr" },
      { "name": "beta", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MakeIterator",
    "summary": "Makes a new iterator from the given `dataset` and stores it in `iterator`.",
    "description": "This operation may be executed multiple times. Each execution will reset the\niterator in `iterator` to the first element of `dataset`.",
    "inputs": [
      { "name": "dataset", "type": "TF_VariantTensor" },
      { "name": "iterator", "type": "Arg" }
    ]
  },
  {
    "name": "tf.MakeUnique",
    "summary": "Make all elements in the non-Batch dimension unique, but \\\"close\\\" to",
    "description": "their initial value. Never returns a sub-normal number. Never returns\nzero. The sign of each input element is always identical to the sign\nof the corresponding output element. Behavior for infinite elements is\nundefined. Behavior for subnormal elements is undefined.",
    "inputs": [
      { "name": "input", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Float32Tensor" }
    ]
  },
  {
    "name": "tf.MapAndBatchDataset",
    "summary": "Creates a dataset that fuses mapping with batching.",
    "description": "Creates a dataset that applies `f` to the outputs of `input_dataset` and then\nbatches `batch_size` of them.\n\nUnlike a \"MapDataset\", which applies `f` sequentially, this dataset invokes up\nto `batch_size * num_parallel_batches` copies of `f` in parallel.",
    "inputs": [
      { "name": "input_dataset", "type": "Arg" },
      { "name": "other_arguments", "type": "Arg" },
      { "name": "batch_size", "type": "Arg" },
      { "name": "num_parallel_calls", "type": "Arg" },
      { "name": "drop_remainder", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "preserve_cardinality", "type": "DefaultValuedOptionalAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MapDataset",
    "summary": "Creates a dataset that applies `f` to the outputs of `input_dataset`.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "other_arguments", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "use_inter_op_parallelism", "type": "DefaultValuedOptionalAttr" },
      { "name": "preserve_cardinality", "type": "DefaultValuedOptionalAttr" },
      { "name": "force_synchronous", "type": "DefaultValuedOptionalAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MatMul",
    "summary": "Multiply the matrix \"a\" by the matrix \"b\".",
    "description": "The inputs must be two-dimensional matrices and the inner dimension of\n\"a\" (after being transposed if transpose_a is true) must match the\nouter dimension of \"b\" (after being transposed if transposed_b is\ntrue).\n\n*Note*: The default kernel implementation for MatMul on GPUs uses\ncublas.",
    "inputs": [
      { "name": "a", "type": "TensorOf" },
      { "name": "b", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "product", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "transpose_a", "type": "DefaultValuedOptionalAttr" },
      { "name": "transpose_b", "type": "DefaultValuedOptionalAttr" },
      { "name": "grad_a", "type": "DefaultValuedOptionalAttr" },
      { "name": "grad_b", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MatrixBandPart",
    "summary": "Copy a tensor setting everything outside a central band in each innermost matrix to zero.",
    "description": "The `band` part is computed as follows:\nAssume `input` has `k` dimensions `[I, J, K, ..., M, N]`, then the output is a\ntensor with the same shape where\n\n`band[i, j, k, ..., m, n] = in_band(m, n) * input[i, j, k, ..., m, n]`.\n\nThe indicator function\n\n`in_band(m, n) = (num_lower < 0 || (m-n) <= num_lower)) &&\n                 (num_upper < 0 || (n-m) <= num_upper)`.\n\nFor example:\n\n```\n# if 'input' is [[ 0,  1,  2, 3]\n#                [-1,  0,  1, 2]\n#                [-2, -1,  0, 1]\n#                [-3, -2, -1, 0]],\n\ntf.linalg.band_part(input, 1, -1) ==> [[ 0,  1,  2, 3]\n                                       [-1,  0,  1, 2]\n                                       [ 0, -1,  0, 1]\n                                       [ 0,  0, -1, 0]],\n\ntf.linalg.band_part(input, 2, 1) ==> [[ 0,  1,  0, 0]\n                                      [-1,  0,  1, 0]\n                                      [-2, -1,  0, 1]\n                                      [ 0, -2, -1, 0]]\n```\n\nUseful special cases:\n\n```\n tf.linalg.band_part(input, 0, -1) ==> Upper triangular part.\n tf.linalg.band_part(input, -1, 0) ==> Lower triangular part.\n tf.linalg.band_part(input, 0, 0) ==> Diagonal.\n```",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "num_lower", "type": "Arg" },
      { "name": "num_upper", "type": "Arg" }
    ],
    "outputs": [
      { "name": "band", "type": "Res" }
    ]
  },
  {
    "name": "tf.MatrixDiag",
    "summary": "Returns a batched diagonal tensor with a given batched diagonal values.",
    "description": "Given a `diagonal`, this operation returns a tensor with the `diagonal` and\neverything else padded with zeros. The diagonal is computed as follows:\n\nAssume `diagonal` has `k` dimensions `[I, J, K, ..., N]`, then the output is a\ntensor of rank `k+1` with dimensions [I, J, K, ..., N, N]` where:\n\n`output[i, j, k, ..., m, n] = 1{m=n} * diagonal[i, j, k, ..., n]`.\n\nFor example:\n\n```\n# 'diagonal' is [[1, 2, 3, 4], [5, 6, 7, 8]]\n\nand diagonal.shape = (2, 4)\n\ntf.matrix_diag(diagonal) ==> [[[1, 0, 0, 0]\n                                     [0, 2, 0, 0]\n                                     [0, 0, 3, 0]\n                                     [0, 0, 0, 4]],\n                                    [[5, 0, 0, 0]\n                                     [0, 6, 0, 0]\n                                     [0, 0, 7, 0]\n                                     [0, 0, 0, 8]]]\n\nwhich has shape (2, 4, 4)\n```",
    "inputs": [
      { "name": "diagonal", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.MatrixDiagPartV3",
    "summary": "Returns the batched diagonal part of a batched tensor.",
    "description": "Returns a tensor with the `k[0]`-th to `k[1]`-th diagonals of the batched\n`input`.\n\nAssume `input` has `r` dimensions `[I, J, ..., L, M, N]`.\nLet `max_diag_len` be the maximum length among all diagonals to be extracted,\n`max_diag_len = min(M + min(k[1], 0), N + min(-k[0], 0))`\nLet `num_diags` be the number of diagonals to extract,\n`num_diags = k[1] - k[0] + 1`.\n\nIf `num_diags == 1`, the output tensor is of rank `r - 1` with shape\n`[I, J, ..., L, max_diag_len]` and values:\n\n```\ndiagonal[i, j, ..., l, n]\n  = input[i, j, ..., l, n+y, n+x] ; if 0 <= n+y < M and 0 <= n+x < N,\n    padding_value                 ; otherwise.\n```\nwhere `y = max(-k[1], 0)`, `x = max(k[1], 0)`.\n\nOtherwise, the output tensor has rank `r` with dimensions\n`[I, J, ..., L, num_diags, max_diag_len]` with values:\n\n```\ndiagonal[i, j, ..., l, m, n]\n  = input[i, j, ..., l, n+y, n+x] ; if 0 <= n+y < M and 0 <= n+x < N,\n    padding_value                 ; otherwise.\n```\nwhere `d = k[1] - m`, `y = max(-d, 0) - offset`, and `x = max(d, 0) - offset`.\n\n`offset` is zero except when the alignment of the diagonal is to the right.\n```\noffset = max_diag_len - diag_len(d) ; if (`align` in {RIGHT_LEFT, RIGHT_RIGHT}\n                                           and `d >= 0`) or\n                                         (`align` in {LEFT_RIGHT, RIGHT_RIGHT}\n                                           and `d <= 0`)\n         0                          ; otherwise\n```\nwhere `diag_len(d) = min(cols - max(d, 0), rows + min(d, 0))`.\n\nThe input must be at least a matrix.\n\nFor example:\n\n```\ninput = np.array([[[1, 2, 3, 4],  # Input shape: (2, 3, 4)\n                   [5, 6, 7, 8],\n                   [9, 8, 7, 6]],\n                  [[5, 4, 3, 2],\n                   [1, 2, 3, 4],\n                   [5, 6, 7, 8]]])\n\n# A main diagonal from each batch.\ntf.matrix_diag_part(input) ==> [[1, 6, 7],  # Output shape: (2, 3)\n                                [5, 2, 7]]\n\n# A superdiagonal from each batch.\ntf.matrix_diag_part(input, k = 1)\n  ==> [[2, 7, 6],  # Output shape: (2, 3)\n       [4, 3, 8]]\n\n# A band from each batch.\ntf.matrix_diag_part(input, k = (-1, 2))\n  ==> [[[0, 3, 8],  # Output shape: (2, 4, 3)\n        [2, 7, 6],\n        [1, 6, 7],\n        [5, 8, 0]],\n       [[0, 3, 4],\n        [4, 3, 8],\n        [5, 2, 7],\n        [1, 6, 0]]]\n\n# LEFT_RIGHT alignment.\ntf.matrix_diag_part(input, k = (-1, 2), align=\"LEFT_RIGHT\")\n  ==> [[[3, 8, 0],  # Output shape: (2, 4, 3)\n        [2, 7, 6],\n        [1, 6, 7],\n        [0, 5, 8]],\n       [[3, 4, 0],\n        [4, 3, 8],\n        [5, 2, 7],\n        [0, 1, 6]]]\n\n# max_diag_len can be shorter than the main diagonal.\ntf.matrix_diag_part(input, k = (-2, -1))\n  ==> [[[5, 8],\n        [9, 0]],\n       [[1, 6],\n        [5, 0]]]\n\n# padding_value = 9\ntf.matrix_diag_part(input, k = (1, 3), padding_value = 9)\n  ==> [[[9, 9, 4],  # Output shape: (2, 3, 3)\n        [9, 3, 8],\n        [2, 7, 6]],\n       [[9, 9, 2],\n        [9, 3, 4],\n        [4, 3, 8]]]\n\n```",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "k", "type": "Arg" },
      { "name": "padding_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "diagonal", "type": "Res" }
    ],
    "attributes": [
      { "name": "align", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MatrixDiagV2",
    "summary": "Returns a batched diagonal tensor with given batched diagonal values.",
    "description": "Returns a tensor with the contents in `diagonal` as `k[0]`-th to `k[1]`-th\ndiagonals of a matrix, with everything else padded with `padding`. `num_rows`\nand `num_cols` specify the dimension of the innermost matrix of the output. If\nboth are not specified, the op assumes the innermost matrix is square and infers\nits size from `k` and the innermost dimension of `diagonal`. If only one of them\nis specified, the op assumes the unspecified value is the smallest possible\nbased on other criteria.\n\nLet `diagonal` have `r` dimensions `[I, J, ..., L, M, N]`. The output tensor has\nrank `r+1` with shape `[I, J, ..., L, M, num_rows, num_cols]` when only one\ndiagonal is given (`k` is an integer or `k[0] == k[1]`). Otherwise, it has rank\n`r` with shape `[I, J, ..., L, num_rows, num_cols]`.\n\nThe second innermost dimension of `diagonal` has double meaning.\nWhen `k` is scalar or `k[0] == k[1]`, `M` is part of the batch size\n[I, J, ..., M], and the output tensor is:\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, n-max(d_upper, 0)] ; if n - m == d_upper\n    padding_value                             ; otherwise\n```\n\nOtherwise, `M` is treated as the number of diagonals for the matrix in the\nsame batch (`M = k[1]-k[0]+1`), and the output tensor is:\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, diag_index, index_in_diag] ; if k[0] <= d <= k[1]\n    padding_value                                     ; otherwise\n```\nwhere `d = n - m`, `diag_index = k[1] - d`, and `index_in_diag = n - max(d, 0)`.\n\nFor example:\n\n```\n# The main diagonal.\ndiagonal = np.array([[1, 2, 3, 4],            # Input shape: (2, 4)\n                     [5, 6, 7, 8]])\ntf.matrix_diag(diagonal) ==> [[[1, 0, 0, 0],  # Output shape: (2, 4, 4)\n                               [0, 2, 0, 0],\n                               [0, 0, 3, 0],\n                               [0, 0, 0, 4]],\n                              [[5, 0, 0, 0],\n                               [0, 6, 0, 0],\n                               [0, 0, 7, 0],\n                               [0, 0, 0, 8]]]\n\n# A superdiagonal (per batch).\ndiagonal = np.array([[1, 2, 3],  # Input shape: (2, 3)\n                     [4, 5, 6]])\ntf.matrix_diag(diagonal, k = 1)\n  ==> [[[0, 1, 0, 0],  # Output shape: (2, 4, 4)\n        [0, 0, 2, 0],\n        [0, 0, 0, 3],\n        [0, 0, 0, 0]],\n       [[0, 4, 0, 0],\n        [0, 0, 5, 0],\n        [0, 0, 0, 6],\n        [0, 0, 0, 0]]]\n\n# A band of diagonals.\ndiagonals = np.array([[[1, 2, 3],  # Input shape: (2, 2, 3)\n                       [4, 5, 0]],\n                      [[6, 7, 9],\n                       [9, 1, 0]]])\ntf.matrix_diag(diagonals, k = (-1, 0))\n  ==> [[[1, 0, 0],  # Output shape: (2, 3, 3)\n        [4, 2, 0],\n        [0, 5, 3]],\n       [[6, 0, 0],\n        [9, 7, 0],\n        [0, 1, 9]]]\n\n# Rectangular matrix.\ndiagonal = np.array([1, 2])  # Input shape: (2)\ntf.matrix_diag(diagonal, k = -1, num_rows = 3, num_cols = 4)\n  ==> [[0, 0, 0, 0],  # Output shape: (3, 4)\n       [1, 0, 0, 0],\n       [0, 2, 0, 0]]\n\n# Rectangular matrix with inferred num_cols and padding_value = 9.\ntf.matrix_diag(diagonal, k = -1, num_rows = 3, padding_value = 9)\n  ==> [[9, 9],  # Output shape: (3, 2)\n       [1, 9],\n       [9, 2]]\n```",
    "inputs": [
      { "name": "diagonal", "type": "Arg" },
      { "name": "k", "type": "Arg" },
      { "name": "num_rows", "type": "Arg" },
      { "name": "num_cols", "type": "Arg" },
      { "name": "padding_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.MatrixDiagV3",
    "summary": "Returns a batched diagonal tensor with given batched diagonal values.",
    "description": "Returns a tensor with the contents in `diagonal` as `k[0]`-th to `k[1]`-th\ndiagonals of a matrix, with everything else padded with `padding`. `num_rows`\nand `num_cols` specify the dimension of the innermost matrix of the output. If\nboth are not specified, the op assumes the innermost matrix is square and infers\nits size from `k` and the innermost dimension of `diagonal`. If only one of them\nis specified, the op assumes the unspecified value is the smallest possible\nbased on other criteria.\n\nLet `diagonal` have `r` dimensions `[I, J, ..., L, M, N]`. The output tensor has\nrank `r+1` with shape `[I, J, ..., L, M, num_rows, num_cols]` when only one\ndiagonal is given (`k` is an integer or `k[0] == k[1]`). Otherwise, it has rank\n`r` with shape `[I, J, ..., L, num_rows, num_cols]`.\n\nThe second innermost dimension of `diagonal` has double meaning.\nWhen `k` is scalar or `k[0] == k[1]`, `M` is part of the batch size\n[I, J, ..., M], and the output tensor is:\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, n-max(d_upper, 0)] ; if n - m == d_upper\n    padding_value                             ; otherwise\n```\n\nOtherwise, `M` is treated as the number of diagonals for the matrix in the\nsame batch (`M = k[1]-k[0]+1`), and the output tensor is:\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, diag_index, index_in_diag] ; if k[0] <= d <= k[1]\n    padding_value                                     ; otherwise\n```\nwhere `d = n - m`, `diag_index = [k] - d`, and\n`index_in_diag = n - max(d, 0) + offset`.\n\n`offset` is zero except when the alignment of the diagonal is to the right.\n```\noffset = max_diag_len - diag_len(d) ; if (`align` in {RIGHT_LEFT, RIGHT_RIGHT}\n                                           and `d >= 0`) or\n                                         (`align` in {LEFT_RIGHT, RIGHT_RIGHT}\n                                           and `d <= 0`)\n         0                          ; otherwise\n```\nwhere `diag_len(d) = min(cols - max(d, 0), rows + min(d, 0))`.\n\nFor example:\n\n```\n# The main diagonal.\ndiagonal = np.array([[1, 2, 3, 4],            # Input shape: (2, 4)\n                     [5, 6, 7, 8]])\ntf.matrix_diag(diagonal) ==> [[[1, 0, 0, 0],  # Output shape: (2, 4, 4)\n                               [0, 2, 0, 0],\n                               [0, 0, 3, 0],\n                               [0, 0, 0, 4]],\n                              [[5, 0, 0, 0],\n                               [0, 6, 0, 0],\n                               [0, 0, 7, 0],\n                               [0, 0, 0, 8]]]\n\n# A superdiagonal (per batch).\ndiagonal = np.array([[1, 2, 3],  # Input shape: (2, 3)\n                     [4, 5, 6]])\ntf.matrix_diag(diagonal, k = 1)\n  ==> [[[0, 1, 0, 0],  # Output shape: (2, 4, 4)\n        [0, 0, 2, 0],\n        [0, 0, 0, 3],\n        [0, 0, 0, 0]],\n       [[0, 4, 0, 0],\n        [0, 0, 5, 0],\n        [0, 0, 0, 6],\n        [0, 0, 0, 0]]]\n\n# A tridiagonal band (per batch).\ndiagonals = np.array([[[0, 8, 9],  # Input shape: (2, 2, 3)\n                       [1, 2, 3],\n                       [4, 5, 0]],\n                      [[0, 2, 3],\n                       [6, 7, 9],\n                       [9, 1, 0]]])\ntf.matrix_diag(diagonals, k = (-1, 1))\n  ==> [[[1, 8, 0],  # Output shape: (2, 3, 3)\n        [4, 2, 9],\n        [0, 5, 3]],\n       [[6, 2, 0],\n        [9, 7, 3],\n        [0, 1, 9]]]\n\n# LEFT_RIGHT alignment.\ndiagonals = np.array([[[8, 9, 0],  # Input shape: (2, 2, 3)\n                       [1, 2, 3],\n                       [0, 4, 5]],\n                      [[2, 3, 0],\n                       [6, 7, 9],\n                       [0, 9, 1]]])\ntf.matrix_diag(diagonals, k = (-1, 1), align=\"LEFT_RIGHT\")\n  ==> [[[1, 8, 0],  # Output shape: (2, 3, 3)\n        [4, 2, 9],\n        [0, 5, 3]],\n       [[6, 2, 0],\n        [9, 7, 3],\n        [0, 1, 9]]]\n\n# Rectangular matrix.\ndiagonal = np.array([1, 2])  # Input shape: (2)\ntf.matrix_diag(diagonal, k = -1, num_rows = 3, num_cols = 4)\n  ==> [[0, 0, 0, 0],  # Output shape: (3, 4)\n       [1, 0, 0, 0],\n       [0, 2, 0, 0]]\n\n# Rectangular matrix with inferred num_cols and padding_value = 9.\ntf.matrix_diag(diagonal, k = -1, num_rows = 3, padding_value = 9)\n  ==> [[9, 9],  # Output shape: (3, 2)\n       [1, 9],\n       [9, 2]]\n\n```",
    "inputs": [
      { "name": "diagonal", "type": "Arg" },
      { "name": "k", "type": "Arg" },
      { "name": "num_rows", "type": "Arg" },
      { "name": "num_cols", "type": "Arg" },
      { "name": "padding_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "align", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MatrixInverse",
    "summary": "Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes).",
    "description": "The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions\nform square matrices. The output is a tensor of the same shape as the input\ncontaining the inverse for all input submatrices `[..., :, :]`.\n\nThe op uses LU decomposition with partial pivoting to compute the inverses.\n\nIf a matrix is not invertible there is no guarantee what the op does. It\nmay detect the condition and raise an exception or it may simply return a\ngarbage result.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "adjoint", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MatrixSetDiag",
    "summary": "Returns a batched matrix tensor with new batched diagonal values.",
    "description": "Given `input` and `diagonal`, this operation returns a tensor with the\nsame shape and values as `input`, except for the main diagonal of the\ninnermost matrices.  These will be overwritten by the values in `diagonal`.\n\nThe output is computed as follows:\n\nAssume `input` has `k+1` dimensions `[I, J, K, ..., M, N]` and `diagonal` has\n`k` dimensions `[I, J, K, ..., min(M, N)]`.  Then the output is a\ntensor of rank `k+1` with dimensions `[I, J, K, ..., M, N]` where:\n\n  * `output[i, j, k, ..., m, n] = diagonal[i, j, k, ..., n]` for `m == n`.\n  * `output[i, j, k, ..., m, n] = input[i, j, k, ..., m, n]` for `m != n`.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "diagonal", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.MatrixSetDiagV2",
    "summary": "Returns a batched matrix tensor with new batched diagonal values.",
    "description": "Given `input` and `diagonal`, this operation returns a tensor with the\nsame shape and values as `input`, except for the specified diagonals of the\ninnermost matrices. These will be overwritten by the values in `diagonal`.\n\n`input` has `r+1` dimensions `[I, J, ..., L, M, N]`. When `k` is scalar or\n`k[0] == k[1]`, `diagonal` has `r` dimensions `[I, J, ..., L, max_diag_len]`.\nOtherwise, it has `r+1` dimensions `[I, J, ..., L, num_diags, max_diag_len]`.\n`num_diags` is the number of diagonals, `num_diags = k[1] - k[0] + 1`.\n`max_diag_len` is the longest diagonal in the range `[k[0], k[1]]`,\n`max_diag_len = min(M + min(k[1], 0), N + min(-k[0], 0))`\n\nThe output is a tensor of rank `k+1` with dimensions `[I, J, ..., L, M, N]`.\nIf `k` is scalar or `k[0] == k[1]`:\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, n-max(k[1], 0)] ; if n - m == k[1]\n    input[i, j, ..., l, m, n]              ; otherwise\n```\n\nOtherwise,\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, diag_index, index_in_diag] ; if k[0] <= d <= k[1]\n    input[i, j, ..., l, m, n]                         ; otherwise\n```\nwhere `d = n - m`, `diag_index = k[1] - d`, and `index_in_diag = n - max(d, 0)`.\n\nFor example:\n\n```\n# The main diagonal.\ninput = np.array([[[7, 7, 7, 7],              # Input shape: (2, 3, 4)\n                   [7, 7, 7, 7],\n                   [7, 7, 7, 7]],\n                  [[7, 7, 7, 7],\n                   [7, 7, 7, 7],\n                   [7, 7, 7, 7]]])\ndiagonal = np.array([[1, 2, 3],               # Diagonal shape: (2, 3)\n                     [4, 5, 6]])\ntf.matrix_set_diag(diagonal) ==> [[[1, 7, 7, 7],  # Output shape: (2, 3, 4)\n                                   [7, 2, 7, 7],\n                                   [7, 7, 3, 7]],\n                                  [[4, 7, 7, 7],\n                                   [7, 5, 7, 7],\n                                   [7, 7, 6, 7]]]\n\n# A superdiagonal (per batch).\ntf.matrix_set_diag(diagonal, k = 1)\n  ==> [[[7, 1, 7, 7],  # Output shape: (2, 3, 4)\n        [7, 7, 2, 7],\n        [7, 7, 7, 3]],\n       [[7, 4, 7, 7],\n        [7, 7, 5, 7],\n        [7, 7, 7, 6]]]\n\n# A band of diagonals.\ndiagonals = np.array([[[1, 2, 3],  # Diagonal shape: (2, 2, 3)\n                       [4, 5, 0]],\n                      [[6, 1, 2],\n                       [3, 4, 0]]])\ntf.matrix_set_diag(diagonals, k = (-1, 0))\n  ==> [[[1, 7, 7, 7],  # Output shape: (2, 3, 4)\n        [4, 2, 7, 7],\n        [0, 5, 3, 7]],\n       [[6, 7, 7, 7],\n        [3, 1, 7, 7],\n        [7, 4, 2, 7]]]\n\n```",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "diagonal", "type": "Arg" },
      { "name": "k", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.MatrixSetDiagV3",
    "summary": "Returns a batched matrix tensor with new batched diagonal values.",
    "description": "Given `input` and `diagonal`, this operation returns a tensor with the\nsame shape and values as `input`, except for the specified diagonals of the\ninnermost matrices. These will be overwritten by the values in `diagonal`.\n\n`input` has `r+1` dimensions `[I, J, ..., L, M, N]`. When `k` is scalar or\n`k[0] == k[1]`, `diagonal` has `r` dimensions `[I, J, ..., L, max_diag_len]`.\nOtherwise, it has `r+1` dimensions `[I, J, ..., L, num_diags, max_diag_len]`.\n`num_diags` is the number of diagonals, `num_diags = k[1] - k[0] + 1`.\n`max_diag_len` is the longest diagonal in the range `[k[0], k[1]]`,\n`max_diag_len = min(M + min(k[1], 0), N + min(-k[0], 0))`\n\nThe output is a tensor of rank `k+1` with dimensions `[I, J, ..., L, M, N]`.\nIf `k` is scalar or `k[0] == k[1]`:\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, n-max(k[1], 0)] ; if n - m == k[1]\n    input[i, j, ..., l, m, n]              ; otherwise\n```\n\nOtherwise,\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, diag_index, index_in_diag] ; if k[0] <= d <= k[1]\n    input[i, j, ..., l, m, n]                         ; otherwise\n```\nwhere `d = n - m`, `diag_index = k[1] - d`, and\n`index_in_diag = n - max(d, 0) + offset`.\n\n`offset` is zero except when the alignment of the diagonal is to the right.\n```\noffset = max_diag_len - diag_len(d) ; if (`align` in {RIGHT_LEFT, RIGHT_RIGHT}\n                                           and `d >= 0`) or\n                                         (`align` in {LEFT_RIGHT, RIGHT_RIGHT}\n                                           and `d <= 0`)\n         0                          ; otherwise\n```\nwhere `diag_len(d) = min(cols - max(d, 0), rows + min(d, 0))`.\n\nFor example:\n\n```\n# The main diagonal.\ninput = np.array([[[7, 7, 7, 7],              # Input shape: (2, 3, 4)\n                   [7, 7, 7, 7],\n                   [7, 7, 7, 7]],\n                  [[7, 7, 7, 7],\n                   [7, 7, 7, 7],\n                   [7, 7, 7, 7]]])\ndiagonal = np.array([[1, 2, 3],               # Diagonal shape: (2, 3)\n                     [4, 5, 6]])\ntf.matrix_set_diag(input, diagonal)\n  ==> [[[1, 7, 7, 7],  # Output shape: (2, 3, 4)\n        [7, 2, 7, 7],\n        [7, 7, 3, 7]],\n       [[4, 7, 7, 7],\n        [7, 5, 7, 7],\n        [7, 7, 6, 7]]]\n\n# A superdiagonal (per batch).\ntf.matrix_set_diag(input, diagonal, k = 1)\n  ==> [[[7, 1, 7, 7],  # Output shape: (2, 3, 4)\n        [7, 7, 2, 7],\n        [7, 7, 7, 3]],\n       [[7, 4, 7, 7],\n        [7, 7, 5, 7],\n        [7, 7, 7, 6]]]\n\n# A band of diagonals.\ndiagonals = np.array([[[0, 9, 1],  # Diagonal shape: (2, 4, 3)\n                       [6, 5, 8],\n                       [1, 2, 3],\n                       [4, 5, 0]],\n                      [[0, 1, 2],\n                       [5, 6, 4],\n                       [6, 1, 2],\n                       [3, 4, 0]]])\ntf.matrix_set_diag(input, diagonals, k = (-1, 2))\n  ==> [[[1, 6, 9, 7],  # Output shape: (2, 3, 4)\n        [4, 2, 5, 1],\n        [7, 5, 3, 8]],\n       [[6, 5, 1, 7],\n        [3, 1, 6, 2],\n        [7, 4, 2, 4]]]\n\n# LEFT_RIGHT alignment.\ndiagonals = np.array([[[9, 1, 0],  # Diagonal shape: (2, 4, 3)\n                       [6, 5, 8],\n                       [1, 2, 3],\n                       [0, 4, 5]],\n                      [[1, 2, 0],\n                       [5, 6, 4],\n                       [6, 1, 2],\n                       [0, 3, 4]]])\ntf.matrix_set_diag(input, diagonals, k = (-1, 2), align=\"LEFT_RIGHT\")\n  ==> [[[1, 6, 9, 7],  # Output shape: (2, 3, 4)\n        [4, 2, 5, 1],\n        [7, 5, 3, 8]],\n       [[6, 5, 1, 7],\n        [3, 1, 6, 2],\n        [7, 4, 2, 4]]]\n\n```",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "diagonal", "type": "Arg" },
      { "name": "k", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "align", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MatrixSolve",
    "summary": "Solves systems of linear equations.",
    "description": "`Matrix` is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions\nform square matrices. `Rhs` is a tensor of shape `[..., M, K]`. The `output` is\na tensor shape `[..., M, K]`.  If `adjoint` is `False` then each output matrix\nsatisfies `matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]`.\nIf `adjoint` is `True` then each output matrix satisfies\n`adjoint(matrix[..., :, :]) * output[..., :, :] = rhs[..., :, :]`.",
    "inputs": [
      { "name": "matrix", "type": "Arg" },
      { "name": "rhs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "adjoint", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MatrixTriangularSolve",
    "summary": "Solves systems of linear equations with upper or lower triangular matrices by backsubstitution.",
    "description": "`matrix` is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions form\nsquare matrices. If `lower` is `True` then the strictly upper triangular part\nof each inner-most matrix is assumed to be zero and not accessed.\nIf `lower` is False then the strictly lower triangular part of each inner-most\nmatrix is assumed to be zero and not accessed.\n`rhs` is a tensor of shape `[..., M, N]`.\n\nThe output is a tensor of shape `[..., M, N]`. If `adjoint` is\n`True` then the innermost matrices in `output` satisfy matrix equations\n`matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]`.\nIf `adjoint` is `False` then the strictly then the  innermost matrices in\n`output` satisfy matrix equations\n`adjoint(matrix[..., i, k]) * output[..., k, j] = rhs[..., i, j]`.\n\nNote, the batch shapes for the inputs only need to broadcast.\n\nExample:\n```python\n\na = tf.constant([[3,  0,  0,  0],\n                 [2,  1,  0,  0],\n                 [1,  0,  1,  0],\n                 [1,  1,  1,  1]], dtype=tf.float32)\n\nb = tf.constant([[4],\n                 [2],\n                 [4],\n                 [2]], dtype=tf.float32)\n\nx = tf.linalg.triangular_solve(a, b, lower=True)\nx\n# <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n# array([[ 1.3333334 ],\n#        [-0.66666675],\n#        [ 2.6666665 ],\n#        [-1.3333331 ]], dtype=float32)>\n\n# in python3 one can use `a@x`\ntf.matmul(a, x)\n# <tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n# array([[4.       ],\n#        [2.       ],\n#        [4.       ],\n#        [1.9999999]], dtype=float32)>\n```",
    "inputs": [
      { "name": "matrix", "type": "Arg" },
      { "name": "rhs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "lower", "type": "DefaultValuedOptionalAttr" },
      { "name": "adjoint", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Max",
    "summary": "Computes the maximum of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "reduction_indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Maximum",
    "summary": "Returns the max of x and y (i.e. x > y ? x : y) element-wise.",
    "description": "*NOTE*: `Maximum` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_IntOrFpTensor" },
      { "name": "y", "type": "TF_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_IntOrFpTensor" }
    ]
  },
  {
    "name": "tf.MaxPool",
    "summary": "Performs max pooling on the input.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MaxPool3D",
    "summary": "Performs 3D max pooling on the input.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MaxPool3DGrad",
    "summary": "Computes gradients of 3D max pooling function.",
    "inputs": [
      { "name": "orig_input", "type": "Arg" },
      { "name": "orig_output", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MaxPool3DGradGrad",
    "summary": "Computes second-order gradients of the maxpooling function.",
    "inputs": [
      { "name": "orig_input", "type": "Arg" },
      { "name": "orig_output", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MaxPoolGrad",
    "summary": "Computes gradients of the maxpooling function.",
    "inputs": [
      { "name": "orig_input", "type": "Arg" },
      { "name": "orig_output", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "explicit_paddings", "type": "DefaultValuedOptionalAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MaxPoolGradGrad",
    "summary": "Computes second-order gradients of the maxpooling function.",
    "inputs": [
      { "name": "orig_input", "type": "Arg" },
      { "name": "orig_output", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "ksize", "type": "ConfinedAttr" },
      { "name": "strides", "type": "ConfinedAttr" },
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MaxPoolGradGradV2",
    "summary": "Computes second-order gradients of the maxpooling function.",
    "inputs": [
      { "name": "orig_input", "type": "Arg" },
      { "name": "orig_output", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "ksize", "type": "Arg" },
      { "name": "strides", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MaxPoolGradV2",
    "summary": "Computes gradients of the maxpooling function.",
    "inputs": [
      { "name": "orig_input", "type": "Arg" },
      { "name": "orig_output", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "ksize", "type": "Arg" },
      { "name": "strides", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MaxPoolV2",
    "summary": "Performs max pooling on the input.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "ksize", "type": "Arg" },
      { "name": "strides", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "padding", "type": "TF_AnyStrAttrOf" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Mean",
    "summary": "Computes the mean of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "reduction_indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MergeSummary",
    "summary": "Merges summaries.",
    "description": "This op creates a\n[`Summary`](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)\nprotocol buffer that contains the union of all the values in the input\nsummaries.\n\nWhen the Op is run, it reports an `InvalidArgument` error if multiple values\nin the summaries to merge use the same tag.",
    "inputs": [
      { "name": "inputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "summary", "type": "Res" }
    ]
  },
  {
    "name": "tf.MergeV2Checkpoints",
    "summary": "V2 format specific: merges the metadata files of sharded checkpoints.  The",
    "description": "result is one logical checkpoint, with one physical metadata file and renamed\ndata files.\n\nIntended for \"grouping\" multiple checkpoints in a sharded checkpoint setup.\n\nIf delete_old_dirs is true, attempts to delete recursively the dirname of each\npath in the input checkpoint_prefixes.  This is useful when those paths are non\nuser-facing temporary locations.\n\nIf allow_missing_files is true, merges the checkpoint prefixes as long as\nat least one file exists. Otherwise, if no files exist, an error will be thrown.\nThe default value for allow_missing_files is false.",
    "inputs": [
      { "name": "checkpoint_prefixes", "type": "Arg" },
      { "name": "destination_prefix", "type": "Arg" }
    ],
    "attributes": [
      { "name": "delete_old_dirs", "type": "DefaultValuedOptionalAttr" },
      { "name": "allow_missing_files", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Min",
    "summary": "Computes the minimum of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "reduction_indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Minimum",
    "summary": "Returns the min of x and y (i.e. x < y ? x : y) element-wise.",
    "description": "*NOTE*: `Minimum` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_IntOrFpTensor" },
      { "name": "y", "type": "TF_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_IntOrFpTensor" }
    ]
  },
  {
    "name": "tf.MirrorPad",
    "summary": "Pads a tensor with mirrored values.",
    "description": "This operation pads a `input` with mirrored values according to the `paddings`\nyou specify. `paddings` is an integer tensor with shape `[n, 2]`, where n is\nthe rank of `input`. For each dimension D of `input`, `paddings[D, 0]` indicates\nhow many values to add before the contents of `input` in that dimension, and\n`paddings[D, 1]` indicates how many values to add after the contents of `input`\nin that dimension. Both `paddings[D, 0]` and `paddings[D, 1]` must be no greater\nthan `input.dim_size(D)` (or `input.dim_size(D) - 1`) if `copy_border` is true\n(if false, respectively).\n\nThe padded size of each dimension D of the output is:\n\n`paddings(D, 0) + input.dim_size(D) + paddings(D, 1)`\n\nFor example:\n\n```\n# 't' is [[1, 2, 3], [4, 5, 6]].\n# 'paddings' is [[1, 1]], [2, 2]].\n# 'mode' is SYMMETRIC.\n# rank of 't' is 2.\npad(t, paddings) ==> [[2, 1, 1, 2, 3, 3, 2]\n                      [2, 1, 1, 2, 3, 3, 2]\n                      [5, 4, 4, 5, 6, 6, 5]\n                      [5, 4, 4, 5, 6, 6, 5]]\n```",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "paddings", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "mode", "type": "TF_AnyStrAttrOf" }
    ]
  },
  {
    "name": "tf.MirrorPadGrad",
    "summary": "Gradient op for `MirrorPad` op. This op folds a mirror-padded tensor.",
    "description": "This operation folds the padded areas of `input` by `MirrorPad` according to the\n`paddings` you specify. `paddings` must be the same as `paddings` argument\ngiven to the corresponding `MirrorPad` op.\n\nThe folded size of each dimension D of the output is:\n\n`input.dim_size(D) - paddings(D, 0) - paddings(D, 1)`\n\nFor example:\n\n```\n# 't' is [[1, 2, 3], [4, 5, 6], [7, 8, 9]].\n# 'paddings' is [[0, 1]], [0, 1]].\n# 'mode' is SYMMETRIC.\n# rank of 't' is 2.\npad(t, paddings) ==> [[ 1,  5]\n                      [11, 28]]\n```",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "paddings", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "mode", "type": "TF_AnyStrAttrOf" }
    ]
  },
  {
    "name": "tf.MlirLocalVarOp",
    "summary": "Creates a handle to an in-scope variable.",
    "description": "Used by internal passes for temporary representation of local state, which will\nbe eventually removed.",
    "outputs": [
      { "name": "resource", "type": "Res" }
    ]
  },
  {
    "name": "tf.MlirPassthroughOp",
    "summary": "Wraps an arbitrary MLIR computation expressed as a module with a main() function.",
    "description": "This operation does not have an associated kernel and is not intended to be\nexecuted in a regular TensorFlow session. Instead it is intended to be used for\ntesting or for special case where a user intends to pass custom MLIR computation\nthrough a TensorFlow graph with the intent of having custom tooling processing\nit downstream (when targeting a different environment, like TensorFlow lite for\nexample).\nThe MLIR module is expected to have a main() function that will be used as an\nentry point. The inputs to the operations will be passed as argument to the\nmain() function and the returned values of the main function mapped to the\noutputs.\nExample usage:\n\n```\nimport tensorflow as tf\nfrom tensorflow.compiler.mlir.tensorflow.gen_mlir_passthrough_op import mlir_passthrough_op\n\nmlir_module = '''python\nfunc @main(%arg0 : tensor<10xf32>, %arg1 : tensor<10xf32>) -> tensor<10x10xf32> {\n   %add = \"magic.op\"(%arg0, %arg1) : (tensor<10xf32>, tensor<10xf32>) -> tensor<10x10xf32>\n   return %ret : tensor<10x10xf32>\n}\n'''\n\n@tf.function\ndef foo(x, y):\n  return mlir_passthrough_op([x, y], mlir_module, Toutputs=[tf.float32])\n\ngraph_def = foo.get_concrete_function(tf.TensorSpec([10], tf.float32), tf.TensorSpec([10], tf.float32)).graph.as_graph_def()\n```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "mlir_module", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.Mod",
    "summary": "Returns element-wise remainder of division. This emulates C semantics in that",
    "description": "the result here is consistent with a truncating divide. E.g.\n`tf.truncatediv(x, y) * y + truncate_mod(x, y) = x`.\n\n*NOTE*: `Mod` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_FpOrI32OrI64Tensor" },
      { "name": "y", "type": "TF_FpOrI32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrI32OrI64Tensor" }
    ]
  },
  {
    "name": "tf.ModelDataset",
    "summary": "Identity transformation that models performance.",
    "description": "Identity transformation that models performance.",
    "inputs": [
      { "name": "input_dataset", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "algorithm", "type": "DefaultValuedOptionalAttr" },
      { "name": "cpu_budget", "type": "DefaultValuedOptionalAttr" },
      { "name": "ram_budget", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.Mul",
    "summary": "Returns x * y element-wise.",
    "description": "*NOTE*: `Multiply` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.MulNoNan",
    "summary": "Returns x * y element-wise. Returns zero if y is zero, even if x if infinite or NaN.",
    "description": "*NOTE*: `MulNoNan` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" },
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.MultiDeviceIterator",
    "summary": "Creates a MultiDeviceIterator resource.",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "devices", "type": "ConfinedAttr" },
      { "name": "shared_name", "type": "StrAttr" },
      { "name": "container", "type": "StrAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.MultiDeviceIteratorFromStringHandle",
    "summary": "Generates a MultiDeviceIterator resource from its provided string handle.",
    "inputs": [
      { "name": "string_handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "multi_device_iterator", "type": "Res" }
    ],
    "attributes": [
      { "name": "output_types", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_shapes", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MultiDeviceIteratorGetNextFromShard",
    "summary": "Gets next element for the provided shard number.",
    "inputs": [
      { "name": "multi_device_iterator", "type": "Arg" },
      { "name": "shard_num", "type": "Arg" },
      { "name": "incarnation_id", "type": "Arg" }
    ],
    "outputs": [
      { "name": "components", "type": "Res" }
    ]
  },
  {
    "name": "tf.MultiDeviceIteratorInit",
    "summary": "Initializes the multi device iterator with the given dataset.",
    "inputs": [
      { "name": "dataset", "type": "Arg" },
      { "name": "multi_device_iterator", "type": "Arg" },
      { "name": "max_buffer_size", "type": "Arg" }
    ],
    "outputs": [
      { "name": "incarnation_id", "type": "Res" }
    ]
  },
  {
    "name": "tf.MultiDeviceIteratorToStringHandle",
    "summary": "Produces a string handle for the given MultiDeviceIterator.",
    "inputs": [
      { "name": "multi_device_iterator", "type": "Arg" }
    ],
    "outputs": [
      { "name": "string_handle", "type": "Res" }
    ]
  },
  {
    "name": "tf.Multinomial",
    "summary": "Draws samples from a multinomial distribution.",
    "inputs": [
      { "name": "logits", "type": "Arg" },
      { "name": "num_samples", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MutableDenseHashTableV2",
    "summary": "Creates an empty hash table that uses tensors as the backing store.",
    "description": "It uses \"open addressing\" with quadratic reprobing to resolve\ncollisions.\n\nThis op creates a mutable hash table, specifying the type of its keys and\nvalues. Each value must be a scalar. Data can be inserted into the table using\nthe insert operations. It does not support the initialization operation.",
    "inputs": [
      { "name": "empty_key", "type": "Arg" },
      { "name": "deleted_key", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "table_handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_node_name_sharing", "type": "DefaultValuedOptionalAttr" },
      { "name": "value_dtype", "type": "TypeAttr" },
      { "name": "value_shape", "type": "DefaultValuedOptionalAttr" },
      { "name": "initial_num_buckets", "type": "DefaultValuedOptionalAttr" },
      { "name": "max_load_factor", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MutableHashTableOfTensorsV2",
    "summary": "Creates an empty hash table.",
    "description": "This op creates a mutable hash table, specifying the type of its keys and\nvalues. Each value must be a vector. Data can be inserted into the table using\nthe insert operations. It does not support the initialization operation.",
    "outputs": [
      { "name": "table_handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_node_name_sharing", "type": "DefaultValuedOptionalAttr" },
      { "name": "key_dtype", "type": "TypeAttr" },
      { "name": "value_dtype", "type": "TypeAttr" },
      { "name": "value_shape", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.MutableHashTableV2",
    "summary": "Creates an empty hash table.",
    "description": "This op creates a mutable hash table, specifying the type of its keys and\nvalues. Each value must be a scalar. Data can be inserted into the table using\nthe insert operations. It does not support the initialization operation.",
    "outputs": [
      { "name": "table_handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_node_name_sharing", "type": "DefaultValuedOptionalAttr" },
      { "name": "key_dtype", "type": "TypeAttr" },
      { "name": "value_dtype", "type": "TypeAttr" }
    ]
  },
  {
    "name": "tf.NcclAllReduce",
    "summary": "Outputs a tensor containing the reduction across all input tensors.",
    "description": "Outputs a tensor containing the reduction across all input tensors passed to ops\nwithin the same `shared_name.\n\nThe graph should be constructed so if one op runs with shared_name value `c`,\nthen `num_devices` ops will run with shared_name value `c`.  Failure to do so\nwill cause the graph execution to fail to complete.\n\ninput: the input to the reduction\ndata: the value of the reduction across all `num_devices` devices.\nreduction: the reduction operation to perform.\nnum_devices: The number of devices participating in this reduction.\nshared_name: Identifier that shared between ops of the same reduction.",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "data", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "reduction", "type": "TF_AnyStrAttrOf" },
      { "name": "num_devices", "type": "I64Attr" },
      { "name": "shared_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.Ndtri",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.Neg",
    "summary": "Computes numerical negative value element-wise.",
    "description": "I.e., \\\\(y = -x\\\\).",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.NextAfter",
    "summary": "Returns the next representable value of `x1` in the direction of `x2`, element-wise.",
    "description": "This operation returns the same result as the C++ std::nextafter function.\n\nIt can also return a subnormal number.\n\n@compatibility(cpp)\nEquivalent to C++ std::nextafter function.\n@end_compatibility",
    "inputs": [
      { "name": "x1", "type": "TF_F32OrF64Tensor" },
      { "name": "x2", "type": "TF_F32OrF64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.NonMaxSuppressionV3",
    "summary": "Greedily selects a subset of bounding boxes in descending order of score,",
    "description": "pruning away boxes that have high intersection-over-union (IOU) overlap\nwith previously selected boxes.  Bounding boxes with score less than\n`score_threshold` are removed.  Bounding boxes are supplied as\n[y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any\ndiagonal pair of box corners and the coordinates can be provided as normalized\n(i.e., lying in the interval [0, 1]) or absolute.  Note that this algorithm\nis agnostic to where the origin is in the coordinate system and more\ngenerally is invariant to orthogonal transformations and translations\nof the coordinate system; thus translating or reflections of the coordinate\nsystem result in the same boxes being selected by the algorithm.\nThe output of this operation is a set of integers indexing into the input\ncollection of bounding boxes representing the selected boxes.  The bounding\nbox coordinates corresponding to the selected indices can then be obtained\nusing the `tf.gather operation`.  For example:\n  selected_indices = tf.image.non_max_suppression_v2(\n      boxes, scores, max_output_size, iou_threshold, score_threshold)\n  selected_boxes = tf.gather(boxes, selected_indices)",
    "inputs": [
      { "name": "boxes", "type": "Arg" },
      { "name": "scores", "type": "Arg" },
      { "name": "max_output_size", "type": "Arg" },
      { "name": "iou_threshold", "type": "Arg" },
      { "name": "score_threshold", "type": "Arg" }
    ],
    "outputs": [
      { "name": "selected_indices", "type": "Res" }
    ]
  },
  {
    "name": "tf.NonMaxSuppressionV4",
    "summary": "Greedily selects a subset of bounding boxes in descending order of score,",
    "description": "pruning away boxes that have high intersection-over-union (IOU) overlap\nwith previously selected boxes.  Bounding boxes with score less than\n`score_threshold` are removed.  Bounding boxes are supplied as\n[y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any\ndiagonal pair of box corners and the coordinates can be provided as normalized\n(i.e., lying in the interval [0, 1]) or absolute.  Note that this algorithm\nis agnostic to where the origin is in the coordinate system and more\ngenerally is invariant to orthogonal transformations and translations\nof the coordinate system; thus translating or reflections of the coordinate\nsystem result in the same boxes being selected by the algorithm.\nThe output of this operation is a set of integers indexing into the input\ncollection of bounding boxes representing the selected boxes.  The bounding\nbox coordinates corresponding to the selected indices can then be obtained\nusing the `tf.gather operation`.  For example:\n  selected_indices = tf.image.non_max_suppression_v2(\n      boxes, scores, max_output_size, iou_threshold, score_threshold)\n  selected_boxes = tf.gather(boxes, selected_indices)",
    "inputs": [
      { "name": "boxes", "type": "Arg" },
      { "name": "scores", "type": "Arg" },
      { "name": "max_output_size", "type": "Arg" },
      { "name": "iou_threshold", "type": "Arg" },
      { "name": "score_threshold", "type": "Arg" }
    ],
    "outputs": [
      { "name": "selected_indices", "type": "Res" },
      { "name": "valid_outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "pad_to_max_output_size", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.NonMaxSuppressionV5",
    "summary": "Greedily selects a subset of bounding boxes in descending order of score,",
    "description": "pruning away boxes that have high intersection-over-union (IOU) overlap\nwith previously selected boxes.  Bounding boxes with score less than\n`score_threshold` are removed.  Bounding boxes are supplied as\n[y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any\ndiagonal pair of box corners and the coordinates can be provided as normalized\n(i.e., lying in the interval [0, 1]) or absolute.  Note that this algorithm\nis agnostic to where the origin is in the coordinate system and more\ngenerally is invariant to orthogonal transformations and translations\nof the coordinate system; thus translating or reflections of the coordinate\nsystem result in the same boxes being selected by the algorithm.\nThe output of this operation is a set of integers indexing into the input\ncollection of bounding boxes representing the selected boxes.  The bounding\nbox coordinates corresponding to the selected indices can then be obtained\nusing the `tf.gather operation`.  For example:\n  selected_indices = tf.image.non_max_suppression_v2(\n      boxes, scores, max_output_size, iou_threshold, score_threshold)\n  selected_boxes = tf.gather(boxes, selected_indices)\nThis op also supports a Soft-NMS (with Gaussian weighting) mode (c.f.\nBodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score\nof other overlapping boxes instead of directly causing them to be pruned.\nTo enable this Soft-NMS mode, set the `soft_nms_sigma` parameter to be\nlarger than 0.",
    "inputs": [
      { "name": "boxes", "type": "Arg" },
      { "name": "scores", "type": "Arg" },
      { "name": "max_output_size", "type": "Arg" },
      { "name": "iou_threshold", "type": "Arg" },
      { "name": "score_threshold", "type": "Arg" },
      { "name": "soft_nms_sigma", "type": "Arg" }
    ],
    "outputs": [
      { "name": "selected_indices", "type": "Res" },
      { "name": "selected_scores", "type": "Res" },
      { "name": "valid_outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "pad_to_max_output_size", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.NoOp",
    "summary": "Does nothing. Only useful as a placeholder for control edges."
  },
  {
    "name": "tf.NotEqual",
    "summary": "Returns the truth value of (x != y) element-wise.",
    "description": "*NOTE*: `NotEqual` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_Tensor" },
      { "name": "y", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_BoolTensor" }
    ],
    "attributes": [
      { "name": "incompatible_shape_error", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.OneHot",
    "summary": "Returns a one-hot tensor.",
    "description": "The locations represented by indices in `indices` take value `on_value`,\nwhile all other locations take value `off_value`.\n\nIf the input `indices` is rank `N`, the output will have rank `N+1`,\nThe new axis is created at dimension `axis` (default: the new axis is\nappended at the end).\n\nIf `indices` is a scalar the output shape will be a vector of length `depth`.\n\nIf `indices` is a vector of length `features`, the output shape will be:\n```\n  features x depth if axis == -1\n  depth x features if axis == 0\n```\n\nIf `indices` is a matrix (batch) with shape `[batch, features]`,\nthe output shape will be:\n```\n  batch x features x depth if axis == -1\n  batch x depth x features if axis == 1\n  depth x batch x features if axis == 0\n```\n\n\nExamples\n=========\n\nSuppose that\n```\n  indices = [0, 2, -1, 1]\n  depth = 3\n  on_value = 5.0\n  off_value = 0.0\n  axis = -1\n```\n\nThen output is `[4 x 3]`:\n```\noutput =\n  [5.0 0.0 0.0]  // one_hot(0)\n  [0.0 0.0 5.0]  // one_hot(2)\n  [0.0 0.0 0.0]  // one_hot(-1)\n  [0.0 5.0 0.0]  // one_hot(1)\n```\n\nSuppose that\n```\n  indices = [0, 2, -1, 1]\n  depth = 3\n  on_value = 0.0\n  off_value = 3.0\n  axis = 0\n```\n\nThen output is `[3 x 4]`:\n```\noutput =\n  [0.0 3.0 3.0 3.0]\n  [3.0 3.0 3.0 0.0]\n  [3.0 3.0 3.0 3.0]\n  [3.0 0.0 3.0 3.0]\n//  ^                one_hot(0)\n//      ^            one_hot(2)\n//          ^        one_hot(-1)\n//              ^    one_hot(1)\n```\n\nSuppose that\n```\n  indices = [[0, 2], [1, -1]]\n  depth = 3\n  on_value = 1.0\n  off_value = 0.0\n  axis = -1\n```\n\nThen output is `[2 x 2 x 3]`:\n```\noutput =\n  [\n    [1.0, 0.0, 0.0]  // one_hot(0)\n    [0.0, 0.0, 1.0]  // one_hot(2)\n  ][\n    [0.0, 1.0, 0.0]  // one_hot(1)\n    [0.0, 0.0, 0.0]  // one_hot(-1)\n  ]\n```",
    "inputs": [
      { "name": "indices", "type": "Arg" },
      { "name": "depth", "type": "Arg" },
      { "name": "on_value", "type": "Arg" },
      { "name": "off_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.OneShotIterator",
    "summary": "Makes a \"one-shot\" iterator that can be iterated only once.",
    "description": "A one-shot iterator bundles the logic for defining the dataset and\nthe state of the iterator in a single op, which allows simple input\npipelines to be defined without an additional initialization\n(\"MakeIterator\") step.\n\nOne-shot iterators have the following limitations:\n\n* They do not support parameterization: all logic for creating the underlying\n  dataset must be bundled in the `dataset_factory` function.\n* They are not resettable. Once a one-shot iterator reaches the end of its\n  underlying dataset, subsequent \"IteratorGetNext\" operations on that\n  iterator will always produce an `OutOfRange` error.\n\nFor greater flexibility, use \"Iterator\" and \"MakeIterator\" to define\nan iterator using an arbitrary subgraph, which may capture tensors\n(including fed values) as parameters, and which may be reset multiple\ntimes by rerunning \"MakeIterator\".",
    "outputs": [
      { "name": "handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "dataset_factory", "type": "SymbolRefAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.OnesLike",
    "summary": "Returns a tensor of ones with the same shape and type as x.",
    "inputs": [
      { "name": "x", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" }
    ]
  },
  {
    "name": "tf.OptimizeDatasetV2",
    "summary": "Creates a dataset by applying related optimizations to `input_dataset`.",
    "description": "Creates a dataset by applying related optimizations to `input_dataset`.",
    "inputs": [
      { "name": "input_dataset", "type": "Arg" },
      { "name": "optimizations_enabled", "type": "Arg" },
      { "name": "optimizations_disabled", "type": "Arg" },
      { "name": "optimizations_default", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "optimization_configs", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.OptionalFromValue",
    "summary": "Constructs an Optional variant from a tuple of tensors.",
    "inputs": [
      { "name": "components", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "optional", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.OptionalGetValue",
    "summary": "Returns the value stored in an Optional variant or raises an error if none exists.",
    "inputs": [
      { "name": "optional", "type": "TF_VariantTensor" }
    ],
    "outputs": [
      { "name": "components", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.OptionalHasValue",
    "summary": "Returns true if and only if the given Optional variant has a value.",
    "inputs": [
      { "name": "optional", "type": "TF_VariantTensor" }
    ],
    "outputs": [
      { "name": "has_value", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.OptionalNone",
    "summary": "Creates an Optional variant with no value.",
    "outputs": [
      { "name": "optional", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.OutfeedEnqueue",
    "summary": "Enqueue a Tensor on the computation outfeed.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ]
  },
  {
    "name": "tf.OutfeedEnqueueTuple",
    "summary": "Enqueue multiple Tensor values on the computation outfeed.",
    "inputs": [
      { "name": "inputs", "type": "Arg" }
    ]
  },
  {
    "name": "tf.Pack",
    "summary": "Packs a list of `N` rank-`R` tensors into one rank-`(R+1)` tensor.",
    "description": "Packs the `N` tensors in `values` into a tensor with rank one higher than each\ntensor in `values`, by packing them along the `axis` dimension.\nGiven a list of tensors of shape `(A, B, C)`;\n\nif `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`.\nif `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`.\nEtc.\n\nFor example:\n\n```\n# 'x' is [1, 4]\n# 'y' is [2, 5]\n# 'z' is [3, 6]\npack([x, y, z]) => [[1, 4], [2, 5], [3, 6]]  # Pack along first dim.\npack([x, y, z], axis=1) => [[1, 2, 3], [4, 5, 6]]\n```\n\nThis is the opposite of `unpack`.",
    "inputs": [
      { "name": "values", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Pad",
    "summary": "Pads a tensor with zeros.",
    "description": "This operation pads a `input` with zeros according to the `paddings` you\nspecify. `paddings` is an integer tensor with shape `[Dn, 2]`, where n is the\nrank of `input`. For each dimension D of `input`, `paddings[D, 0]` indicates\nhow many zeros to add before the contents of `input` in that dimension, and\n`paddings[D, 1]` indicates how many zeros to add after the contents of `input`\nin that dimension.\n\nThe padded size of each dimension D of the output is:\n\n`paddings(D, 0) + input.dim_size(D) + paddings(D, 1)`\n\nFor example:\n\n```\n# 't' is [[1, 1], [2, 2]]\n# 'paddings' is [[1, 1], [2, 2]]\n# rank of 't' is 2\npad(t, paddings) ==> [[0, 0, 0, 0, 0, 0]\n                      [0, 0, 1, 1, 0, 0]\n                      [0, 0, 2, 2, 0, 0]\n                      [0, 0, 0, 0, 0, 0]]\n```",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "paddings", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "category": "Transform"
  },
  {
    "name": "tf.PadV2",
    "summary": "Pads a tensor.",
    "description": "This operation pads `input` according to the `paddings` and `constant_values`\nyou specify. `paddings` is an integer tensor with shape `[Dn, 2]`, where n is\nthe rank of `input`. For each dimension D of `input`, `paddings[D, 0]` indicates\nhow many padding values to add before the contents of `input` in that dimension,\nand `paddings[D, 1]` indicates how many padding values to add after the contents\nof `input` in that dimension. `constant_values` is a scalar tensor of the same\ntype as `input` that indicates the value to use for padding `input`.\n\nThe padded size of each dimension D of the output is:\n\n`paddings(D, 0) + input.dim_size(D) + paddings(D, 1)`\n\nFor example:\n\n```\n# 't' is [[1, 1], [2, 2]]\n# 'paddings' is [[1, 1], [2, 2]]\n# 'constant_values' is 0\n# rank of 't' is 2\npad(t, paddings) ==> [[0, 0, 0, 0, 0, 0]\n                      [0, 0, 1, 1, 0, 0]\n                      [0, 0, 2, 2, 0, 0]\n                      [0, 0, 0, 0, 0, 0]]\n```",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "paddings", "type": "TF_I32OrI64Tensor" },
      { "name": "constant_values", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.ParallelDynamicStitch",
    "summary": "Interleave the values from the `data` tensors into a single tensor.",
    "description": "Builds a merged tensor such that\n\n```python\n    merged[indices[m][i, ..., j], ...] = data[m][i, ..., j, ...]\n```\n\nFor example, if each `indices[m]` is scalar or vector, we have\n\n```python\n    # Scalar indices:\n    merged[indices[m], ...] = data[m][...]\n\n    # Vector indices:\n    merged[indices[m][i], ...] = data[m][i, ...]\n```\n\nEach `data[i].shape` must start with the corresponding `indices[i].shape`,\nand the rest of `data[i].shape` must be constant w.r.t. `i`.  That is, we\nmust have `data[i].shape = indices[i].shape + constant`.  In terms of this\n`constant`, the output shape is\n\n    merged.shape = [max(indices)] + constant\n\nValues may be merged in parallel, so if an index appears in both `indices[m][i]`\nand `indices[n][j]`, the result may be invalid. This differs from the normal\nDynamicStitch operator that defines the behavior in that case.\n\nFor example:\n\n```python\n    indices[0] = 6\n    indices[1] = [4, 1]\n    indices[2] = [[5, 2], [0, 3]]\n    data[0] = [61, 62]\n    data[1] = [[41, 42], [11, 12]]\n    data[2] = [[[51, 52], [21, 22]], [[1, 2], [31, 32]]]\n    merged = [[1, 2], [11, 12], [21, 22], [31, 32], [41, 42],\n              [51, 52], [61, 62]]\n```\n\nThis method can be used to merge partitions created by `dynamic_partition`\nas illustrated on the following example:\n\n```python\n    # Apply function (increments x_i) on elements for which a certain condition\n    # apply (x_i != -1 in this example).\n    x=tf.constant([0.1, -1., 5.2, 4.3, -1., 7.4])\n    condition_mask=tf.not_equal(x,tf.constant(-1.))\n    partitioned_data = tf.dynamic_partition(\n        x, tf.cast(condition_mask, tf.int32) , 2)\n    partitioned_data[1] = partitioned_data[1] + 1.0\n    condition_indices = tf.dynamic_partition(\n        tf.range(tf.shape(x)[0]), tf.cast(condition_mask, tf.int32) , 2)\n    x = tf.dynamic_stitch(condition_indices, partitioned_data)\n    # Here x=[1.1, -1., 6.2, 5.3, -1, 8.4], the -1. values remain\n    # unchanged.\n```\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/DynamicStitch.png\" alt>\n</div>",
    "inputs": [
      { "name": "indices", "type": "Variadic" },
      { "name": "data", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "merged", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.ParallelMapDataset",
    "summary": "Creates a dataset that applies `f` to the outputs of `input_dataset`.",
    "description": "Unlike a \"MapDataset\", which applies `f` sequentially, this dataset invokes up\nto `num_parallel_calls` copies of `f` in parallel.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "other_arguments", "type": "Variadic" },
      { "name": "num_parallel_calls", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "use_inter_op_parallelism", "type": "DefaultValuedOptionalAttr" },
      { "name": "sloppy", "type": "DefaultValuedOptionalAttr" },
      { "name": "preserve_cardinality", "type": "DefaultValuedOptionalAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ParallelMapDatasetV2",
    "summary": "Creates a dataset that applies `f` to the outputs of `input_dataset`.",
    "description": "Unlike a \"MapDataset\", which applies `f` sequentially, this dataset invokes up\nto `num_parallel_calls` copies of `f` in parallel.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "other_arguments", "type": "Variadic" },
      { "name": "num_parallel_calls", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "use_inter_op_parallelism", "type": "DefaultValuedOptionalAttr" },
      { "name": "deterministic", "type": "DefaultValuedOptionalAttr" },
      { "name": "preserve_cardinality", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_unbounded_threadpool", "type": "DefaultValuedOptionalAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ParameterizedTruncatedNormal",
    "summary": "Outputs random values from a normal distribution. The parameters may each be a",
    "description": "scalar which applies to the entire output, or a vector of length shape[0] which\nstores the parameters for each batch.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "means", "type": "Arg" },
      { "name": "stdevs", "type": "Arg" },
      { "name": "minvals", "type": "Arg" },
      { "name": "maxvals", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ParseExample",
    "summary": "Transforms a vector of tf.Example protos (as strings) into typed tensors.",
    "inputs": [
      { "name": "serialized", "type": "TF_StrTensor" },
      { "name": "names", "type": "TF_StrTensor" },
      { "name": "sparse_keys", "type": "Variadic" },
      { "name": "dense_keys", "type": "Variadic" },
      { "name": "dense_defaults", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "sparse_indices", "type": "Variadic" },
      { "name": "sparse_values", "type": "Variadic" },
      { "name": "sparse_shapes", "type": "Variadic" },
      { "name": "dense_values", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dense_shapes", "type": "TF_ShapeAttrArray" }
    ]
  },
  {
    "name": "tf.ParseExampleV2",
    "summary": "Transforms a vector of tf.Example protos (as strings) into typed tensors.",
    "inputs": [
      { "name": "serialized", "type": "TF_StrTensor" },
      { "name": "names", "type": "TF_StrTensor" },
      { "name": "sparse_keys", "type": "TF_StrTensor" },
      { "name": "dense_keys", "type": "TF_StrTensor" },
      { "name": "ragged_keys", "type": "TF_StrTensor" },
      { "name": "dense_defaults", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "sparse_indices", "type": "Variadic" },
      { "name": "sparse_values", "type": "Variadic" },
      { "name": "sparse_shapes", "type": "Variadic" },
      { "name": "dense_values", "type": "Variadic" },
      { "name": "ragged_values", "type": "Variadic" },
      { "name": "ragged_row_splits", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "num_sparse", "type": "ConfinedAttr" },
      { "name": "dense_shapes", "type": "TF_ShapeAttrArray" }
    ]
  },
  {
    "name": "tf.PartitionedCall",
    "summary": "returns `f(inputs)`, where `f`'s body is placed and partitioned.",
    "description": "Asynchronously executes a function, potentially across multiple devices but\nwithin a single process. The kernel places and partitions a given function's\nunderlying graph, and executes each of the partitioned subgraphs as a function.",
    "inputs": [
      { "name": "args", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" },
      { "name": "config_proto", "type": "DefaultValuedOptionalAttr" },
      { "name": "executor_type", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Placeholder",
    "summary": "Placeholder op",
    "description": "Inserts a placeholder for a tensor that will be always fed.",
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.PlaceholderWithDefault",
    "summary": "Placeholder op",
    "description": "A placeholder op that passes through input when its output is not fed.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.Polygamma",
    "summary": "Compute the polygamma function \\\\(\\psi^{(n)}(x)\\\\).",
    "description": "The polygamma function is defined as:\n\n\n\\\\(\\psi^{(a)}(x) = \\frac{d^a}{dx^a} \\psi(x)\\\\)\n\nwhere \\\\(\\psi(x)\\\\) is the digamma function.\nThe polygamma function is defined only for non-negative integer orders \\\\a\\\\.",
    "inputs": [
      { "name": "a", "type": "TF_F32OrF64Tensor" },
      { "name": "x", "type": "TF_F32OrF64Tensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.PopulationCount",
    "summary": "Computes element-wise population count (a.k.a. popcount, bitsum, bitcount).",
    "description": "For each entry in `x`, calculates the number of `1` (on) bits in the binary\nrepresentation of that entry.\n\n**NOTE**: It is more efficient to first `tf.bitcast` your tensors into\n`int32` or `int64` and perform the bitcount on the result, than to feed in\n8- or 16-bit inputs and then aggregate the resulting counts.",
    "inputs": [
      { "name": "x", "type": "TF_IntTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_Uint8Tensor" }
    ]
  },
  {
    "name": "tf.Pow",
    "summary": "Computes the power of one value to another.",
    "description": "Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\ncorresponding elements in `x` and `y`. For example:\n\n```\n# tensor 'x' is [[2, 2]], [3, 3]]\n# tensor 'y' is [[8, 16], [2, 3]]\ntf.pow(x, y) ==> [[256, 65536], [9, 27]]\n```",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.PrefetchDataset",
    "summary": "Creates a dataset that asynchronously prefetches elements from `input_dataset`.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "buffer_size", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "slack_period", "type": "DefaultValuedOptionalAttr" },
      { "name": "legacy_autotune", "type": "DefaultValuedOptionalAttr" },
      { "name": "buffer_size_min", "type": "DefaultValuedOptionalAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.PreventGradient",
    "summary": "An identity op that triggers an error if a gradient is requested.",
    "description": "When executed in a graph, this op outputs its input tensor as-is.\n\nWhen building ops to compute gradients, the TensorFlow gradient system\nwill return an error when trying to lookup the gradient of this op,\nbecause no gradient must ever be registered for this function.  This\nop exists to prevent subtle bugs from silently returning unimplemented\ngradients in some corner cases.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "message", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Print",
    "summary": "Prints a list of tensors.",
    "description": "Passes `input` through to `output` and prints `data` when evaluating.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "data", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "message", "type": "DefaultValuedOptionalAttr" },
      { "name": "first_n", "type": "DefaultValuedOptionalAttr" },
      { "name": "summarize", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.PrintV2",
    "summary": "Prints a string scalar.",
    "description": "Prints a string scalar to the desired output_stream.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "attributes": [
      { "name": "output_stream", "type": "DefaultValuedOptionalAttr" },
      { "name": "end", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Prod",
    "summary": "Computes the product of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "reduction_indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Qr",
    "summary": "Computes the QR decompositions of one or more matrices.",
    "description": "Computes the QR decomposition of each inner matrix in `tensor` such that\n`tensor[..., :, :] = q[..., :, :] * r[..., :,:])`\n\nCurrently, the gradient for the QR decomposition is well-defined only when\nthe first `P` columns of the inner matrix are linearly independent, where\n`P` is the minimum of `M` and `N`, the 2 inner-most dimmensions of `tensor`.\n\n```python\n# a is a tensor.\n# q is a tensor of orthonormal matrices.\n# r is a tensor of upper triangular matrices.\nq, r = qr(a)\nq_full, r_full = qr(a, full_matrices=True)\n```",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "q", "type": "Res" },
      { "name": "r", "type": "Res" }
    ],
    "attributes": [
      { "name": "full_matrices", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.QuantizeAndDequantize",
    "summary": "Use QuantizeAndDequantizeV2 instead.",
    "inputs": [
      { "name": "input", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "signed_input", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "range_given", "type": "DefaultValuedOptionalAttr" },
      { "name": "input_min", "type": "DefaultValuedOptionalAttr" },
      { "name": "input_max", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.QuantizeAndDequantizeV2",
    "summary": "Quantizes then dequantizes a tensor.",
    "description": "This op simulates the precision loss from the quantized forward pass by:\n\n1. Quantizing the tensor to fixed point numbers, which should match the target\n   quantization method when it is used in inference.\n2. Dequantizing it back to floating point numbers for the following ops, most\n   likely matmul.\n\nThere are different ways to quantize. This version uses only scaling, so 0.0\nmaps to 0.\n\nFrom the specified 'num_bits' in the quantized output type, it determines\nminimum and maximum representable quantized values.\n\ne.g.\n\n*   [-128, 127] for signed, num_bits = 8, or\n*   [0, 255] for unsigned, num_bits = 8.\n\nIf range_given == False, the initial input_min, input_max will be determined\nautomatically as the minimum and maximum values in the input tensor, otherwise\nthe specified values of input_min, input_max are used.\n\nNote: If the input_min, input_max are specified, they do not need to equal the\nactual minimum and maximum values in the tensor. e.g. in some cases it may be\nbeneficial to specify these values such that the low probability extremes of the\ninput distribution are clipped.\n\nThis op determines the maximum scale_factor that would map the initial\n[input_min, input_max] range to a range that lies within the representable\nquantized range.\n\nIt determines the scale from one of input_min and input_max, then updates the\nother one to maximize the representable range.\n\ne.g.\n\n*   if the output is signed, num_bits = 8, [input_min, input_max] = [-10.0,\n    5.0]: it would use a scale_factor of -128 / -10.0 = 12.8 In this case, it\n    would update input_max to be 127 / 12.8 = 9.921875\n*   if the output is signed, num_bits = 8, [input_min, input_max] = [-10.0,\n    10.0]: it would use a scale_factor of 127 / 10.0 = 12.7 In this case, it\n    would update input_min to be 128.0 / 12.7 = -10.07874\n*   if the output is unsigned, input_min is forced to be 0, and only the\n    specified input_max is used.\n\nAfter determining the scale_factor and updating the input range, it applies the\nfollowing to each value in the 'input' tensor.\n\noutput = round(clamp(value, input_min, input_max) * scale_factor) / scale_factor.\n\nThe above round function rounds the value based on the given round_mode.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "input_min", "type": "Arg" },
      { "name": "input_max", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "signed_input", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "range_given", "type": "DefaultValuedOptionalAttr" },
      { "name": "round_mode", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" },
      { "name": "axis", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.QuantizeAndDequantizeV3",
    "summary": "Quantizes then dequantizes a tensor.",
    "description": "This is almost identical to QuantizeAndDequantizeV2, except that num_bits is a\ntensor, so its value can change during training.",
    "inputs": [
      { "name": "input", "type": "TF_FloatTensor" },
      { "name": "input_min", "type": "TF_FloatTensor" },
      { "name": "input_max", "type": "TF_FloatTensor" },
      { "name": "num_bits", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "signed_input", "type": "DefaultValuedOptionalAttr" },
      { "name": "range_given", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" },
      { "name": "axis", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.QuantizeAndDequantizeV4",
    "summary": "Quantizes then dequantizes a tensor.",
    "description": "This is almost identical to QuantizeAndDequantizeV2, except that it returns a\ngradient of 1 for inputs that are within the quantization range, or 0 otherwise.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "input_min", "type": "Arg" },
      { "name": "input_max", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "signed_input", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_bits", "type": "DefaultValuedOptionalAttr" },
      { "name": "range_given", "type": "DefaultValuedOptionalAttr" },
      { "name": "round_mode", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" },
      { "name": "axis", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.QuantizeV2",
    "summary": "Quantize the 'input' tensor of type float to 'output' tensor of type 'T'.",
    "description": "[min_range, max_range] are scalar floats that specify the range for\nthe 'input' data. The 'mode' attribute controls exactly which calculations are\nused to convert the float values to their quantized equivalents.  The\n'round_mode' attribute controls which rounding tie-breaking algorithm is used\nwhen rounding float values to their quantized equivalents.\n\nIn 'MIN_COMBINED' mode, each value of the tensor will undergo the following:\n\n```\nout[i] = (in[i] - min_range) * range(T) / (max_range - min_range)\nif T == qint8: out[i] -= (range(T) + 1) / 2.0\n```\n\nhere `range(T) = numeric_limits<T>::max() - numeric_limits<T>::min()`\n\n*MIN_COMBINED Mode Example*\n\nAssume the input is type float and has a possible range of [0.0, 6.0] and the\noutput type is quint8 ([0, 255]). The min_range and max_range values should be\nspecified as 0.0 and 6.0. Quantizing from float to quint8 will multiply each\nvalue of the input by 255/6 and cast to quint8.\n\nIf the output type was qint8 ([-128, 127]), the operation will additionally\nsubtract each value by 128 prior to casting, so that the range of values aligns\nwith the range of qint8.\n\nIf the mode is 'MIN_FIRST', then this approach is used:\n\n```\nnum_discrete_values = 1 << (# of bits in T)\nrange_adjust = num_discrete_values / (num_discrete_values - 1)\nrange = (range_max - range_min) * range_adjust\nrange_scale = num_discrete_values / range\nquantized = round(input * range_scale) - round(range_min * range_scale) +\n  numeric_limits<T>::min()\nquantized = max(quantized, numeric_limits<T>::min())\nquantized = min(quantized, numeric_limits<T>::max())\n```\n\nThe biggest difference between this and MIN_COMBINED is that the minimum range\nis rounded first, before it's subtracted from the rounded value. With\nMIN_COMBINED, a small bias is introduced where repeated iterations of quantizing\nand dequantizing will introduce a larger and larger error.\n\n*SCALED mode Example*\n\n`SCALED` mode matches the quantization approach used in\n`QuantizeAndDequantize{V2|V3}`.\n\nIf the mode is `SCALED`, the quantization is performed by multiplying each\ninput value by a scaling_factor.\nThe scaling_factor is determined from `min_range` and `max_range` to be as large\nas possible such that the range from `min_range` to `max_range` is representable\nwithin values of type T.\n\n```c++\n\n  const int min_T = std::numeric_limits<T>::min();\n  const int max_T = std::numeric_limits<T>::max();\n  const float max_float = std::numeric_limits<float>::max();\n\n  const float scale_factor_from_min_side =\n      (min_T * min_range > 0) ? min_T / min_range : max_float;\n  const float scale_factor_from_max_side =\n      (max_T * max_range > 0) ? max_T / max_range : max_float;\n\n  const float scale_factor = std::min(scale_factor_from_min_side,\n                                      scale_factor_from_max_side);\n```\n\nWe next use the scale_factor to adjust min_range and max_range as follows:\n\n```c++\n      min_range = min_T / scale_factor;\n      max_range = max_T / scale_factor;\n```\n\n\ne.g. if T = qint8, and initially min_range = -10, and max_range = 9, we would\ncompare -128/-10.0 = 12.8 to 127/9.0 = 14.11, and set scaling_factor = 12.8\nIn this case, min_range would remain -10, but max_range would be adjusted to\n127 / 12.8 = 9.921875\n\nSo we will quantize input values in the range (-10, 9.921875) to (-128, 127).\n\nThe input tensor can now be quantized by clipping values to the range\n`min_range` to `max_range`, then multiplying by scale_factor as follows:\n\n```c++\nresult = round(min(max_range, max(min_range, input)) * scale_factor)\n```\n\nThe adjusted `min_range` and `max_range` are returned as outputs 2 and 3 of\nthis operation. These outputs should be used as the range for any further\ncalculations.\n\n\n*narrow_range (bool) attribute*\n\nIf true, we do not use the minimum quantized value.\ni.e. for int8 the quantized output, it would be restricted to the range\n-127..127 instead of the full -128..127 range.\nThis is provided for compatibility with certain inference backends.\n(Only applies to SCALED mode)\n\n\n*axis (int) attribute*\n\nAn optional `axis` attribute can specify a dimension index of the input tensor,\nsuch that quantization ranges will be calculated and applied separately for each\nslice of the tensor along that dimension. This is useful for per-channel\nquantization.\n\nIf axis is specified, min_range and max_range\n\nif `axis`=None, per-tensor quantization is performed as normal.\n\n\n*ensure_minimum_range (float) attribute*\n\nEnsures the minimum quantization range is at least this value.\nThe legacy default value for this is 0.01, but it is strongly suggested to\nset it to 0 for new uses.",
    "inputs": [
      { "name": "input", "type": "TF_Float32Tensor" },
      { "name": "min_range", "type": "Arg" },
      { "name": "max_range", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" },
      { "name": "output_min", "type": "Res" },
      { "name": "output_max", "type": "Res" }
    ],
    "attributes": [
      { "name": "mode", "type": "DefaultValuedOptionalAttr" },
      { "name": "round_mode", "type": "DefaultValuedOptionalAttr" },
      { "name": "narrow_range", "type": "DefaultValuedOptionalAttr" },
      { "name": "axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "ensure_minimum_range", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.QueueDequeueV2",
    "summary": "Dequeues a tuple of one or more tensors from the given queue.",
    "description": "This operation has k outputs, where k is the number of components\nin the tuples stored in the given queue, and output i is the ith\ncomponent of the dequeued tuple.\n\nN.B. If the queue is empty, this operation will block until an element\nhas been dequeued (or 'timeout_ms' elapses, if specified).",
    "inputs": [
      { "name": "handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "components", "type": "Res" }
    ],
    "attributes": [
      { "name": "timeout_ms", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RaggedGather",
    "summary": "Gather ragged slices from `params` axis `0` according to `indices`.",
    "description": "Outputs a `RaggedTensor` output composed from `output_dense_values` and\n`output_nested_splits`, such that:\n\n```python\noutput.shape = indices.shape + params.shape[1:]\noutput.ragged_rank = indices.shape.ndims + params.ragged_rank\noutput[i...j, d0...dn] = params[indices[i...j], d0...dn]\n```\n\nwhere\n\n* `params =\n   ragged.from_nested_row_splits(params_dense_values, params_nested_splits)`\n   provides the values that should be gathered.\n* `indices` ia a dense tensor with dtype `int32` or `int64`, indicating which\n   values should be gathered.\n* `output =\n   ragged.from_nested_row_splits(output_dense_values, output_nested_splits)`\n   is the output tensor.\n\n(Note: This c++ op is used to implement the higher-level python\n`tf.ragged.gather` op, which also supports ragged indices.)",
    "inputs": [
      { "name": "params_nested_splits", "type": "Arg" },
      { "name": "params_dense_values", "type": "Arg" },
      { "name": "indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output_nested_splits", "type": "Res" },
      { "name": "output_dense_values", "type": "Res" }
    ]
  },
  {
    "name": "tf.RaggedRange",
    "summary": "Returns a `RaggedTensor` containing the specified sequences of numbers.",
    "description": "Returns a `RaggedTensor` `result` composed from `rt_dense_values` and\n`rt_nested_splits`, such that\n`result[i] = range(starts[i], limits[i], deltas[i])`.\n\n```python\n(rt_nested_splits, rt_dense_values) = ragged_range(\n      starts=[2, 5, 8], limits=[3, 5, 12], deltas=1)\nresult = tf.ragged.from_row_splits(rt_dense_values, rt_nested_splits)\nprint(result)\n<tf.RaggedTensor [[2], [], [8, 9, 10, 11]] >\n```\n\nThe input tensors `starts`, `limits`, and `deltas` may be scalars or vectors.\nThe vector inputs must all have the same size.  Scalar inputs are broadcast\nto match the size of the vector inputs.",
    "inputs": [
      { "name": "starts", "type": "Arg" },
      { "name": "limits", "type": "Arg" },
      { "name": "deltas", "type": "Arg" }
    ],
    "outputs": [
      { "name": "rt_nested_splits", "type": "Res" },
      { "name": "rt_dense_values", "type": "Res" }
    ]
  },
  {
    "name": "tf.RandomGamma",
    "summary": "Outputs random values from the Gamma distribution(s) described by alpha.",
    "description": "This op uses the algorithm by Marsaglia et al. to acquire samples via\ntransformation-rejection from pairs of uniform and normal random variables.\nSee http://dl.acm.org/citation.cfm?id=358414",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "alpha", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RandomGammaGrad",
    "summary": "Computes the derivative of a Gamma random sample w.r.t. `alpha`.",
    "inputs": [
      { "name": "alpha", "type": "TF_F32OrF64Tensor" },
      { "name": "sample", "type": "TF_F32OrF64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.RandomPoisson",
    "summary": "Use RandomPoissonV2 instead.",
    "inputs": [
      { "name": "shape", "type": "TF_I32OrI64Tensor" },
      { "name": "rate", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RandomPoissonV2",
    "summary": "Outputs random values from the Poisson distribution(s) described by rate.",
    "description": "This op uses two algorithms, depending on rate. If rate >= 10, then\nthe algorithm by Hormann is used to acquire samples via\ntransformation-rejection.\nSee http://www.sciencedirect.com/science/article/pii/0167668793909974.\n\nOtherwise, Knuth's algorithm is used to acquire samples via multiplying uniform\nrandom variables.\nSee Donald E. Knuth (1969). Seminumerical Algorithms. The Art of Computer\nProgramming, Volume 2. Addison Wesley",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "rate", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RandomShuffle",
    "summary": "Randomly shuffles a tensor along its first dimension.",
    "description": "The tensor is shuffled along dimension 0, such that each `value[j]` is mapped\n  to one and only one `output[i]`. For example, a mapping that might occur for a\n  3x2 tensor is:\n\n```\n[[1, 2],       [[5, 6],\n [3, 4],  ==>   [1, 2],\n [5, 6]]        [3, 4]]\n```",
    "inputs": [
      { "name": "value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RandomStandardNormal",
    "summary": "Outputs random values from a normal distribution.",
    "description": "The generated values will have mean 0 and standard deviation 1.",
    "inputs": [
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RandomUniform",
    "summary": "Outputs random values from a uniform distribution.",
    "description": "The generated values follow a uniform distribution in the range `[0, 1)`. The\nlower bound 0 is included in the range, while the upper bound 1 is excluded.",
    "inputs": [
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RandomUniformInt",
    "summary": "Outputs random integers from a uniform distribution.",
    "description": "The generated values are uniform integers in the range `[minval, maxval)`.\nThe lower bound `minval` is included in the range, while the upper bound\n`maxval` is excluded.\n\nThe random integers are slightly biased unless `maxval - minval` is an exact\npower of two.  The bias is small for values of `maxval - minval` significantly\nsmaller than the range of the output (either `2^32` or `2^64`).",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "minval", "type": "Arg" },
      { "name": "maxval", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Range",
    "summary": "Creates a sequence of numbers.",
    "description": "This operation creates a sequence of numbers that begins at `start` and\nextends by increments of `delta` up to but not including `limit`.\n\nFor example:\n\n```\n# 'start' is 3\n# 'limit' is 18\n# 'delta' is 3\ntf.range(start, limit, delta) ==> [3, 6, 9, 12, 15]\n```",
    "inputs": [
      { "name": "start", "type": "Arg" },
      { "name": "limit", "type": "Arg" },
      { "name": "delta", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.RangeDataset",
    "summary": "Creates a dataset with a range of values. Corresponds to python's xrange.",
    "inputs": [
      { "name": "start", "type": "Arg" },
      { "name": "stop", "type": "Arg" },
      { "name": "step", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" },
      { "name": "replicate_on_split", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Rank",
    "summary": "Returns the rank of a tensor.",
    "description": "This operation returns an integer representing the rank of `input`.\n\nFor example:\n\n```\n# 't' is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]\n# shape of tensor 't' is [2, 2, 3]\nrank(t) ==> 3\n```\n\n**Note**: The rank of a tensor is not the same as the rank of a matrix. The rank\nof a tensor is the number of indices required to uniquely select each element\nof the tensor. Rank is also known as \"order\", \"degree\", or \"ndims.\"",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Int32Tensor" }
    ]
  },
  {
    "name": "tf.ReadFile",
    "summary": "Reads and outputs the entire contents of the input filename.",
    "inputs": [
      { "name": "filename", "type": "TF_StrTensor" }
    ],
    "outputs": [
      { "name": "contents", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.ReadVariableOp",
    "summary": "Reads the value of a variable.",
    "description": "The tensor returned by this operation is immutable.\n\nThe value returned by this operation is guaranteed to be influenced by all the\nwrites on which this operation depends directly or indirectly, and to not be\ninfluenced by any of the writes which depend directly or indirectly on this\noperation.",
    "inputs": [
      { "name": "resource", "type": "Arg" }
    ],
    "outputs": [
      { "name": "value", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.Real",
    "summary": "Returns the real part of a complex number.",
    "description": "Given a tensor `input` of complex numbers, this operation returns a tensor of\ntype `float` that is the real part of each element in `input`. All elements in\n`input` must be complex numbers of the form \\\\(a + bj\\\\), where *a* is the real\n part returned by this operation and *b* is the imaginary part.\n\nFor example:\n\n```\n# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]\ntf.real(input) ==> [-2.25, 3.25]\n```",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tf.RealDiv",
    "summary": "Returns x / y element-wise for real types.",
    "description": "If `x` and `y` are reals, this will return the floating-point division.\n\n*NOTE*: `Div` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.Reciprocal",
    "summary": "Computes the reciprocal of x element-wise.",
    "description": "I.e., \\\\(y = 1 / x\\\\).",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.ReciprocalGrad",
    "summary": "Computes the gradient for the inverse of `x` wrt its input.",
    "description": "Specifically, `grad = -dy * y*y`, where `y = 1/x`, and `dy`\nis the corresponding input gradient.",
    "inputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" },
      { "name": "dy", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Recv",
    "summary": "Receives the named tensor from send_device on recv_device.",
    "outputs": [
      { "name": "tensor", "type": "Res" }
    ],
    "attributes": [
      { "name": "tensor_name", "type": "StrAttr" },
      { "name": "send_device", "type": "StrAttr" },
      { "name": "send_device_incarnation", "type": "I64Attr" },
      { "name": "recv_device", "type": "StrAttr" },
      { "name": "client_terminated", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RecvTPUEmbeddingActivations",
    "summary": "An op that receives embedding activations on the TPU.",
    "description": "The TPU system performs the embedding lookups and aggregations specified by\nthe arguments to TPUEmbeddingEnqueue(Integer/Sparse/SparseTensor)Batch. The\nresults of these aggregations are visible to the Tensorflow Graph as the\noutputs of a RecvTPUEmbeddingActivations op. This op returns a list containing\none Tensor of activations per table specified in the model. There can be at\nmost one RecvTPUEmbeddingActivations op in the TPU graph.",
    "outputs": [
      { "name": "outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.ReduceDataset",
    "summary": "Reduces the input dataset to a singleton using a reduce function.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "initial_state", "type": "Variadic" },
      { "name": "other_arguments", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "components", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "Tstate", "type": "ConfinedAttr" },
      { "name": "Targuments", "type": "ConfinedAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "use_inter_op_parallelism", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ReduceJoin",
    "summary": "Joins a string Tensor across the given dimensions.",
    "description": "Computes the string join across dimensions in the given string Tensor of shape\n`[\\\\(d_0, d_1, ..., d_{n-1}\\\\)]`.  Returns a new Tensor created by joining the input\nstrings with the given separator (default: empty string).  Negative indices are\ncounted backwards from the end, with `-1` being equivalent to `n - 1`.  If\nindices are not specified, joins across all dimensions beginning from `n - 1`\nthrough `0`.\n\nFor example:\n\n```python\n# tensor `a` is [[\"a\", \"b\"], [\"c\", \"d\"]]\ntf.reduce_join(a, 0) ==> [\"ac\", \"bd\"]\ntf.reduce_join(a, 1) ==> [\"ab\", \"cd\"]\ntf.reduce_join(a, -2) = tf.reduce_join(a, 0) ==> [\"ac\", \"bd\"]\ntf.reduce_join(a, -1) = tf.reduce_join(a, 1) ==> [\"ab\", \"cd\"]\ntf.reduce_join(a, 0, keep_dims=True) ==> [[\"ac\", \"bd\"]]\ntf.reduce_join(a, 1, keep_dims=True) ==> [[\"ab\"], [\"cd\"]]\ntf.reduce_join(a, 0, separator=\".\") ==> [\"a.c\", \"b.d\"]\ntf.reduce_join(a, [0, 1]) ==> \"acbd\"\ntf.reduce_join(a, [1, 0]) ==> \"abcd\"\ntf.reduce_join(a, []) ==> [[\"a\", \"b\"], [\"c\", \"d\"]]\ntf.reduce_join(a) = tf.reduce_join(a, [1, 0]) ==> \"abcd\"\n```",
    "inputs": [
      { "name": "inputs", "type": "Arg" },
      { "name": "reduction_indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" },
      { "name": "separator", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Relu",
    "summary": "Computes rectified linear: `max(features, 0)`.",
    "description": "See: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\nExample usage:\n>>> tf.nn.relu([-2., 0., 3.]).numpy()\narray([0., 0., 3.], dtype=float32)",
    "inputs": [
      { "name": "features", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "activations", "type": "TensorOf" }
    ],
    "category": "Activation"
  },
  {
    "name": "tf.Relu6",
    "summary": "Computes rectified linear 6: `min(max(features, 0), 6)`.",
    "inputs": [
      { "name": "features", "type": "TF_IntOrFpTensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_IntOrFpTensor" }
    ]
  },
  {
    "name": "tf.Relu6Grad",
    "summary": "Computes rectified linear 6 gradients for a Relu6 operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "features", "type": "Arg" }
    ],
    "outputs": [
      { "name": "backprops", "type": "Res" }
    ]
  },
  {
    "name": "tf.ReluGrad",
    "summary": "Computes rectified linear gradients for a Relu operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "features", "type": "Arg" }
    ],
    "outputs": [
      { "name": "backprops", "type": "Res" }
    ]
  },
  {
    "name": "tf.RemoteCall",
    "summary": "Runs function `f` on a remote device indicated by `target`.",
    "inputs": [
      { "name": "target", "type": "Arg" },
      { "name": "args", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.RepeatDataset",
    "summary": "Creates a dataset that emits the outputs of `input_dataset` `count` times.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "count", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Reshape",
    "summary": "Reshapes a tensor.",
    "description": "Given `tensor`, this operation returns a tensor that has the same values\nas `tensor` with shape `shape`.\n\nIf one component of 1-D tensor `shape` is the special value -1, the size of that\ndimension is computed so that the total size remains constant.  In particular, a\n`shape` of `[-1]` flattens into 1-D.  At most one component of `shape` may be\nunknown.\n\nThe `shape` must be 1-D and the operation returns a tensor with shape\n`shape` filled with the values of `tensor`. In this case, the number of elements\nimplied by `shape` must be the same as the number of elements in `tensor`.\n\nIt is an error if `shape` is not 1-D.\n\nFor example:\n\n```\n# tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]\n# tensor 't' has shape [9]\nreshape(t, [3, 3]) ==> [[1, 2, 3],\n                        [4, 5, 6],\n                        [7, 8, 9]]\n\n# tensor 't' is [[[1, 1], [2, 2]],\n#                [[3, 3], [4, 4]]]\n# tensor 't' has shape [2, 2, 2]\nreshape(t, [2, 4]) ==> [[1, 1, 2, 2],\n                        [3, 3, 4, 4]]\n\n# tensor 't' is [[[1, 1, 1],\n#                 [2, 2, 2]],\n#                [[3, 3, 3],\n#                 [4, 4, 4]],\n#                [[5, 5, 5],\n#                 [6, 6, 6]]]\n# tensor 't' has shape [3, 2, 3]\n# pass '[-1]' to flatten 't'\nreshape(t, [-1]) ==> [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]\n\n# -1 can also be used to infer the shape\n\n# -1 is inferred to be 9:\nreshape(t, [2, -1]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n# -1 is inferred to be 2:\nreshape(t, [-1, 9]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n# -1 is inferred to be 3:\nreshape(t, [ 2, -1, 3]) ==> [[[1, 1, 1],\n                              [2, 2, 2],\n                              [3, 3, 3]],\n                             [[4, 4, 4],\n                              [5, 5, 5],\n                              [6, 6, 6]]]\n\n# tensor 't' is [7]\n# shape `[]` reshapes to a scalar\nreshape(t, []) ==> 7\n```",
    "inputs": [
      { "name": "tensor", "type": "TF_Tensor" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "category": "Shape"
  },
  {
    "name": "tf.ResizeBilinear",
    "summary": "Resize `images` to `size` using bilinear interpolation.",
    "description": "Input images can be of different types but output images are always float.",
    "inputs": [
      { "name": "images", "type": "Arg" },
      { "name": "size", "type": "Arg" }
    ],
    "outputs": [
      { "name": "resized_images", "type": "Res" }
    ],
    "attributes": [
      { "name": "align_corners", "type": "DefaultValuedOptionalAttr" },
      { "name": "half_pixel_centers", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResizeBilinearGrad",
    "summary": "Computes the gradient of bilinear interpolation.",
    "inputs": [
      { "name": "grads", "type": "Arg" },
      { "name": "original_image", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "align_corners", "type": "DefaultValuedOptionalAttr" },
      { "name": "half_pixel_centers", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResizeNearestNeighbor",
    "summary": "Resize `images` to `size` using nearest neighbor interpolation.",
    "inputs": [
      { "name": "images", "type": "Arg" },
      { "name": "size", "type": "Arg" }
    ],
    "outputs": [
      { "name": "resized_images", "type": "Res" }
    ],
    "attributes": [
      { "name": "align_corners", "type": "DefaultValuedOptionalAttr" },
      { "name": "half_pixel_centers", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResizeNearestNeighborGrad",
    "summary": "Computes the gradient of nearest neighbor interpolation.",
    "inputs": [
      { "name": "grads", "type": "Arg" },
      { "name": "size", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "align_corners", "type": "DefaultValuedOptionalAttr" },
      { "name": "half_pixel_centers", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyAdadelta",
    "summary": "Update '*var' according to the adadelta scheme.",
    "description": "accum = rho() * accum + (1 - rho()) * grad.square();\nupdate = (update_accum + epsilon).sqrt() * (accum + epsilon()).rsqrt() * grad;\nupdate_accum = rho() * update_accum + (1 - rho()) * update.square();\nvar -= update;",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "accum_update", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "rho", "type": "Arg" },
      { "name": "epsilon", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyAdagrad",
    "summary": "Update '*var' according to the adagrad scheme.",
    "description": "accum += grad * grad\nvar -= lr * grad * (1 / sqrt(accum))",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "update_slots", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyAdagradDA",
    "summary": "Update '*var' according to the proximal adagrad scheme.",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "gradient_accumulator", "type": "Arg" },
      { "name": "gradient_squared_accumulator", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "l1", "type": "Arg" },
      { "name": "l2", "type": "Arg" },
      { "name": "global_step", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyAdagradV2",
    "summary": "Update '*var' according to the adagrad scheme.",
    "description": "accum += grad * grad\nvar -= lr * grad * (1 / (sqrt(accum) + epsilon))",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "epsilon", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "update_slots", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyAdam",
    "summary": "Update '*var' according to the Adam algorithm.",
    "description": "$$\\text{lr}_t := \\mathrm{lr} \\cdot \\frac{\\sqrt{1 - \\beta_2^t}}{1 - \\beta_1^t}$$\n$$m_t := \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g$$\n$$v_t := \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g^2$$\n$$\\text{var} := \\begin{cases} \\text{var} - (m_t \\beta_1 + g \\cdot (1 - \\beta_1))\\cdot\\text{lr}_t/(\\sqrt{v_t} + \\epsilon), &\\text{if use_nesterov}\\\\\\\\  \\text{var} - m_t \\cdot \\text{lr}_t /(\\sqrt{v_t} + \\epsilon), &\\text{otherwise} \\end{cases}$$",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "m", "type": "Arg" },
      { "name": "v", "type": "Arg" },
      { "name": "beta1_power", "type": "Arg" },
      { "name": "beta2_power", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "beta1", "type": "Arg" },
      { "name": "beta2", "type": "Arg" },
      { "name": "epsilon", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_nesterov", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyAdaMax",
    "summary": "Update '*var' according to the AdaMax algorithm.",
    "description": "m_t <- beta1 * m_{t-1} + (1 - beta1) * g\nv_t <- max(beta2 * v_{t-1}, abs(g))\nvariable <- variable - learning_rate / (1 - beta1^t) * m_t / (v_t + epsilon)",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "m", "type": "Arg" },
      { "name": "v", "type": "Arg" },
      { "name": "beta1_power", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "beta1", "type": "Arg" },
      { "name": "beta2", "type": "Arg" },
      { "name": "epsilon", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyAddSign",
    "summary": "Update '*var' according to the AddSign update.",
    "description": "m_t <- beta1 * m_{t-1} + (1 - beta1) * g\nupdate <- (alpha + sign_decay * sign(g) *sign(m)) * g\nvariable <- variable - lr_t * update",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "m", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "alpha", "type": "Arg" },
      { "name": "sign_decay", "type": "Arg" },
      { "name": "beta", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyCenteredRMSProp",
    "summary": "Update '*var' according to the centered RMSProp algorithm.",
    "description": "The centered RMSProp algorithm uses an estimate of the centered second moment\n(i.e., the variance) for normalization, as opposed to regular RMSProp, which\nuses the (uncentered) second moment. This often helps with training, but is\nslightly more expensive in terms of computation and memory.\n\nNote that in dense implementation of this algorithm, mg, ms, and mom will\nupdate even if the grad is zero, but in this sparse implementation, mg, ms,\nand mom will not update in iterations during which the grad is zero.\n\nmean_square = decay * mean_square + (1-decay) * gradient ** 2\nmean_grad = decay * mean_grad + (1-decay) * gradient\n\nDelta = learning_rate * gradient / sqrt(mean_square + epsilon - mean_grad ** 2)\n\nmg <- rho * mg_{t-1} + (1-rho) * grad\nms <- rho * ms_{t-1} + (1-rho) * grad * grad\nmom <- momentum * mom_{t-1} + lr * grad / sqrt(ms - mg * mg + epsilon)\nvar <- var - mom",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "mg", "type": "Arg" },
      { "name": "ms", "type": "Arg" },
      { "name": "mom", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "rho", "type": "Arg" },
      { "name": "momentum", "type": "Arg" },
      { "name": "epsilon", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyFtrl",
    "summary": "Update '*var' according to the Ftrl-proximal scheme.",
    "description": "accum_new = accum + grad * grad\nlinear += grad - (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var\nquadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2\nvar = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0\naccum = accum_new",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "linear", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "l1", "type": "Arg" },
      { "name": "l2", "type": "Arg" },
      { "name": "lr_power", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "multiply_linear_by_lr", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyFtrlV2",
    "summary": "Update '*var' according to the Ftrl-proximal scheme.",
    "description": "accum_new = accum + grad * grad\ngrad_with_shrinkage = grad + 2 * l2_shrinkage * var\nlinear += grad_with_shrinkage +\n    (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var\nquadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2\nvar = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0\naccum = accum_new",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "linear", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "l1", "type": "Arg" },
      { "name": "l2", "type": "Arg" },
      { "name": "l2_shrinkage", "type": "TF_NumberTensor" },
      { "name": "lr_power", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "multiply_linear_by_lr", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyGradientDescent",
    "summary": "Update '*var' by subtracting 'alpha' * 'delta' from it.",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "alpha", "type": "Arg" },
      { "name": "delta", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyKerasMomentum",
    "summary": "Update '*var' according to the momentum scheme.",
    "description": "Set use_nesterov = True if you want to use Nesterov momentum.\n\naccum = accum * momentum - lr * grad\nvar += accum",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "momentum", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_nesterov", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyMomentum",
    "summary": "Update '*var' according to the momentum scheme.",
    "description": "Set use_nesterov = True if you want to use Nesterov momentum.\n\naccum = accum * momentum + grad\nvar -= lr * accum",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "momentum", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_nesterov", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyPowerSign",
    "summary": "Update '*var' according to the AddSign update.",
    "description": "m_t <- beta1 * m_{t-1} + (1 - beta1) * g\nupdate <- exp(logbase * sign_decay * sign(g) * sign(m_t)) * g\nvariable <- variable - lr_t * update",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "m", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "logbase", "type": "Arg" },
      { "name": "sign_decay", "type": "Arg" },
      { "name": "beta", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyProximalAdagrad",
    "summary": "Update '*var' and '*accum' according to FOBOS with Adagrad learning rate.",
    "description": "accum += grad * grad\nprox_v = var - lr * grad * (1 / sqrt(accum))\nvar = sign(prox_v)/(1+lr*l2) * max{|prox_v|-lr*l1,0}",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "l1", "type": "Arg" },
      { "name": "l2", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyProximalGradientDescent",
    "summary": "Update '*var' as FOBOS algorithm with fixed learning rate.",
    "description": "prox_v = var - alpha * delta\nvar = sign(prox_v)/(1+alpha*l2) * max{|prox_v|-alpha*l1,0}",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "alpha", "type": "Arg" },
      { "name": "l1", "type": "Arg" },
      { "name": "l2", "type": "Arg" },
      { "name": "delta", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceApplyRMSProp",
    "summary": "Update '*var' according to the RMSProp algorithm.",
    "description": "Note that in dense implementation of this algorithm, ms and mom will\nupdate even if the grad is zero, but in this sparse implementation, ms\nand mom will not update in iterations during which the grad is zero.\n\nmean_square = decay * mean_square + (1-decay) * gradient ** 2\nDelta = learning_rate * gradient / sqrt(mean_square + epsilon)\n\nms <- rho * ms_{t-1} + (1-rho) * grad * grad\nmom <- momentum * mom_{t-1} + lr * grad / sqrt(ms + epsilon)\nvar <- var - mom",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "ms", "type": "Arg" },
      { "name": "mom", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "rho", "type": "Arg" },
      { "name": "momentum", "type": "TF_NumberTensor" },
      { "name": "epsilon", "type": "Arg" },
      { "name": "grad", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceGather",
    "summary": "Gather slices from the variable pointed to by `resource` according to `indices`.",
    "description": "`indices` must be an integer tensor of any dimension (usually 0-D or 1-D).\nProduces an output tensor with shape `indices.shape + params.shape[1:]` where:\n\n```python\n    # Scalar indices\n    output[:, ..., :] = params[indices, :, ... :]\n\n    # Vector indices\n    output[i, :, ..., :] = params[indices[i], :, ... :]\n\n    # Higher rank indices\n    output[i, ..., j, :, ... :] = params[indices[i, ..., j], :, ..., :]\n```",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "batch_dims", "type": "DefaultValuedOptionalAttr" },
      { "name": "validate_indices", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceGatherNd",
    "summary": "GatherNd on a resource.",
    "description": "This op reads the variable referenced by the first argument, and\nthen performs a GatherNd operation on it.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.ResourceScatterAdd",
    "summary": "Adds sparse updates to the variable referenced by `resource`.",
    "description": "This operation computes\n\n    # Scalar indices\n    ref[indices, ...] += updates[...]\n\n    # Vector indices (for each i)\n    ref[indices[i], ...] += updates[i, ...]\n\n    # High rank indices (for each i, ..., j)\n    ref[indices[i, ..., j], ...] += updates[i, ..., j, ...]\n\nDuplicate entries are handled correctly: if multiple `indices` reference\nthe same location, their contributions add.\n\nRequires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src='https://www.tensorflow.org/images/ScatterAdd.png' alt>\n</div>",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ]
  },
  {
    "name": "tf.ResourceScatterDiv",
    "summary": "Divides sparse updates into the variable referenced by `resource`.",
    "description": "This operation computes\n\n    # Scalar indices\n    ref[indices, ...] /= updates[...]\n\n    # Vector indices (for each i)\n    ref[indices[i], ...] /= updates[i, ...]\n\n    # High rank indices (for each i, ..., j)\n    ref[indices[i, ..., j], ...] /= updates[i, ..., j, ...]\n\nDuplicate entries are handled correctly: if multiple `indices` reference\nthe same location, their contributions multiply.\n\nRequires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src='https://www.tensorflow.org/images/ScatterAdd.png' alt>\n</div>",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ]
  },
  {
    "name": "tf.ResourceScatterMax",
    "summary": "Reduces sparse updates into the variable referenced by `resource` using the `max` operation.",
    "description": "This operation computes\n\n    # Scalar indices\n    ref[indices, ...] = max(ref[indices, ...], updates[...])\n\n    # Vector indices (for each i)\n    ref[indices[i], ...] = max(ref[indices[i], ...], updates[i, ...])\n\n    # High rank indices (for each i, ..., j)\n    ref[indices[i, ..., j], ...] = max(ref[indices[i, ..., j], ...], updates[i, ..., j, ...])\n\nDuplicate entries are handled correctly: if multiple `indices` reference\nthe same location, their contributions are combined.\n\nRequires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src='https://www.tensorflow.org/images/ScatterAdd.png' alt>\n</div>",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ]
  },
  {
    "name": "tf.ResourceScatterMin",
    "summary": "Reduces sparse updates into the variable referenced by `resource` using the `min` operation.",
    "description": "This operation computes\n\n    # Scalar indices\n    ref[indices, ...] = min(ref[indices, ...], updates[...])\n\n    # Vector indices (for each i)\n    ref[indices[i], ...] = min(ref[indices[i], ...], updates[i, ...])\n\n    # High rank indices (for each i, ..., j)\n    ref[indices[i, ..., j], ...] = min(ref[indices[i, ..., j], ...], updates[i, ..., j, ...])\n\nDuplicate entries are handled correctly: if multiple `indices` reference\nthe same location, their contributions are combined.\n\nRequires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src='https://www.tensorflow.org/images/ScatterAdd.png' alt>\n</div>",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ]
  },
  {
    "name": "tf.ResourceScatterMul",
    "summary": "Multiplies sparse updates into the variable referenced by `resource`.",
    "description": "This operation computes\n\n    # Scalar indices\n    ref[indices, ...] *= updates[...]\n\n    # Vector indices (for each i)\n    ref[indices[i], ...] *= updates[i, ...]\n\n    # High rank indices (for each i, ..., j)\n    ref[indices[i, ..., j], ...] *= updates[i, ..., j, ...]\n\nDuplicate entries are handled correctly: if multiple `indices` reference\nthe same location, their contributions multiply.\n\nRequires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src='https://www.tensorflow.org/images/ScatterAdd.png' alt>\n</div>",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ]
  },
  {
    "name": "tf.ResourceScatterNdAdd",
    "summary": "Applies sparse addition to individual values or slices in a Variable.",
    "description": "`ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.\n\n`indices` must be integer tensor, containing indices into `ref`.\nIt must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n\nThe innermost dimension of `indices` (with length `K`) corresponds to\nindices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\ndimension of `ref`.\n\n`updates` is `Tensor` of rank `Q-1+P-K` with shape:\n\n```\n[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]]\n```\n\nFor example, say we want to add 4 scattered elements to a rank-1 tensor to\n8 elements. In Python, that addition would look like this:\n\n```python\nref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8], use_resource=True)\nindices = tf.constant([[4], [3], [1], [7]])\nupdates = tf.constant([9, 10, 11, 12])\nadd = tf.scatter_nd_add(ref, indices, updates)\nwith tf.Session() as sess:\n  print sess.run(add)\n```\n\nThe resulting update to ref would look like this:\n\n    [1, 13, 3, 14, 14, 6, 7, 20]\n\nSee `tf.scatter_nd` for more details about how to make updates to\nslices.",
    "inputs": [
      { "name": "ref", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceScatterNdSub",
    "summary": "Applies sparse subtraction to individual values or slices in a Variable.",
    "description": "`ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.\n\n`indices` must be integer tensor, containing indices into `ref`.\nIt must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n\nThe innermost dimension of `indices` (with length `K`) corresponds to\nindices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\ndimension of `ref`.\n\n`updates` is `Tensor` of rank `Q-1+P-K` with shape:\n\n```\n[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]]\n```\n\nFor example, say we want to subtract 4 scattered elements from a rank-1 tensor\nwith 8 elements. In Python, that subtraction would look like this:\n\n```python\nref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8], use_resource=True)\nindices = tf.constant([[4], [3], [1], [7]])\nupdates = tf.constant([9, 10, 11, 12])\nsub = tf.scatter_nd_sub(ref, indices, updates)\nwith tf.Session() as sess:\n  print sess.run(sub)\n```\n\nThe resulting update to ref would look like this:\n\n    [1, -9, 3, -6, -4, 6, 7, -4]\n\nSee `tf.scatter_nd` for more details about how to make updates to\nslices.",
    "inputs": [
      { "name": "ref", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceScatterNdUpdate",
    "summary": "Applies sparse `updates` to individual values or slices within a given",
    "description": "variable according to `indices`.\n\n`ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.\n\n`indices` must be integer tensor, containing indices into `ref`.\nIt must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n\nThe innermost dimension of `indices` (with length `K`) corresponds to\nindices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\ndimension of `ref`.\n\n`updates` is `Tensor` of rank `Q-1+P-K` with shape:\n\n```\n[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].\n```\n\nFor example, say we want to update 4 scattered elements to a rank-1 tensor to\n8 elements. In Python, that update would look like this:\n\n```python\n    ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n    indices = tf.constant([[4], [3], [1] ,[7]])\n    updates = tf.constant([9, 10, 11, 12])\n    update = tf.scatter_nd_update(ref, indices, updates)\n    with tf.Session() as sess:\n      print sess.run(update)\n```\n\nThe resulting update to ref would look like this:\n\n    [1, 11, 3, 10, 9, 6, 7, 12]\n\nSee `tf.scatter_nd` for more details about how to make updates to\nslices.",
    "inputs": [
      { "name": "ref", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceScatterSub",
    "summary": "Subtracts sparse updates from the variable referenced by `resource`.",
    "description": "This operation computes\n\n    # Scalar indices\n    ref[indices, ...] -= updates[...]\n\n    # Vector indices (for each i)\n    ref[indices[i], ...] -= updates[i, ...]\n\n    # High rank indices (for each i, ..., j)\n    ref[indices[i, ..., j], ...] -= updates[i, ..., j, ...]\n\nDuplicate entries are handled correctly: if multiple `indices` reference\nthe same location, their contributions add.\n\nRequires `updates.shape = indices.shape + ref.shape[1:]` or `updates.shape = []`.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src='https://www.tensorflow.org/images/ScatterAdd.png' alt>\n</div>",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ]
  },
  {
    "name": "tf.ResourceScatterUpdate",
    "summary": "Assigns sparse updates to the variable referenced by `resource`.",
    "description": "This operation computes\n\n    # Scalar indices\n    ref[indices, ...] = updates[...]\n\n    # Vector indices (for each i)\n    ref[indices[i], ...] = updates[i, ...]\n\n    # High rank indices (for each i, ..., j)\n    ref[indices[i, ..., j], ...] = updates[i, ..., j, ...]",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ]
  },
  {
    "name": "tf.ResourceSparseApplyAdagrad",
    "summary": "Update relevant entries in '*var' and '*accum' according to the adagrad scheme.",
    "description": "That is for rows we have grad for, we update var and accum as follows:\naccum += grad * grad\nvar -= lr * grad * (1 / sqrt(accum))",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "indices", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "update_slots", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceSparseApplyAdagradV2",
    "summary": "Update relevant entries in '*var' and '*accum' according to the adagrad scheme.",
    "description": "That is for rows we have grad for, we update var and accum as follows:\naccum += grad * grad\nvar -= lr * grad * (1 / sqrt(accum))",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "epsilon", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "indices", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "update_slots", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceSparseApplyFtrl",
    "summary": "Update relevant entries in '*var' according to the Ftrl-proximal scheme.",
    "description": "That is for rows we have grad for, we update var, accum and linear as follows:\naccum_new = accum + grad * grad\nlinear += grad - (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var\nquadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2\nvar = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0\naccum = accum_new",
    "inputs": [
      { "name": "var", "type": "Arg" },
      { "name": "accum", "type": "Arg" },
      { "name": "linear", "type": "Arg" },
      { "name": "grad", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "lr", "type": "Arg" },
      { "name": "l1", "type": "Arg" },
      { "name": "l2", "type": "Arg" },
      { "name": "lr_power", "type": "Arg" }
    ],
    "attributes": [
      { "name": "use_locking", "type": "DefaultValuedOptionalAttr" },
      { "name": "multiply_linear_by_lr", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ResourceStridedSliceAssign",
    "summary": "Assign `value` to the sliced l-value reference of `ref`.",
    "description": "The values of `value` are assigned to the positions in the variable\n`ref` that are selected by the slice parameters. The slice parameters\n`begin, `end`, `strides`, etc. work exactly as in `StridedSlice`.\n\nNOTE this op currently does not support broadcasting and so `value`'s\nshape must be exactly the shape produced by the slice of `ref`.",
    "inputs": [
      { "name": "ref", "type": "Arg" },
      { "name": "begin", "type": "TF_I32OrI64Tensor" },
      { "name": "end", "type": "TF_I32OrI64Tensor" },
      { "name": "strides", "type": "TF_I32OrI64Tensor" },
      { "name": "value", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "begin_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "end_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "ellipsis_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "new_axis_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "shrink_axis_mask", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Restore",
    "summary": "Restores a tensor from checkpoint files.",
    "description": "Reads a tensor stored in one or several files. If there are several files (for\ninstance because a tensor was saved as slices), `file_pattern` may contain\nwildcard symbols (`*` and `?`) in the filename portion only, not in the\ndirectory portion.\n\nIf a `file_pattern` matches several files, `preferred_shard` can be used to hint\nin which file the requested tensor is likely to be found. This op will first\nopen the file at index `preferred_shard` in the list of matching files and try\nto restore tensors from that file.  Only if some tensors or tensor slices are\nnot found in that first file, then the Op opens all the files. Setting\n`preferred_shard` to match the value passed as the `shard` input\nof a matching `Save` Op may speed up Restore.  This attribute only affects\nperformance, not correctness.  The default value -1 means files are processed in\norder.\n\nSee also `RestoreSlice`.",
    "inputs": [
      { "name": "file_pattern", "type": "Arg" },
      { "name": "tensor_name", "type": "Arg" }
    ],
    "outputs": [
      { "name": "tensor", "type": "Res" }
    ],
    "attributes": [
      { "name": "preferred_shard", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RestoreV2",
    "summary": "Restores tensors from a V2 checkpoint.",
    "description": "For backward compatibility with the V1 format, this Op currently allows\nrestoring from a V1 checkpoint as well:\n  - This Op first attempts to find the V2 index file pointed to by \"prefix\", and\n    if found proceed to read it as a V2 checkpoint;\n  - Otherwise the V1 read path is invoked.\nRelying on this behavior is not recommended, as the ability to fall back to read\nV1 might be deprecated and eventually removed.\n\nBy default, restores the named tensors in full.  If the caller wishes to restore\nspecific slices of stored tensors, \"shape_and_slices\" should be non-empty\nstrings and correspondingly well-formed.\n\nCallers must ensure all the named tensors are indeed stored in the checkpoint.",
    "inputs": [
      { "name": "prefix", "type": "Arg" },
      { "name": "tensor_names", "type": "Arg" },
      { "name": "shape_and_slices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "tensors", "type": "Res" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingAdadeltaParameters",
    "summary": "Retrieve Adadelta embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "accumulators", "type": "Res" },
      { "name": "updates", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingAdadeltaParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "accumulators", "type": "TF_Float32Tensor" },
      { "name": "updates", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingAdagradParameters",
    "summary": "Retrieve Adagrad embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "accumulators", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingAdagradParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "accumulators", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingADAMParameters",
    "summary": "Retrieve ADAM embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "momenta", "type": "Res" },
      { "name": "velocities", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingADAMParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "velocities", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingCenteredRMSPropParameters",
    "summary": "Retrieve centered RMSProp embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "ms", "type": "Res" },
      { "name": "mom", "type": "Res" },
      { "name": "mg", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingFTRLParameters",
    "summary": "Retrieve FTRL embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "accumulators", "type": "Res" },
      { "name": "linears", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingFTRLParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "accumulators", "type": "TF_Float32Tensor" },
      { "name": "linears", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingMDLAdagradLightParameters",
    "summary": "Retrieve MDL Adagrad Light embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "accumulators", "type": "Res" },
      { "name": "weights", "type": "Res" },
      { "name": "benefits", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingMomentumParameters",
    "summary": "Retrieve Momentum embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "momenta", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingMomentumParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingProximalAdagradParameters",
    "summary": "Retrieve proximal Adagrad embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "accumulators", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingProximalAdagradParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "accumulators", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingProximalYogiParameters",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "v", "type": "TF_Float32Tensor" },
      { "name": "m", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingProximalYogiParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "v", "type": "TF_Float32Tensor" },
      { "name": "m", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingRMSPropParameters",
    "summary": "Retrieve RMSProp embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" },
      { "name": "ms", "type": "Res" },
      { "name": "mom", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingRMSPropParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "ms", "type": "TF_Float32Tensor" },
      { "name": "mom", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingStochasticGradientDescentParameters",
    "summary": "Retrieve SGD embedding parameters.",
    "description": "An op that retrieves optimization parameters from embedding to host\nmemory. Must be preceded by a ConfigureTPUEmbeddingHost op that sets up\nthe correct embedding table configuration. For example, this op is\nused to retrieve updated parameters before saving a checkpoint.",
    "outputs": [
      { "name": "parameters", "type": "Res" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RetrieveTPUEmbeddingStochasticGradientDescentParametersGradAccumDebug",
    "outputs": [
      { "name": "parameters", "type": "TF_Float32Tensor" },
      { "name": "gradient_accumulators", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "DefaultValuedOptionalAttr" },
      { "name": "table_name", "type": "DefaultValuedOptionalAttr" },
      { "name": "num_shards", "type": "I64Attr" },
      { "name": "shard_id", "type": "I64Attr" },
      { "name": "config", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Reverse",
    "summary": "Reverses specific dimensions of a tensor.",
    "description": "Given a `tensor`, and a `bool` tensor `dims` representing the dimensions\nof `tensor`, this operation reverses each dimension i of `tensor` where\n`dims[i]` is `True`.\n\n`tensor` can have up to 8 dimensions. The number of dimensions\nof `tensor` must equal the number of elements in `dims`. In other words:\n\n`rank(tensor) = size(dims)`\n\nFor example:\n\n```\n# tensor 't' is [[[[ 0,  1,  2,  3],\n#                  [ 4,  5,  6,  7],\n#                  [ 8,  9, 10, 11]],\n#                 [[12, 13, 14, 15],\n#                  [16, 17, 18, 19],\n#                  [20, 21, 22, 23]]]]\n# tensor 't' shape is [1, 2, 3, 4]\n\n# 'dims' is [False, False, False, True]\nreverse(t, dims) ==> [[[[ 3,  2,  1,  0],\n                        [ 7,  6,  5,  4],\n                        [ 11, 10, 9, 8]],\n                       [[15, 14, 13, 12],\n                        [19, 18, 17, 16],\n                        [23, 22, 21, 20]]]]\n\n# 'dims' is [False, True, False, False]\nreverse(t, dims) ==> [[[[12, 13, 14, 15],\n                        [16, 17, 18, 19],\n                        [20, 21, 22, 23]\n                       [[ 0,  1,  2,  3],\n                        [ 4,  5,  6,  7],\n                        [ 8,  9, 10, 11]]]]\n\n# 'dims' is [False, False, True, False]\nreverse(t, dims) ==> [[[[8, 9, 10, 11],\n                        [4, 5, 6, 7],\n                        [0, 1, 2, 3]]\n                       [[20, 21, 22, 23],\n                        [16, 17, 18, 19],\n                        [12, 13, 14, 15]]]]\n```",
    "inputs": [
      { "name": "tensor", "type": "Arg" },
      { "name": "dims", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.ReverseSequence",
    "summary": "Reverses variable length slices.",
    "description": "This op first slices `input` along the dimension `batch_dim`, and for each\nslice `i`, reverses the first `seq_lengths[i]` elements along\nthe dimension `seq_dim`.\n\nThe elements of `seq_lengths` must obey `seq_lengths[i] <= input.dims[seq_dim]`,\nand `seq_lengths` must be a vector of length `input.dims[batch_dim]`.\n\nThe output slice `i` along dimension `batch_dim` is then given by input\nslice `i`, with the first `seq_lengths[i]` slices along dimension\n`seq_dim` reversed.\n\nFor example:\n\n```\n# Given this:\nbatch_dim = 0\nseq_dim = 1\ninput.dims = (4, 8, ...)\nseq_lengths = [7, 2, 3, 5]\n\n# then slices of input are reversed on seq_dim, but only up to seq_lengths:\noutput[0, 0:7, :, ...] = input[0, 7:0:-1, :, ...]\noutput[1, 0:2, :, ...] = input[1, 2:0:-1, :, ...]\noutput[2, 0:3, :, ...] = input[2, 3:0:-1, :, ...]\noutput[3, 0:5, :, ...] = input[3, 5:0:-1, :, ...]\n\n# while entries past seq_lens are copied through:\noutput[0, 7:, :, ...] = input[0, 7:, :, ...]\noutput[1, 2:, :, ...] = input[1, 2:, :, ...]\noutput[2, 3:, :, ...] = input[2, 3:, :, ...]\noutput[3, 2:, :, ...] = input[3, 2:, :, ...]\n```\n\nIn contrast, if:\n\n```\n# Given this:\nbatch_dim = 2\nseq_dim = 0\ninput.dims = (8, ?, 4, ...)\nseq_lengths = [7, 2, 3, 5]\n\n# then slices of input are reversed on seq_dim, but only up to seq_lengths:\noutput[0:7, :, 0, :, ...] = input[7:0:-1, :, 0, :, ...]\noutput[0:2, :, 1, :, ...] = input[2:0:-1, :, 1, :, ...]\noutput[0:3, :, 2, :, ...] = input[3:0:-1, :, 2, :, ...]\noutput[0:5, :, 3, :, ...] = input[5:0:-1, :, 3, :, ...]\n\n# while entries past seq_lens are copied through:\noutput[7:, :, 0, :, ...] = input[7:, :, 0, :, ...]\noutput[2:, :, 1, :, ...] = input[2:, :, 1, :, ...]\noutput[3:, :, 2, :, ...] = input[3:, :, 2, :, ...]\noutput[2:, :, 3, :, ...] = input[2:, :, 3, :, ...]\n```",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "seq_lengths", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seq_dim", "type": "I64Attr" },
      { "name": "batch_dim", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ReverseV2",
    "summary": "Reverses specific dimensions of a tensor.",
    "description": "Given a `tensor`, and a `int32` tensor `axis` representing the set of\ndimensions of `tensor` to reverse. This operation reverses each dimension\n`i` for which there exists `j` s.t. `axis[j] == i`.\n\n`tensor` can have up to 8 dimensions. The number of dimensions specified\nin `axis` may be 0 or more entries. If an index is specified more than\nonce, a InvalidArgument error is raised.\n\nFor example:\n\n```\n# tensor 't' is [[[[ 0,  1,  2,  3],\n#                  [ 4,  5,  6,  7],\n#                  [ 8,  9, 10, 11]],\n#                 [[12, 13, 14, 15],\n#                  [16, 17, 18, 19],\n#                  [20, 21, 22, 23]]]]\n# tensor 't' shape is [1, 2, 3, 4]\n\n# 'dims' is [3] or 'dims' is [-1]\nreverse(t, dims) ==> [[[[ 3,  2,  1,  0],\n                        [ 7,  6,  5,  4],\n                        [ 11, 10, 9, 8]],\n                       [[15, 14, 13, 12],\n                        [19, 18, 17, 16],\n                        [23, 22, 21, 20]]]]\n\n# 'dims' is '[1]' (or 'dims' is '[-3]')\nreverse(t, dims) ==> [[[[12, 13, 14, 15],\n                        [16, 17, 18, 19],\n                        [20, 21, 22, 23]\n                       [[ 0,  1,  2,  3],\n                        [ 4,  5,  6,  7],\n                        [ 8,  9, 10, 11]]]]\n\n# 'dims' is '[2]' (or 'dims' is '[-2]')\nreverse(t, dims) ==> [[[[8, 9, 10, 11],\n                        [4, 5, 6, 7],\n                        [0, 1, 2, 3]]\n                       [[20, 21, 22, 23],\n                        [16, 17, 18, 19],\n                        [12, 13, 14, 15]]]]\n```",
    "inputs": [
      { "name": "tensor", "type": "Arg" },
      { "name": "axis", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.RFFT",
    "summary": "Real-valued fast Fourier transform.",
    "description": "Computes the 1-dimensional discrete Fourier transform of a real-valued signal\nover the inner-most dimension of `input`.\n\nSince the DFT of a real signal is Hermitian-symmetric, `RFFT` only returns the\n`fft_length / 2 + 1` unique components of the FFT: the zero-frequency term,\nfollowed by the `fft_length / 2` positive-frequency terms.\n\nAlong the axis `RFFT` is computed on, if `fft_length` is smaller than the\ncorresponding dimension of `input`, the dimension is cropped. If it is larger,\nthe dimension is padded with zeros.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "fft_length", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.RFFT2D",
    "summary": "2D real-valued fast Fourier transform.",
    "description": "Computes the 2-dimensional discrete Fourier transform of a real-valued signal\nover the inner-most 2 dimensions of `input`.\n\nSince the DFT of a real signal is Hermitian-symmetric, `RFFT2D` only returns the\n`fft_length / 2 + 1` unique components of the FFT for the inner-most dimension\nof `output`: the zero-frequency term, followed by the `fft_length / 2`\npositive-frequency terms.\n\nAlong each axis `RFFT2D` is computed on, if `fft_length` is smaller than the\ncorresponding dimension of `input`, the dimension is cropped. If it is larger,\nthe dimension is padded with zeros.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "fft_length", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.RFFT3D",
    "summary": "3D real-valued fast Fourier transform.",
    "description": "Computes the 3-dimensional discrete Fourier transform of a real-valued signal\nover the inner-most 3 dimensions of `input`.\n\nSince the DFT of a real signal is Hermitian-symmetric, `RFFT3D` only returns the\n`fft_length / 2 + 1` unique components of the FFT for the inner-most dimension\nof `output`: the zero-frequency term, followed by the `fft_length / 2`\npositive-frequency terms.\n\nAlong each axis `RFFT3D` is computed on, if `fft_length` is smaller than the\ncorresponding dimension of `input`, the dimension is cropped. If it is larger,\nthe dimension is padded with zeros.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "fft_length", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.RGBToHSV",
    "summary": "Converts one or more images from RGB to HSV.",
    "description": "Outputs a tensor of the same shape as the `images` tensor, containing the HSV\nvalue of the pixels. The output is only well defined if the value in `images`\nare in `[0,1]`.\n\n`output[..., 0]` contains hue, `output[..., 1]` contains saturation, and\n`output[..., 2]` contains value. All HSV values are in `[0,1]`. A hue of 0\ncorresponds to pure red, hue 1/3 is pure green, and 2/3 is pure blue.\n\nUsage Example:\n\n>>> blue_image = tf.stack([\n...    tf.zeros([5,5]),\n...    tf.zeros([5,5]),\n...    tf.ones([5,5])],\n...    axis=-1)\n>>> blue_hsv_image = tf.image.rgb_to_hsv(blue_image)\n>>> blue_hsv_image[0,0].numpy()\narray([0.6666667, 1. , 1. ], dtype=float32)",
    "inputs": [
      { "name": "images", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.RightShift",
    "summary": "Elementwise computes the bitwise right-shift of `x` and `y`.",
    "description": "Performs a logical shift for unsigned integer types, and an arithmetic shift\nfor signed integer types.\n\nIf `y` is negative, or greater than or equal to than the width of `x` in bits\nthe result is implementation defined.\n\nExample:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\nimport numpy as np\ndtype_list = [tf.int8, tf.int16, tf.int32, tf.int64]\n\nfor dtype in dtype_list:\n  lhs = tf.constant([-1, -5, -3, -14], dtype=dtype)\n  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)\n\n  right_shift_result = bitwise_ops.right_shift(lhs, rhs)\n\n  print(right_shift_result)\n\n# This will print:\n# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int8)\n# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int16)\n# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int32)\n# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int64)\n\nlhs = np.array([-2, 64, 101, 32], dtype=np.int8)\nrhs = np.array([-1, -5, -3, -14], dtype=np.int8)\nbitwise_ops.right_shift(lhs, rhs)\n# <tf.Tensor: shape=(4,), dtype=int8, numpy=array([ -2,  64, 101,  32], dtype=int8)>\n```",
    "inputs": [
      { "name": "x", "type": "TF_IntTensor" },
      { "name": "y", "type": "TF_IntTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_IntTensor" }
    ]
  },
  {
    "name": "tf.Rint",
    "summary": "Returns element-wise integer closest to x.",
    "description": "If the result is midway between two representable values,\nthe even representable is chosen.\nFor example:\n\n```\nrint(-1.5) ==> -2.0\nrint(0.5000001) ==> 1.0\nrint([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0]) ==> [-2., -2., -0., 0., 2., 2., 2.]\n```",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.RiscAdd",
    "summary": "Returns x + y element-wise.",
    "description": "*NOTE*: `RiscAdd` does not supports broadcasting.\n\nGiven two input tensors, the `tf.risc_add` operation computes the sum for every element in the tensor.\n\nBoth input and output have a range `(-inf, inf)`.",
    "inputs": [
      { "name": "x", "type": "TF_FloatTensor" },
      { "name": "y", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.RiscDot",
    "inputs": [
      { "name": "a", "type": "TF_FloatTensor" },
      { "name": "b", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "product", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "transpose_a", "type": "DefaultValuedOptionalAttr" },
      { "name": "transpose_b", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.RngReadAndSkip",
    "summary": "Advance the counter of a counter-based RNG.",
    "description": "The state of the RNG after\n`rng_read_and_skip(n)` will be the same as that after `uniform([n])`\n(or any other distribution). The actual increment added to the\ncounter is an unspecified implementation choice.\n\nIn the case that the input algorithm is RNG_ALG_AUTO_SELECT, the counter in the state needs to be of size int64[2], the current maximal counter size among algorithms. In this case, this op will manage the counter as if it is an 128-bit integer with layout [lower_64bits, higher_64bits]. If an algorithm needs less than 128 bits for the counter, it should use the left portion of the int64[2]. In this way, the int64[2] is compatible with all current RNG algorithms (Philox, ThreeFry and xla::RandomAlgorithm::RNG_DEFAULT). Downstream RNG ops can thus use this counter with any RNG algorithm.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "alg", "type": "Arg" },
      { "name": "delta", "type": "Arg" }
    ],
    "outputs": [
      { "name": "value", "type": "Res" }
    ]
  },
  {
    "name": "tf.Roll",
    "summary": "Rolls the elements of a tensor along an axis.",
    "description": "The elements are shifted positively (towards larger indices) by the offset of\n`shift` along the dimension of `axis`. Negative `shift` values will shift\nelements in the opposite direction. Elements that roll passed the last position\nwill wrap around to the first and vice versa. Multiple shifts along multiple\naxes may be specified.\n\nFor example:\n\n```\n# 't' is [0, 1, 2, 3, 4]\nroll(t, shift=2, axis=0) ==> [3, 4, 0, 1, 2]\n\n# shifting along multiple dimensions\n# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\nroll(t, shift=[1, -2], axis=[0, 1]) ==> [[7, 8, 9, 5, 6], [2, 3, 4, 0, 1]]\n\n# shifting along the same axis multiple times\n# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\nroll(t, shift=[2, -3], axis=[1, 1]) ==> [[1, 2, 3, 4, 0], [6, 7, 8, 9, 5]]\n```",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "shift", "type": "Arg" },
      { "name": "axis", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Round",
    "summary": "Rounds the values of a tensor to the nearest integer, element-wise.",
    "description": "Rounds half to even.  Also known as bankers rounding. If you want to round\naccording to the current system rounding mode use std::cint.",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.Rsqrt",
    "summary": "Computes reciprocal of square root of x element-wise.",
    "description": "I.e., \\\\(y = 1 / \\sqrt{x}\\\\).",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.RsqrtGrad",
    "summary": "Computes the gradient for the rsqrt of `x` wrt its input.",
    "description": "Specifically, `grad = dy * -0.5 * y^3`, where `y = rsqrt(x)`, and `dy`\nis the corresponding input gradient.",
    "inputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" },
      { "name": "dy", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Save",
    "summary": "Saves the input tensors to disk.",
    "description": "The size of `tensor_names` must match the number of tensors in `data`. `data[i]`\nis written to `filename` with name `tensor_names[i]`.\n\nSee also `SaveSlices`.",
    "inputs": [
      { "name": "filename", "type": "Arg" },
      { "name": "tensor_names", "type": "Arg" },
      { "name": "data", "type": "Arg" }
    ]
  },
  {
    "name": "tf.SaveSlices",
    "summary": "Saves input tensors slices to disk.",
    "description": "This is like `Save` except that tensors can be listed in the saved file as being\na slice of a larger tensor.  `shapes_and_slices` specifies the shape of the\nlarger tensor and the slice that this tensor covers. `shapes_and_slices` must\nhave as many elements as `tensor_names`.\n\nElements of the `shapes_and_slices` input must either be:\n\n*  The empty string, in which case the corresponding tensor is\n   saved normally.\n*  A string of the form `dim0 dim1 ... dimN-1 slice-spec` where the\n   `dimI` are the dimensions of the larger tensor and `slice-spec`\n   specifies what part is covered by the tensor to save.\n\n`slice-spec` itself is a `:`-separated list: `slice0:slice1:...:sliceN-1`\nwhere each `sliceI` is either:\n\n*  The string `-` meaning that the slice covers all indices of this dimension\n*  `start,length` where `start` and `length` are integers.  In that\n   case the slice covers `length` indices starting at `start`.\n\nSee also `Save`.",
    "inputs": [
      { "name": "filename", "type": "Arg" },
      { "name": "tensor_names", "type": "Arg" },
      { "name": "shapes_and_slices", "type": "Arg" },
      { "name": "data", "type": "Arg" }
    ]
  },
  {
    "name": "tf.SaveV2",
    "summary": "Saves tensors in V2 checkpoint format.",
    "description": "By default, saves the named tensors in full.  If the caller wishes to save\nspecific slices of full tensors, \"shape_and_slices\" should be non-empty strings\nand correspondingly well-formed.",
    "inputs": [
      { "name": "prefix", "type": "Arg" },
      { "name": "tensor_names", "type": "Arg" },
      { "name": "shape_and_slices", "type": "Arg" },
      { "name": "tensors", "type": "Arg" }
    ]
  },
  {
    "name": "tf.ScatterNd",
    "summary": "Scatters `updates` into a tensor of shape `shape` according to `indices`.",
    "description": "Scatter sparse `updates` according to individual values at the specified\n`indices`. This op returns an output tensor with the `shape` you specify. This\nop is the inverse of the `tf.gather_nd` operator which extracts values or slices\nfrom a given tensor.\n\nThis operation is similar to `tf.tensor_scatter_nd_add`, except that the tensor\nis zero-initialized. Calling `tf.scatter_nd(indices, updates, shape)`\nis identical to calling\n`tf.tensor_scatter_nd_add(tf.zeros(shape, updates.dtype), indices, updates)`\n\nIf `indices` contains duplicates, the associated `updates` are accumulated\n(summed) into the output tensor.\n\n**WARNING**: For floating-point data types, the output may be nondeterministic.\nThis is because the order in which the updates are applied is nondeterministic\nand when floating-point numbers are added in different orders the resulting\nnumerical approximation error can be slightly different. However, the output\nwill be deterministic if op determinism is enabled via\n`tf.config.experimental.enable_op_determinism`.\n\n`indices` is an integer tensor containing indices into the output tensor. The\nlast dimension of `indices` can be at most the rank of `shape`:\n\n    indices.shape[-1] <= shape.rank\n\nThe last dimension of `indices` corresponds to indices of elements\n(if `indices.shape[-1] = shape.rank`) or slices\n(if `indices.shape[-1] < shape.rank`) along dimension `indices.shape[-1]` of\n`shape`.\n\n`updates` is a tensor with shape:\n\n    indices.shape[:-1] + shape[indices.shape[-1]:]\n\nThe simplest form of the scatter op is to insert individual elements in\na tensor by index. Consider an example where you want to insert 4 scattered\nelements in a rank-1 tensor with 8 elements.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/ScatterNd1.png\" alt>\n</div>\n\nIn Python, this scatter operation would look like this:\n\n```python\n    indices = tf.constant([[4], [3], [1], [7]])\n    updates = tf.constant([9, 10, 11, 12])\n    shape = tf.constant([8])\n    scatter = tf.scatter_nd(indices, updates, shape)\n    print(scatter)\n```\n\nThe resulting tensor would look like this:\n\n    [0, 11, 0, 10, 9, 0, 0, 12]\n\nYou can also insert entire slices of a higher rank tensor all at once. For\nexample, you can insert two slices in the first dimension of a rank-3 tensor\nwith two matrices of new values.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/ScatterNd2.png\" alt>\n</div>\n\nIn Python, this scatter operation would look like this:\n\n```python\n    indices = tf.constant([[1], [3]])\n    updates = tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],\n                            [7, 7, 7, 7], [8, 8, 8, 8]],\n                           [[5, 5, 5, 5], [6, 6, 6, 6],\n                            [7, 7, 7, 7], [8, 8, 8, 8]]])\n    shape = tf.constant([4, 4, 4])\n    scatter = tf.scatter_nd(indices, updates, shape)\n    print(scatter)\n```\n\nThe resulting tensor would look like this:\n\n    [[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],\n     [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],\n     [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],\n     [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]\n\nIf `indices` contains any out-of-bound indices, depending on\n`bad_indices_policy`, the op will either return an error or ignore the\nout-of-bound indices. `bad_indices_policy` can be one of the following values:\n1. \"\" or \"DEFAULT\": raises on CPU and ignore on GPU. This is because\n   historically on CPU and GPU we handle errors in different ways, and for\n   backward compatibility we keep the default behavior.\n2. \"ERROR\": raises error; GPU does not support this value.\n3. \"IGNORE\": ignore the bad indices; supported on both CPU and GPU.",
    "inputs": [
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SegmentMax",
    "summary": "Computes the maximum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\max_j(data_j)\\\\) where `max` is over `j` such\nthat `segment_ids[j] == i`.\n\nIf the max is empty for a given segment ID `i`, `output[i] = 0`.\n\nCaution: On CPU, values in `segment_ids` are always validated to be sorted,\nand an error is thrown for indices that are not increasing. On GPU, this\ndoes not throw an error for unsorted indices. On GPU, out-of-order indices\nresult in safe but unspecified behavior, which may include treating\nout-of-order indices as the same as a smaller following index.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/SegmentMax.png\" alt>\n</div>\n\nFor example:\n\n>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n>>> tf.math.segment_max(c, tf.constant([0, 0, 1])).numpy()\narray([[4, 3, 3, 4],\n       [5, 6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_IntOrFpTensor" },
      { "name": "segment_ids", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SegmentMaxV2",
    "summary": "Computes the maximum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\max_j(data_j)\\\\) where `max` is over `j` such\nthat `segment_ids[j] == i`.\n\nIf the maximum is empty for a given segment ID `i`, it outputs the smallest\npossible value for the specific numeric type,\n`output[i] = numeric_limits<T>::lowest()`.\n\nNote: That this op is currently only supported with jit_compile=True.\n\nCaution: On CPU, values in `segment_ids` are always validated to be sorted,\nand an error is thrown for indices that are not increasing. On GPU, this\ndoes not throw an error for unsorted indices. On GPU, out-of-order indices\nresult in safe but unspecified behavior, which may include treating\nout-of-order indices as the same as a smaller following index.\n\nThe only difference with SegmentMax is the additional input  `num_segments`.\nThis helps in evaluating the output shape in compile time.\n`num_segments` should be consistent with segment_ids.\ne.g. Max(segment_ids) should be equal to `num_segments` - 1 for a 1-d segment_ids\nWith inconsistent num_segments, the op still runs. only difference is,\nthe output takes the size of num_segments irrespective of size of segment_ids and data.\nfor num_segments less than expected output size, the last elements are ignored\nfor num_segments more than the expected output size, last elements are assigned \nsmallest possible value for the specific numeric type.\n\nFor example:\n\n>>> @tf.function(jit_compile=True)\n... def test(c):\n...   return tf.raw_ops.SegmentMaxV2(data=c, segment_ids=tf.constant([0, 0, 1]), num_segments=2)\n>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n>>> test(c).numpy()\narray([[4, 3, 3, 4],\n       [5, 6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_IntOrFpTensor" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SegmentMean",
    "summary": "Computes the mean along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\frac{\\sum_j data_j}{N}\\\\) where `mean` is\nover `j` such that `segment_ids[j] == i` and `N` is the total number of\nvalues summed.\n\nIf the mean is empty for a given segment ID `i`, `output[i] = 0`.\n\nCaution: On CPU, values in `segment_ids` are always validated to be sorted,\nand an error is thrown for indices that are not increasing. On GPU, this\ndoes not throw an error for unsorted indices. On GPU, out-of-order indices\nresult in safe but unspecified behavior, which may include treating\nout-of-order indices as a smaller following index when computing the numerator\nof the mean.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/SegmentMean.png\" alt>\n</div>\n\nFor example:\n\n>>> c = tf.constant([[1.0,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n>>> tf.math.segment_mean(c, tf.constant([0, 0, 1])).numpy()\narray([[2.5, 2.5, 2.5, 2.5],\n       [5., 6., 7., 8.]], dtype=float32)",
    "inputs": [
      { "name": "data", "type": "TF_NumberTensor" },
      { "name": "segment_ids", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SegmentMin",
    "summary": "Computes the minimum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\min_j(data_j)\\\\) where `min` is over `j` such\nthat `segment_ids[j] == i`.\n\nIf the min is empty for a given segment ID `i`, `output[i] = 0`.\n\nCaution: On CPU, values in `segment_ids` are always validated to be sorted,\nand an error is thrown for indices that are not increasing. On GPU, this\ndoes not throw an error for unsorted indices. On GPU, out-of-order indices\nresult in safe but unspecified behavior, which may include treating\nout-of-order indices as the same as a smaller following index.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/SegmentMin.png\" alt>\n</div>\n\nFor example:\n\n>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n>>> tf.math.segment_min(c, tf.constant([0, 0, 1])).numpy()\narray([[1, 2, 2, 1],\n       [5, 6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_IntOrFpTensor" },
      { "name": "segment_ids", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SegmentMinV2",
    "summary": "Computes the minimum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\min_j(data_j)\\\\) where `min` is over `j` such\nthat `segment_ids[j] == i`.\n\nIf the minimum is empty for a given segment ID `i`, it outputs the largest\npossible value for the specific numeric type,\n`output[i] = numeric_limits<T>::max()`.\n\nNote: That this op is currently only supported with jit_compile=True.\n\nCaution: On CPU, values in `segment_ids` are always validated to be sorted,\nand an error is thrown for indices that are not increasing. On GPU, this\ndoes not throw an error for unsorted indices. On GPU, out-of-order indices\nresult in safe but unspecified behavior, which may include treating\nout-of-order indices as the same as a smaller following index.\n\nThe only difference with SegmentMin is the additional input  `num_segments`.\nThis helps in evaluating the output shape in compile time.\n`num_segments` should be consistent with segment_ids.\ne.g. Max(segment_ids) should be equal to `num_segments` - 1 for a 1-d segment_ids\nWith inconsistent num_segments, the op still runs. only difference is,\nthe output takes the size of num_segments irrespective of size of segment_ids and data.\nfor num_segments less than expected output size, the last elements are ignored\nfor num_segments more than the expected output size, last elements are assigned \nthe largest possible value for the specific numeric type.\n\nFor example:\n\n>>> @tf.function(jit_compile=True)\n... def test(c):\n...   return tf.raw_ops.SegmentMinV2(data=c, segment_ids=tf.constant([0, 0, 1]), num_segments=2)\n>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n>>> test(c).numpy()\narray([[1, 2, 2, 1],\n       [5, 6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_IntOrFpTensor" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SegmentProd",
    "summary": "Computes the product along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\prod_j data_j\\\\) where the product is over `j` such\nthat `segment_ids[j] == i`.\n\nIf the product is empty for a given segment ID `i`, `output[i] = 1`.\n\nCaution: On CPU, values in `segment_ids` are always validated to be sorted,\nand an error is thrown for indices that are not increasing. On GPU, this\ndoes not throw an error for unsorted indices. On GPU, out-of-order indices\nresult in safe but unspecified behavior, which may include treating\nout-of-order indices as the same as a smaller following index.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/SegmentProd.png\" alt>\n</div>\n\nFor example:\n\n>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n>>> tf.math.segment_prod(c, tf.constant([0, 0, 1])).numpy()\narray([[4, 6, 6, 4],\n       [5, 6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_NumberTensor" },
      { "name": "segment_ids", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SegmentProdV2",
    "summary": "Computes the product along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\prod_j data_j\\\\) where the product is over `j` such\nthat `segment_ids[j] == i`.\n\nIf the product is empty for a given segment ID `i`, `output[i] = 1`.\n\nNote: That this op is currently only supported with jit_compile=True.\n\nThe only difference with SegmentProd is the additional input  `num_segments`.\nThis helps in evaluating the output shape in compile time.\n`num_segments` should be consistent with segment_ids.\ne.g. Max(segment_ids) - 1 should be equal to `num_segments` for a 1-d segment_ids\nWith inconsistent num_segments, the op still runs. only difference is, \nthe output takes the size of num_segments irrespective of size of segment_ids and data.\nfor num_segments less than expected output size, the last elements are ignored\nfor num_segments more than the expected output size, last elements are assigned 1.\n\nFor example:\n\n>>> @tf.function(jit_compile=True)\n... def test(c):\n...   return tf.raw_ops.SegmentProdV2(data=c, segment_ids=tf.constant([0, 0, 1]), num_segments=2)\n>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n>>> test(c).numpy()\narray([[4, 6, 6, 4],\n       [5, 6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_NumberTensor" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SegmentSum",
    "summary": "Computes the sum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\sum_j data_j\\\\) where sum is over `j` such\nthat `segment_ids[j] == i`.\n\nIf the sum is empty for a given segment ID `i`, `output[i] = 0`.\n\nCaution: On CPU, values in `segment_ids` are always validated to be sorted,\nand an error is thrown for indices that are not increasing. On GPU, this\ndoes not throw an error for unsorted indices. On GPU, out-of-order indices\nresult in safe but unspecified behavior, which may include treating\nout-of-order indices as the same as a smaller following index.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/SegmentSum.png\" alt>\n</div>\n\nFor example:\n\n>>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n>>> tf.math.segment_sum(c, tf.constant([0, 0, 1])).numpy()\narray([[5, 5, 5, 5],\n       [5, 6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_NumberTensor" },
      { "name": "segment_ids", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SegmentSumV2",
    "summary": "Computes the sum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output_i = \\sum_j data_j\\\\) where sum is over `j` such\nthat `segment_ids[j] == i`.\n\nIf the sum is empty for a given segment ID `i`, `output[i] = 0`.\n\nNote that this op is currently only supported with jit_compile=True.\n</div>",
    "inputs": [
      { "name": "data", "type": "TF_NumberTensor" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Select",
    "summary": "Selects elements from `x` or `y`, depending on `condition`.",
    "description": "The `x`, and `y` tensors must all have the same shape, and the\noutput will also have that shape.\n\nThe `condition` tensor must be a scalar if `x` and `y` are scalars.\nIf `x` and `y` are vectors or higher rank, then `condition` must be either a\nscalar, a vector with size matching the first dimension of `x`, or must have\nthe same shape as `x`.\n\nThe `condition` tensor acts as a mask that chooses, based on the value at each\nelement, whether the corresponding element / row in the output should be\ntaken from `x` (if true) or `y` (if false).\n\nIf `condition` is a vector and `x` and `y` are higher rank matrices, then\nit chooses which row (outer dimension) to copy from `x` and `y`.\nIf `condition` has the same shape as `x` and `y`, then it chooses which\nelement to copy from `x` and `y`.\n\nFor example:\n\n```python\n# 'condition' tensor is [[True,  False]\n#                        [False, True]]\n# 't' is [[1, 2],\n#         [3, 4]]\n# 'e' is [[5, 6],\n#         [7, 8]]\nselect(condition, t, e)  # => [[1, 6], [7, 4]]\n\n\n# 'condition' tensor is [True, False]\n# 't' is [[1, 2],\n#         [3, 4]]\n# 'e' is [[5, 6],\n#         [7, 8]]\nselect(condition, t, e) ==> [[1, 2],\n                             [7, 8]]\n\n```",
    "inputs": [
      { "name": "condition", "type": "TF_BoolTensor" },
      { "name": "then_value", "type": "Arg" },
      { "name": "else_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SelectV2",
    "inputs": [
      { "name": "condition", "type": "TF_BoolTensor" },
      { "name": "then_value", "type": "TF_Tensor" },
      { "name": "else_value", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.SelfAdjointEigV2",
    "summary": "Computes the eigen decomposition of one or more square self-adjoint matrices.",
    "description": "Computes the eigenvalues and (optionally) eigenvectors of each inner matrix in\n`input` such that `input[..., :, :] = v[..., :, :] * diag(e[..., :])`. The eigenvalues\nare sorted in non-decreasing order.\n\n```python\n# a is a tensor.\n# e is a tensor of eigenvalues.\n# v is a tensor of eigenvectors.\ne, v = self_adjoint_eig(a)\ne = self_adjoint_eig(a, compute_v=False)\n```",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "e", "type": "Res" },
      { "name": "v", "type": "Res" }
    ],
    "attributes": [
      { "name": "compute_v", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Selu",
    "summary": "Computes scaled exponential linear: `scale * alpha * (exp(features) - 1)`",
    "description": "if < 0, `scale * features` otherwise.\n\nTo be used together with\n`initializer = tf.variance_scaling_initializer(factor=1.0, mode='FAN_IN')`.\nFor correct dropout, use `tf.contrib.nn.alpha_dropout`.\n\nSee [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)",
    "inputs": [
      { "name": "features", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.SeluGrad",
    "summary": "Computes gradients for the scaled exponential linear (Selu) operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "outputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "backprops", "type": "Res" }
    ]
  },
  {
    "name": "tf.Send",
    "summary": "Sends the named tensor from send_device to recv_device.",
    "inputs": [
      { "name": "tensor", "type": "Arg" }
    ],
    "attributes": [
      { "name": "tensor_name", "type": "StrAttr" },
      { "name": "send_device", "type": "StrAttr" },
      { "name": "send_device_incarnation", "type": "I64Attr" },
      { "name": "recv_device", "type": "StrAttr" },
      { "name": "client_terminated", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SendTPUEmbeddingGradients",
    "summary": "Performs gradient updates of embedding tables.",
    "inputs": [
      { "name": "inputs", "type": "Arg" },
      { "name": "learning_rates", "type": "Arg" }
    ],
    "attributes": [
      { "name": "config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.SerializeIterator",
    "summary": "Converts the given `resource_handle` representing an iterator to a variant tensor.",
    "inputs": [
      { "name": "resource_handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "serialized", "type": "Res" }
    ],
    "attributes": [
      { "name": "external_state_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SerializeSparse",
    "summary": "Serialize a `SparseTensor` into a `[3]` `Tensor` object.",
    "inputs": [
      { "name": "sparse_indices", "type": "Arg" },
      { "name": "sparse_values", "type": "Arg" },
      { "name": "sparse_shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "serialized_sparse", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.SetStaticDimensionBounds",
    "summary": "Op used to indicate to the compiler and runtime the static bounds of a tensor.",
    "description": "The information passed through this op can possibly be used by the compiler and\nruntime to perform certain optimizations such as more efficient DMAs. The\nbounds passed via this op should be considered advisory only, and depending on\nthe implementation, might do nothing and simply be an identity\n\n`input`: The tensor that has dynamic dimensions.\n`static_shape`: The static shape of the tensor, corresponds to the maximum bounds of each dimension.\n`output` is the input tensor with no changes done to it.\n\nExample usage:\n\ndef tpu_call(args):\n  def model_fn(args):\n    # do something with dynamic tensor\n\n  @function.Defun(capture_resource_var_by_value=False)\n  def tpu_subgraph():\n      return tf.tpu.rewrite(model_fn, args)\n\n  return tf.raw_ops.TPUPartitionedCall(\n      args=tpu_subgraph.captured_inputs,\n      Tout=[o.type for o in tpu_subgraph.definition.signature.output_arg],\n      f=tpu_subgraph,\n      device_ordinal=[0])\n\nstatic_shape = tf.placeholder(tf.int32, shape=([3]), name='static_size')\n\nw = tf.Variable(tf.constant([[1.0], [2.0], [3.0]]), name='w')\n\nw_dyn = tf.SetDynamicDimensionBounds(w, static_size])\ntpu_call([w_dyn])",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "static_shape", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.Shape",
    "summary": "Returns the shape of a tensor.",
    "description": "This operation returns a 1-D integer tensor representing the shape of `input`.\n\nFor example:\n\n```\n# 't' is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]\nshape(t) ==> [2, 2, 3]\n```",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_I32OrI64Tensor" }
    ],
    "category": "Shape"
  },
  {
    "name": "tf.ShapeN",
    "summary": "Returns shape of tensors.",
    "description": "This operation returns N 1-D integer tensors representing shape of `input[i]s`.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.ShardedFilename",
    "summary": "Generate a sharded filename. The filename is printf formatted as",
    "description": "%s-%05d-of-%05d, basename, shard, num_shards.",
    "inputs": [
      { "name": "basename", "type": "TF_StrTensor" },
      { "name": "shard", "type": "TF_Int32Tensor" },
      { "name": "num_shards", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "filename", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.ShuffleAndRepeatDatasetV2",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "buffer_size", "type": "TF_Int64Tensor" },
      { "name": "seed", "type": "TF_Int64Tensor" },
      { "name": "seed2", "type": "TF_Int64Tensor" },
      { "name": "count", "type": "TF_Int64Tensor" },
      { "name": "seed_generator", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "reshuffle_each_iteration", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ShuffleDatasetV2",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "buffer_size", "type": "TF_Int64Tensor" },
      { "name": "seed_generator", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ShuffleDatasetV3",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "buffer_size", "type": "TF_Int64Tensor" },
      { "name": "seed", "type": "TF_Int64Tensor" },
      { "name": "seed2", "type": "TF_Int64Tensor" },
      { "name": "seed_generator", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "reshuffle_each_iteration", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.ShutdownDistributedTPU",
    "summary": "Shuts down a running distributed TPU system.",
    "description": "The op returns an error if no system is running."
  },
  {
    "name": "tf.ShutdownTPUSystem",
    "summary": "An op that shuts down the TPU system.",
    "outputs": [
      { "name": "success", "type": "TF_BoolTensor" }
    ]
  },
  {
    "name": "tf.Sigmoid",
    "summary": "Computes sigmoid of `x` element-wise.",
    "description": "Specifically, `y = 1 / (1 + exp(-x))`.",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ],
    "category": "Activation"
  },
  {
    "name": "tf.SigmoidGrad",
    "summary": "Computes the gradient of the sigmoid of `x` wrt its input.",
    "description": "Specifically, `grad = dy * y * (1 - y)`, where `y = sigmoid(x)`, and\n`dy` is the corresponding input gradient.",
    "inputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" },
      { "name": "dy", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Sign",
    "summary": "Returns an element-wise indication of the sign of a number.",
    "description": "`y = sign(x) = -1` if `x < 0`; 0 if `x == 0`; 1 if `x > 0`.\n\nFor complex numbers, `y = sign(x) = x / |x|` if `x != 0`, otherwise `y = 0`.\n\nExample usage:\n>>> tf.math.sign([0., 2., -3.])\n<tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.,  1., -1.], dtype=float32)>",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.Sin",
    "summary": "Computes sine of x element-wise.",
    "description": "Given an input tensor, this function computes sine of every\n  element in the tensor. Input range is `(-inf, inf)` and\n  output range is `[-1,1]`.\n\n  ```python\n  x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 200, 10, float(\"inf\")])\n  tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Sinh",
    "summary": "Computes hyperbolic sine of x element-wise.",
    "description": "Given an input tensor, this function computes hyperbolic sine of every\n  element in the tensor. Input range is `[-inf,inf]` and output range\n  is `[-inf,inf]`.\n\n  ```python\n  x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 2, 10, float(\"inf\")])\n  tf.math.sinh(x) ==> [-inf -4.0515420e+03 -5.2109528e-01 1.1752012e+00 1.5094614e+00 3.6268604e+00 1.1013232e+04 inf]\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Size",
    "summary": "Returns the size of a tensor.",
    "description": "This operation returns an integer representing the number of elements in\n`input`.\n\nFor example:\n\n```\n# 't' is [[[1, 1,, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]]\nsize(t) ==> 12\n```",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_I32OrI64Tensor" }
    ],
    "category": "Shape"
  },
  {
    "name": "tf.Slice",
    "summary": "Return a slice from 'input'.",
    "description": "The output tensor is a tensor with dimensions described by 'size'\nwhose values are extracted from 'input' starting at the offsets in\n'begin'.\n\n*Requirements*:\n  0 <= begin[i] <= begin[i] + size[i] <= Di  for i in [0, n)",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "begin", "type": "Arg" },
      { "name": "size", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "category": "Tensor"
  },
  {
    "name": "tf.Snapshot",
    "summary": "Returns a copy of the input tensor.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.Softmax",
    "summary": "Computes softmax activations.",
    "description": "For each batch `i` and class `j` we have\n\n    $$softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))$$",
    "inputs": [
      { "name": "logits", "type": "Arg" }
    ],
    "outputs": [
      { "name": "softmax", "type": "Res" }
    ],
    "category": "Activation"
  },
  {
    "name": "tf.SoftmaxCrossEntropyWithLogits",
    "summary": "Computes softmax cross entropy cost and gradients to backpropagate.",
    "description": "Inputs are the logits, not probabilities.",
    "inputs": [
      { "name": "features", "type": "Arg" },
      { "name": "labels", "type": "Arg" }
    ],
    "outputs": [
      { "name": "loss", "type": "Res" },
      { "name": "backprop", "type": "Res" }
    ]
  },
  {
    "name": "tf.Softplus",
    "inputs": [
      { "name": "features", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.SoftplusGrad",
    "summary": "Computes softplus gradients for a softplus operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "features", "type": "Arg" }
    ],
    "outputs": [
      { "name": "backprops", "type": "Res" }
    ]
  },
  {
    "name": "tf.Softsign",
    "summary": "Computes softsign: `features / (abs(features) + 1)`.",
    "inputs": [
      { "name": "features", "type": "TF_FloatTensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.SoftsignGrad",
    "summary": "Computes softsign gradients for a softsign operation.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "features", "type": "Arg" }
    ],
    "outputs": [
      { "name": "backprops", "type": "Res" }
    ]
  },
  {
    "name": "tf.SortListOfSparseCoreCooTensors",
    "summary": "An op which sorts each COO tensors in the list by which SparseCore the id will go to. This op should be used along with the ConvertToSparseCoreCsrWrappedCooTensorOp.",
    "inputs": [
      { "name": "row_ids_list", "type": "Variadic" },
      { "name": "col_ids_list", "type": "Variadic" },
      { "name": "gains_list", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "sorted_row_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_col_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "id_counts", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "sample_count_list", "type": "I64ArrayAttr" },
      { "name": "col_offset_list", "type": "I64ArrayAttr" },
      { "name": "num_replica", "type": "ConfinedAttr" },
      { "name": "table_vocab_size", "type": "ConfinedAttr" },
      { "name": "feature_width", "type": "ConfinedAttr" },
      { "name": "num_sc_per_chip", "type": "ConfinedAttr" },
      { "name": "max_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "max_unique_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.SpaceToBatch",
    "summary": "SpaceToBatch for 4-D tensors of type T.",
    "description": "This is a legacy version of the more general SpaceToBatchND.\n\nZero-pads and then rearranges (permutes) blocks of spatial data into batch.\nMore specifically, this op outputs a copy of the input tensor where values from\nthe `height` and `width` dimensions are moved to the `batch` dimension. After\nthe zero-padding, both `height` and `width` of the input must be divisible by the\nblock size.\n\nThe attr `block_size` must be greater than one. It indicates the block size.\n\n  * Non-overlapping blocks of size `block_size x block size` in the height and\n    width dimensions are rearranged into the batch dimension at each location.\n  * The batch of the output tensor is `batch * block_size * block_size`.\n  * Both height_pad and width_pad must be divisible by block_size.\n\nThe shape of the output will be:\n\n    [batch*block_size*block_size, height_pad/block_size, width_pad/block_size,\n     depth]\n\nSome examples:\n\n(1) For the following input of shape `[1, 2, 2, 1]` and block_size of 2:\n\n```\nx = [[[[1], [2]], [[3], [4]]]]\n```\n\nThe output tensor has shape `[4, 1, 1, 1]` and value:\n\n```\n[[[[1]]], [[[2]]], [[[3]]], [[[4]]]]\n```\n\n(2) For the following input of shape `[1, 2, 2, 3]` and block_size of 2:\n\n```\nx = [[[[1, 2, 3], [4, 5, 6]],\n      [[7, 8, 9], [10, 11, 12]]]]\n```\n\nThe output tensor has shape `[4, 1, 1, 3]` and value:\n\n```\n[[[[1, 2, 3]]], [[[4, 5, 6]]], [[[7, 8, 9]]], [[[10, 11, 12]]]]\n```\n\n(3) For the following input of shape `[1, 4, 4, 1]` and block_size of 2:\n\n```\nx = [[[[1],   [2],  [3],  [4]],\n      [[5],   [6],  [7],  [8]],\n      [[9],  [10], [11],  [12]],\n      [[13], [14], [15],  [16]]]]\n```\n\nThe output tensor has shape `[4, 2, 2, 1]` and value:\n\n```\nx = [[[[1], [3]], [[9], [11]]],\n     [[[2], [4]], [[10], [12]]],\n     [[[5], [7]], [[13], [15]]],\n     [[[6], [8]], [[14], [16]]]]\n```\n\n(4) For the following input of shape `[2, 2, 4, 1]` and block_size of 2:\n\n```\nx = [[[[1],   [2],  [3],  [4]],\n      [[5],   [6],  [7],  [8]]],\n     [[[9],  [10], [11],  [12]],\n      [[13], [14], [15],  [16]]]]\n```\n\nThe output tensor has shape `[8, 1, 2, 1]` and value:\n\n```\nx = [[[[1], [3]]], [[[9], [11]]], [[[2], [4]]], [[[10], [12]]],\n     [[[5], [7]]], [[[13], [15]]], [[[6], [8]]], [[[14], [16]]]]\n```\n\nAmong others, this operation is useful for reducing atrous convolution into\nregular convolution.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "paddings", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "block_size", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.SpaceToBatchND",
    "summary": "SpaceToBatch for N-D tensors of type T.",
    "description": "This operation divides \"spatial\" dimensions `[1, ..., M]` of the input into a\ngrid of blocks of shape `block_shape`, and interleaves these blocks with the\n\"batch\" dimension (0) such that in the output, the spatial dimensions\n`[1, ..., M]` correspond to the position within the grid, and the batch\ndimension combines both the position within a spatial block and the original\nbatch position.  Prior to division into blocks, the spatial dimensions of the\ninput are optionally zero padded according to `paddings`. See below for a\nprecise description.\n\nThis operation is equivalent to the following steps:\n\n1. Zero-pad the start and end of dimensions `[1, ..., M]` of the\n   input according to `paddings` to produce `padded` of shape `padded_shape`.\n\n2. Reshape `padded` to `reshaped_padded` of shape:\n\n     [batch] +\n     [padded_shape[1] / block_shape[0],\n       block_shape[0],\n      ...,\n      padded_shape[M] / block_shape[M-1],\n      block_shape[M-1]] +\n     remaining_shape\n\n3. Permute dimensions of `reshaped_padded` to produce\n   `permuted_reshaped_padded` of shape:\n\n     block_shape +\n     [batch] +\n     [padded_shape[1] / block_shape[0],\n      ...,\n      padded_shape[M] / block_shape[M-1]] +\n     remaining_shape\n\n4. Reshape `permuted_reshaped_padded` to flatten `block_shape` into the batch\n   dimension, producing an output tensor of shape:\n\n     [batch * prod(block_shape)] +\n     [padded_shape[1] / block_shape[0],\n      ...,\n      padded_shape[M] / block_shape[M-1]] +\n     remaining_shape\n\nSome examples:\n\n(1) For the following input of shape `[1, 2, 2, 1]`, `block_shape = [2, 2]`, and\n    `paddings = [[0, 0], [0, 0]]`:\n\n```\nx = [[[[1], [2]], [[3], [4]]]]\n```\n\nThe output tensor has shape `[4, 1, 1, 1]` and value:\n\n```\n[[[[1]]], [[[2]]], [[[3]]], [[[4]]]]\n```\n\n(2) For the following input of shape `[1, 2, 2, 3]`, `block_shape = [2, 2]`, and\n    `paddings = [[0, 0], [0, 0]]`:\n\n```\nx = [[[[1, 2, 3], [4, 5, 6]],\n      [[7, 8, 9], [10, 11, 12]]]]\n```\n\nThe output tensor has shape `[4, 1, 1, 3]` and value:\n\n```\n[[[[1, 2, 3]]], [[[4, 5, 6]]], [[[7, 8, 9]]], [[[10, 11, 12]]]]\n```\n\n(3) For the following input of shape `[1, 4, 4, 1]`, `block_shape = [2, 2]`, and\n    `paddings = [[0, 0], [0, 0]]`:\n\n```\nx = [[[[1],   [2],  [3],  [4]],\n      [[5],   [6],  [7],  [8]],\n      [[9],  [10], [11],  [12]],\n      [[13], [14], [15],  [16]]]]\n```\n\nThe output tensor has shape `[4, 2, 2, 1]` and value:\n\n```\nx = [[[[1], [3]], [[9], [11]]],\n     [[[2], [4]], [[10], [12]]],\n     [[[5], [7]], [[13], [15]]],\n     [[[6], [8]], [[14], [16]]]]\n```\n\n(4) For the following input of shape `[2, 2, 4, 1]`, block_shape = `[2, 2]`, and\n    paddings = `[[0, 0], [2, 0]]`:\n\n```\nx = [[[[1],   [2],  [3],  [4]],\n      [[5],   [6],  [7],  [8]]],\n     [[[9],  [10], [11],  [12]],\n      [[13], [14], [15],  [16]]]]\n```\n\nThe output tensor has shape `[8, 1, 3, 1]` and value:\n\n```\nx = [[[[0], [1], [3]]], [[[0], [9], [11]]],\n     [[[0], [2], [4]]], [[[0], [10], [12]]],\n     [[[0], [5], [7]]], [[[0], [13], [15]]],\n     [[[0], [6], [8]]], [[[0], [14], [16]]]]\n```\n\nAmong others, this operation is useful for reducing atrous convolution into\nregular convolution.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "block_shape", "type": "Arg" },
      { "name": "paddings", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.SpaceToDepth",
    "summary": "SpaceToDepth for tensors of type T.",
    "description": "Rearranges blocks of spatial data, into depth. More specifically,\nthis op outputs a copy of the input tensor where values from the `height`\nand `width` dimensions are moved to the `depth` dimension.\nThe attr `block_size` indicates the input block size.\n\n  * Non-overlapping blocks of size `block_size x block size` are rearranged\n    into depth at each location.\n  * The depth of the output tensor is `block_size * block_size * input_depth`.\n  * The Y, X coordinates within each block of the input become the high order\n    component of the output channel index.\n  * The input tensor's height and width must be divisible by block_size.\n\nThe `data_format` attr specifies the layout of the input and output tensors\nwith the following options:\n  \"NHWC\": `[ batch, height, width, channels ]`\n  \"NCHW\": `[ batch, channels, height, width ]`\n  \"NCHW_VECT_C\":\n      `qint8 [ batch, channels / 4, height, width, 4 ]`\n\nIt is useful to consider the operation as transforming a 6-D Tensor.\ne.g. for data_format = NHWC,\n     Each element in the input tensor can be specified via 6 coordinates,\n     ordered by decreasing memory layout significance as:\n     n,oY,bY,oX,bX,iC  (where n=batch index, oX, oY means X or Y coordinates\n                        within the output image, bX, bY means coordinates\n                        within the input block, iC means input channels).\n     The output would be a transpose to the following layout:\n     n,oY,oX,bY,bX,iC\n\nThis operation is useful for resizing the activations between convolutions\n(but keeping all data), e.g. instead of pooling. It is also useful for training\npurely convolutional models.\n\nFor example, given an input of shape `[1, 2, 2, 1]`, data_format = \"NHWC\" and\nblock_size = 2:\n\n```\nx = [[[[1], [2]],\n      [[3], [4]]]]\n```\n\nThis operation will output a tensor of shape `[1, 1, 1, 4]`:\n\n```\n[[[[1, 2, 3, 4]]]]\n```\n\nHere, the input has a batch of 1 and each batch element has shape `[2, 2, 1]`,\nthe corresponding output will have a single element (i.e. width and height are\nboth 1) and will have a depth of 4 channels (1 * block_size * block_size).\nThe output element shape is `[1, 1, 4]`.\n\nFor an input tensor with larger depth, here of shape `[1, 2, 2, 3]`, e.g.\n\n```\nx = [[[[1, 2, 3], [4, 5, 6]],\n      [[7, 8, 9], [10, 11, 12]]]]\n```\n\nThis operation, for block_size of 2, will return the following tensor of shape\n`[1, 1, 1, 12]`\n\n```\n[[[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]]]]\n```\n\nSimilarly, for the following input of shape `[1 4 4 1]`, and a block size of 2:\n\n```\nx = [[[[1],   [2],  [5],  [6]],\n      [[3],   [4],  [7],  [8]],\n      [[9],  [10], [13],  [14]],\n      [[11], [12], [15],  [16]]]]\n```\n\nthe operator will return the following tensor of shape `[1 2 2 4]`:\n\n```\nx = [[[[1, 2, 3, 4],\n       [5, 6, 7, 8]],\n      [[9, 10, 11, 12],\n       [13, 14, 15, 16]]]]\n```",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "block_size", "type": "ConfinedAttr" },
      { "name": "data_format", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseAdd",
    "summary": "Adds two `SparseTensor` objects to produce another `SparseTensor`.",
    "description": "The input `SparseTensor` objects' indices are assumed ordered in standard\nlexicographic order.  If this is not the case, before this step run\n`SparseReorder` to restore index ordering.\n\nBy default, if two values sum to zero at some index, the output `SparseTensor`\nwould still include that particular location in its index, storing a zero in the\ncorresponding value slot.  To override this, callers can specify `thresh`,\nindicating that if the sum has a magnitude strictly smaller than `thresh`, its\ncorresponding value and index would then not be included.  In particular,\n`thresh == 0` (default) means everything is kept and actual thresholding happens\nonly for a positive value.\n\nIn the following shapes, `nnz` is the count after taking `thresh` into account.",
    "inputs": [
      { "name": "a_indices", "type": "Arg" },
      { "name": "a_values", "type": "Arg" },
      { "name": "a_shape", "type": "Arg" },
      { "name": "b_indices", "type": "Arg" },
      { "name": "b_values", "type": "Arg" },
      { "name": "b_shape", "type": "Arg" },
      { "name": "thresh", "type": "Arg" }
    ],
    "outputs": [
      { "name": "sum_indices", "type": "TF_Int64Tensor" },
      { "name": "sum_values", "type": "TF_NumberTensor" },
      { "name": "sum_shape", "type": "TF_Int64Tensor" }
    ]
  },
  {
    "name": "tf.SparseFillEmptyRows",
    "summary": "Fills empty rows in the input 2-D `SparseTensor` with a default value.",
    "description": "The input `SparseTensor` is represented via the tuple of inputs\n(`indices`, `values`, `dense_shape`).  The output `SparseTensor` has the\nsame `dense_shape` but with indices `output_indices` and values\n`output_values`.\n\nThis op inserts a single entry for every row that doesn't have any values.\nThe index is created as `[row, 0, ..., 0]` and the inserted value\nis `default_value`.\n\nFor example, suppose `sp_input` has shape `[5, 6]` and non-empty values:\n\n    [0, 1]: a\n    [0, 3]: b\n    [2, 0]: c\n    [3, 1]: d\n\nRows 1 and 4 are empty, so the output will be of shape `[5, 6]` with values:\n\n    [0, 1]: a\n    [0, 3]: b\n    [1, 0]: default_value\n    [2, 0]: c\n    [3, 1]: d\n    [4, 0]: default_value\n\nThe output `SparseTensor` will be in row-major order and will have the\nsame shape as the input.\n\nThis op also returns an indicator vector shaped `[dense_shape[0]]` such that\n\n    empty_row_indicator[i] = True iff row i was an empty row.\n\nAnd a reverse index map vector shaped `[indices.shape[0]]` that is used during\nbackpropagation,\n\n    reverse_index_map[j] = out_j s.t. indices[j, :] == output_indices[out_j, :]",
    "inputs": [
      { "name": "indices", "type": "Arg" },
      { "name": "values", "type": "Arg" },
      { "name": "dense_shape", "type": "Arg" },
      { "name": "default_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output_indices", "type": "TF_Int64Tensor" },
      { "name": "output_values", "type": "Res" },
      { "name": "empty_row_indicator", "type": "Res" },
      { "name": "reverse_index_map", "type": "Res" }
    ]
  },
  {
    "name": "tf.SparseMatMul",
    "summary": "Multiply matrix \"a\" by matrix \"b\".",
    "description": "The inputs must be two-dimensional matrices and the inner dimension of \"a\" must\nmatch the outer dimension of \"b\". Both \"a\" and \"b\" must be `Tensor`s not\n`SparseTensor`s.  This op is optimized for the case where at least one of \"a\" or\n\"b\" is sparse, in the sense that they have a large proportion of zero values.\nThe breakeven for using this versus a dense matrix multiply on one platform was\n30% zero values in the sparse matrix.\n\nThe gradient computation of this operation will only take advantage of sparsity\nin the input gradient when that gradient comes from a Relu.",
    "inputs": [
      { "name": "a", "type": "TensorOf" },
      { "name": "b", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "product", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "transpose_a", "type": "DefaultValuedOptionalAttr" },
      { "name": "transpose_b", "type": "DefaultValuedOptionalAttr" },
      { "name": "a_is_sparse", "type": "DefaultValuedOptionalAttr" },
      { "name": "b_is_sparse", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseReduceSum",
    "summary": "Computes the sum of elements across dimensions of a SparseTensor.",
    "description": "This Op takes a SparseTensor and is the sparse counterpart to\n`tf.reduce_sum()`.  In particular, this Op also returns a dense `Tensor`\ninstead of a sparse one.\n\nReduces `sp_input` along the dimensions given in `reduction_axes`.  Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`reduction_axes`. If `keep_dims` is true, the reduced dimensions are retained\nwith length 1.\n\nIf `reduction_axes` has no entries, all dimensions are reduced, and a tensor\nwith a single element is returned.  Additionally, the axes can be negative,\nwhich are interpreted according to the indexing rules in Python.",
    "inputs": [
      { "name": "input_indices", "type": "Arg" },
      { "name": "input_values", "type": "Arg" },
      { "name": "input_shape", "type": "Arg" },
      { "name": "reduction_axes", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseReshape",
    "summary": "Reshapes a SparseTensor to represent values in a new dense shape.",
    "description": "This operation has the same semantics as reshape on the represented dense\ntensor.  The `input_indices` are recomputed based on the requested `new_shape`.\n\nIf one component of `new_shape` is the special value -1, the size of that\ndimension is computed so that the total dense size remains constant.  At\nmost one component of `new_shape` can be -1.  The number of dense elements\nimplied by `new_shape` must be the same as the number of dense elements\noriginally implied by `input_shape`.\n\nReshaping does not affect the order of values in the SparseTensor.\n\nIf the input tensor has rank `R_in` and `N` non-empty values, and `new_shape`\nhas length `R_out`, then `input_indices` has shape `[N, R_in]`,\n`input_shape` has length `R_in`, `output_indices` has shape `[N, R_out]`, and\n`output_shape` has length `R_out`.",
    "inputs": [
      { "name": "input_indices", "type": "Arg" },
      { "name": "input_shape", "type": "Arg" },
      { "name": "new_shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output_indices", "type": "Res" },
      { "name": "output_shape", "type": "Res" }
    ]
  },
  {
    "name": "tf.SparseSegmentMean",
    "summary": "Computes the mean along sparse segments of a tensor.",
    "description": "See `tf.sparse.segment_sum` for usage examples.\n\nLike `SegmentMean`, but `segment_ids` can have rank less than `data`'s first\ndimension, selecting a subset of dimension 0, specified by `indices`.",
    "inputs": [
      { "name": "data", "type": "TF_FloatTensor" },
      { "name": "indices", "type": "Arg" },
      { "name": "segment_ids", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "sparse_gradient", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseSegmentMeanGrad",
    "summary": "Computes gradients for SparseSegmentMean.",
    "description": "Returns tensor \"output\" with same shape as grad, except for dimension 0 whose\nvalue is output_dim0.",
    "inputs": [
      { "name": "grad", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "output_dim0", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.SparseSegmentMeanWithNumSegments",
    "summary": "Computes the mean along sparse segments of a tensor.",
    "description": "Like `SparseSegmentMean`, but allows missing ids in `segment_ids`. If an id is\nmissing, the `output` tensor at that position will be zeroed.\n\nRead\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.",
    "inputs": [
      { "name": "data", "type": "TF_FloatTensor" },
      { "name": "indices", "type": "Arg" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "sparse_gradient", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseSegmentSqrtN",
    "summary": "Computes the sum along sparse segments of a tensor divided by the sqrt of N.",
    "description": "N is the size of the segment being reduced.\n\nSee `tf.sparse.segment_sum` for usage examples.",
    "inputs": [
      { "name": "data", "type": "TF_FloatTensor" },
      { "name": "indices", "type": "Arg" },
      { "name": "segment_ids", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "sparse_gradient", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseSegmentSqrtNGrad",
    "summary": "Computes gradients for SparseSegmentSqrtN.",
    "description": "Returns tensor \"output\" with same shape as grad, except for dimension 0 whose\nvalue is output_dim0.",
    "inputs": [
      { "name": "grad", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "output_dim0", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ]
  },
  {
    "name": "tf.SparseSegmentSqrtNWithNumSegments",
    "summary": "Computes the sum along sparse segments of a tensor divided by the sqrt of N.",
    "description": "N is the size of the segment being reduced.\n\nLike `SparseSegmentSqrtN`, but allows missing ids in `segment_ids`. If an id is\nmissing, the `output` tensor at that position will be zeroed.\n\nRead\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.",
    "inputs": [
      { "name": "data", "type": "TF_FloatTensor" },
      { "name": "indices", "type": "Arg" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "sparse_gradient", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseSegmentSum",
    "summary": "Computes the sum along sparse segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nLike `SegmentSum`, but `segment_ids` can have rank less than `data`'s first\ndimension, selecting a subset of dimension 0, specified by `indices`.\n\nFor example:\n\n```python\nc = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])\n\n# Select two rows, one segment.\ntf.sparse_segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0]))\n# => [[0 0 0 0]]\n\n# Select two rows, two segment.\ntf.sparse_segment_sum(c, tf.constant([0, 1]), tf.constant([0, 1]))\n# => [[ 1  2  3  4]\n#     [-1 -2 -3 -4]]\n\n# Select all rows, two segments.\ntf.sparse_segment_sum(c, tf.constant([0, 1, 2]), tf.constant([0, 0, 1]))\n# => [[0 0 0 0]\n#     [5 6 7 8]]\n\n# Which is equivalent to:\ntf.segment_sum(c, tf.constant([0, 0, 1]))\n```",
    "inputs": [
      { "name": "data", "type": "TF_IntOrFpTensor" },
      { "name": "indices", "type": "Arg" },
      { "name": "segment_ids", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "sparse_gradient", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseSoftmaxCrossEntropyWithLogits",
    "summary": "Computes softmax cross entropy cost and gradients to backpropagate.",
    "description": "Unlike `SoftmaxCrossEntropyWithLogits`, this operation does not accept\na matrix of label probabilities, but rather a single label per row\nof features.  This label is considered to have probability 1.0 for the\ngiven row.\n\nInputs are the logits, not probabilities.",
    "inputs": [
      { "name": "features", "type": "Arg" },
      { "name": "labels", "type": "Arg" }
    ],
    "outputs": [
      { "name": "loss", "type": "Res" },
      { "name": "backprop", "type": "Res" }
    ]
  },
  {
    "name": "tf.SparseTensorDenseMatMul",
    "summary": "Multiply SparseTensor (of rank 2) \"A\" by dense matrix \"B\".",
    "description": "No validity checking is performed on the indices of A.  However, the following\ninput format is recommended for optimal behavior:\n\nif adjoint_a == false:\n  A should be sorted in lexicographically increasing order.  Use SparseReorder\n  if you're not sure.\nif adjoint_a == true:\n  A should be sorted in order of increasing dimension 1 (i.e., \"column major\"\n  order instead of \"row major\" order).",
    "inputs": [
      { "name": "a_indices", "type": "Arg" },
      { "name": "a_values", "type": "Arg" },
      { "name": "a_shape", "type": "Arg" },
      { "name": "b", "type": "Arg" }
    ],
    "outputs": [
      { "name": "product", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "adjoint_a", "type": "DefaultValuedOptionalAttr" },
      { "name": "adjoint_b", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SparseToDense",
    "summary": "Converts a sparse representation into a dense tensor.",
    "description": "Builds an array `dense` with shape `output_shape` such that\n\n```\n# If sparse_indices is scalar\ndense[i] = (i == sparse_indices ? sparse_values : default_value)\n\n# If sparse_indices is a vector, then for each i\ndense[sparse_indices[i]] = sparse_values[i]\n\n# If sparse_indices is an n by d matrix, then for each i in [0, n)\ndense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]\n```\n\nAll other values in `dense` are set to `default_value`.  If `sparse_values` is a\nscalar, all sparse indices are set to this single value.\n\nIndices should be sorted in lexicographic order, and indices must not\ncontain any repeats. If `validate_indices` is true, these properties\nare checked during execution.",
    "inputs": [
      { "name": "sparse_indices", "type": "Arg" },
      { "name": "output_shape", "type": "Arg" },
      { "name": "sparse_values", "type": "Arg" },
      { "name": "default_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "dense", "type": "Res" }
    ],
    "attributes": [
      { "name": "validate_indices", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Split",
    "summary": "Splits a tensor into `num_split` tensors along one dimension.",
    "inputs": [
      { "name": "split_dim", "type": "Arg" },
      { "name": "value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.SplitV",
    "summary": "Splits a tensor into `num_split` tensors along one dimension.",
    "inputs": [
      { "name": "value", "type": "Arg" },
      { "name": "size_splits", "type": "Arg" },
      { "name": "split_dim", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.Sqrt",
    "summary": "Computes square root of x element-wise.",
    "description": "I.e., \\\\(y = \\sqrt{x} = x^{1/2}\\\\).",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.SqrtGrad",
    "summary": "Computes the gradient for the sqrt of `x` wrt its input.",
    "description": "Specifically, `grad = dy * 0.5 / y`, where `y = sqrt(x)`, and `dy`\nis the corresponding input gradient.",
    "inputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" },
      { "name": "dy", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Square",
    "summary": "Computes square of x element-wise.",
    "description": "I.e., \\\\(y = x * x = x^2\\\\).",
    "inputs": [
      { "name": "x", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.SquaredDifference",
    "summary": "Returns conj(x - y)(x - y) element-wise.",
    "description": "*NOTE*: `SquaredDifference` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.Squeeze",
    "summary": "Removes dimensions of size 1 from the shape of a tensor.",
    "description": "Given a tensor `input`, this operation returns a tensor of the same type with\nall dimensions of size 1 removed. If you don't want to remove all size 1\ndimensions, you can remove specific size 1 dimensions by specifying\n`axis`.\n\nFor example:\n\n```\n# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\nshape(squeeze(t)) ==> [2, 3]\n```\n\nOr, to remove specific size 1 dimensions:\n\n```\n# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\nshape(squeeze(t, [2, 4])) ==> [1, 2, 3, 1]\n```",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "squeeze_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.StackCloseV2",
    "summary": "Delete the stack from its resource container.",
    "inputs": [
      { "name": "handle", "type": "Arg" }
    ]
  },
  {
    "name": "tf.StackPopV2",
    "summary": "Pop the element at the top of the stack.",
    "inputs": [
      { "name": "handle", "type": "Arg" }
    ],
    "outputs": [
      { "name": "elem", "type": "Res" }
    ]
  },
  {
    "name": "tf.StackPushV2",
    "summary": "Push an element onto the stack.",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "elem", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "swap_memory", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.StackV2",
    "summary": "A stack that produces elements in first-in last-out order.",
    "inputs": [
      { "name": "max_size", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "Res" }
    ],
    "attributes": [
      { "name": "elem_type", "type": "TypeAttr" },
      { "name": "stack_name", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.StatefulPartitionedCall",
    "summary": "returns `f(inputs)`, where `f`'s body is placed and partitioned.",
    "description": "Asynchronously executes a function, potentially across multiple devices but\nwithin a single process. The kernel places and partitions a given function's\nunderlying graph, and executes each of the partitioned subgraphs as a function.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "f", "type": "FlatSymbolRefAttr" },
      { "name": "config", "type": "StrAttr" },
      { "name": "config_proto", "type": "StrAttr" },
      { "name": "executor_type", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.StatefulStandardNormalV2",
    "summary": "Outputs random values from a normal distribution.",
    "description": "The generated values will have mean 0 and standard deviation 1.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "algorithm", "type": "Arg" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatefulTruncatedNormal",
    "summary": "Outputs random values from a truncated normal distribution.",
    "description": "The generated values follow a normal distribution with mean 0 and standard\ndeviation 1, except that values whose magnitude is more than 2 standard\ndeviations from the mean are dropped and re-picked.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "algorithm", "type": "Arg" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatefulUniform",
    "summary": "Outputs random values from a uniform distribution.",
    "description": "The generated values follow a uniform distribution in the range `[0, 1)`. The\nlower bound 0 is included in the range, while the upper bound 1 is excluded.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "algorithm", "type": "Arg" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatefulUniformFullInt",
    "summary": "Outputs random integers from a uniform distribution.",
    "description": "The generated values are uniform integers covering the whole range of `dtype`.",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "algorithm", "type": "TF_Int64Tensor" },
      { "name": "shape", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.StatefulUniformInt",
    "summary": "Outputs random integers from a uniform distribution.",
    "description": "The generated values are uniform integers in the range `[minval, maxval)`.\nThe lower bound `minval` is included in the range, while the upper bound\n`maxval` is excluded.\n\nThe random integers are slightly biased unless `maxval - minval` is an exact\npower of two.  The bias is small for values of `maxval - minval` significantly\nsmaller than the range of the output (either `2^32` or `2^64`).",
    "inputs": [
      { "name": "resource", "type": "Arg" },
      { "name": "algorithm", "type": "TF_Int64Tensor" },
      { "name": "shape", "type": "TF_I32OrI64Tensor" },
      { "name": "minval", "type": "TensorOf" },
      { "name": "maxval", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.StatelessMultinomial",
    "summary": "Draws samples from a multinomial distribution.",
    "inputs": [
      { "name": "logits", "type": "Arg" },
      { "name": "num_samples", "type": "Arg" },
      { "name": "seed", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessParameterizedTruncatedNormal",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" },
      { "name": "means", "type": "Arg" },
      { "name": "stddevs", "type": "Arg" },
      { "name": "minvals", "type": "Arg" },
      { "name": "maxvals", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomBinomial",
    "summary": "Outputs deterministic pseudorandom random numbers from a binomial distribution.",
    "description": "Outputs random values from a binomial distribution.\n\nThe outputs are a deterministic function of `shape`, `seed`, `counts`, and `probs`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" },
      { "name": "counts", "type": "Arg" },
      { "name": "probs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomGammaV2",
    "summary": "Outputs deterministic pseudorandom random numbers from a gamma distribution.",
    "description": "Outputs random values from a gamma distribution.\n\nThe outputs are a deterministic function of `shape`, `seed`, and `alpha`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" },
      { "name": "alpha", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomGetAlg",
    "summary": "Picks the best counter-based RNG algorithm based on device.",
    "description": "This op picks the best counter-based RNG algorithm based on device.",
    "outputs": [
      { "name": "alg", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomGetKeyCounter",
    "summary": "Scrambles seed into key and counter, using the best algorithm based on device.",
    "description": "This op scrambles a shape-[2] seed into a key and a counter, both needed by counter-based RNG algorithms. The scrambing uses the best algorithm based on device. The scrambling is opaque but approximately satisfies the property that different seed results in different key/counter pair (which will in turn result in different random numbers).",
    "inputs": [
      { "name": "seed", "type": "Arg" }
    ],
    "outputs": [
      { "name": "key", "type": "Res" },
      { "name": "counter", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomGetKeyCounterAlg",
    "summary": "Picks the best algorithm based on device, and scrambles seed into key and counter.",
    "description": "This op picks the best counter-based RNG algorithm based on device, and scrambles a shape-[2] seed into a key and a counter, both needed by the counter-based algorithm. The scrambling is opaque but approximately satisfies the property that different seed results in different key/counter pair (which will in turn result in different random numbers).",
    "inputs": [
      { "name": "seed", "type": "Arg" }
    ],
    "outputs": [
      { "name": "key", "type": "Res" },
      { "name": "counter", "type": "Res" },
      { "name": "alg", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomNormal",
    "summary": "Outputs deterministic pseudorandom values from a normal distribution.",
    "description": "The generated values will have mean 0 and standard deviation 1.\n\nThe outputs are a deterministic function of `shape` and `seed`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomNormalV2",
    "summary": "Outputs deterministic pseudorandom values from a normal distribution.",
    "description": "The generated values will have mean 0 and standard deviation 1.\n\nThe outputs are a deterministic function of `shape`, `key`, `counter` and `alg`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "key", "type": "Arg" },
      { "name": "counter", "type": "Arg" },
      { "name": "alg", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomPoisson",
    "summary": "Outputs deterministic pseudorandom random numbers from a Poisson distribution.",
    "description": "Outputs random values from a Poisson distribution.\n\nThe outputs are a deterministic function of `shape`, `seed`, and `lam`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" },
      { "name": "lam", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomUniform",
    "summary": "Outputs deterministic pseudorandom random values from a uniform distribution.",
    "description": "The generated values follow a uniform distribution in the range `[0, 1)`. The\nlower bound 0 is included in the range, while the upper bound 1 is excluded.\n\nThe outputs are a deterministic function of `shape` and `seed`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomUniformFullInt",
    "summary": "Outputs deterministic pseudorandom random integers from a uniform distribution.",
    "description": "The generated values are uniform integers covering the whole range of `dtype`.\n\nThe outputs are a deterministic function of `shape` and `seed`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomUniformFullIntV2",
    "summary": "Outputs deterministic pseudorandom random integers from a uniform distribution.",
    "description": "The generated values are uniform integers covering the whole range of `dtype`.\n\nThe outputs are a deterministic function of `shape`, `key`, `counter` and `alg`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "key", "type": "Arg" },
      { "name": "counter", "type": "Arg" },
      { "name": "alg", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomUniformInt",
    "summary": "Outputs deterministic pseudorandom random integers from a uniform distribution.",
    "description": "The generated values follow a uniform distribution in the range `[minval, maxval)`.\n\nThe outputs are a deterministic function of `shape`, `seed`, `minval`, and `maxval`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" },
      { "name": "minval", "type": "Arg" },
      { "name": "maxval", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomUniformIntV2",
    "summary": "Outputs deterministic pseudorandom random integers from a uniform distribution.",
    "description": "The generated values follow a uniform distribution in the range `[minval, maxval)`.\n\nThe outputs are a deterministic function of `shape`, `key`, `counter`, `alg`, `minval` and `maxval`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "key", "type": "Arg" },
      { "name": "counter", "type": "Arg" },
      { "name": "alg", "type": "Arg" },
      { "name": "minval", "type": "Arg" },
      { "name": "maxval", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessRandomUniformV2",
    "summary": "Outputs deterministic pseudorandom random values from a uniform distribution.",
    "description": "The generated values follow a uniform distribution in the range `[0, 1)`. The\nlower bound 0 is included in the range, while the upper bound 1 is excluded.\n\nThe outputs are a deterministic function of `shape`, `key`, `counter` and `alg`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "key", "type": "Arg" },
      { "name": "counter", "type": "Arg" },
      { "name": "alg", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessTruncatedNormal",
    "summary": "Outputs deterministic pseudorandom values from a truncated normal distribution.",
    "description": "The generated values follow a normal distribution with mean 0 and standard\ndeviation 1, except that values whose magnitude is more than 2 standard\ndeviations from the mean are dropped and re-picked.\n\nThe outputs are a deterministic function of `shape` and `seed`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "seed", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StatelessTruncatedNormalV2",
    "summary": "Outputs deterministic pseudorandom values from a truncated normal distribution.",
    "description": "The generated values follow a normal distribution with mean 0 and standard\ndeviation 1, except that values whose magnitude is more than 2 standard\ndeviations from the mean are dropped and re-picked.\n\nThe outputs are a deterministic function of `shape`, `key`, `counter` and `alg`.",
    "inputs": [
      { "name": "shape", "type": "Arg" },
      { "name": "key", "type": "Arg" },
      { "name": "counter", "type": "Arg" },
      { "name": "alg", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StaticRegexFullMatch",
    "summary": "Check if the input matches the regex pattern.",
    "description": "The input is a string tensor of any shape. The pattern is the\nregular expression to be matched with every element of the input tensor.\nThe boolean values (True or False) of the output tensor indicate\nif the input matches the regex pattern provided.\n\nThe pattern follows the re2 syntax (https://github.com/google/re2/wiki/Syntax)",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "pattern", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.StopGradient",
    "summary": "Stops gradient computation.",
    "description": "When executed in a graph, this op outputs its input tensor as-is.\n\nWhen building ops to compute gradients, this op prevents the contribution of\nits inputs to be taken into account.  Normally, the gradient generator adds ops\nto a graph to compute the derivatives of a specified 'loss' by recursively\nfinding out inputs that contributed to its computation.  If you insert this op\nin the graph it inputs are masked from the gradient generator.  They are not\ntaken into account for computing gradients.\n\nThis is useful any time you want to compute a value with TensorFlow but need\nto pretend that the value was a constant. For example, the softmax function\nfor a vector x can be written as\n\n```python\n\n  def softmax(x):\n    numerator = tf.exp(x)\n    denominator = tf.reduce_sum(numerator)\n    return numerator / denominator\n```\n\nThis however is susceptible to overflow if the values in x are large. An\nalternative more stable way is to subtract the maximum of x from each of the\nvalues.\n\n```python\n\n  def stable_softmax(x):\n    z = x - tf.reduce_max(x)\n    numerator = tf.exp(z)\n    denominator = tf.reduce_sum(numerator)\n    return numerator / denominator\n```\n\nHowever, when we backprop through the softmax to x, we dont want to backprop\nthrough the `tf.reduce_max(x)` (if the max values are not unique then the\ngradient could flow to the wrong input) calculation and treat that as a\nconstant. Therefore, we should write this out as\n\n```python\n\n  def stable_softmax(x):\n    z = x - tf.stop_gradient(tf.reduce_max(x))\n    numerator = tf.exp(z)\n    denominator = tf.reduce_sum(numerator)\n    return numerator / denominator\n```\n\nSome other examples include:\n\n*  The *EM* algorithm where the *M-step* should not involve backpropagation\n   through the output of the *E-step*.\n*  Contrastive divergence training of Boltzmann machines where, when\n   differentiating the energy function, the training must not backpropagate\n   through the graph that generated the samples from the model.\n*  Adversarial training, where no backprop should happen through the adversarial\n   example generation process.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.StoreMinibatchStatisticsInFdo",
    "summary": "Store the number of IDs and unique IDs in an FDO table.",
    "inputs": [
      { "name": "program_key", "type": "TF_StrTensor" },
      { "name": "max_ids", "type": "TF_Int32Tensor" },
      { "name": "max_uniques", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "sample_count", "type": "ConfinedAttr" },
      { "name": "num_replica", "type": "ConfinedAttr" },
      { "name": "feature_width", "type": "ConfinedAttr" },
      { "name": "num_sc_per_chip", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" },
      { "name": "mini_batch_splits", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.StridedSlice",
    "summary": "Return a strided slice from `input`.",
    "description": "Note, most python users will want to use the Python `Tensor.__getitem__`\nor `Variable.__getitem__` rather than this op directly.\n\nThe goal of this op is to produce a new tensor with a subset of\nthe elements from the `n` dimensional `input` tensor. The subset is chosen using\na sequence of `m` sparse range specifications encoded into the arguments\nof this function. Note, in some cases\n`m` could be equal to `n`, but this need not be the case. Each\nrange specification entry can be one of the following:\n\n- An ellipsis (...). Ellipses are used to imply zero or more\n  dimensions of full-dimension selection and are produced using\n  `ellipsis_mask`. For example, `foo[...]` is the identity slice.\n\n- A new axis. This is used to insert a new shape=1 dimension and is\n  produced using `new_axis_mask`. For example, `foo[:, ...]` where\n  `foo` is shape `(3, 4)` produces a `(1, 3, 4)` tensor.\n\n\n- A range `begin:end:stride`. This is used to specify how much to choose from\n  a given dimension. `stride` can be any integer but 0.  `begin` is an integer\n  which represents the index of the first value to select while `end` represents\n  the index of the last value to select. The number of values selected in each\n  dimension is `end - begin` if `stride > 0` and `begin - end` if `stride < 0`.\n  `begin` and `end` can be negative where `-1` is the last element, `-2` is\n  the second to last. `begin_mask` controls whether to replace the explicitly\n  given `begin` with an implicit effective value of `0` if `stride > 0` and\n  `-1` if `stride < 0`. `end_mask` is analogous but produces the number\n  required to create the largest open interval. For example, given a shape\n  `(3,)` tensor `foo[:]`, the effective `begin` and `end` are `0` and `3`. Do\n  not assume this is equivalent to `foo[0:-1]` which has an effective `begin`\n  and `end` of `0` and `2`. Another example is `foo[-2::-1]` which reverses the\n  first dimension of a tensor while dropping the last two (in the original\n  order elements). For example `foo = [1,2,3,4]; foo[-2::-1]` is `[4,3]`.\n\n- A single index. This is used to keep only elements that have a given\n  index. For example (`foo[2, :]` on a shape `(5,6)` tensor produces a\n  shape `(6,)` tensor. This is encoded in `begin` and `end` and\n  `shrink_axis_mask`.\n\nEach conceptual range specification is encoded in the op's argument. This\nencoding is best understand by considering a non-trivial example. In\nparticular,\n`foo[1, 2:4, None, ..., :-3:-1, :]` will be encoded as\n\n```\nbegin = [1, 2, x, x, 0, x] # x denotes don't care (usually 0)\nend = [2, 4, x, x, -3, x]\nstrides = [1, 1, x, x, -1, 1]\nbegin_mask = 1<<4 | 1<<5 = 48\nend_mask = 1<<5 = 32\nellipsis_mask = 1<<3 = 8\nnew_axis_mask = 1<<2 = 4\nshrink_axis_mask = 1<<0 = 1\n```\n\nIn this case if `foo.shape` is (5, 5, 5, 5, 5, 5) the final shape of\nthe slice becomes (2, 1, 5, 5, 2, 5).\nLet us walk step by step through each argument specification.\n\n1.  The first argument in the example slice is turned into `begin = 1` and\n`end = begin + 1 = 2`. To disambiguate from the original spec `2:4` we\nalso set the appropriate bit in `shrink_axis_mask`.\n\n2. `2:4` is contributes 2, 4, 1 to begin, end, and stride. All masks have\nzero bits contributed.\n\n3. None is a synonym for `tf.newaxis`. This means insert a dimension of size 1\ndimension in the final shape. Dummy values are contributed to begin,\nend and stride, while the new_axis_mask bit is set.\n\n4. `...` grab the full ranges from as many dimensions as needed to\nfully specify a slice for every dimension of the input shape.\n\n5. `:-3:-1` shows the use of negative indices. A negative index `i` associated\nwith a dimension that has shape `s` is converted to a positive index\n`s + i`. So `-1` becomes `s-1` (i.e. the last element). This conversion\nis done internally so begin, end and strides receive x, -3, and -1.\nThe appropriate begin_mask bit is set to indicate the start range is the\nfull range (ignoring the x).\n\n6. `:` indicates that the entire contents of the corresponding dimension\nis selected. This is equivalent to `::` or `0::1`. begin, end, and strides\nreceive 0, 0, and 1, respectively. The appropriate bits in `begin_mask` and\n`end_mask` are also set.\n\n*Requirements*:\n  `0 != strides[i] for i in [0, m)`\n  `ellipsis_mask must be a power of two (only one ellipsis)`",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "begin", "type": "Arg" },
      { "name": "end", "type": "Arg" },
      { "name": "strides", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "begin_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "end_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "ellipsis_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "new_axis_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "shrink_axis_mask", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.StridedSliceGrad",
    "summary": "Returns the gradient of `StridedSlice`.",
    "description": "Since `StridedSlice` cuts out pieces of its `input` which is size\n`shape`, its gradient will have the same shape (which is passed here\nas `shape`). The gradient will be zero in any element that the slice\ndoes not select.\n\nArguments are the same as StridedSliceGrad with the exception that\n`dy` is the input gradient to be propagated and `shape` is the\nshape of `StridedSlice`'s `input`.",
    "inputs": [
      { "name": "shape", "type": "TF_I32OrI64Tensor" },
      { "name": "begin", "type": "TF_I32OrI64Tensor" },
      { "name": "end", "type": "TF_I32OrI64Tensor" },
      { "name": "strides", "type": "TF_I32OrI64Tensor" },
      { "name": "dy", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "begin_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "end_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "ellipsis_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "new_axis_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "shrink_axis_mask", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.StringFormat",
    "summary": "Formats a string template using a list of tensors.",
    "description": "Formats a string template using a list of tensors, pretty-printing tensor summaries.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_StrTensor" }
    ],
    "attributes": [
      { "name": "strtemplate", "type": "DefaultValuedStrAttr" },
      { "name": "placeholder", "type": "DefaultValuedStrAttr" },
      { "name": "summarize", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.StringJoin",
    "summary": "Joins the strings in the given list of string tensors into one tensor;",
    "description": "with the given separator (default is an empty separator).\n\nExamples:\n\n>>> s = [\"hello\", \"world\", \"tensorflow\"]\n>>> tf.strings.join(s, \" \")\n<tf.Tensor: shape=(), dtype=string, numpy=b'hello world tensorflow'>",
    "inputs": [
      { "name": "inputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_StrTensor" }
    ],
    "attributes": [
      { "name": "separator", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.StringStrip",
    "summary": "Strip leading and trailing whitespaces from the Tensor.",
    "description": "Examples:\n\n>>> tf.strings.strip([\"\\nTensorFlow\", \"     The python library    \"]).numpy()\narray([b'TensorFlow', b'The python library'], dtype=object)",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.StringToHashBucketFast",
    "summary": "Converts each string in the input Tensor to its hash mod by a number of buckets.",
    "description": "The hash function is deterministic on the content of the string within the\nprocess and will never change. However, it is not suitable for cryptography.\nThis function may be used when CPU time is scarce and inputs are trusted or\nunimportant. There is a risk of adversaries constructing inputs that all hash\nto the same bucket. To prevent this problem, use a strong hash function with\n`tf.string_to_hash_bucket_strong`.\n\nExamples:\n\n>>> tf.strings.to_hash_bucket_fast([\"Hello\", \"TensorFlow\", \"2.x\"], 3).numpy()\narray([0, 2, 2])",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "num_buckets", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.Sub",
    "summary": "Returns x - y element-wise.",
    "description": "*NOTE*: `Subtract` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.Sum",
    "summary": "Computes the sum of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "reduction_indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SummaryWriter",
    "summary": "Returns a handle to be used to access a summary writer.",
    "description": "The summary writer is an in-graph resource which can be used by ops to write\nsummaries to event files.\n\nwriter: the summary writer resource. Scalar handle.",
    "outputs": [
      { "name": "writer", "type": "Res" }
    ],
    "attributes": [
      { "name": "shared_name", "type": "StrAttr" },
      { "name": "container", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.Svd",
    "summary": "Computes the singular value decompositions of one or more matrices.",
    "description": "Computes the SVD of each inner matrix in `input` such that\n`input[..., :, :] = u[..., :, :] * diag(s[..., :, :]) * transpose(v[..., :, :])`\n\n```python\n# a is a tensor containing a batch of matrices.\n# s is a tensor of singular values for each matrix.\n# u is the tensor containing the left singular vectors for each matrix.\n# v is the tensor containing the right singular vectors for each matrix.\ns, u, v = svd(a)\ns, _, _ = svd(a, compute_uv=False)\n```",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "s", "type": "Res" },
      { "name": "u", "type": "Res" },
      { "name": "v", "type": "Res" }
    ],
    "attributes": [
      { "name": "compute_uv", "type": "DefaultValuedOptionalAttr" },
      { "name": "full_matrices", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.SymbolicGradient",
    "summary": "Computes the gradient function for function f via backpropagation.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "f", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.TakeDataset",
    "summary": "Creates a dataset that contains `count` elements from the `input_dataset`.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "count", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TakeWhileDataset",
    "summary": "Creates a dataset that stops iteration when predicate` is false.",
    "description": "The `predicate` function must return a scalar boolean and accept the\nfollowing arguments:\n\n* One tensor for each component of an element of `input_dataset`.\n* One tensor for each value in `other_arguments`.",
    "inputs": [
      { "name": "input_dataset", "type": "TF_VariantTensor" },
      { "name": "other_arguments", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "predicate", "type": "SymbolRefAttr" },
      { "name": "output_types", "type": "ConfinedAttr" },
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Tan",
    "summary": "Computes tan of x element-wise.",
    "description": "Given an input tensor, this function computes tangent of every\n  element in the tensor. Input range is `(-inf, inf)` and\n  output range is `(-inf, inf)`. If input lies outside the boundary, `nan`\n  is returned.\n\n  ```python\n  x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 200, 10000, float(\"inf\")])\n  tf.math.tan(x) ==> [nan 0.45231566 -0.5463025 1.5574077 2.572152 -1.7925274 0.32097113 nan]\n  ```",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Tanh",
    "summary": "Computes hyperbolic tangent of `x` element-wise.",
    "description": "Given an input tensor, this function computes hyperbolic tangent of every\n  element in the tensor. Input range is `[-inf, inf]` and\n  output range is `[-1,1]`.\n\n  >>> x = tf.constant([-float(\"inf\"), -5, -0.5, 1, 1.2, 2, 3, float(\"inf\")])\n  >>> tf.math.tanh(x)\n  <tf.Tensor: shape=(8,), dtype=float32, numpy=\n  array([-1.0, -0.99990916, -0.46211717,  0.7615942 ,  0.8336547 ,\n          0.9640276 ,  0.9950547 ,  1.0], dtype=float32)>",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ],
    "category": "Activation"
  },
  {
    "name": "tf.TanhGrad",
    "summary": "Computes the gradient for the tanh of `x` wrt its input.",
    "description": "Specifically, `grad = dy * (1 - y*y)`, where `y = tanh(x)`, and `dy`\nis the corresponding input gradient.",
    "inputs": [
      { "name": "y", "type": "TF_FpOrComplexTensor" },
      { "name": "dy", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.TensorArrayCloseV3",
    "summary": "Delete the TensorArray from its resource container.",
    "description": "This enables the user to close and release the resource in the middle\nof a step/run.",
    "inputs": [
      { "name": "handle", "type": "Arg" }
    ]
  },
  {
    "name": "tf.TensorArrayConcatV3",
    "summary": "Concat the elements from the TensorArray into value `value`.",
    "description": "Takes `T` elements of shapes\n\n  ```\n  (n0 x d0 x d1 x ...), (n1 x d0 x d1 x ...), ..., (n(T-1) x d0 x d1 x ...)\n  ```\n\nand concatenates them into a Tensor of shape:\n\n  ```\n  (n0 + n1 + ... + n(T-1) x d0 x d1 x ...)\n  ```\n\nAll elements must have the same shape (excepting the first dimension).",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "flow_in", "type": "Arg" }
    ],
    "outputs": [
      { "name": "value", "type": "Res" },
      { "name": "lengths", "type": "Res" }
    ],
    "attributes": [
      { "name": "element_shape_except0", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorArrayGatherV3",
    "summary": "Gather specific elements from the TensorArray into output `value`.",
    "description": "All elements selected by `indices` must have the same shape.",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "flow_in", "type": "Arg" }
    ],
    "outputs": [
      { "name": "value", "type": "Res" }
    ],
    "attributes": [
      { "name": "element_shape", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorArrayGradV3",
    "summary": "Creates a TensorArray for storing the gradients of values in the given handle.",
    "description": "If the given TensorArray gradient already exists, returns a reference to it.\n\nLocks the size of the original TensorArray by disabling its dynamic size flag.\n\n**A note about the input flow_in:**\n\nThe handle flow_in forces the execution of the gradient lookup to occur\nonly after certain other operations have occurred.  For example, when\nthe forward TensorArray is dynamically sized, writes to this TensorArray\nmay resize the object.  The gradient TensorArray is statically sized based\non the size of the forward TensorArray when this operation executes.\nFurthermore, the size of the forward TensorArray is frozen by this call.\nAs a result, the flow is used to ensure that the call to generate the gradient\nTensorArray only happens after all writes are executed.\n\nIn the case of dynamically sized TensorArrays, gradient computation should\nonly be performed on read operations that have themselves been chained via\nflow to occur only after all writes have executed. That way the final size\nof the forward TensorArray is known when this operation is called.\n\n**A note about the source attribute:**\n\nTensorArray gradient calls use an accumulator TensorArray object.  If\nmultiple gradients are calculated and run in the same session, the multiple\ngradient nodes may accidentally flow through the same accumulator TensorArray.\nThis double counts and generally breaks the TensorArray gradient flow.\n\nThe solution is to identify which gradient call this particular\nTensorArray gradient is being called in.  This is performed by identifying\na unique string (e.g. \"gradients\", \"gradients_1\", ...) from the input\ngradient Tensor's name.  This string is used as a suffix when creating\nthe TensorArray gradient object here (the attribute `source`).\n\nThe attribute `source` is added as a suffix to the forward TensorArray's\nname when performing the creation / lookup, so that each separate gradient\ncalculation gets its own TensorArray accumulator.",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "flow_in", "type": "Arg" }
    ],
    "outputs": [
      { "name": "grad_handle", "type": "Res" },
      { "name": "flow_out", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "source", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.TensorArrayReadV3",
    "summary": "Read an element from the TensorArray into output `value`.",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "index", "type": "TF_Int32Tensor" },
      { "name": "flow_in", "type": "Arg" }
    ],
    "outputs": [
      { "name": "value", "type": "Res" }
    ]
  },
  {
    "name": "tf.TensorArrayScatterV3",
    "summary": "Scatter the data from the input value into specific TensorArray elements.",
    "description": "`indices` must be a vector, its length must match the first dim of `value`.",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "value", "type": "Arg" },
      { "name": "flow_in", "type": "Arg" }
    ],
    "outputs": [
      { "name": "flow_out", "type": "Res" }
    ]
  },
  {
    "name": "tf.TensorArraySizeV3",
    "summary": "Get the current size of the TensorArray.",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "flow_in", "type": "Arg" }
    ],
    "outputs": [
      { "name": "size", "type": "Res" }
    ]
  },
  {
    "name": "tf.TensorArraySplitV3",
    "summary": "Split the data from the input value into TensorArray elements.",
    "description": "Assuming that `lengths` takes on values\n\n  ```\n  (n0, n1, ..., n(T-1))\n  ```\n\nand that `value` has shape\n\n  ```\n  (n0 + n1 + ... + n(T-1) x d0 x d1 x ...),\n  ```\n\nthis splits values into a TensorArray with T tensors.\n\nTensorArray index t will be the subtensor of values with starting position\n\n  ```\n  (n0 + n1 + ... + n(t-1), 0, 0, ...)\n  ```\n\nand having size\n\n  ```\n  nt x d0 x d1 x ...\n  ```",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "value", "type": "Arg" },
      { "name": "lengths", "type": "Arg" },
      { "name": "flow_in", "type": "Arg" }
    ],
    "outputs": [
      { "name": "flow_out", "type": "Res" }
    ]
  },
  {
    "name": "tf.TensorArrayV3",
    "summary": "An array of Tensors of given size.",
    "description": "Write data via Write and read via Read or Pack.",
    "inputs": [
      { "name": "size", "type": "Arg" }
    ],
    "outputs": [
      { "name": "handle", "type": "Res" },
      { "name": "flow", "type": "Res" }
    ],
    "attributes": [
      { "name": "dtype", "type": "TypeAttr" },
      { "name": "element_shape", "type": "DefaultValuedOptionalAttr" },
      { "name": "dynamic_size", "type": "DefaultValuedOptionalAttr" },
      { "name": "clear_after_read", "type": "DefaultValuedOptionalAttr" },
      { "name": "identical_element_shapes", "type": "DefaultValuedOptionalAttr" },
      { "name": "tensor_array_name", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorArrayWriteV3",
    "summary": "Push an element onto the tensor_array.",
    "inputs": [
      { "name": "handle", "type": "Arg" },
      { "name": "index", "type": "Arg" },
      { "name": "value", "type": "Arg" },
      { "name": "flow_in", "type": "Arg" }
    ],
    "outputs": [
      { "name": "flow_out", "type": "Res" }
    ]
  },
  {
    "name": "tf.TensorListConcatV2",
    "summary": "Concats all tensors in the list along the 0th dimension.",
    "description": "Requires that all tensors have the same shape except the first dimension.\n\ninput_handle: The input list.\nelement_shape: The shape of the uninitialized elements in the list. If the first\n  dimension is not -1, it is assumed that all list elements have the same\n  leading dim.\nleading_dims: The list of leading dims of uninitialized list elements. Used if\n  the leading dim of input_handle.element_shape or the element_shape input arg\n  is not already set.\ntensor: The concated result.\nlengths: Output tensor containing sizes of the 0th dimension of tensors in the list, used for computing the gradient.",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "element_shape", "type": "TF_I32OrI64Tensor" },
      { "name": "leading_dims", "type": "TF_Int64Tensor" }
    ],
    "outputs": [
      { "name": "tensor", "type": "TF_Tensor" },
      { "name": "lengths", "type": "TF_Int64Tensor" }
    ]
  },
  {
    "name": "tf.TensorListElementShape",
    "summary": "The shape of the elements of the given list, as a tensor.",
    "description": "input_handle: the list\n  element_shape: the shape of elements of the list",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" }
    ],
    "outputs": [
      { "name": "element_shape", "type": "TF_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tf.TensorListFromTensor",
    "summary": "Creates a TensorList which, when stacked, has the value of `tensor`.",
    "description": "Each tensor in the result list corresponds to one row of the input tensor.\n\ntensor: The input tensor.\noutput_handle: The list.",
    "inputs": [
      { "name": "tensor", "type": "TF_Tensor" },
      { "name": "element_shape", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output_handle", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.TensorListGather",
    "summary": "Creates a Tensor by indexing into the TensorList.",
    "description": "Each row in the produced Tensor corresponds to the element in the TensorList\nspecified by the given index (see `tf.gather`).\n\ninput_handle: The input tensor list.\nindices: The indices used to index into the list.\nvalues: The tensor.",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "indices", "type": "TF_Int32Tensor" },
      { "name": "element_shape", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "values", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.TensorListGetItem",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "index", "type": "TF_Int32Tensor" },
      { "name": "element_shape", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "item", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.TensorListLength",
    "summary": "Returns the number of tensors in the input tensor list.",
    "description": "input_handle: the input list\nlength: the number of tensors in the list",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" }
    ],
    "outputs": [
      { "name": "length", "type": "TF_Int32Tensor" }
    ]
  },
  {
    "name": "tf.TensorListPopBack",
    "summary": "Returns the last element of the input list as well as a list with all but that element.",
    "description": "Fails if the list is empty.\n\ninput_handle: the input list\ntensor: the withdrawn last element of the list\nelement_dtype: the type of elements in the list\nelement_shape: the shape of the output tensor",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "element_shape", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output_handle", "type": "TF_VariantTensor" },
      { "name": "tensor", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.TensorListPushBack",
    "summary": "Returns a list which has the passed-in `Tensor` as last element and the other elements of the given list in `input_handle`.",
    "description": "tensor: The tensor to put on the list.\ninput_handle: The old list.\noutput_handle: A list with the elements of the old list followed by tensor.\nelement_dtype: the type of elements in the list.\nelement_shape: a shape compatible with that of elements in the list.",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "tensor", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output_handle", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.TensorListReserve",
    "summary": "List of the given size with empty elements.",
    "description": "element_shape: the shape of the future elements of the list\nnum_elements: the number of elements to reserve\nhandle: the output list\nelement_dtype: the desired type of elements in the list.",
    "inputs": [
      { "name": "element_shape", "type": "TF_I32OrI64Tensor" },
      { "name": "num_elements", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.TensorListResize",
    "summary": "Resizes the list.",
    "description": "input_handle: the input list\nsize: size of the output list",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "size", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output_handle", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.TensorListScatterIntoExistingList",
    "summary": "Scatters tensor at indices in an input list.",
    "description": "Each member of the TensorList corresponds to one row of the input tensor,\nspecified by the given index (see `tf.gather`).\n\ninput_handle: The list to scatter into.\ntensor: The input tensor.\nindices: The indices used to index into the list.\noutput_handle: The TensorList.",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "tensor", "type": "TF_Tensor" },
      { "name": "indices", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output_handle", "type": "TF_VariantTensor" }
    ]
  },
  {
    "name": "tf.TensorListSetItem",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "index", "type": "TF_Int32Tensor" },
      { "name": "item", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output_handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "resize_if_index_out_of_bounds", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorListStack",
    "summary": "Stacks all tensors in the list.",
    "description": "Requires that all tensors have the same shape.\n\ninput_handle: the input list\ntensor: the gathered result\nnum_elements: optional. If not -1, the number of elements in the list.",
    "inputs": [
      { "name": "input_handle", "type": "TF_VariantTensor" },
      { "name": "element_shape", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "tensor", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "num_elements", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorScatterAdd",
    "summary": "Adds sparse `updates` to an existing tensor according to `indices`.",
    "description": "This operation creates a new tensor by adding sparse `updates` to the passed\nin `tensor`.\nThis operation is very similar to `tf.compat.v1.scatter_nd_add`, except that the\nupdates are added onto an existing tensor (as opposed to a variable). If the\nmemory for the existing tensor cannot be re-used, a copy is made and updated.\n\n`indices` is an integer tensor containing indices into a new tensor of shape\n`tensor.shape`.  The last dimension of `indices` can be at most the rank of\n`tensor.shape`:\n\n```\nindices.shape[-1] <= tensor.shape.rank\n```\n\nThe last dimension of `indices` corresponds to indices into elements\n(if `indices.shape[-1] = tensor.shape.rank`) or slices\n(if `indices.shape[-1] < tensor.shape.rank`) along dimension\n`indices.shape[-1]` of `tensor.shape`.  `updates` is a tensor with shape\n\n```\nindices.shape[:-1] + tensor.shape[indices.shape[-1]:]\n```\n\nThe simplest form of `tensor_scatter_nd_add` is to add individual elements to a\ntensor by index. For example, say we want to add 4 elements in a rank-1\ntensor with 8 elements.\n\nIn Python, this scatter add operation would look like this:\n\n>>> indices = tf.constant([[4], [3], [1], [7]])\n>>> updates = tf.constant([9, 10, 11, 12])\n>>> tensor = tf.ones([8], dtype=tf.int32)\n>>> updated = tf.tensor_scatter_nd_add(tensor, indices, updates)\n>>> updated\n<tf.Tensor: shape=(8,), dtype=int32,\nnumpy=array([ 1, 12,  1, 11, 10,  1,  1, 13], dtype=int32)>\n\nWe can also, insert entire slices of a higher rank tensor all at once. For\nexample, if we wanted to insert two slices in the first dimension of a\nrank-3 tensor with two matrices of new values.\n\nIn Python, this scatter add operation would look like this:\n\n>>> indices = tf.constant([[0], [2]])\n>>> updates = tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],\n...                         [7, 7, 7, 7], [8, 8, 8, 8]],\n...                        [[5, 5, 5, 5], [6, 6, 6, 6],\n...                         [7, 7, 7, 7], [8, 8, 8, 8]]])\n>>> tensor = tf.ones([4, 4, 4],dtype=tf.int32)\n>>> updated = tf.tensor_scatter_nd_add(tensor, indices, updates)\n>>> updated\n<tf.Tensor: shape=(4, 4, 4), dtype=int32,\nnumpy=array([[[6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8], [9, 9, 9, 9]],\n             [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],\n             [[6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8], [9, 9, 9, 9]],\n             [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]], dtype=int32)>\n\n\nIf `indices` contains any out-of-bound indices, depending on\n`bad_indices_policy`, the op will either return an error or ignore the\nout-of-bound indices. `bad_indices_policy` can be one of the following values:\n1. \"\" or \"DEFAULT\": raises on CPU and ignore on GPU. This is because\n   historically on CPU and GPU we handle errors in different ways, and for\n   backward compatibility we keep the default behavior.\n2. \"ERROR\": raises error; GPU does not support this value.\n3. \"IGNORE\": ignore the bad indices; supported on both CPU and GPU.",
    "inputs": [
      { "name": "tensor", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorScatterMax",
    "summary": "Apply a sparse update to a tensor taking the element-wise maximum.",
    "description": "Returns a new tensor copied from `tensor` whose values are element-wise maximum between\ntensor and updates according to the indices.\n\n>>> tensor = [0, 0, 0, 0, 0, 0, 0, 0]\n>>> indices = [[1], [4], [5]]\n>>> updates = [1, -1, 1]\n>>> tf.tensor_scatter_nd_max(tensor, indices, updates).numpy()\narray([0, 1, 0, 0, 0, 1, 0, 0], dtype=int32)\n\nRefer to `tf.tensor_scatter_nd_update` for more details.",
    "inputs": [
      { "name": "tensor", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorScatterMin",
    "inputs": [
      { "name": "tensor", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorScatterSub",
    "summary": "Subtracts sparse `updates` from an existing tensor according to `indices`.",
    "description": "This operation creates a new tensor by subtracting sparse `updates` from the\npassed in `tensor`.\nThis operation is very similar to `tf.scatter_nd_sub`, except that the updates\nare subtracted from an existing tensor (as opposed to a variable). If the memory\nfor the existing tensor cannot be re-used, a copy is made and updated.\n\n`indices` is an integer tensor containing indices into a new tensor of shape\n`shape`.  The last dimension of `indices` can be at most the rank of `shape`:\n\n    indices.shape[-1] <= shape.rank\n\nThe last dimension of `indices` corresponds to indices into elements\n(if `indices.shape[-1] = shape.rank`) or slices\n(if `indices.shape[-1] < shape.rank`) along dimension `indices.shape[-1]` of\n`shape`.  `updates` is a tensor with shape\n\n    indices.shape[:-1] + shape[indices.shape[-1]:]\n\nThe simplest form of tensor_scatter_sub is to subtract individual elements\nfrom a tensor by index. For example, say we want to insert 4 scattered elements\nin a rank-1 tensor with 8 elements.\n\nIn Python, this scatter subtract operation would look like this:\n\n```python\n    indices = tf.constant([[4], [3], [1], [7]])\n    updates = tf.constant([9, 10, 11, 12])\n    tensor = tf.ones([8], dtype=tf.int32)\n    updated = tf.tensor_scatter_nd_sub(tensor, indices, updates)\n    print(updated)\n```\n\nThe resulting tensor would look like this:\n\n    [1, -10, 1, -9, -8, 1, 1, -11]\n\nWe can also, insert entire slices of a higher rank tensor all at once. For\nexample, if we wanted to insert two slices in the first dimension of a\nrank-3 tensor with two matrices of new values.\n\nIn Python, this scatter add operation would look like this:\n\n```python\n    indices = tf.constant([[0], [2]])\n    updates = tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],\n                            [7, 7, 7, 7], [8, 8, 8, 8]],\n                           [[5, 5, 5, 5], [6, 6, 6, 6],\n                            [7, 7, 7, 7], [8, 8, 8, 8]]])\n    tensor = tf.ones([4, 4, 4],dtype=tf.int32)\n    updated = tf.tensor_scatter_nd_sub(tensor, indices, updates)\n    print(updated)\n```\n\nThe resulting tensor would look like this:\n\n    [[[-4, -4, -4, -4], [-5, -5, -5, -5], [-6, -6, -6, -6], [-7, -7, -7, -7]],\n     [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],\n     [[-4, -4, -4, -4], [-5, -5, -5, -5], [-6, -6, -6, -6], [-7, -7, -7, -7]],\n     [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]]\n\nNote that on CPU, if an out of bound index is found, an error is returned.\nOn GPU, if an out of bound index is found, the index is ignored.",
    "inputs": [
      { "name": "tensor", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorScatterUpdate",
    "summary": "Scatter `updates` into an existing tensor according to `indices`.",
    "description": "This operation creates a new tensor by applying sparse `updates` to the passed\nin `tensor`.\nThis operation is very similar to `tf.scatter_nd`, except that the updates are\nscattered onto an existing tensor (as opposed to a zero-tensor). If the memory\nfor the existing tensor cannot be re-used, a copy is made and updated.\n\nIf `indices` contains duplicates, then we pick the last update for the index.\n\n**WARNING**: There are some GPU specific semantics for this operation.\n- If an out of bound index is found, the index is ignored.\n- The order in which updates are applied is nondeterministic, so the output\nwill be nondeterministic if `indices` contains duplicates.\n\n`indices` is an integer tensor containing indices into a new tensor of shape\n`shape`.\n\n* `indices` must have at least 2 axes: `(num_updates, index_depth)`.\n* The last axis of `indices` is how deep to index into `tensor` so  this index\n  depth must be less than the rank of `tensor`: `indices.shape[-1] <= tensor.ndim`\n\nif `indices.shape[-1] = tensor.rank` this Op indexes and updates scalar elements.\nif `indices.shape[-1] < tensor.rank` it indexes and updates slices of the input\n`tensor`.\n\nEach `update` has a rank of `tensor.rank - indices.shape[-1]`.\nThe overall shape of `updates` is:\n\n```\nindices.shape[:-1] + tensor.shape[indices.shape[-1]:]\n```\n\nIf `indices` contains any out-of-bound indices, depending on\n`bad_indices_policy`, the op will either return an error or ignore the\nout-of-bound indices. `bad_indices_policy` can be one of the following values:\n1. \"\" or \"DEFAULT\": raises on CPU and ignore on GPU. This is because\n   historically on CPU and GPU we handle errors in different ways, and for\n   backward compatibility we keep the default behavior.\n2. \"ERROR\": raises error; GPU does not support this value.\n3. \"IGNORE\": ignore the bad indices; supported on both CPU and GPU.\n\nFor usage examples see the python [tf.tensor_scatter_nd_update](\nhttps://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update) function",
    "inputs": [
      { "name": "tensor", "type": "Arg" },
      { "name": "indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "bad_indices_policy", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorSliceDataset",
    "summary": "Creates a dataset that emits each dim-0 slice of `components` once.",
    "inputs": [
      { "name": "components", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "handle", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "output_shapes", "type": "ConfinedAttr" },
      { "name": "is_files", "type": "DefaultValuedOptionalAttr" },
      { "name": "metadata", "type": "DefaultValuedOptionalAttr" },
      { "name": "replicate_on_split", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TensorStridedSliceUpdate",
    "summary": "Assign `value` to the sliced l-value reference of `input`.",
    "description": "The values of `value` are assigned to the positions in the tensor `input` that\nare selected by the slice parameters. The slice parameters `begin` `end`\n`strides` etc. work exactly as in `StridedSlice`.\n\nNOTE this op currently does not support broadcasting and so `value`'s shape\nmust be exactly the shape produced by the slice of `input`.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "begin", "type": "TF_I32OrI64Tensor" },
      { "name": "end", "type": "TF_I32OrI64Tensor" },
      { "name": "strides", "type": "TF_I32OrI64Tensor" },
      { "name": "value", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "begin_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "end_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "ellipsis_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "new_axis_mask", "type": "DefaultValuedOptionalAttr" },
      { "name": "shrink_axis_mask", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Tile",
    "summary": "Constructs a tensor by tiling a given tensor.",
    "description": "This operation creates a new tensor by replicating `input` `multiples` times.\nThe output tensor's i'th dimension has `input.dims(i) * multiples[i]` elements,\nand the values of `input` are replicated `multiples[i]` times along the 'i'th\ndimension. For example, tiling `[a b c d]` by `[2]` produces\n`[a b c d a b c d]`.\n\n>>> a = tf.constant([[1,2,3],[4,5,6]], tf.int32)\n>>> b = tf.constant([1,2], tf.int32)\n>>> tf.tile(a, b)\n<tf.Tensor: shape=(2, 6), dtype=int32, numpy=\narray([[1, 2, 3, 1, 2, 3],\n       [4, 5, 6, 4, 5, 6]], dtype=int32)>\n>>> c = tf.constant([2,1], tf.int32)\n>>> tf.tile(a, c)\n<tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6],\n       [1, 2, 3],\n       [4, 5, 6]], dtype=int32)>\n>>> d = tf.constant([2,2], tf.int32)\n>>> tf.tile(a, d)\n<tf.Tensor: shape=(4, 6), dtype=int32, numpy=\narray([[1, 2, 3, 1, 2, 3],\n       [4, 5, 6, 4, 5, 6],\n       [1, 2, 3, 1, 2, 3],\n       [4, 5, 6, 4, 5, 6]], dtype=int32)>",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "multiples", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.Timestamp",
    "summary": "Provides the time since epoch in seconds.",
    "description": "Returns the timestamp as a `float64` for seconds since the Unix epoch.\n\nCommon usages include:\n* Logging\n* Providing a random number seed\n* Debugging graph execution\n* Generating timing information, mainly through comparison of timestamps\n\nNote: In graph mode, the timestamp is computed when the op is executed,\nnot when it is added to the graph.  In eager mode, the timestamp is computed\nwhen the op is eagerly executed.",
    "outputs": [
      { "name": "ts", "type": "TF_Float64Tensor" }
    ]
  },
  {
    "name": "tf.ToBool",
    "summary": "Converts a tensor to a scalar predicate.",
    "description": "Converts a tensor to a scalar predicate with the following rules:\n\n- For 0D tensors, truthiness is determined by comparing against a \"zero\"\n  value. For numerical types it is the obvious zero. For strings it is the\n  empty string.\n\n- For >0D tensors, truthiness is determined by looking at the number of\n  elements. If has zero elements, then the result is false. Otherwise the\n  result is true.\n\nThis matches the behavior of If and While for determining if a tensor counts\nas true/false for a branch condition.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "I1Tensor" }
    ]
  },
  {
    "name": "tf.TopKUnique",
    "summary": "Returns the TopK unique values in the array in sorted order.",
    "description": "The running time is proportional to the product of K and the input\nsize. Sorting the whole array is more efficient for sufficiently large\nvalues of K. The median-of-medians algorithm is probably faster, but\ndifficult to implement efficiently in XLA. If there are fewer than K\nunique numbers (not NANs), the results are padded with negative\ninfinity. NaNs are never returned. Subnormal numbers are flushed to\nzero. If an element appears at multiple indices, the highest index is\nreturned. If a TopK element never appears in the input due to padding\nvalues, the indices are padded with negative one. If a padding value\nappears in the input and padding is needed, the highest index of the\npadding value will be returned. The semantics are not the same as\nkth_order_statistic.",
    "inputs": [
      { "name": "input", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "topk", "type": "TF_Float32Tensor" },
      { "name": "topk_indices", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "k", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.TopKV2",
    "summary": "Finds values and indices of the `k` largest elements for the last dimension.",
    "description": "If the input is a vector (rank-1), finds the `k` largest entries in the vector\nand outputs their values and indices as vectors.  Thus `values[j]` is the\n`j`-th largest entry in `input`, and its index is `indices[j]`.\n\nFor matrices (resp. higher rank input), computes the top `k` entries in each\nrow (resp. vector along the last dimension).  Thus,\n\n    values.shape = indices.shape = input.shape[:-1] + [k]\n\nIf two elements are equal, the lower-index element appears first.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "k", "type": "Arg" }
    ],
    "outputs": [
      { "name": "values", "type": "Res" },
      { "name": "indices", "type": "Res" }
    ],
    "attributes": [
      { "name": "sorted", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TopKWithUnique",
    "summary": "Returns the TopK values in the array in sorted order.",
    "description": "This is a combination of MakeUnique and TopKUnique. The returned top-K will\nhave its lower bits replaced by iota, thus it will be close to the original\nvalue but not exactly the same. The running time is proportional to the product\nof K and the input size. NaNs are never returned. Subnormal numbers are flushed\nto zero.",
    "inputs": [
      { "name": "input", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "topk", "type": "TF_Float32Tensor" },
      { "name": "topk_indices", "type": "TF_Int32Tensor" }
    ],
    "attributes": [
      { "name": "k", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.TPUAnnotateTensorsWithDynamicShape",
    "summary": "Placeholder op which takes the output of TPUCopyWithDynamicShapeOp and pass\nthem to the following tpu ops.",
    "description": "This op serves as an annotation for the dynamic shaped tensor and will be\nremoved during the bridge rewrite.",
    "inputs": [
      { "name": "tensors", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "tpu_tensors", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.TPUCompilationResult",
    "summary": "Returns the result of a TPU compilation.",
    "description": "This operation returns the result of a TPU compilation as a serialized\nCompilationResultProto, which holds a status and an error message if an error\noccurred during compilation.",
    "outputs": [
      { "name": "output", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.TPUCompileMlirAndExecute",
    "summary": "Op that compiles a computation in MLIR into a TPU program, and loads and executes it on a TPU device.",
    "description": "For the internal use of the TPU compiler.\n\n'static_shapes' are tensors specifying the maximum dimension sizes for the tensors specified in `dynamic_operands`.\n'args' are inputs to the TPU computation.\n'operands_with_static_shape' are the indices of the operands that have a maximal static shape specified.\n'mlir_module' is a serialized MLIR module with a `main` function that contains\ntarget computation.\n'metadata' is a serialized TPUCompileMetadataProto describing the shapes and\ntypes of the inputs to the computation, as well as a mapping onto the TPU pod\ntopology.\n'producer_name' is a string describing the name of the framework that add support for running this portion of the model on TPUs.",
    "inputs": [
      { "name": "args", "type": "Variadic" },
      { "name": "static_shapes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "rendezvous_key_base", "type": "TF_Tensor" },
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "operands_with_static_shape", "type": "OptionalAttr" },
      { "name": "mlir_module", "type": "DefaultValuedStrAttr" },
      { "name": "metadata", "type": "StrAttr" },
      { "name": "producer_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.TPUCompileSucceededAssert",
    "summary": "Asserts that compilation succeeded.",
    "description": "This op produces no output and closes the device during failure to ensure all\npending device interactions fail.\n\n'compilation_status' is a serialized CompilationResultProto.",
    "inputs": [
      { "name": "compilation_status", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.TPUCopyWithDynamicShape",
    "summary": "Op that copies host tensors to device with bounded dynamic shape support.",
    "description": "This op copies the padded tensor on cpu to TPU without the padded data. `tensors`\nis a list of cpu tensors with padded data. `unpadded_sizes` is a list of shape\ntensors which describes unpadded size of each dimension for each cpu tensor.\nThe size of the `unpadded_sizes` should be the same as `tensors`. They are both\non host. `tpu_tensors` are list of tpu device tensors without the padded data.\n`tpu_tensors` also has the same size of the `tensors` and the shapes of\n`tpu_tensors` are determined by the `unpadded_sizes`.",
    "inputs": [
      { "name": "tensors", "type": "Variadic" },
      { "name": "unpadded_sizes", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "tpu_tensors", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.TPUCopyWithLayout",
    "summary": "Op that copies host tensor to device with specified layout.",
    "description": "For internal use only.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "layout", "type": "TF_Int64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.TPUDummyInput",
    "summary": "Generates a zero-valued tensor for use as a dummy input to a TPU.",
    "description": "For the internal use of the TF2XLA bridge in the XLA Broadcast pass. This op",
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "shape", "type": "TF_ShapeAttr" }
    ]
  },
  {
    "name": "tf.TPUEmbeddingActivations",
    "summary": "An op enabling differentiation of TPU Embeddings.",
    "description": "This op simply returns its first input, which is assumed to have been sliced\nfrom the Tensors returned by TPUEmbeddingDequeueActivations. The presence of\nthis op, and its first argument being a trainable Variable, enables automatic\ndifferentiation of graphs containing embeddings via the TPU Embedding Python\nlibraries.",
    "inputs": [
      { "name": "embedding_variable", "type": "Arg" },
      { "name": "sliced_activations", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "ConfinedAttr" },
      { "name": "lookup_id", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.TPUExecute",
    "summary": "Op that loads and executes a TPU program on a TPU device.",
    "description": "For the internal use of the distributed TPU compiler.",
    "inputs": [
      { "name": "args", "type": "Variadic" },
      { "name": "key", "type": "TF_StrTensor" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.TPUExecuteAndUpdateVariables",
    "summary": "Op that executes a program with optional in-place variable updates.",
    "description": "It (optionally) reads device variables, loads and executes a TPU program on a\nTPU device, and then (optionally) in-place updates variables using the program\noutputs, as specified in attributes device_var_reads_indices (program input\nindices from directly reading variables) and device_var_updates_indices (program\noutput indices used to update variables, -1 means no-update/read-only). Such\nprogram outputs are consumed by these variables will not appear in the op\noutput. For the internal use of the distributed TPU compiler.",
    "inputs": [
      { "name": "args", "type": "Variadic" },
      { "name": "key", "type": "TF_StrTensor" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "device_var_reads_indices", "type": "I64ArrayAttr" },
      { "name": "device_var_updates_indices", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "tf.TPUGetLayoutOp",
    "summary": "Op that retrieves the layout of an input or output determined by TPUCompile.",
    "description": "For internal use only.",
    "inputs": [
      { "name": "cache_key", "type": "TF_StrTensor" }
    ],
    "outputs": [
      { "name": "layout", "type": "TF_Int64Tensor" }
    ],
    "attributes": [
      { "name": "index", "type": "I64Attr" },
      { "name": "is_output", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.TPUOrdinalSelector",
    "summary": "A TPU core selector Op.",
    "description": "This Op produces a set of TPU cores (for warm-up) or a single TPU core\n(for regular inference) to execute the TPU program on. The output is\nconsumed by TPUPartitionedCall.",
    "outputs": [
      { "name": "device_ordinals", "type": "Res" }
    ]
  },
  {
    "name": "tf.TPUPartitionedCall",
    "summary": "Calls a function placed on a specified TPU device.",
    "inputs": [
      { "name": "args", "type": "Variadic" },
      { "name": "device_ordinal", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "f", "type": "SymbolRefAttr" },
      { "name": "autotuner_thresh", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TPUPartitionedInput",
    "summary": "An op that groups a list of partitioned inputs together. This op",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "partition_dim", "type": "DefaultValuedOptionalAttr" },
      { "name": "_XlaSharding", "type": "OptionalAttr" },
      { "name": "_XlaShardingV2", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tf.TPUPartitionedInputV2",
    "summary": "An op that groups a list of partitioned inputs together. Supports ND sharding.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "partition_dims", "type": "I64ArrayAttr" },
      { "name": "is_packed", "type": "DefaultValuedOptionalAttr" },
      { "name": "_XlaSharding", "type": "OptionalAttr" },
      { "name": "_XlaShardingV2", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tf.TPUPartitionedOutput",
    "summary": "An op that demultiplexes a tensor to be sharded by XLA to a list of partitioned",
    "description": "outputs outside the XLA computation.",
    "inputs": [
      { "name": "inputs", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "partition_dim", "type": "DefaultValuedOptionalAttr" },
      { "name": "_XlaSharding", "type": "OptionalAttr" },
      { "name": "_XlaShardingV2", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tf.TPUPartitionedOutputV2",
    "summary": "An op that demultiplexes a tensor to be sharded by XLA to a list of partitioned",
    "description": "outputs outside the XLA computation. Supports ND sharding.",
    "inputs": [
      { "name": "inputs", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "partition_dims", "type": "I64ArrayAttr" },
      { "name": "_XlaSharding", "type": "OptionalAttr" },
      { "name": "_XlaShardingV2", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tf.TPUReplicatedInput",
    "summary": "Connects N inputs to an N-way replicated TPU computation.",
    "description": "This operation holds a replicated input to a `tpu.replicate()` computation subgraph.\nEach replicated input has the same shape and type alongside the output.\n\nFor example:\n```\n%a = \"tf.opA\"()\n%b = \"tf.opB\"()\n%replicated_input = \"tf.TPUReplicatedInput\"(%a, %b)\n%computation = \"tf.Computation\"(%replicated_input)\n```\nThe above computation has a replicated input of two replicas.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "is_mirrored_variable", "type": "DefaultValuedOptionalAttr" },
      { "name": "index", "type": "DefaultValuedOptionalAttr" },
      { "name": "is_packed", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TPUReplicatedOutput",
    "summary": "Connects N outputs from an N-way replicated TPU computation.",
    "description": "This operation holds a replicated output from a `tpu.replicate()` computation subgraph.\nEach replicated output has the same shape and type alongside the input.\n\nFor example:\n```\n%computation = \"tf.Computation\"()\n%replicated_output:2 = \"tf.TPUReplicatedOutput\"(%computation)\n```\nThe above computation has a replicated output of two replicas.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.TPUReplicateMetadata",
    "summary": "Metadata indicating how the TPU computation should be replicated.",
    "description": "This operation holds the metadata common to operations of a `tpu.replicate()` computation subgraph.",
    "attributes": [
      { "name": "num_replicas", "type": "ConfinedAttr" },
      { "name": "num_cores_per_replica", "type": "DefaultValuedOptionalAttr" },
      { "name": "topology", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_tpu", "type": "DefaultValuedOptionalAttr" },
      { "name": "device_assignment", "type": "DefaultValuedOptionalAttr" },
      { "name": "computation_shape", "type": "DefaultValuedOptionalAttr" },
      { "name": "host_compute_core", "type": "DefaultValuedOptionalAttr" },
      { "name": "padding_map", "type": "DefaultValuedOptionalAttr" },
      { "name": "step_marker_location", "type": "DefaultValuedOptionalAttr" },
      { "name": "allow_soft_placement", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_spmd_for_xla_partitioning", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_shardy_partitioner", "type": "DefaultValuedOptionalAttr" },
      { "name": "tpu_compile_options_proto", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TPUReshardVariables",
    "summary": "Op that reshards on-device TPU variables to specified state.",
    "description": "Op that reshards on-device TPU variables to specified state. Internal use only.\n\nThe sharding state is represented as the key of the compilation that generated\nthe sharding/unsharding programs along with the main program. new_format_key\nspecifies the desired state, and format_state_var is the current state of the\nvariables.",
    "inputs": [
      { "name": "vars", "type": "Arg" },
      { "name": "new_format_key", "type": "TF_StrTensor" },
      { "name": "format_state_var", "type": "Arg" }
    ]
  },
  {
    "name": "tf.TPURoundRobin",
    "summary": "Round-robin load balancing on TPU cores.",
    "description": "A load balancing op that round-robins among TPU cores.\n\nThis op round-robins between the integers in [0, NumTPUCoresVisiblePerHost]. It\nis useful for interfacing with TensorFlow ops that take as input a TPU core on\nwhich to execute computations, such as `TPUPartitionedCall`.\n\ndevice_ordinal: An integer in [0, NumTPUCoresVisiblePerHost].",
    "outputs": [
      { "name": "device_ordinal", "type": "TF_Int32Tensor" }
    ]
  },
  {
    "name": "tf.Transpose",
    "summary": "Shuffle dimensions of x according to a permutation.",
    "description": "The output `y` has the same rank as `x`. The shapes of `x` and `y` satisfy:\n  `y.shape[i] == x.shape[perm[i]] for i in [0, 1, ..., rank(x) - 1]`",
    "inputs": [
      { "name": "x", "type": "TF_Tensor" },
      { "name": "perm", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TF_Tensor" }
    ],
    "category": "Transform"
  },
  {
    "name": "tf.TridiagonalMatMul",
    "summary": "Calculate product with tridiagonal matrix.",
    "description": "Calculates product of two matrices, where left matrix is a tridiagonal matrix.",
    "inputs": [
      { "name": "superdiag", "type": "Arg" },
      { "name": "maindiag", "type": "Arg" },
      { "name": "subdiag", "type": "Arg" },
      { "name": "rhs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.TridiagonalSolve",
    "summary": "Solves tridiagonal systems of equations.",
    "description": "Solves tridiagonal systems of equations.\n  Supports batch dimensions and multiple right-hand sides per each left-hand\n  side.\n  On CPU, solution is computed via Gaussian elimination with or without partial\n  pivoting, depending on `partial_pivoting` attribute. On GPU, Nvidia's cuSPARSE\n  library is used: https://docs.nvidia.com/cuda/cusparse/index.html#gtsv\n  Partial pivoting is not yet supported by XLA backends.",
    "inputs": [
      { "name": "diagonals", "type": "Arg" },
      { "name": "rhs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "partial_pivoting", "type": "DefaultValuedOptionalAttr" },
      { "name": "perturb_singular", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TruncateDiv",
    "summary": "Returns x / y element-wise, rounded towards zero.",
    "description": "Truncation designates that negative numbers will round fractional quantities\ntoward zero. I.e. -7 / 5 = -1. This matches C semantics but it is different\nthan Python semantics. See `FloorDiv` for a division function that matches\nPython Semantics.\n\n*NOTE*: `TruncateDiv` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TensorOf" },
      { "name": "y", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "z", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.TruncatedNormal",
    "summary": "Outputs random values from a truncated normal distribution.",
    "description": "The generated values follow a normal distribution with mean 0 and standard\ndeviation 1, except that values whose magnitude is more than 2 standard\ndeviations from the mean are dropped and re-picked.",
    "inputs": [
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.TruncateMod",
    "summary": "Returns element-wise remainder of division. This emulates C semantics in that",
    "description": "the result here is consistent with a truncating divide. E.g. `truncate(x / y) *\ny + truncate_mod(x, y) = x`.\n\n*NOTE*: `TruncateMod` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)",
    "inputs": [
      { "name": "x", "type": "TF_FpOrI32OrI64Tensor" },
      { "name": "y", "type": "TF_FpOrI32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrI32OrI64Tensor" }
    ]
  },
  {
    "name": "tf.UncompressElement",
    "summary": "Uncompresses a compressed dataset element.",
    "inputs": [
      { "name": "compressed", "type": "TF_VariantTensor" }
    ],
    "outputs": [
      { "name": "components", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.UniformDequantize",
    "summary": "Perform dequantization on the quantized Tensor `input`.",
    "description": "Given quantized `input` which was quantized using `scales` and `zero_points`, performs dequantization using the formula:\ndequantized_data = (quantized_data - zero_point) * scale.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "scales", "type": "Arg" },
      { "name": "zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "quantization_min_val", "type": "I64Attr" },
      { "name": "quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.UniformQuantize",
    "summary": "Perform quantization on Tensor `input`.",
    "description": "Given `input`, `scales` and `zero_points`, performs quantization using the formula:\nquantized_data = floor(input_data * (1.0f / scale) + 0.5f) + zero_point",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "scales", "type": "Arg" },
      { "name": "zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "quantization_min_val", "type": "I64Attr" },
      { "name": "quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.UniformQuantizedAdd",
    "summary": "Perform quantized add of quantized Tensor `lhs` and quantized Tensor `rhs` to make quantized `output`.",
    "description": "Given quantized `lhs` and quantized `rhs`, performs quantized add on `lhs` and `rhs` to make quantized `output`.\n\n`UniformQuantizedAdd` follows Numpy broadcasting rules.\nThe two input array shapes are compared element-wise.\nStarting with the trailing dimensions, the two dimensions either have to be equal or one of them needs to be 1.\n\n`lhs` and `rhs` must be quantized Tensor, where data value is quantized using the formula:\n```\nquantized_data = clip(original_data / scale + zero_point, quantization_min_val, quantization_max_val)\n```\n`output` is also quantized, using the same formula.\n\nIf `lhs` and `output` is both per-axis quantized, the quantization axis must match.\nAlso, if `rhs` and `output` is both per-axis quantized, the quantization axis must match.\n*Match* means the axis must match when adding, regarding the broadcasting.\ni.e. For both operands `lhs` and `rhs`,\nif `operand.quantization_axis` >= 0 and `output.quantization_axis` >= 0,\n`operand.dims` - `operand.quantization_axis` must be equal to `output.dims` - `output.quantization_axis`.",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" },
      { "name": "lhs_scales", "type": "Arg" },
      { "name": "lhs_zero_points", "type": "Arg" },
      { "name": "rhs_scales", "type": "Arg" },
      { "name": "rhs_zero_points", "type": "Arg" },
      { "name": "output_scales", "type": "Arg" },
      { "name": "output_zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "lhs_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "lhs_quantization_min_val", "type": "I64Attr" },
      { "name": "lhs_quantization_max_val", "type": "I64Attr" },
      { "name": "rhs_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "rhs_quantization_min_val", "type": "I64Attr" },
      { "name": "rhs_quantization_max_val", "type": "I64Attr" },
      { "name": "output_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_quantization_min_val", "type": "I64Attr" },
      { "name": "output_quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.UniformQuantizedClipByValue",
    "summary": "Perform clip by value on the quantized Tensor `operand`.",
    "description": "Given quantized `operand` which was quantized using `scales` and `zero_points`, performs clip by value using `min` and `max` values.\nIf quantization_axis is -1 (per-tensor quantized), the entire operand is clipped using scalar min, max.\nOtherwise (per-channel quantized), the clipping is also done per-channel.",
    "inputs": [
      { "name": "operand", "type": "Arg" },
      { "name": "min", "type": "Arg" },
      { "name": "max", "type": "Arg" },
      { "name": "scales", "type": "Arg" },
      { "name": "zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "quantization_min_val", "type": "I64Attr" },
      { "name": "quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.UniformQuantizedConvolution",
    "summary": "Perform quantized convolution of quantized Tensor `lhs` and quantized Tensor `rhs`. to make quantized `output`.",
    "description": "Given quantized `lhs` and quantized `rhs`, performs quantized dot on `lhs` and `rhs` to make quantized `output`.\n\n`lhs` and `rhs` must be Tensors of same rank, and meet following shape conditions.\n- `lhs_feature` % `feature_group_count` == 0\n- `lhs_feature` % `rhs_input_feature` == 0\n- `lhs_feature` / `feature_group_count` == `rhs_input_feature`\n- `rhs_output_feature` % `feature_group_count` == 0\n- `lhs_batch` % `batch_group_count` == 0\n- `rhs_output_feature` % `batch_group_count` == 0\n\n`lhs` and `rhs` must be quantized Tensor, where data value is quantized using the formula:\n```\nquantized_data = clip(original_data / scale + zero_point, quantization_min_val, quantization_max_val)\n```\n`output` is also quantized, using the same formula.\nIf `rhs` is per-tensor quantized, `output` must be also per-tensor quantized.",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" },
      { "name": "lhs_scales", "type": "Arg" },
      { "name": "lhs_zero_points", "type": "Arg" },
      { "name": "rhs_scales", "type": "Arg" },
      { "name": "rhs_zero_points", "type": "Arg" },
      { "name": "output_scales", "type": "Arg" },
      { "name": "output_zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "window_strides", "type": "DefaultValuedOptionalAttr" },
      { "name": "padding", "type": "StrAttr" },
      { "name": "explicit_padding", "type": "DefaultValuedOptionalAttr" },
      { "name": "lhs_dilation", "type": "DefaultValuedOptionalAttr" },
      { "name": "rhs_dilation", "type": "DefaultValuedOptionalAttr" },
      { "name": "batch_group_count", "type": "DefaultValuedOptionalAttr" },
      { "name": "feature_group_count", "type": "DefaultValuedOptionalAttr" },
      { "name": "dimension_numbers", "type": "DefaultValuedOptionalAttr" },
      { "name": "lhs_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "lhs_quantization_min_val", "type": "I64Attr" },
      { "name": "lhs_quantization_max_val", "type": "I64Attr" },
      { "name": "rhs_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "rhs_quantization_min_val", "type": "I64Attr" },
      { "name": "rhs_quantization_max_val", "type": "I64Attr" },
      { "name": "output_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_quantization_min_val", "type": "I64Attr" },
      { "name": "output_quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.UniformQuantizedConvolutionHybrid",
    "summary": "Perform hybrid quantized convolution of float Tensor `lhs` and quantized Tensor `rhs`.",
    "description": "Given float `lhs` and quantized `rhs`, internally performs quantization on `lhs`,\nand then performs quantized convolution on quantized `lhs` and `rhs`.\n\nThe internal quantization on `lhs` is a quantization to `Trhs`, dynamic range,\nper-batch (per-axis along axis `dimension_numbers.input_batch_dimension`), asymmetric,\nand not narrow range (the range is [Trhs_MIN, Trhs_MAX]).\n\n`lhs` and `rhs` must be Tensors of same rank, and meet following shape conditions.\n- lhs_feature % feature_group_count == 0\n- lhs_feature % rhs_input_feature == 0\n- lhs_feature / feature_group_count == rhs_input_feature\n- rhs_output_feature % feature_group_count == 0\n- lhs_batch % batch_group_count == 0\n- rhs_output_feature % batch_group_count == 0\n\n`rhs` must be quantized Tensor, where its data value is quantized using the formula:\nquantized_data = clip(original_data / scale + zero_point, quantization_min_val, quantization_max_val).",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" },
      { "name": "rhs_scales", "type": "Arg" },
      { "name": "rhs_zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "window_strides", "type": "DefaultValuedOptionalAttr" },
      { "name": "padding", "type": "StrAttr" },
      { "name": "explicit_padding", "type": "DefaultValuedOptionalAttr" },
      { "name": "lhs_dilation", "type": "DefaultValuedOptionalAttr" },
      { "name": "rhs_dilation", "type": "DefaultValuedOptionalAttr" },
      { "name": "batch_group_count", "type": "DefaultValuedOptionalAttr" },
      { "name": "feature_group_count", "type": "DefaultValuedOptionalAttr" },
      { "name": "dimension_numbers", "type": "DefaultValuedOptionalAttr" },
      { "name": "rhs_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "rhs_quantization_min_val", "type": "I64Attr" },
      { "name": "rhs_quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.UniformQuantizedDot",
    "summary": "Perform quantized dot of quantized Tensor `lhs` and quantized Tensor `rhs` to make quantized `output`.",
    "description": "Given quantized `lhs` and quantized `rhs`, performs quantized dot on `lhs` and `rhs` to make quantized `output`.\n`lhs` and `rhs` must be 2D Tensors and the lhs.dim_size(1) must match rhs.dim_size(0).\n`lhs` and `rhs` must be quantized Tensor, where data value is quantized using the formula:\nquantized_data = clip(original_data / scale + zero_point, quantization_min_val, quantization_max_val).\n`output` is also quantized, using the same formula.\nIf `rhs` is per-tensor quantized, `output` must be also per-tensor quantized.",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" },
      { "name": "lhs_scales", "type": "Arg" },
      { "name": "lhs_zero_points", "type": "Arg" },
      { "name": "rhs_scales", "type": "Arg" },
      { "name": "rhs_zero_points", "type": "Arg" },
      { "name": "output_scales", "type": "Arg" },
      { "name": "output_zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "lhs_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "lhs_quantization_min_val", "type": "I64Attr" },
      { "name": "lhs_quantization_max_val", "type": "I64Attr" },
      { "name": "rhs_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "rhs_quantization_min_val", "type": "I64Attr" },
      { "name": "rhs_quantization_max_val", "type": "I64Attr" },
      { "name": "output_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_quantization_min_val", "type": "I64Attr" },
      { "name": "output_quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.UniformQuantizedDotHybrid",
    "summary": "Perform hybrid quantized dot of float Tensor `lhs` and quantized Tensor `rhs`.",
    "description": "Given float `lhs` and quantized `rhs`, internally performs quantization on `lhs`, and then performs quantized dot on quantized lhs and `rhs`.\nThe internal quantization on `lhs` is a quantization to qint8, dynamic range, per-batch (per-axis along axis 0), asymmetric, and not narrow range (the range is [-128, 127]).\n`lhs` and `rhs` must be 2D Tensors and the lhs.dim_size(1) must match rhs.dim_size(0).\n`rhs` must be quantized Tensor, where its data value is quantized using the formula:\nquantized_data = clip(original_data / scale + zero_point, quantization_min_val, quantization_max_val).",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" },
      { "name": "rhs_scales", "type": "Arg" },
      { "name": "rhs_zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "rhs_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "rhs_quantization_min_val", "type": "I64Attr" },
      { "name": "rhs_quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.UniformRequantize",
    "summary": "Given quantized tensor `input`, requantize it with new quantization parameters.",
    "description": "Given quantized tensor `input`, which was quantized using {input_scales, input_zero_points, input_quantization_axis, input_quantization_min_val, input_quantization_max_val},\nrequantize it to a tensor, which is quantized using {output_scales, output_zero_points, output_quantization_axis, output_quantization_min_val, output_quantization_max_val}.\nThe requantization is done by using the formula:\noutput_quantized_data = clip(\n  (input_quantized_data - input_zero_point) * (input_scale / output_scale) + output_zero_point,\n  output_quantization_min_val,\n  output_quantization_max_val)\n\nPer-tensor and per-axis quantization supported cases are followings:\n* per-tensor -> per-tensor\n* per-tensor -> per-axis\n* per-axis -> per-axis where input_quantization_axis equals output_quantization_axis.\ni.e. At least one among input_quantization_axis and output_quantization_axis must be -1, or two must be equal.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "input_scales", "type": "Arg" },
      { "name": "input_zero_points", "type": "Arg" },
      { "name": "output_scales", "type": "Arg" },
      { "name": "output_zero_points", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "input_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "input_quantization_min_val", "type": "I64Attr" },
      { "name": "input_quantization_max_val", "type": "I64Attr" },
      { "name": "output_quantization_axis", "type": "DefaultValuedOptionalAttr" },
      { "name": "output_quantization_min_val", "type": "I64Attr" },
      { "name": "output_quantization_max_val", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.Unique",
    "summary": "Finds unique elements in a 1-D tensor.",
    "description": "This operation returns a tensor `y` containing all of the unique elements of `x`\nsorted in the same order that they occur in `x`; `x` does not need to be sorted.\nThis operation also returns a tensor `idx` the same size as `x` that contains\nthe index of each value of `x` in the unique output `y`. In other words:\n\n`y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1]`\n\nExamples:\n\n```\n# tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]\ny, idx = unique(x)\ny ==> [1, 2, 4, 7, 8]\nidx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]\n```\n\n```\n# tensor 'x' is [4, 5, 1, 2, 3, 3, 4, 5]\ny, idx = unique(x)\ny ==> [4, 5, 1, 2, 3]\nidx ==> [0, 1, 2, 3, 4, 4, 0, 1]\n```",
    "inputs": [
      { "name": "x", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" },
      { "name": "idx", "type": "Res" }
    ]
  },
  {
    "name": "tf.UniqueV2",
    "summary": "Finds unique elements along an axis of a tensor.",
    "description": "This operation either returns a tensor `y` containing unique elements\nalong the `axis` of a tensor. The returned unique elements is sorted\nin the same order as they occur along `axis` in `x`.\nThis operation also returns a tensor `idx` that is the same size as\nthe number of the elements in `x` along the `axis` dimension. It\ncontains the index in the unique output `y`.\nIn other words, for an `1-D` tensor `x` with `axis = None:\n\n`y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1]`\n\nFor example:\n\n```\n# tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]\ny, idx = unique(x)\ny ==> [1, 2, 4, 7, 8]\nidx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]\n```\n\nFor an `2-D` tensor `x` with `axis = 0`:\n\n```\n# tensor 'x' is [[1, 0, 0],\n#                [1, 0, 0],\n#                [2, 0, 0]]\ny, idx = unique(x, axis=0)\ny ==> [[1, 0, 0],\n       [2, 0, 0]]\nidx ==> [0, 0, 1]\n```\n\nFor an `2-D` tensor `x` with `axis = 1`:\n\n```\n# tensor 'x' is [[1, 0, 0],\n#                [1, 0, 0],\n#                [2, 0, 0]]\ny, idx = unique(x, axis=1)\ny ==> [[1, 0],\n       [1, 0],\n       [2, 0]]\nidx ==> [0, 1, 1]\n```",
    "inputs": [
      { "name": "x", "type": "Arg" },
      { "name": "axis", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" },
      { "name": "idx", "type": "Res" }
    ]
  },
  {
    "name": "tf.Unpack",
    "summary": "Unpacks a given dimension of a rank-`R` tensor into `num` rank-`(R-1)` tensors.",
    "description": "Unpacks `num` tensors from `value` by chipping it along the `axis` dimension.\nFor example, given a tensor of shape `(A, B, C, D)`;\n\nIf `axis == 0` then the i'th tensor in `output` is the slice `value[i, :, :, :]`\n  and each tensor in `output` will have shape `(B, C, D)`. (Note that the\n  dimension unpacked along is gone, unlike `split`).\n\nIf `axis == 1` then the i'th tensor in `output` is the slice `value[:, i, :, :]`\n  and each tensor in `output` will have shape `(A, C, D)`.\nEtc.\n\nThis is the opposite of `pack`.",
    "inputs": [
      { "name": "value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.UnsortedSegmentMax",
    "summary": "Computes the maximum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nThis operator is similar to `tf.math.unsorted_segment_sum`,\nInstead of computing the sum over segments, it computes the maximum such that:\n\n\\\\(output_i = \\max_{j...} data[j...]\\\\) where max is over tuples `j...` such\nthat `segment_ids[j...] == i`.\n\nIf the maximum is empty for a given segment ID `i`, it outputs the smallest\npossible value for the specific numeric type,\n`output[i] = numeric_limits<T>::lowest()`.\n\nIf the given segment ID `i` is negative, then the corresponding value is\ndropped, and will not be included in the result.\n\nCaution: On CPU, values in `segment_ids` are always validated to be less than\n`num_segments`, and an error is thrown for out-of-bound indices. On GPU, this\ndoes not throw an error for out-of-bound indices. On Gpu, out-of-bound indices\nresult in safe but unspecified behavior, which may include ignoring\nout-of-bound indices or outputting a tensor with a 0 stored in the first\ndimension of its shape if `num_segments` is 0.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/UnsortedSegmentMax.png\" alt>\n</div>\n\nFor example:\n\n>>> c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])\n>>> tf.math.unsorted_segment_max(c, tf.constant([0, 1, 0]), num_segments=2).numpy()\narray([[4, 3, 3, 4],\n       [5,  6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_IntOrFpTensor" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.UnsortedSegmentMin",
    "summary": "Computes the minimum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nThis operator is similar to `tf.math.unsorted_segment_sum`,\nInstead of computing the sum over segments, it computes the minimum such that:\n\n\\\\(output_i = \\min_{j...} data_[j...]\\\\) where min is over tuples `j...` such\nthat `segment_ids[j...] == i`.\n\nIf the minimum is empty for a given segment ID `i`, it outputs the largest\npossible value for the specific numeric type,\n`output[i] = numeric_limits<T>::max()`.\n\nFor example:\n\n>>> c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])\n>>> tf.math.unsorted_segment_min(c, tf.constant([0, 1, 0]), num_segments=2).numpy()\narray([[1, 2, 2, 1],\n       [5, 6, 7, 8]], dtype=int32)\n\nIf the given segment ID `i` is negative, then the corresponding value is\ndropped, and will not be included in the result.\n\nCaution: On CPU, values in `segment_ids` are always validated to be less than\n`num_segments`, and an error is thrown for out-of-bound indices. On GPU, this\ndoes not throw an error for out-of-bound indices. On Gpu, out-of-bound indices\nresult in safe but unspecified behavior, which may include ignoring\nout-of-bound indices or outputting a tensor with a 0 stored in the first\ndimension of its shape if `num_segments` is 0.",
    "inputs": [
      { "name": "data", "type": "TF_IntOrFpTensor" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.UnsortedSegmentProd",
    "summary": "Computes the product along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nThis operator is similar to `tf.math.unsorted_segment_sum`,\nInstead of computing the sum over segments, it computes the product of all\nentries belonging to a segment such that:\n\n\\\\(output_i = \\prod_{j...} data[j...]\\\\) where the product is over tuples\n`j...` such that `segment_ids[j...] == i`.\n\nFor example:\n\n>>> c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])\n>>> tf.math.unsorted_segment_prod(c, tf.constant([0, 1, 0]), num_segments=2).numpy()\narray([[4, 6, 6, 4],\n       [5, 6, 7, 8]], dtype=int32)\n\nIf there is no entry for a given segment ID `i`, it outputs 1.\n\nIf the given segment ID `i` is negative, then the corresponding value is\ndropped, and will not be included in the result.\nCaution: On CPU, values in `segment_ids` are always validated to be less than\n`num_segments`, and an error is thrown for out-of-bound indices. On GPU, this\ndoes not throw an error for out-of-bound indices. On Gpu, out-of-bound indices\nresult in safe but unspecified behavior, which may include ignoring\nout-of-bound indices or outputting a tensor with a 0 stored in the first\ndimension of its shape if `num_segments` is 0.",
    "inputs": [
      { "name": "data", "type": "TF_NumberTensor" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.UnsortedSegmentSum",
    "summary": "Computes the sum along segments of a tensor.",
    "description": "Read\n[the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\nfor an explanation of segments.\n\nComputes a tensor such that\n\\\\(output[i] = \\sum_{j...} data[j...]\\\\) where the sum is over tuples `j...` such\nthat `segment_ids[j...] == i`.  Unlike `SegmentSum`, `segment_ids`\nneed not be sorted and need not cover all values in the full\nrange of valid values.\n\nIf the sum is empty for a given segment ID `i`, `output[i] = 0`.\nIf the given segment ID `i` is negative, the value is dropped and will not be\nadded to the sum of the segment.\n\n`num_segments` should equal the number of distinct segment IDs.\n\nCaution: On CPU, values in `segment_ids` are always validated to be less than\n`num_segments`, and an error is thrown for out-of-bound indices. On GPU, this\ndoes not throw an error for out-of-bound indices. On Gpu, out-of-bound indices\nresult in safe but unspecified behavior, which may include ignoring\nout-of-bound indices or outputting a tensor with a 0 stored in the first\ndimension of its shape if `num_segments` is 0.\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/UnsortedSegmentSum.png\" alt>\n</div>\n\n>>> c = [[1,2,3,4], [5,6,7,8], [4,3,2,1]]\n>>> tf.math.unsorted_segment_sum(c, [0, 1, 0], num_segments=2).numpy()\narray([[5, 5, 5, 5],\n       [5, 6, 7, 8]], dtype=int32)",
    "inputs": [
      { "name": "data", "type": "TF_NumberTensor" },
      { "name": "segment_ids", "type": "Arg" },
      { "name": "num_segments", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.UpperBound",
    "summary": "Applies upper_bound(sorted_search_values, values) along each row.",
    "description": "Each set of rows with the same index in (sorted_inputs, values) is treated\nindependently.  The resulting row is the equivalent of calling\n`np.searchsorted(sorted_inputs, values, side='right')`.\n\nThe result is not a global index to the entire\n`Tensor`, but rather just the index in the last dimension.\n\nA 2-D example:\n  sorted_sequence = [[0, 3, 9, 9, 10],\n                     [1, 2, 3, 4, 5]]\n  values = [[2, 4, 9],\n            [0, 2, 6]]\n\n  result = UpperBound(sorted_sequence, values)\n\n  result == [[1, 2, 4],\n             [0, 2, 5]]",
    "inputs": [
      { "name": "sorted_inputs", "type": "Arg" },
      { "name": "values", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.VarHandleOp",
    "summary": "Creates a handle to a Variable resource from its name.",
    "description": "container: the container this variable is placed in.\nshared_name: the name by which this variable is referred to.\ndtype and shape: attributes representing the data type and shape held in the\n  variable.\n\nExample:\n    resource_variable_ops.var_handle_op(\n          dtype=dtypes.int32, shape=[8, 16], container=\"foo\", shared_name=\"bar\")\n  returns a handle for a variable with name \"bar\" in container \"foo\", and the\n  variable holds a tensor of shape [8, 16] and dtype int32.",
    "outputs": [
      { "name": "resource", "type": "Res" }
    ],
    "attributes": [
      { "name": "container", "type": "DefaultValuedStrAttr" },
      { "name": "shared_name", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "tf.Variable",
    "summary": "Use VariableV2 instead.",
    "outputs": [
      { "name": "ref", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "shape", "type": "TF_ShapeAttr" },
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.VariableShape",
    "summary": "Returns the shape of the variable pointed to by `resource`.",
    "description": "This operation returns a 1-D integer tensor representing the shape of `input`.\n\nFor example:\n\n```\n# 't' is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]\nshape(t) ==> [2, 2, 3]\n```",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tf.VariableV2",
    "summary": "Holds state in the form of a tensor that persists across steps.",
    "description": "Outputs a ref to the tensor state so it may be read or modified.\nTODO(zhifengc/mrry): Adds a pointer to a more detail document\nabout sharing states in tensorflow.",
    "outputs": [
      { "name": "ref", "type": "Res" }
    ],
    "attributes": [
      { "name": "shape", "type": "TF_ShapeAttr" },
      { "name": "container", "type": "DefaultValuedOptionalAttr" },
      { "name": "shared_name", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.VarIsInitializedOp",
    "summary": "Checks whether a resource handle-based variable has been initialized.",
    "inputs": [
      { "name": "resource", "type": "Arg" }
    ],
    "outputs": [
      { "name": "is_initialized", "type": "Res" }
    ]
  },
  {
    "name": "tf.Where",
    "summary": "Returns locations of nonzero / true values in a tensor.",
    "description": "This operation returns the coordinates of true elements in `condition`. The\ncoordinates are returned in a 2-D tensor where the first dimension (rows)\nrepresents the number of true elements, and the second dimension (columns)\nrepresents the coordinates of the true elements. Keep in mind, the shape of\nthe output tensor can vary depending on how many true values there are in\n`condition`. Indices are output in row-major order.\n\nFor example:\n\n```\n# 'input' tensor is [[True, False]\n#                    [True, False]]\n# 'input' has two true values, so output has two coordinates.\n# 'input' has rank of 2, so coordinates have two indices.\nwhere(input) ==> [[0, 0],\n                  [1, 0]]\n\n# `condition` tensor is [[[True, False]\n#                     [True, False]]\n#                    [[False, True]\n#                     [False, True]]\n#                    [[False, False]\n#                     [False, True]]]\n# 'input' has 5 true values, so output has 5 coordinates.\n# 'input' has rank of 3, so coordinates have three indices.\nwhere(input) ==> [[0, 0, 0],\n                  [0, 1, 0],\n                  [1, 0, 1],\n                  [1, 1, 1],\n                  [2, 1, 1]]\n\n# `condition` tensor is [[[1.5,  0.0]\n#                     [-0.5, 0.0]]\n#                    [[0.0,  0.25]\n#                     [0.0,  0.75]]\n#                    [[0.0,  0.0]\n#                     [0.0,  0.01]]]\n# 'input' has 5 nonzero values, so output has 5 coordinates.\n# 'input' has rank of 3, so coordinates have three indices.\nwhere(input) ==> [[0, 0, 0],\n                  [0, 1, 0],\n                  [1, 0, 1],\n                  [1, 1, 1],\n                  [2, 1, 1]]\n\n# `condition` tensor is [[[1.5 + 0.0j, 0.0  + 0.0j]\n#                     [0.0 + 0.5j, 0.0  + 0.0j]]\n#                    [[0.0 + 0.0j, 0.25 + 1.5j]\n#                     [0.0 + 0.0j, 0.75 + 0.0j]]\n#                    [[0.0 + 0.0j, 0.0  + 0.0j]\n#                     [0.0 + 0.0j, 0.01 + 0.0j]]]\n# 'input' has 5 nonzero magnitude values, so output has 5 coordinates.\n# 'input' has rank of 3, so coordinates have three indices.\nwhere(input) ==> [[0, 0, 0],\n                  [0, 1, 0],\n                  [1, 0, 1],\n                  [1, 1, 1],\n                  [2, 1, 1]]\n```",
    "inputs": [
      { "name": "input", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "index", "type": "TF_Int64Tensor" }
    ]
  },
  {
    "name": "tf.While",
    "summary": "output = input; While (Cond(output)) { output = Body(output) }",
    "description": "output = input; While (Cond(output)) { output = Body(output) }\n\ninput: A list of input tensors whose types are T.\noutput: A list of output tensors whose types are T.\ncond: A function that takes 'input' and returns a tensor.  If the tensor is\n    a scalar of non-boolean, the scalar is converted to a boolean\n    according to the following rule: if the scalar is a numerical\n    value, non-zero means True and zero means False; if the scalar is\n    a string, non-empty means True and empty means False. If the\n    tensor is not a scalar, non-emptiness means True and False\n    otherwise.\nbody: A function that takes a list of tensors and returns another\n      list of tensors. Both lists have the same types as specified\n      by T.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "cond", "type": "FlatSymbolRefAttr" },
      { "name": "body", "type": "FlatSymbolRefAttr" },
      { "name": "parallel_iterations", "type": "ConfinedAttr" },
      { "name": "is_stateless", "type": "BoolAttr" },
      { "name": "shape_invariant", "type": "UnitAttr" }
    ]
  },
  {
    "name": "tf.WhileRegion",
    "summary": "while operation",
    "description": "The tf.WhileRegion op represents a while loop using 2 regions and a set of\n  iteration variables. The iteration variables maintained by this Op have the\n  same types as the inputs. The Op executes a while loop described by the\n  following pseudo code:\n\n  ```\n     func WhileRegionOp(inputs) {\n       iteration_vars = inputs;\n       while (cond(iteration_vars)) {\n           iteration_vars = body(iteration_vars);\n       }\n       return iteration_vars;\n     }\n  ```\n\n  `cond` is the condition region and `body` is the body region. Both these\n  regions accept the current value of the iteration variables as inputs.\n\n  The condition region yields a tensor<i1> which, if false, will exit the loop.\n  It can also, optionally and additionally, yield the iteration variables, which\n  must be unchanged.\n\n  The body region always has to yield the (possibly updated) iteration variables.\n\n  The iteration variables are initialized to the Op input, and the results of the\n  tf.WhileRegion op are the final values of the iteration variables.\n\n  This implies that the operand and result types for tf.WhileRegion should be\n  the same. Note that the condition and body regions can implicitly capture\n  loop invariant values directly. In canonical form, iteration variables that\n  pass through the loop body unmodified are converted to implicitly captured\n  references to their values outside the loop.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "parallel_iterations", "type": "ConfinedAttr" },
      { "name": "is_stateless", "type": "BoolAttr" },
      { "name": "shape_invariant", "type": "UnitAttr" }
    ]
  },
  {
    "name": "tf.WriteAudioSummary",
    "summary": "Writes a `Summary` protocol buffer with audio.",
    "description": "The summary has up to `max_outputs` summary values containing audio. The\naudio is built from `tensor` which must be 3-D with shape `[batch_size,\nframes, channels]` or 2-D with shape `[batch_size, frames]`. The values are\nassumed to be in the range of `[-1.0, 1.0]` with a sample rate of `sample_rate`.\n\nThe `tag` argument is a scalar `Tensor` of type `string`.  It is used to\nbuild the `tag` of the summary values:\n\n*  If `max_outputs` is 1, the summary value tag is '*tag*/audio'.\n*  If `max_outputs` is greater than 1, the summary value tags are\n   generated sequentially as '*tag*/audio/0', '*tag*/audio/1', etc.\n\nwriter: A handle to a summary writer.\nstep: The step to write the summary for.\ntag: Scalar. Used to build the `tag` attribute of the summary values.\ntensor: 2-D of shape `[batch_size, frames]`.\nsample_rate: The sample rate of the signal in hertz.\nmax_outputs: Max number of batch elements to generate audio for.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "step", "type": "TF_Int64Tensor" },
      { "name": "tag", "type": "TF_StrTensor" },
      { "name": "tensor", "type": "TF_Float32Tensor" },
      { "name": "sample_rate", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "max_outputs", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.WriteGraphSummary",
    "summary": "Writes a `GraphDef` protocol buffer to a `SummaryWriter`.",
    "description": "writer: Handle of `SummaryWriter`.\nstep: The step to write the summary for.\ntensor: A scalar string of the serialized tf.GraphDef proto.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "step", "type": "TF_Int64Tensor" },
      { "name": "tensor", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.WriteHistogramSummary",
    "summary": "Writes a histogram summary.",
    "description": "The generated\n[`Summary`](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)\nhas one summary value containing a histogram for `values`.\n\nThis op reports an `InvalidArgument` error if any value is not finite.\n\nwriter: A handle to a summary writer.\nstep: The step to write the summary for.\ntag: Scalar.  Tag to use for the `Summary.Value`.\nvalues: Any shape. Values to use to build the histogram.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "step", "type": "TF_Int64Tensor" },
      { "name": "tag", "type": "TF_StrTensor" },
      { "name": "values", "type": "TF_IntOrFpTensor" }
    ]
  },
  {
    "name": "tf.WriteImageSummary",
    "summary": "Writes a `Summary` protocol buffer with images.",
    "description": "The summary has up to `max_images` summary values containing images. The\nimages are built from `tensor` which must be 4-D with shape `[batch_size,\nheight, width, channels]` and where `channels` can be:\n\n*  1: `tensor` is interpreted as Grayscale.\n*  3: `tensor` is interpreted as RGB.\n*  4: `tensor` is interpreted as RGBA.\n\nThe images have the same number of channels as the input tensor. For float\ninput, the values are normalized one image at a time to fit in the range\n`[0, 255]`.  `uint8` values are unchanged.  The op uses two different\nnormalization algorithms:\n\n*  If the input values are all positive, they are rescaled so the largest one\n   is 255.\n\n*  If any input value is negative, the values are shifted so input value 0.0\n   is at 127.  They are then rescaled so that either the smallest value is 0,\n   or the largest one is 255.\n\nThe `tag` argument is a scalar `Tensor` of type `string`.  It is used to\nbuild the `tag` of the summary values:\n\n*  If `max_images` is 1, the summary value tag is '*tag*/image'.\n*  If `max_images` is greater than 1, the summary value tags are\n   generated sequentially as '*tag*/image/0', '*tag*/image/1', etc.\n\nThe `bad_color` argument is the color to use in the generated images for\nnon-finite input values.  It is a `unit8` 1-D tensor of length `channels`.\nEach element must be in the range `[0, 255]` (It represents the value of a\npixel in the output image).  Non-finite values in the input tensor are\nreplaced by this tensor in the output image.  The default value is the color\nred.\n\nwriter: A handle to a summary writer.\nstep: The step to write the summary for.\ntag: Scalar. Used to build the `tag` attribute of the summary values.\ntensor: 4-D of shape `[batch_size, height, width, channels]` where\n  `channels` is 1, 3, or 4.\nmax_images: Max number of batch elements to generate images for.\nbad_color: Color to use for pixels with non-finite values.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "step", "type": "TF_Int64Tensor" },
      { "name": "tag", "type": "TF_StrTensor" },
      { "name": "tensor", "type": "TensorOf" },
      { "name": "bad_color", "type": "TF_Uint8Tensor" }
    ],
    "attributes": [
      { "name": "max_images", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tf.WriteRawProtoSummary",
    "summary": "Writes a `Summary` protocol buffer with serialized string `Summary` protocol buffers.",
    "description": "writer: A handle to a summary writer.\nstep: The step to write the summary for.\ntensor: A tensor holding one or more serialized `Summary` protobufs to write.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "step", "type": "TF_Int64Tensor" },
      { "name": "tensor", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.WriteScalarSummary",
    "summary": "Writes a `Summary` protocol buffer with scalar values.",
    "description": "The input `tag` and `value` must have the scalars.\n\nwriter: A handle to a summary writer.\nstep: The step to write the summary for.\ntag: Tag for the summary.\nvalue: Value for the summary.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "step", "type": "TF_Int64Tensor" },
      { "name": "tag", "type": "TF_StrTensor" },
      { "name": "value", "type": "TF_IntOrFpTensor" }
    ]
  },
  {
    "name": "tf.WriteSummary",
    "summary": "Outputs a `Summary` protocol buffer with a tensor.",
    "description": "writer: A handle to a summary writer.\nstep: The step to write the summary for.\ntensor: A tensor to serialize.\ntag: The summary's tag.\nsummary_metadata: Serialized SummaryMetadata protocol buffer containing\n plugin-related metadata for this summary.",
    "inputs": [
      { "name": "writer", "type": "Arg" },
      { "name": "step", "type": "TF_Int64Tensor" },
      { "name": "tensor", "type": "TF_Tensor" },
      { "name": "tag", "type": "TF_StrTensor" },
      { "name": "summary_metadata", "type": "TF_StrTensor" }
    ]
  },
  {
    "name": "tf.WriteTrainingPredictions",
    "summary": "Writes the given predictions into a RecordIO file using a previously",
    "description": "initialized global TrainingPredictionWriter. The predictions are transformed\ninto a PredictionData proto before they are written to the file.",
    "inputs": [
      { "name": "keys", "type": "Arg" },
      { "name": "predictions_list", "type": "Arg" },
      { "name": "step", "type": "Arg" },
      { "name": "timestamp_usec", "type": "Arg" }
    ],
    "attributes": [
      { "name": "prediction_names", "type": "StrArrayAttr" },
      { "name": "training", "type": "BoolAttr" },
      { "name": "file_path", "type": "StrAttr" },
      { "name": "write_vector_predictions", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.Xdivy",
    "summary": "Returns 0 if x == 0, and x / y otherwise, elementwise.",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" },
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.XlaAllReduce",
    "summary": "Wraps the XLA AllReduce operator",
    "description": "documented at https://www.tensorflow.org/xla/operation_semantics#allreduce.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "group_assignment", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "reduce_op", "type": "TF_AnyStrAttrOf" },
      { "name": "mode", "type": "TF_AnyStrAttrOf" }
    ]
  },
  {
    "name": "tf.XlaBroadcastHelper",
    "summary": "Helper operator for performing XLA-style broadcasts",
    "description": "Broadcasts `lhs` and `rhs` to the same rank, by adding size 1 dimensions to\nwhichever of `lhs` and `rhs` has the lower rank, using XLA's broadcasting rules\nfor binary operators.",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" },
      { "name": "broadcast_dims", "type": "Arg" }
    ],
    "outputs": [
      { "name": "lhs_output", "type": "Res" },
      { "name": "rhs_output", "type": "Res" }
    ]
  },
  {
    "name": "tf.XlaCallModule",
    "summary": "Invokes a StableHLO module.",
    "description": "This op is used with JAX native serialization in a TensorFlow context with\nstability guarantees.",
    "inputs": [
      { "name": "args", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "version", "type": "I64Attr" },
      { "name": "module", "type": "StrAttr" },
      { "name": "Sout", "type": "TF_ShapeAttrArray" },
      { "name": "dim_args_spec", "type": "DefaultValuedOptionalAttr" },
      { "name": "platforms", "type": "DefaultValuedOptionalAttr" },
      { "name": "function_list", "type": "DefaultValuedOptionalAttr" },
      { "name": "has_token_input_output", "type": "DefaultValuedOptionalAttr" },
      { "name": "disabled_checks", "type": "DefaultValuedOptionalAttr" },
      { "name": "use_shardy_partitioner", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.XlaClusterOutput",
    "summary": "Operator that connects the output of an XLA computation to other consumer graph nodes.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "outputs", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.XlaConcatND",
    "summary": "Concats input tensor across all dimensions.",
    "description": "An op which merges slices the input tensor based on the given num_splits\nattribute, strips paddings optionally, and returns the merged tensor without\npaddings.\n\nThis op may be generated via the TPU bridge.\n\nFor example, with `input` tensor:\n```\n[[0, 1],\n [4, 5]]\n[[2, 3],\n [6, 7]]\n[[8, 9],\n [12, 13]]\n[[10, 11],\n [14, 15]]\n```\n`num_splits`:\n```\n[2, 2]\n```\nand `paddings`:\n```\n[1, 1]\n```\nthe expected `outputs` is:\n```\n[[0, 1, 2],\n [4, 5, 6],\n [8, 9, 10]]\n```",
    "inputs": [
      { "name": "inputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ],
    "attributes": [
      { "name": "num_concats", "type": "I64ArrayAttr" },
      { "name": "paddings", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.XlaConv",
    "summary": "Wraps the XLA ConvGeneralDilated operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#conv_convolution\n.",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" },
      { "name": "window_strides", "type": "Arg" },
      { "name": "padding", "type": "Arg" },
      { "name": "lhs_dilation", "type": "Arg" },
      { "name": "rhs_dilation", "type": "Arg" },
      { "name": "feature_group_count", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_NumberTensor" }
    ],
    "attributes": [
      { "name": "dimension_numbers", "type": "StrAttr" },
      { "name": "precision_config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaConvV2",
    "summary": "Wraps the XLA ConvGeneralDilated operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#conv_convolution\n.",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" },
      { "name": "window_strides", "type": "Arg" },
      { "name": "padding", "type": "Arg" },
      { "name": "lhs_dilation", "type": "Arg" },
      { "name": "rhs_dilation", "type": "Arg" },
      { "name": "feature_group_count", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_NumberTensor" }
    ],
    "attributes": [
      { "name": "dimension_numbers", "type": "StrAttr" },
      { "name": "precision_config", "type": "StrAttr" },
      { "name": "batch_group_count", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.XlaCustomCallV2",
    "summary": "Emits an HLO `CustomCall` operation with multiple outputs.",
    "description": "As opposed to `XlaCustomCall`, this operation supports multiple outputs.\n\nSee `CustomCall` specification at\n  https://tensorflow.org/xla/operation_semantics#customcall,\nand `mhlo.custom_call` specification at\n  https://tensorflow.org/mlir/hlo_ops#mhlocustom_call_mlirmhlocustomcallop.",
    "inputs": [
      { "name": "operands", "type": "Arg" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "call_target_name", "type": "StrAttr" },
      { "name": "backend_config", "type": "StrAttr" },
      { "name": "has_side_effect", "type": "BoolAttr" },
      { "name": "result_shapes", "type": "TF_ShapeAttrArray" }
    ]
  },
  {
    "name": "tf.XlaDot",
    "summary": "Wraps the XLA DotGeneral operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#dotgeneral\n.",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_NumberTensor" }
    ],
    "attributes": [
      { "name": "dimension_numbers", "type": "StrAttr" },
      { "name": "precision_config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaDotV2",
    "summary": "Wraps the XLA DotGeneral operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#dotgeneral\n.",
    "inputs": [
      { "name": "lhs", "type": "Arg" },
      { "name": "rhs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_NumberTensor" }
    ],
    "attributes": [
      { "name": "dimension_numbers", "type": "StrAttr" },
      { "name": "precision_config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaDynamicSlice",
    "summary": "Wraps the XLA DynamicSlice operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#dynamicslice\n.\n\nDynamicSlice extracts a sub-array from the input array at dynamic\nstart_indices. The size of the slice in each dimension is passed in\nsize_indices, which specify the end point of exclusive slice intervals in each\ndimension -- [start, start + size). The shape of start_indices must have rank 1,\nwith dimension size equal to the rank of operand.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "start_indices", "type": "Arg" },
      { "name": "size_indices", "type": "TF_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.XlaDynamicUpdateSlice",
    "summary": "Wraps the XLA DynamicUpdateSlice operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#dynamicupdateslice\n.\n\nXlaDynamicUpdateSlice generates a result which is the value of the `input`\noperand, with a slice update overwritten at `indices`. The shape of `update`\ndetermines the shape of the sub-array of the result which is updated. The shape\nof indices must be rank == 1, with dimension size equal to the rank of `input`.\n\nHandling of out-of-bounds slice indices is implementation-defined.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "update", "type": "Arg" },
      { "name": "indices", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.XlaEinsum",
    "summary": "An op which supports basic einsum op with 2 inputs and 1 output.",
    "description": "This op has better TPU performance since it doesn't have explicitly reshape and\ntranspose operations as tf.einsum does.",
    "inputs": [
      { "name": "a", "type": "TensorOf" },
      { "name": "b", "type": "TensorOf" }
    ],
    "outputs": [
      { "name": "product", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "equation", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaGather",
    "summary": "Wraps the XLA Gather operator documented at",
    "description": "https://www.tensorflow.org/xla/operation_semantics#gather",
    "inputs": [
      { "name": "operand", "type": "Arg" },
      { "name": "start_indices", "type": "Arg" },
      { "name": "slice_sizes", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "dimension_numbers", "type": "StrAttr" },
      { "name": "indices_are_sorted", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.XlaHostCompute",
    "summary": "A pseudo-op to represent host-side computation in an XLA program.",
    "inputs": [
      { "name": "inputs", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "ancestors", "type": "StrArrayAttr" },
      { "name": "shapes", "type": "TF_ShapeAttrArray" },
      { "name": "shape_inference_graph", "type": "OptionalAttr" },
      { "name": "key", "type": "StrAttr" },
      { "name": "send_key", "type": "DefaultValuedStrAttr" },
      { "name": "recv_key", "type": "DefaultValuedStrAttr" },
      { "name": "cost_estimate_ns", "type": "DefaultValuedOptionalAttr" },
      { "name": "tpu_core", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.XlaKeyValueSort",
    "summary": "Wraps the XLA Sort operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#sort\n.\n\nSorts a tensor. Currently only sorts in ascending order are supported.",
    "inputs": [
      { "name": "keys", "type": "Arg" },
      { "name": "values", "type": "Arg" }
    ],
    "outputs": [
      { "name": "sorted_keys", "type": "Res" },
      { "name": "sorted_values", "type": "Res" }
    ]
  },
  {
    "name": "tf.XlaLaunch",
    "summary": "XLA Launch Op. For use by the XLA JIT only.",
    "inputs": [
      { "name": "constants", "type": "Variadic" },
      { "name": "args", "type": "Variadic" },
      { "name": "resources", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "function", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.XlaLaunchV2",
    "summary": "XLA Launch Op. For use by the XLA JIT only.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "constants", "type": "I64ArrayAttr" },
      { "name": "resources", "type": "I64ArrayAttr" },
      { "name": "function", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.XlaLocalSparseDenseMatmul",
    "summary": "Performs embedding lookup on SparseCore for a single table.",
    "inputs": [
      { "name": "embedding_ids", "type": "TF_Int32Tensor" },
      { "name": "sample_ids", "type": "TF_Int32Tensor" },
      { "name": "gains", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "result", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "batch_size", "type": "ConfinedAttr" },
      { "name": "T", "type": "TypeAttr" }
    ]
  },
  {
    "name": "tf.XlaOptimizationBarrier",
    "summary": "Wraps the XLA OptimizationBarrier operator.",
    "description": "Documented at https://www.tensorflow.org/xla/operation_semantics#optimizationbarrier.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ]
  },
  {
    "name": "tf.XlaPad",
    "summary": "Wraps the XLA Pad operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#pad\n.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "padding_value", "type": "Arg" },
      { "name": "padding_low", "type": "Arg" },
      { "name": "padding_high", "type": "Arg" },
      { "name": "padding_interior", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.XlaRecv",
    "summary": "Receives the named tensor from another XLA computation. Wraps the XLA Recv",
    "description": "operator documented at\n https://www.tensorflow.org/performance/xla/operation_semantics#recv .",
    "outputs": [
      { "name": "tensor", "type": "Res" }
    ],
    "attributes": [
      { "name": "tensor_name", "type": "StrAttr" },
      { "name": "shape", "type": "TF_ShapeAttr" }
    ]
  },
  {
    "name": "tf.XlaRecvFromHost",
    "summary": "An op to receive a tensor from the host.",
    "description": "output: the tensor that will be received from the host.\nToutput: element type for output.\nshape: shape for output.\nkey: A unique identifier for this region used to match up host transfers.",
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "shape", "type": "TF_ShapeAttr" },
      { "name": "key", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaRecvTPUEmbeddingActivations",
    "summary": "An op that receives embedding activations on the TPU.",
    "description": "The TPU system performs the embedding lookups and aggregations. The results of\nthese aggregations are visible to the Tensorflow Graph as the outputs of a\nXlaRecvTPUEmbeddingActivations Op. This op returns a list containing one\nTensor of activations per table specified in the model.",
    "inputs": [
      { "name": "deduplication_data", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaRecvTPUEmbeddingDeduplicationData",
    "summary": "Receives deduplication data (indices and weights) from the embedding core.",
    "description": "The deduplication data is a Tensor with type=DT_VARIANT. The tensor itself is an\nXLA nested tuple containing N elements (where N is the ratio of the number of\nembedding to tensor cores per TPU chip). Each element of the nested tuple is a\ntuple of rank 1 tensors. Each tensor either contains indices (DT_UINT32) for\nembedding lookup on the TensorCore or weights (DT_FLOAT) to apply to the output\nof the embedding lookup operation.",
    "outputs": [
      { "name": "output", "type": "TF_VariantTensor" }
    ],
    "attributes": [
      { "name": "config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaReduce",
    "summary": "Wraps the XLA Reduce operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#reduce .",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "init_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "dimensions_to_reduce", "type": "I64ArrayAttr" },
      { "name": "reducer", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.XlaReducePrecision",
    "summary": "Wraps the XLA ReducePrecision operator",
    "description": "documented at https://www.tensorflow.org/xla/operation_semantics#reduceprecision.",
    "inputs": [
      { "name": "operand", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_FloatTensor" }
    ],
    "attributes": [
      { "name": "exponent_bits", "type": "I64Attr" },
      { "name": "mantissa_bits", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.XlaReduceScatter",
    "summary": "Wraps the XLA ReduceScatter operator",
    "description": "documented at https://www.tensorflow.org/xla/operation_semantics#reducescatter.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "group_assignment", "type": "Arg" },
      { "name": "scatter_dimension", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "reduce_op", "type": "TF_AnyStrAttrOf" }
    ]
  },
  {
    "name": "tf.XlaReduceWindow",
    "summary": "Wraps the XLA ReduceWindow operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#reducewindow .",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "init_value", "type": "Arg" },
      { "name": "window_dimensions", "type": "Arg" },
      { "name": "window_strides", "type": "Arg" },
      { "name": "base_dilations", "type": "TF_I32OrI64Tensor" },
      { "name": "window_dilations", "type": "TF_I32OrI64Tensor" },
      { "name": "padding", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "computation", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.XlaRemoveDynamicDimensionSize",
    "summary": "Inverse of XlaSetDynamicDimensionSize.",
    "description": "Make an xla bounded dynamic dimension into a static dimension. The bound of the\nsize of dimension `dim_index` becomes the static dimension size.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "dim_index", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.XlaReplicaId",
    "summary": "Replica ID.",
    "outputs": [
      { "name": "id", "type": "TF_Int32Tensor" }
    ]
  },
  {
    "name": "tf.XlaRngBitGenerator",
    "summary": "Stateless PRNG bit generator.",
    "description": "Wraps the XLA RngBitGenerator operator, documented at\n https://www.tensorflow.org/performance/xla/operation_semantics#rngbitgenerator.",
    "inputs": [
      { "name": "algorithm", "type": "Arg" },
      { "name": "initial_state", "type": "Arg" },
      { "name": "shape", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output_key", "type": "TF_Uint64Tensor" },
      { "name": "output", "type": "TensorOf" }
    ]
  },
  {
    "name": "tf.XlaScatter",
    "summary": "Wraps the XLA Scatter operator documented at",
    "description": "https://www.tensorflow.org/xla/operation_semantics#scatter.",
    "inputs": [
      { "name": "operand", "type": "Arg" },
      { "name": "scatter_indices", "type": "Arg" },
      { "name": "updates", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TensorOf" }
    ],
    "attributes": [
      { "name": "update_computation", "type": "SymbolRefAttr" },
      { "name": "dimension_numbers", "type": "StrAttr" },
      { "name": "indices_are_sorted", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.XlaSelectAndScatter",
    "summary": "Wraps the XLA SelectAndScatter operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#selectandscatter\n.",
    "inputs": [
      { "name": "operand", "type": "Arg" },
      { "name": "window_dimensions", "type": "Arg" },
      { "name": "window_strides", "type": "Arg" },
      { "name": "padding", "type": "Arg" },
      { "name": "source", "type": "Arg" },
      { "name": "init_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_NumberTensor" }
    ],
    "attributes": [
      { "name": "select", "type": "SymbolRefAttr" },
      { "name": "scatter", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.XlaSelfAdjointEig",
    "summary": "Computes the eigen decomposition of a batch of self-adjoint matrices",
    "description": "(Note: Only real inputs are supported).\n\nComputes the eigenvalues and eigenvectors of the innermost N-by-N matrices in\ntensor such that tensor[...,:,:] * v[..., :,i] = e[..., i] * v[...,:,i], for\ni=0...N-1.",
    "inputs": [
      { "name": "a", "type": "Arg" }
    ],
    "outputs": [
      { "name": "w", "type": "Res" },
      { "name": "v", "type": "Res" }
    ],
    "attributes": [
      { "name": "lower", "type": "BoolAttr" },
      { "name": "max_iter", "type": "I64Attr" },
      { "name": "epsilon", "type": "F32Attr" }
    ]
  },
  {
    "name": "tf.XlaSend",
    "summary": "Sends the named tensor to another XLA computation. Wraps the XLA Send operator",
    "description": "documented at\n https://www.tensorflow.org/performance/xla/operation_semantics#send .",
    "inputs": [
      { "name": "tensor", "type": "Arg" }
    ],
    "attributes": [
      { "name": "tensor_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSendToHost",
    "summary": "An op to send a tensor to the host.",
    "description": "input: the tensor that will be sent to the host.\nTinput: element type for input.\nkey: A unique identifier for this region used to match up host transfers.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "key", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSendTPUEmbeddingGradients",
    "summary": "An op that performs gradient updates of embedding tables.",
    "description": "The gradients argument is a TensorList having the same length and shapes as the\nreturn value of XlaRecvTPUEmbeddingActivations, but contains gradients of the\nmodel's loss with respect to the embedding activations. The embedding tables are\nupdated from these gradients via the optimizer specified in the\nTPUEmbeddingConfiguration proto given to tpu.initialize_system.",
    "inputs": [
      { "name": "gradients", "type": "Arg" },
      { "name": "learning_rates", "type": "Arg" },
      { "name": "deduplication_data", "type": "Arg" }
    ],
    "attributes": [
      { "name": "config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSetBound",
    "summary": "Set a bound for the given input value as a hint to Xla compiler,",
    "description": "returns the same value.",
    "inputs": [
      { "name": "input", "type": "TF_Int32Tensor" },
      { "name": "bound", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Int32Tensor" }
    ]
  },
  {
    "name": "tf.XlaSetDynamicDimensionSize",
    "summary": "Make a static dimension into a xla bounded dynamic dimension.",
    "description": "The current static dimension size will become the bound and the second\n        operand becomes the dynamic size of the dimension.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" },
      { "name": "dim_index", "type": "TF_Int32Tensor" },
      { "name": "size", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ]
  },
  {
    "name": "tf.XlaSharding",
    "summary": "An op which shards the input based on the given sharding attribute.\n\nSince TF runtime still relies on V1 sharding but the compiler requires V2\nSharding to support the new partitioner Shardy, when the op can reach both the\ncompiler and the runtime, we record V1 sharding attribute in _XlaSharding and V2\nsharding attribute in _XlaShardingV2, and verify that they are equivalent when\nthey both exist and _XlaShardingV2 is used.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "sharding", "type": "DefaultValuedStrAttr" },
      { "name": "_XlaSharding", "type": "OptionalAttr" },
      { "name": "_XlaShardingV2", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tf.XlaSort",
    "summary": "Wraps the XLA Sort operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#sort\n.\n\nSorts a tensor. Currently only sorts in ascending order are supported.",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Res" }
    ]
  },
  {
    "name": "tf.XlaSparseActivationsUnstack",
    "summary": "XlaSparseActivationsUnstackOp attempts to fuse transpose, relayout,\n    conversion, stacking and optionally interleaving of the embedding\n    activations, while also offloading this work to SparseCore.\n\n    The op assumes its operand is in SparseCore layout.\n    The output is a tuple of tensors in TensorCore layout.",
    "inputs": [
      { "name": "stacked_activations", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "unstacked_embedding_activations", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "num_tables", "type": "ConfinedAttr" },
      { "name": "sample_counts", "type": "ConfinedAttr" },
      { "name": "features", "type": "ConfinedAttr" },
      { "name": "interleaved", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseCoreAdagrad",
    "summary": "aaa",
    "inputs": [
      { "name": "indices", "type": "TF_Int32Tensor" },
      { "name": "gradient", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "feature_width", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.XlaSparseCoreAdagradMomentum",
    "summary": "aaa",
    "inputs": [
      { "name": "indices", "type": "TF_Int32Tensor" },
      { "name": "gradient", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "beta_1", "type": "TF_Float32Tensor" },
      { "name": "epsilon", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "momentum", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_momentum", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "feature_width", "type": "I64Attr" },
      { "name": "use_nesterov", "type": "BoolAttr" },
      { "name": "beta_2", "type": "F32Attr" },
      { "name": "exponent", "type": "F32Attr" }
    ]
  },
  {
    "name": "tf.XlaSparseCoreAdam",
    "summary": "aaa",
    "inputs": [
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "indices", "type": "TF_Int32Tensor" },
      { "name": "gradient", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "momentum", "type": "TF_Float32Tensor" },
      { "name": "velocity", "type": "TF_Float32Tensor" },
      { "name": "beta_1", "type": "TF_Float32Tensor" },
      { "name": "beta_2", "type": "TF_Float32Tensor" },
      { "name": "epsilon", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_velocity", "type": "TF_Float32Tensor" },
      { "name": "updated_momentum", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "feature_width", "type": "I64Attr" },
      { "name": "use_sum_inside_sqrt", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseCoreFtrl",
    "summary": "aaa",
    "inputs": [
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "linear", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "indices", "type": "TF_Int32Tensor" },
      { "name": "gradient", "type": "TF_Float32Tensor" },
      { "name": "beta", "type": "TF_Float32Tensor" },
      { "name": "learning_rate_power", "type": "TF_Float32Tensor" },
      { "name": "l2_regularization_strength", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_linear", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "feature_width", "type": "I64Attr" },
      { "name": "multiply_linear_by_learning_rate", "type": "BoolAttr" },
      { "name": "l1_regularization_strength", "type": "F32Attr" }
    ]
  },
  {
    "name": "tf.XlaSparseCoreSgd",
    "summary": "aaa",
    "inputs": [
      { "name": "indices", "type": "TF_Int32Tensor" },
      { "name": "gradient", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "feature_width", "type": "I64Attr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulCustomCombinerOnTcGradWithAdagradAndCsrInput",
    "summary": "This op back-propagates the activation gradients to the embedding table and the combiner weights.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_pos_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" },
      { "name": "preserved_valencies", "type": "TF_Int32Tensor" },
      { "name": "preserved_vectors", "type": "TF_Float32Tensor" },
      { "name": "preserved_weights", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "combiner_weights_learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_weights", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "max_valency", "type": "ConfinedAttr" },
      { "name": "num_weights", "type": "ConfinedAttr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "combiner_table_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "combiner_weights_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulCustomCombinerOnTcGradWithAdagradMomentumAndCsrInput",
    "summary": "This op back-propagates the activation gradients to the embedding table and the combiner weights.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_pos_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" },
      { "name": "preserved_valencies", "type": "TF_Int32Tensor" },
      { "name": "preserved_vectors", "type": "TF_Float32Tensor" },
      { "name": "preserved_weights", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "combiner_weights_learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_momenta", "type": "TF_Float32Tensor" },
      { "name": "updated_weights", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "max_valency", "type": "ConfinedAttr" },
      { "name": "num_weights", "type": "ConfinedAttr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "use_nesterov", "type": "BoolAttr" },
      { "name": "exponent", "type": "F32Attr" },
      { "name": "beta1", "type": "F32Attr" },
      { "name": "beta2", "type": "F32Attr" },
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "combiner_table_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "combiner_weights_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulCustomCombinerOnTcGradWithAdamAndCsrInput",
    "summary": "This op back-propagates the activation gradients to the embedding table and the combiner weights.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_pos_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" },
      { "name": "preserved_valencies", "type": "TF_Int32Tensor" },
      { "name": "preserved_vectors", "type": "TF_Float32Tensor" },
      { "name": "preserved_weights", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "combiner_weights_learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "velocity", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_momenta", "type": "TF_Float32Tensor" },
      { "name": "updated_velocity", "type": "TF_Float32Tensor" },
      { "name": "updated_weights", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "max_valency", "type": "ConfinedAttr" },
      { "name": "num_weights", "type": "ConfinedAttr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "use_sum_inside_sqrt", "type": "BoolAttr" },
      { "name": "beta1", "type": "F32Attr" },
      { "name": "beta2", "type": "F32Attr" },
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "combiner_table_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "combiner_weights_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulCustomCombinerOnTcGradWithCsrInput",
    "summary": "This op back-propagates the activation gradients to the embedding table and the combiner weights.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_pos_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" },
      { "name": "preserved_valencies", "type": "TF_Int32Tensor" },
      { "name": "preserved_vectors", "type": "TF_Float32Tensor" },
      { "name": "preserved_weights", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "tables", "type": "Variadic" },
      { "name": "hyperparameters", "type": "Variadic" },
      { "name": "combiner_weights_learning_rate", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_tables", "type": "Variadic" },
      { "name": "updated_weights", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "max_valency", "type": "ConfinedAttr" },
      { "name": "num_weights", "type": "ConfinedAttr" },
      { "name": "combiner_table_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "combiner_weights_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "optimizer_custom_computation", "type": "SymbolRefAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulCustomCombinerOnTcGradWithFtrlAndCsrInput",
    "summary": "This op back-propagates the activation gradients to the embedding table and the combiner weights.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_pos_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" },
      { "name": "preserved_valencies", "type": "TF_Int32Tensor" },
      { "name": "preserved_vectors", "type": "TF_Float32Tensor" },
      { "name": "preserved_weights", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "combiner_weights_learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "linear", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_linear", "type": "TF_Float32Tensor" },
      { "name": "updated_weights", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "max_valency", "type": "ConfinedAttr" },
      { "name": "num_weights", "type": "ConfinedAttr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "multiply_linear_by_learning_rate", "type": "BoolAttr" },
      { "name": "beta", "type": "F32Attr" },
      { "name": "learning_rate_power", "type": "F32Attr" },
      { "name": "l1_regularization_strength", "type": "F32Attr" },
      { "name": "l2_regularization_strength", "type": "F32Attr" },
      { "name": "combiner_table_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "combiner_weights_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulCustomCombinerOnTcGradWithSgdAndCsrInput",
    "summary": "This op back-propagates the activation gradients to the embedding table and the combiner weights.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_pos_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" },
      { "name": "preserved_valencies", "type": "TF_Int32Tensor" },
      { "name": "preserved_vectors", "type": "TF_Float32Tensor" },
      { "name": "preserved_weights", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "combiner_weights_learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_weights", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "max_valency", "type": "ConfinedAttr" },
      { "name": "num_weights", "type": "ConfinedAttr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "combiner_table_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "combiner_weights_vjp_computation", "type": "SymbolRefAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulCustomCombinerOnTcWithCsrInput",
    "summary": "This op looks up the embedding vectors on SparseCores and performs the given combiner computation on TensorCores.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_pos_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "weights", "type": "TF_Float32Tensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_Float32Tensor" },
      { "name": "preserved_valencies", "type": "TF_Int32Tensor" },
      { "name": "preserved_vectors", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "input_size", "type": "ConfinedAttr" },
      { "name": "max_valency", "type": "ConfinedAttr" },
      { "name": "num_weights", "type": "ConfinedAttr" },
      { "name": "quantization_config_low", "type": "OptionalAttr" },
      { "name": "quantization_config_high", "type": "OptionalAttr" },
      { "name": "quantization_config_num_buckets", "type": "OptionalAttr" },
      { "name": "combiner_computation", "type": "SymbolRefAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithAdagradAndCsrInput",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithAdagradAndStaticBufferSize",
    "summary": "A XLA op which performs the Adagrad optimizer update for the dense-sparse matrix multiplication.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "max_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "max_unique_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithAdagradMomentumAndCsrInput",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_momenta", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "use_nesterov", "type": "BoolAttr" },
      { "name": "exponent", "type": "F32Attr" },
      { "name": "beta1", "type": "F32Attr" },
      { "name": "beta2", "type": "F32Attr" },
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithAdagradMomentumAndStaticBufferSize",
    "summary": "A XLA op which performs the Adagrad momentumoptimizer update for the dense-sparse matrix multiplication.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_momenta", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "use_nesterov", "type": "BoolAttr" },
      { "name": "exponent", "type": "F32Attr" },
      { "name": "beta1", "type": "F32Attr" },
      { "name": "beta2", "type": "F32Attr" },
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "max_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "max_unique_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithAdamAndCsrInput",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "velocity", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_momenta", "type": "TF_Float32Tensor" },
      { "name": "updated_velocity", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "use_sum_inside_sqrt", "type": "BoolAttr" },
      { "name": "beta1", "type": "F32Attr" },
      { "name": "beta2", "type": "F32Attr" },
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithAdamAndStaticBufferSize",
    "summary": "A XLA op which performs the Adam optimizer update for the dense-sparse matrix multiplication.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "momenta", "type": "TF_Float32Tensor" },
      { "name": "velocity", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_momenta", "type": "TF_Float32Tensor" },
      { "name": "updated_velocity", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "use_sum_inside_sqrt", "type": "BoolAttr" },
      { "name": "beta1", "type": "F32Attr" },
      { "name": "beta2", "type": "F32Attr" },
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "max_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "max_unique_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithCsrInput",
    "summary": "A XLA op which performs the custom optimizer per-row update for the dense-sparse matrix multiplication.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "tables", "type": "Variadic" },
      { "name": "hyperparameters", "type": "Variadic" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_tables", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "custom_computation", "type": "SymbolRefAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithFtrlAndCsrInput",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "linear", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_linear", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "multiply_linear_by_learning_rate", "type": "BoolAttr" },
      { "name": "beta", "type": "F32Attr" },
      { "name": "learning_rate_power", "type": "F32Attr" },
      { "name": "l1_regularization_strength", "type": "F32Attr" },
      { "name": "l2_regularization_strength", "type": "F32Attr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithFtrlAndStaticBufferSize",
    "summary": "A XLA op which performs the Ftrl optimizer update for the dense-sparse matrix multiplication.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "accumulator", "type": "TF_Float32Tensor" },
      { "name": "linear", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" },
      { "name": "updated_accumulator", "type": "TF_Float32Tensor" },
      { "name": "updated_linear", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "multiply_linear_by_learning_rate", "type": "BoolAttr" },
      { "name": "beta", "type": "F32Attr" },
      { "name": "learning_rate_power", "type": "F32Attr" },
      { "name": "l1_regularization_strength", "type": "F32Attr" },
      { "name": "l2_regularization_strength", "type": "F32Attr" },
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "max_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "max_unique_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithSgdAndCsrInput",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulGradWithSgdAndStaticBufferSize",
    "summary": "A XLA op which performs the SGD optimizer update for the dense-sparse matrix multiplication.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "activation_gradients", "type": "TF_Float32Tensor" },
      { "name": "learning_rate", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "updated_embedding_table", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "clip_weight_min", "type": "F32Attr" },
      { "name": "clip_weight_max", "type": "F32Attr" },
      { "name": "max_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "max_unique_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulWithCsrInput",
    "summary": "aaa",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "input_size", "type": "ConfinedAttr" },
      { "name": "quantization_config_low", "type": "OptionalAttr" },
      { "name": "quantization_config_high", "type": "OptionalAttr" },
      { "name": "quantization_config_num_buckets", "type": "OptionalAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseDenseMatmulWithStaticBufferSize",
    "summary": "A XLA op which performs the dense-sparse matrix multiplication.",
    "inputs": [
      { "name": "row_pointers", "type": "TF_Int32Tensor" },
      { "name": "sorted_sample_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_token_ids", "type": "TF_Int32Tensor" },
      { "name": "sorted_gains", "type": "TF_Float32Tensor" },
      { "name": "embedding_table", "type": "TF_Float32Tensor" },
      { "name": "num_minibatches_per_physical_sparse_core", "type": "TF_Int32Tensor" }
    ],
    "outputs": [
      { "name": "activations", "type": "TF_Float32Tensor" }
    ],
    "attributes": [
      { "name": "input_size", "type": "ConfinedAttr" },
      { "name": "quantization_config_low", "type": "OptionalAttr" },
      { "name": "quantization_config_high", "type": "OptionalAttr" },
      { "name": "quantization_config_num_buckets", "type": "OptionalAttr" },
      { "name": "max_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "max_unique_ids_per_sparse_core", "type": "ConfinedAttr" },
      { "name": "table_name", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaSparseGradientsStack",
    "summary": "XlaSparseGradientsStackOp attempts to fuse transpose, relayout, conversion,\n    unstacking and optionally interleaving of the embedding gradients, while\n    also offloading this work to SparseCore.\n\n    The op assumes its operands are in TensoreCore layout.\n    The output is in SparseCore layout.",
    "inputs": [
      { "name": "unstacked_gradients", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "stacked_gradients", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "num_tables", "type": "ConfinedAttr" },
      { "name": "interleaved", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.XlaSplitND",
    "summary": "Splits input tensor across all dimensions.",
    "description": "An op which slices the input tensor based on the given num_splits attribute,\npads slices optionally, and returned the slices. Slices are returned in\nrow-major order.\n\nThis op may be generated via the TPU bridge.\n\nFor example, with `input` tensor:\n```\n[[0, 1, 2],\n [3, 4, 5],\n [6, 7, 8]]\n```\n`num_splits`:\n```\n[2, 2]\n```\nand `paddings`:\n```\n[1, 1]\n```\nthe expected `outputs` is:\n```\n[[0, 1],\n [3, 4]]\n[[2, 0],\n [5, 0]]\n[[6, 7],\n [0, 0]]\n[[8, 0],\n [0, 0]]\n```",
    "inputs": [
      { "name": "input", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "num_splits", "type": "I64ArrayAttr" },
      { "name": "paddings", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.XlaSpmdFullToShardShape",
    "summary": "An op used by XLA SPMD partitioner to switch from automatic partitioning to",
    "description": "manual partitioning. It annotates the input (full-shape, to be automatically\npartitioned) with the same sharding used by manual partitioning, and outputs a\nshard-shaped tensor to be consumed by later manually-partitioned ops. If the\nshape is not evenly partitionable, the padding region will be masked with 0s.\nThe conversion can happen partially in subgroups, by specifying the dim\nattribute, where only that dim will be converted.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "manual_sharding", "type": "StrAttr" },
      { "name": "dim", "type": "DefaultValuedOptionalAttr" },
      { "name": "unspecified_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.XlaSpmdShardToFullShape",
    "summary": "An op used by XLA SPMD partitioner to switch from manual partitioning to",
    "description": "automatic partitioning. It converts the shard-shaped, manually partitioned input\ninto full-shaped tensor to be partitioned automatically with the same sharding\nused by manual partitioning. The conversion can happen partially in subgroups,\nby specifying the dim attribute, where only that dim will be converted.",
    "inputs": [
      { "name": "input", "type": "TF_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TF_Tensor" }
    ],
    "attributes": [
      { "name": "manual_sharding", "type": "StrAttr" },
      { "name": "full_shape", "type": "TF_ShapeAttr" },
      { "name": "dim", "type": "DefaultValuedOptionalAttr" },
      { "name": "unspecified_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tf.XlaSvd",
    "summary": "Computes the eigen decomposition of a batch of self-adjoint matrices",
    "description": "(Note: Only real inputs are supported).\n\nComputes the eigenvalues and eigenvectors of the innermost M-by-N matrices in\ntensor such that tensor[...,:,:] = u[..., :, :] * Diag(s[..., :]) * Transpose(v[...,:,:]).",
    "inputs": [
      { "name": "a", "type": "Arg" }
    ],
    "outputs": [
      { "name": "s", "type": "Res" },
      { "name": "u", "type": "Res" },
      { "name": "v", "type": "Res" }
    ],
    "attributes": [
      { "name": "max_iter", "type": "I64Attr" },
      { "name": "epsilon", "type": "F32Attr" },
      { "name": "precision_config", "type": "StrAttr" }
    ]
  },
  {
    "name": "tf.XlaVariadicReduce",
    "summary": "Wraps the variadic XLA Reduce operator.",
    "description": "Semantics are documented at\n https://www.tensorflow.org/performance/xla/operation_semantics#variadic_reduce.\n\nThis version is limited to operands of the same dtype.\nXlaVariadicReduceV2 is a version that supports heterogeneous operands.",
    "inputs": [
      { "name": "input", "type": "Arg" },
      { "name": "init_value", "type": "Arg" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimensions_to_reduce", "type": "I64ArrayAttr" },
      { "name": "reducer", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.XlaVariadicReduceV2",
    "summary": "Wraps the variadic XLA Reduce operator.",
    "description": "Semantics are documented at\n https://www.tensorflow.org/performance/xla/operation_semantics#variadic_reduce.\n\nThis is an expanded version of XlaVariadicReduce, with support for\noperands of different dtypes, and improved shape inference.",
    "inputs": [
      { "name": "inputs", "type": "Arg" },
      { "name": "init_values", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimensions_to_reduce", "type": "I64ArrayAttr" },
      { "name": "reducer", "type": "SymbolRefAttr" }
    ]
  },
  {
    "name": "tf.XlaVariadicSort",
    "summary": "Wraps the XLA Sort operator, documented at",
    "description": "https://www.tensorflow.org/performance/xla/operation_semantics#sort\n.\n\nSorts one or more tensors, with support for custom comparator, dimension, and\nis_stable attributes.",
    "inputs": [
      { "name": "inputs", "type": "Arg" },
      { "name": "dimension", "type": "Arg" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Res" }
    ],
    "attributes": [
      { "name": "comparator", "type": "SymbolRefAttr" },
      { "name": "is_stable", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tf.Xlog1py",
    "summary": "Returns 0 if x == 0, and x * log1p(y) otherwise, elementwise.",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" },
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Xlogy",
    "summary": "Returns 0 if x == 0, and x * log(y) otherwise, elementwise.",
    "inputs": [
      { "name": "x", "type": "TF_FpOrComplexTensor" },
      { "name": "y", "type": "TF_FpOrComplexTensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_FpOrComplexTensor" }
    ]
  },
  {
    "name": "tf.Yield",
    "summary": "Yield operation",
    "description": "The \"yield\" operation represents a return operation within the conditional\n    and body of structured control flow (e.g., if and while). The operation\n    takes a variable number of operands and produces no results. The number and\n    types of inputs must match the signature of the operation that contains the\n    region."
  },
  {
    "name": "tf.ZerosLike",
    "summary": "Returns a tensor of zeros with the same shape and type as x.",
    "inputs": [
      { "name": "x", "type": "Arg" }
    ],
    "outputs": [
      { "name": "y", "type": "Res" }
    ]
  },
  {
    "name": "tf.Zeta",
    "summary": "Compute the Hurwitz zeta function \\\\(\\zeta(x, q)\\\\).",
    "description": "The Hurwitz zeta function is defined as:\n\n\n\\\\(\\zeta(x, q) = \\sum_{n=0}^{\\infty} (q + n)^{-x}\\\\)",
    "inputs": [
      { "name": "x", "type": "TF_F32OrF64Tensor" },
      { "name": "q", "type": "TF_F32OrF64Tensor" }
    ],
    "outputs": [
      { "name": "z", "type": "TF_F32OrF64Tensor" }
    ]
  },
  {
    "name": "tfl.abs",
    "summary": "Absolute value operator",
    "description": "Given a tensor `x`, this operation returns a tensor containing the absolute\nvalue of each element in `x`. For example, if x is an input element and y is\nan output element, this operation computes \\\\(y = |x|\\\\).",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.add",
    "summary": "Addition operator",
    "description": "Element-wise addition operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.add_n",
    "summary": "add_n operator",
    "description": "Adds all input tensors element-wise.",
    "inputs": [
      { "name": "inputs", "type": "TFL_VariadicTensorOf" }
    ],
    "outputs": [
      { "name": "sum", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.arg_max",
    "summary": "ArgMax operator",
    "description": "Returns the index with the largest value across dimensions of a tensor.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "dim", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tfl.arg_min",
    "summary": "ArgMin operator",
    "description": "Returns the index with the smallest value across dimensions of a tensor.\n      a = [1, 10, 26.9, 2.8, 166.32, 62.3]\n      b = tf.math.argmin(input = a)\n      c = tf.keras.backend.eval(b)",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "dim", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tfl.assign_variable",
    "summary": "Assigns a new value to a variable.",
    "description": "Any ReadVariableOp with a control dependency on this op is guaranteed to return\nthis value or a subsequent newer value of the variable.",
    "inputs": [
      { "name": "resource_id", "type": "TFL_ResourceTensor" },
      { "name": "value", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.atan2",
    "summary": "Atan2 operation",
    "description": "The \"atan2\" operation computes the arctangent of y/x element-wise,\n    respecting signs of the arguments.",
    "inputs": [
      { "name": "y", "type": "TFL_TensorOf" },
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.average_pool_2d",
    "summary": "Average_pool_2d operator",
    "description": "Performs average-pooling operation on input.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "filter_height", "type": "I32Attr" },
      { "name": "filter_width", "type": "I32Attr" },
      { "name": "padding", "type": "TFL_PaddingAttr" },
      { "name": "stride_h", "type": "I32Attr" },
      { "name": "stride_w", "type": "I32Attr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.basic_lstm",
    "summary": "The basic lstm operator",
    "description": "basic LSTM Cell Operator.",
    "inputs": [
      { "name": "data_input", "type": "TFL_TensorOf" },
      { "name": "prev_activ_input", "type": "TFL_TensorOf" },
      { "name": "weights_input", "type": "TFL_TensorOf" },
      { "name": "biases_input", "type": "TFL_TensorOf" },
      { "name": "prev_state_input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "activ_output", "type": "TFL_2DTensorOf" },
      { "name": "state_output", "type": "TFL_2DTensorOf" },
      { "name": "concat_temp", "type": "TFL_2DTensorOf" },
      { "name": "activ_temp", "type": "TFL_2DTensorOf" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "DefaultValuedStrAttr" },
      { "name": "cell_clip", "type": "ConfinedAttr" },
      { "name": "proj_clip", "type": "ConfinedAttr" },
      { "name": "kernel_type", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tfl.batch_matmul",
    "summary": "Batch Matrix Multiply Operator",
    "description": "Performs a batched matrix multiplication on the inputs. Follows the\nconventions of TensorFlow BatchMatMulV2, with support for unknown dimensions\nin the batch dimensions and broadcasting.\n\n    Inputs:\n      `inputs[0]`: required: input LHS\n      `inputs[1]`: required: input RHS\n      `adjoint_lhs`: optional: Transpose LHS (default false)\n      `adjoint_rhs`: optional: Transpose RHS (default false)",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" },
      { "name": "y", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "adj_x", "type": "DefaultValuedOptionalAttr" },
      { "name": "adj_y", "type": "DefaultValuedOptionalAttr" },
      { "name": "asymmetric_quantize_inputs", "type": "OptionalAttr" }
    ],
    "category": "Layer"
  },
  {
    "name": "tfl.batch_to_space_nd",
    "summary": "BatchToSpaceNd operator",
    "description": "This operation reshapes the \"batch\" dimension 0 into space dimensions.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "block_shape", "type": "TFL_TensorOf" },
      { "name": "indices", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.bidirectional_sequence_lstm",
    "summary": "Bidirectional sequence lstm operator",
    "description": "Bidirectional lstm is essentially two lstms, one running forward & the\n    other running backward. And the output is the concatenation of the two\n    lstms.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "fw_input_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_input_to_forget_weights", "type": "TFL_TensorOf" },
      { "name": "fw_input_to_cell_weights", "type": "TFL_TensorOf" },
      { "name": "fw_input_to_output_weights", "type": "TFL_TensorOf" },
      { "name": "fw_recurrent_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_recurrent_to_forget_weights", "type": "TFL_TensorOf" },
      { "name": "fw_recurrent_to_cell_weights", "type": "TFL_TensorOf" },
      { "name": "fw_recurrent_to_output_weights", "type": "TFL_TensorOf" },
      { "name": "fw_cell_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_cell_to_forget_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_cell_to_output_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_input_gate_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_forget_gate_bias", "type": "TFL_TensorOf" },
      { "name": "fw_cell_bias", "type": "TFL_TensorOf" },
      { "name": "fw_output_gate_bias", "type": "TFL_TensorOf" },
      { "name": "fw_projection_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_projection_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_input_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_input_to_forget_weights", "type": "TFL_TensorOf" },
      { "name": "bw_input_to_cell_weights", "type": "TFL_TensorOf" },
      { "name": "bw_input_to_output_weights", "type": "TFL_TensorOf" },
      { "name": "bw_recurrent_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_recurrent_to_forget_weights", "type": "TFL_TensorOf" },
      { "name": "bw_recurrent_to_cell_weights", "type": "TFL_TensorOf" },
      { "name": "bw_recurrent_to_output_weights", "type": "TFL_TensorOf" },
      { "name": "bw_cell_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_cell_to_forget_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_cell_to_output_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_input_gate_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_forget_gate_bias", "type": "TFL_TensorOf" },
      { "name": "bw_cell_bias", "type": "TFL_TensorOf" },
      { "name": "bw_output_gate_bias", "type": "TFL_TensorOf" },
      { "name": "bw_projection_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_projection_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_input_activation_state", "type": "TFL_StatefulTensor" },
      { "name": "fw_input_cell_state", "type": "TFL_StatefulTensor" },
      { "name": "bw_input_activation_state", "type": "TFL_StatefulTensor" },
      { "name": "bw_input_cell_state", "type": "TFL_StatefulTensor" },
      { "name": "aux_input", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_aux_input_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_aux_input_to_forget_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_aux_input_to_cell_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "fw_aux_input_to_output_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_aux_input_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_aux_input_to_forget_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_aux_input_to_cell_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "bw_aux_input_to_output_weights", "type": "TFL_TensorOfOrNone" }
    ],
    "outputs": [
      { "name": "fw_output", "type": "AnyTensor" },
      { "name": "bw_output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "cell_clip", "type": "ConfinedAttr" },
      { "name": "proj_clip", "type": "ConfinedAttr" },
      { "name": "merge_outputs", "type": "BoolAttr" },
      { "name": "time_major", "type": "BoolAttr" },
      { "name": "asymmetric_quantize_inputs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tfl.bitcast",
    "summary": "Bitcast operator",
    "description": "Bitcasts a tensor from one type to another.",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "tfl.bitwise_xor",
    "summary": "Bitwise Xor operator",
    "description": "Elementwise computes the bitwise XOR of `lhs` and `rhs`.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.broadcast_args",
    "summary": "Return the shape of s0 op s1 with broadcast.",
    "description": "Given `s0` and `s1`, tensors that represent shapes, compute `r0`, the\nbroadcasted shape. `s0`, `s1` and `r0` are all integer vectors.",
    "inputs": [
      { "name": "s0", "type": "TFL_I32OrI64Tensor" },
      { "name": "s1", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "r0", "type": "TFL_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tfl.broadcast_to",
    "summary": "Broadcast an array for a compatible shape.",
    "description": "Broadcasting is the process of making arrays to have compatible shapes\nfor arithmetic operations. Two shapes are compatible if for each\ndimension pair they are either equal or one of them is one. When trying\nto broadcast a Tensor to a shape, it starts with the trailing dimensions,\nand works its way forward.\n\nFor example,\n\n>>> x = tf.constant([1, 2, 3])\n>>> y = tf.broadcast_to(x, [3, 3])\n>>> print(y)\ntf.Tensor(\n    [[1 2 3]\n     [1 2 3]\n     [1 2 3]], shape=(3, 3), dtype=int32)\n\nIn the above example, the input Tensor with the shape of `[1, 3]`\nis broadcasted to output Tensor with shape of `[3, 3]`.\n\nWhen doing broadcasted operations such as multiplying a tensor\nby a scalar, broadcasting (usually) confers some time or space\nbenefit, as the broadcasted tensor is never materialized.\n\nHowever, `broadcast_to` does not carry with it any such benefits.\nThe newly-created tensor takes the full memory of the broadcasted\nshape. (In a graph context, `broadcast_to` might be fused to\nsubsequent operation and then be optimized away, however.)",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "shape", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.bucketize",
    "summary": "Bucketizes 'input' based on 'boundaries'.",
    "description": "Example:\n\nIf the inputs are `boundaries = [0, 10, 100]` and\n`input = [[-5, 10000][150, 10][5, 100]]`,\nthen the output will be `output = [[0, 3][3, 2][1, 3]]`.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "boundaries", "type": "F32ArrayAttr" }
    ]
  },
  {
    "name": "tfl.call_once",
    "summary": "Invokes an initialization function",
    "description": "This operation invokes the given initialization function for the session\ninitializer in tf saved model dialect.",
    "attributes": [
      { "name": "session_init_function", "type": "StrAttr" }
    ]
  },
  {
    "name": "tfl.cast",
    "summary": "Cast operator",
    "description": "Casts input from input type to output type.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.ceil",
    "summary": "Ceil operator",
    "description": "Returns element-wise ceil value of the input.",
    "inputs": [
      { "name": "x", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_FpTensor" }
    ]
  },
  {
    "name": "tfl.complex_abs",
    "summary": "Computes the complex absolute value of a tensor.",
    "description": "Given a tensor `x` of complex numbers, this operation returns a tensor of type\n`float` or `double` that is the absolute value of each element in `x`. All\nelements in `x` must be complex numbers of the form \\\\(a + bj\\\\). The absolute\nvalue is computed as \\\\( \\sqrt{a^2 + b^2}\\\\).",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.concatenation",
    "summary": "Concatenation operator",
    "description": "Concatenates tensors along one dimension",
    "inputs": [
      { "name": "values", "type": "TFL_VariadicTensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.control_node",
    "summary": "The `TFL.control_node` operation wraps single-block operations in order to attach control edges.",
    "description": "This is used to wrap regions and attach control dependencies to them. Typically,\n    this will happen in one of the last steps before emitting the flatbuffer model\n    in order to enable optimizations that rely on a fixed order of operations (such\n    as rematerialization.)\n    The flatbuffer exporter will unwrap the wrapped region and annotate the generated\n    model with metadata such that any runtime reorderings will respect the order\n    given by the control dependencies.",
    "inputs": [
      { "name": "controlInputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" },
      { "name": "control", "type": "TFL_Control" }
    ]
  },
  {
    "name": "tfl.conv_2d",
    "summary": "operator",
    "description": "Performs convolution operation on inputs.\n\n    Inputs:\n      `inputs[0]`: required: the input activation tensor\n      `inputs[1]`: required: the filter weight tensor\n      `inputs[2]`: optional: the bias tensor",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "filter", "type": "TFL_TensorOf" },
      { "name": "bias", "type": "TFL_1DTensorOfOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "dilation_h_factor", "type": "I32Attr" },
      { "name": "dilation_w_factor", "type": "I32Attr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "padding", "type": "TFL_PaddingAttr" },
      { "name": "stride_h", "type": "I32Attr" },
      { "name": "stride_w", "type": "I32Attr" }
    ],
    "category": "Layer"
  },
  {
    "name": "tfl.conv_3d",
    "summary": "Convolution 3D operator",
    "description": "Performs convolution operation on 3D inputs.\n    Inputs:\n      `inputs[0]`: required: the input activation tensor\n      `inputs[1]`: required: the filter weight tensor\n      `inputs[2]`: optional: the bias tensor",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "filter", "type": "TFL_TensorOf" },
      { "name": "bias", "type": "TFL_TensorOfOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "dilation_d_factor", "type": "I32Attr" },
      { "name": "dilation_h_factor", "type": "I32Attr" },
      { "name": "dilation_w_factor", "type": "I32Attr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "padding", "type": "TFL_PaddingAttr" },
      { "name": "stride_d", "type": "I32Attr" },
      { "name": "stride_h", "type": "I32Attr" },
      { "name": "stride_w", "type": "I32Attr" }
    ]
  },
  {
    "name": "tfl.conv_3d_transpose",
    "summary": "Transposed Convolution 3D operator",
    "description": "Performs transposed convolution operation on 3D inputs.\n    Inputs:\n      `inputs[0]`: required: the shape of output tensor\n      `inputs[1]`: required: the filter weight tensor\n      `inputs[2]`: required: the input activation tensor\n      `inputs[3]`: optional: the bias tensor",
    "inputs": [
      { "name": "output_shape", "type": "TFL_I32Tensor" },
      { "name": "filter", "type": "TFL_TensorOf" },
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "bias", "type": "TFL_TensorOfOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "dilation_d_factor", "type": "I32Attr" },
      { "name": "dilation_h_factor", "type": "I32Attr" },
      { "name": "dilation_w_factor", "type": "I32Attr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "padding", "type": "TFL_PaddingAttr" },
      { "name": "stride_d", "type": "I32Attr" },
      { "name": "stride_h", "type": "I32Attr" },
      { "name": "stride_w", "type": "I32Attr" }
    ]
  },
  {
    "name": "tfl.cos",
    "summary": "Cosine operator",
    "description": "Computes element-wise Cosine of input",
    "inputs": [
      { "name": "x", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_FpTensor" }
    ]
  },
  {
    "name": "tfl.cumsum",
    "summary": "Cumsum operator",
    "description": "Compute the cumulative sum of the tensor x along axis.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "axis", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "exclusive", "type": "DefaultValuedOptionalAttr" },
      { "name": "reverse", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.custom",
    "summary": "Custom op",
    "description": "A generic op for any TFLite custom operation.\n\n    input: A list of inputs in the original op.\n    custom_code: A string used to identify which exactly this op is, which\n                 corresponds to operator_codes.custom_code in the flatbuffer.\n    custom_option: a holder to save the op attributes in bytes fashion.\n    output: A list of outputs in the original op.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "custom_code", "type": "StrAttr" },
      { "name": "custom_option", "type": "TFL_ConstBytesAttr" }
    ]
  },
  {
    "name": "tfl.custom_tf",
    "summary": "Wrapper Op for TF custom ops.",
    "description": "A wrapper op around any Custom TF op. These includes ops defined using\n    custom_opdefs or linked which are not defined in TF dialect.\n    This Op just wraps the custom op inside a region.\n    Note #1, this Op will not include TF Lite custom ops defined using CustomOp.\n    Note #2, this op is just internal representation inside the converter and\n    are not exposed/exported when the model is exported to Flatbuffer.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ]
  },
  {
    "name": "tfl.densify",
    "summary": "Densify operator",
    "description": "Converts sparse tensor to dense format.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.depth_to_space",
    "summary": "DepthToSpace operator",
    "description": "Rearranges data from depth into blocks of spatial data.\n    This is the reverse transformation of SpaceToDepth. More specifically,\n    this op outputs a copy of the input tensor where values from the `depth`\n    dimension are moved in spatial blocks to the `height` and `width`\n    dimensions. The attr `block_size` indicates the input block size and how\n    the data is moved.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "block_size", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tfl.depthwise_conv_2d",
    "summary": "operator",
    "description": "Performs convolution operation on inputs.\n\n    Inputs:\n      `inputs[0]`: required: the input activation tensor\n      `inputs[1]`: required: the filter weight tensor\n      `inputs[2]`: optional: the bias tensor",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "filter", "type": "TFL_TensorOf" },
      { "name": "bias", "type": "TFL_1DTensorOfOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "dilation_h_factor", "type": "I32Attr" },
      { "name": "dilation_w_factor", "type": "I32Attr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "padding", "type": "TFL_PaddingAttr" },
      { "name": "stride_h", "type": "I32Attr" },
      { "name": "stride_w", "type": "I32Attr" },
      { "name": "depth_multiplier", "type": "I32Attr" }
    ]
  },
  {
    "name": "tfl.dequantize",
    "summary": "Dequantize operator",
    "description": "Converts quantized array of integers to floating-points according to the\n    quantization parameters.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_FpTensor" }
    ]
  },
  {
    "name": "tfl.dilate",
    "summary": "Dilation operator",
    "description": "Extends a tensor by adding new elements between the existing ones.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "dilations", "type": "TFL_TensorOf" },
      { "name": "padding_value", "type": "TFL_0DTensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.div",
    "summary": "Division operator",
    "description": "Element-wise division operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.dynamic_update_slice",
    "summary": "DynamicUpdateSlice.",
    "description": "DynamicUpdateSlice op that have the same semantics with XLA\n    DynamicUpdateSlice.\n    Generates a result which is the value of the input array\n    operand, with a slice update overwritten at start_indices.\n\n    See https://www.tensorflow.org/xla/operation_semantics#dynamicupdateslice.",
    "inputs": [
      { "name": "operand", "type": "TFL_TensorOf" },
      { "name": "update", "type": "TFL_TensorOf" },
      { "name": "start_indices", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.elu",
    "summary": "Exponential Linear Unit operator",
    "description": "Computes the exponential linear\n      f(x) -> exp(x) - 1 for x < 0, x for x >= 0.\n    element-wise.",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.embedding_lookup",
    "summary": "Embedding lookup operator",
    "description": "Looks up ids in a list of embedding tensors.",
    "inputs": [
      { "name": "lookup", "type": "TFL_TensorOf" },
      { "name": "value", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.equal",
    "summary": "Equal operator",
    "description": "Returns the truth element of x == y element-wise",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" },
      { "name": "y", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.exp",
    "summary": "Natural exponentiation operator",
    "description": "Performs element-wise natural exponentiation operation on input.",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.expand_dims",
    "summary": "Inserts a dimension of 1 into a tensor's shape.",
    "description": "Given a tensor `input`, this operation inserts a dimension of 1 at the\ndimension index `axis` of `input`'s shape. The dimension index `axis` starts at\nzero; if you specify a negative number for `axis` it is counted backward from\nthe end.\n\nThis operation is useful if you want to add a batch dimension to a single\nelement. For example, if you have a single image of shape `[height, width,\nchannels]`, you can make it a batch of 1 image with `expand_dims(image, 0)`,\nwhich will make the shape `[1, height, width, channels]`.\n\nOther examples:\n\n```\n# 't' is a tensor of shape [2]\nshape(expand_dims(t, 0)) ==> [1, 2]\nshape(expand_dims(t, 1)) ==> [2, 1]\nshape(expand_dims(t, -1)) ==> [2, 1]\n\n# 't2' is a tensor of shape [2, 3, 5]\nshape(expand_dims(t2, 0)) ==> [1, 2, 3, 5]\nshape(expand_dims(t2, 2)) ==> [2, 3, 1, 5]\nshape(expand_dims(t2, 3)) ==> [2, 3, 5, 1]\n```\n\nThis operation requires that:\n\n`-1-input.dims() <= dim <= input.dims()`\n\nThis operation is related to `squeeze()`, which removes dimensions of\nsize 1.",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "dim", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "tfl.external_const",
    "summary": "External const op.",
    "description": "External const op holds a `buffer_index` which points to a constant\n    in the flatbuffer.",
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "buffer_index", "type": "I32Attr" }
    ]
  },
  {
    "name": "tfl.fake_quant",
    "summary": "FakeQuant operator",
    "description": "Fake-quantize the 'inputs' tensor of type float via float scalars min and\n    max to 'outputs' tensor of same shape as inputs.",
    "inputs": [
      { "name": "input", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_FpTensor" }
    ],
    "attributes": [
      { "name": "min", "type": "F32Attr" },
      { "name": "max", "type": "F32Attr" },
      { "name": "num_bits", "type": "ConfinedAttr" },
      { "name": "narrow_range", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tfl.fill",
    "summary": "Fill the tensor with given value.",
    "description": "Fill the tensor with given value.",
    "inputs": [
      { "name": "dims", "type": "TFL_I32OrI64Tensor" },
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.floor",
    "summary": "Floor operator",
    "description": "Returns element-wise floor value of the input.",
    "inputs": [
      { "name": "x", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_FpTensor" }
    ]
  },
  {
    "name": "tfl.floor_div",
    "summary": "Floor div operator",
    "description": "Element-wise floor div operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.floor_mod",
    "summary": "Division reminder",
    "description": "Element-wise division reminder operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.fully_connected",
    "summary": "Fully connected op",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "filter", "type": "TFL_TensorOf" },
      { "name": "bias", "type": "TFL_TensorOfOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_VariadicTensorOf" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "weights_format", "type": "TFL_FullyConnectedOptionsWeightFormatAttr" },
      { "name": "keep_num_dims", "type": "BoolAttr" },
      { "name": "asymmetric_quantize_inputs", "type": "OptionalAttr" }
    ],
    "category": "Layer"
  },
  {
    "name": "tfl.gather",
    "summary": "Gather operator",
    "description": "Gather slices from `params` axis `axis` according to `indices`.",
    "inputs": [
      { "name": "params", "type": "TFL_TensorOf" },
      { "name": "indices", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" },
      { "name": "batch_dims", "type": "DefaultValuedOptionalAttr" }
    ],
    "category": "Tensor"
  },
  {
    "name": "tfl.gather_nd",
    "summary": "Gather_nd operator",
    "description": "Gather slices from `params` into a Tensor with shape specified by `indices`.",
    "inputs": [
      { "name": "params", "type": "TFL_TensorOf" },
      { "name": "indices", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.gelu",
    "summary": "GELU activation function.",
    "description": "Computes GELU activation function element-wise.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "approximate", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.greater",
    "summary": "Greater operator",
    "description": "Element-wise greater operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.greater_equal",
    "summary": "Greater_equal operator",
    "description": "Element-wise greater_equal operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.hard_swish",
    "summary": "Hardswish activation function.",
    "description": "Computes hard-swish activation function\n      f(x) -> (x * relu6(x+3))/6\n    element-wise.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.hashtable",
    "summary": "Creates a non-initialized hash table.",
    "description": "This op creates a hash table, specifying the type of its keys and values.\nBefore using the table you will have to initialize it.  After initialization the\ntable will be immutable.",
    "outputs": [
      { "name": "out", "type": "TFL_ResourceTensor" }
    ],
    "attributes": [
      { "name": "table_id", "type": "I32Attr" },
      { "name": "key_dtype", "type": "TypeAttr" },
      { "name": "value_dtype", "type": "TypeAttr" }
    ]
  },
  {
    "name": "tfl.hashtable_find",
    "summary": "Looks up keys in a table, outputs the corresponding values.",
    "description": "The tensor `keys` must of the same type as the keys of the table.\nThe output `values` is of the type of the table values.\n\nThe scalar `default_value` is the value output for keys not present in the\ntable. It must also be of the same type as the table values.",
    "inputs": [
      { "name": "hash_table", "type": "TFL_ResourceTensor" },
      { "name": "keys", "type": "TFL_TensorOf" },
      { "name": "default_value", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "out", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.hashtable_import",
    "summary": "Replaces the contents of the table with the specified keys and values.",
    "description": "The tensor `keys` must be of the same type as the keys of the table.\nThe tensor `values` must be of the type of the table values.",
    "inputs": [
      { "name": "hash_table", "type": "TFL_ResourceTensor" },
      { "name": "keys", "type": "TFL_TensorOf" },
      { "name": "values", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.hashtable_size",
    "summary": "Computes the number of elements in the given table.",
    "inputs": [
      { "name": "hash_table", "type": "TFL_ResourceTensor" }
    ],
    "outputs": [
      { "name": "out", "type": "TFL_I64Tensor" }
    ]
  },
  {
    "name": "tfl.if",
    "summary": "if-then-else operation",
    "description": "The `tfl.if` operation represents an if-then-else construct for\n    conditionally executing two regions of code. The operand to an if operation\n    is a boolean value. For example:\n\n    ```mlir\n    tfl.if %b  {\n      ...\n    } else {\n      ...\n    }\n    ```\n\n    `tfl.if` may also return results that are defined in its regions. The\n    values defined are determined by which execution path is taken.\n\n    Example:\n\n    ```mlir\n    %x, %y = tfl.if %b -> (tensor<f32>, tensor<f32>) {\n      %x_true = ...\n      %y_true = ...\n      tfl.yield %x_true, %y_true : tensor<f32>, tensor<f32>\n    } else {\n      %x_false = ...\n      %y_false = ...\n      tfl.yield %x_false, %y_false : tensor<f32>, tensor<f32>\n    }\n    ```\n\n    `tfl.if` regions are always terminated with \"tfl.yield\". If \"tfl.if\"\n    defines no values, the \"tfl.yield\" can be left out, and will be inserted\n    implicitly. Otherwise, it must be explicit.\n    Also, if \"tfl.if\" defines one or more values, the 'else' block cannot be\n    omitted.\n\n    Example:\n\n    ```mlir\n    tfl.if %b  {\n      ...\n    }\n    ```",
    "inputs": [
      { "name": "cond", "type": "TFL_BoolTensor" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "tfl.imag",
    "summary": "Returns the imaginary part of a complex number.",
    "description": "Given a tensor `input` of complex numbers, this operation returns a tensor of\ntype `float` that is the imaginary part of each element in `input`. All\nelements in `input` must be complex numbers of the form \\\\(a + bj\\\\), where *a*\nis the real part and *b* is the imaginary part returned by this operation.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.l2_normalization",
    "summary": "L2 Normalize Operator",
    "description": "L2Normalization Op",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.leaky_relu",
    "summary": "Leaky Relu operator",
    "description": "Element-wise Leaky ReLU operator\n      x -> x >= 0 ? x : (alpha * x)",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "alpha", "type": "F32Attr" }
    ]
  },
  {
    "name": "tfl.less",
    "summary": "Less operator",
    "description": "Element-wise less operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.less_equal",
    "summary": "Less_equal operator",
    "description": "Element-wise less_equal operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.local_response_normalization",
    "summary": "Local Response Normalization.",
    "description": "The 4-D `input` tensor is treated as a 3-D array of 1-D vectors (along the last\ndimension), and each vector is normalized independently.  Within a given vector,\neach component is divided by the weighted, squared sum of inputs within\n`depth_radius`.  In detail,\n\n    sqr_sum[a, b, c, d] =\n        sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)\n    output = input / (bias + alpha * sqr_sum) ** beta\n\nFor details, see [Krizhevsky et al., ImageNet classification with deep\nconvolutional neural networks (NIPS 2012)](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks).",
    "inputs": [
      { "name": "input", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_FpTensor" }
    ],
    "attributes": [
      { "name": "radius", "type": "I32Attr" },
      { "name": "bias", "type": "F32Attr" },
      { "name": "alpha", "type": "F32Attr" },
      { "name": "beta", "type": "F32Attr" }
    ]
  },
  {
    "name": "tfl.log",
    "summary": "Natural logarithm operator",
    "description": "Performs element-wise natural logarithm operation on input.",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.log_softmax",
    "summary": "Log softmax operator",
    "description": "Computes element-wise log softmax activations with the following formula\n\n      input - log(reduce_sum(exp(input), dim))",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.logical_and",
    "summary": "Logical AND operator",
    "description": "Element-wise logical AND operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_BoolTensor" },
      { "name": "rhs", "type": "TFL_BoolTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.logical_not",
    "summary": "Logical NOT operator",
    "description": "Element-wise logical NOT operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_BoolTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.logical_or",
    "summary": "Logical OR operator",
    "description": "Element-wise logical OR operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_BoolTensor" },
      { "name": "rhs", "type": "TFL_BoolTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.logistic",
    "summary": "Logistic operator",
    "description": "Computes element-wise Sigmoid of input",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.lstm",
    "summary": "The full lstm operator",
    "description": "Long short-term memory unit (LSTM) recurrent network layer.\nThe default non-peephole implementation is based on:\nhttp://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\nS. Hochreiter and J. Schmidhuber. 'Long Short-Term Memory'. Neural Computation,\n9(8):1735-1780, 1997.\nThe peephole implementation is based on:\nhttps://research.google.com/pubs/archive/43905.pdf\nHasim Sak, Andrew Senior, and Francoise Beaufays. 'Long short-term memory\nrecurrent neural network architectures for large scale acoustic modeling.'\nINTERSPEECH, 2014.\nThe coupling of input and forget gate (CIFG) is based on:\nhttp://arxiv.org/pdf/1503.04069.pdf\nGreff et al. 'LSTM: A Search Space Odyssey'\nThe layer normalization is based on:\nhttps://arxiv.org/pdf/1607.06450.pdf\nBa et al. 'Layer Normalization'",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "input_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "input_to_forget_weights", "type": "TFL_TensorOf" },
      { "name": "input_to_cell_weights", "type": "TFL_TensorOf" },
      { "name": "input_to_output_weights", "type": "TFL_TensorOf" },
      { "name": "recurrent_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "recurrent_to_forget_weights", "type": "TFL_TensorOf" },
      { "name": "recurrent_to_cell_weights", "type": "TFL_TensorOf" },
      { "name": "recurrent_to_output_weights", "type": "TFL_TensorOf" },
      { "name": "cell_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "cell_to_forget_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "cell_to_output_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "input_gate_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "forget_gate_bias", "type": "TFL_TensorOf" },
      { "name": "cell_bias", "type": "TFL_TensorOf" },
      { "name": "output_gate_bias", "type": "TFL_TensorOf" },
      { "name": "projection_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "projection_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "input_activation_state", "type": "TFL_StatefulTensor" },
      { "name": "input_cell_state", "type": "TFL_StatefulTensor" },
      { "name": "input_layer_norm_coefficients", "type": "TFL_TensorOfOrNone" },
      { "name": "forget_layer_norm_coefficients", "type": "TFL_TensorOfOrNone" },
      { "name": "cell_layer_norm_coefficients", "type": "TFL_TensorOfOrNone" },
      { "name": "output_layer_norm_coefficients", "type": "TFL_TensorOfOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "cell_clip", "type": "ConfinedAttr" },
      { "name": "proj_clip", "type": "ConfinedAttr" },
      { "name": "kernel_type", "type": "ConfinedAttr" },
      { "name": "asymmetric_quantize_inputs", "type": "OptionalAttr" },
      { "name": "input_to_input_intermediate", "type": "OptionalAttr" },
      { "name": "input_to_forget_intermediate", "type": "OptionalAttr" },
      { "name": "input_to_cell_intermediate", "type": "OptionalAttr" },
      { "name": "input_to_output_intermediate", "type": "OptionalAttr" },
      { "name": "effective_hidden_scale_intermediate", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tfl.matrix_diag",
    "summary": "Returns a tensor with the provided diagonal and everything else padded with zeros.",
    "description": "Given a diagonal, returns a tensor with the diagonal and everything else padded with zeros.\n    Assume diagonal has k dimensions `[I, J, K, ..., N]`, then the output is a tensor of rank `k+1`\n    with dimensions `[I, J, K, ..., N, N]` where:\n       `output[i, j, k, ..., m, n] = 1{m=n} * diagonal[i, j, k, ..., n].`",
    "inputs": [
      { "name": "diagonal", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.matrix_set_diag",
    "summary": "Returns a batched matrix tensor with new batched diagonal values.",
    "description": "Given `input` and `diagonal`, this operation returns a tensor with the\nsame shape and values as `input`, except for the main diagonal of the\ninnermost matrices.  These will be overwritten by the values in `diagonal`.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "diagonal", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.max_pool_2d",
    "summary": "Max Pool 2D op",
    "description": "Performs max pool 2D on input.\n\n    Inputs:\n      `inputs[0]`: required: the input tensor",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "padding", "type": "TFL_PaddingAttr" },
      { "name": "stride_w", "type": "I32Attr" },
      { "name": "stride_h", "type": "I32Attr" },
      { "name": "filter_width", "type": "I32Attr" },
      { "name": "filter_height", "type": "I32Attr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.maximum",
    "summary": "Max operator",
    "description": "Element-wise max operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "max", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.mean",
    "summary": "Mean operator",
    "description": "Computes the mean of elements across dimensions of a tensor.\n    Reduces input_tensor along the dimensions given in axis.\n    Unless keepdims is true, the rank of the tensor is reduced by 1 for\n    each entry in axis. If keepdims is true, the reduced dimensions are retained\n    with length 1.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "axis", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tfl.minimum",
    "summary": "Min operator",
    "description": "Element-wise min operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "min", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.mirror_pad",
    "summary": "MirrorPad Operator. Pads a tensor with mirrored values.",
    "description": "This operation pads a input with mirrored values according to the paddings\n    you specify. paddings is an integer tensor with shape [n, 2],\n    where n is the rank of input.\n    For each dimension D of input, paddings[D, 0] indicates how many values\n    to add before the contents of input in that dimension,\n    and paddings[D, 1] indicates how many values to add after the contents of\n    input in that dimension.\n\n    Both paddings[D, 0] and paddings[D, 1] must be no greater than\n    input.dim_size(D) (or input.dim_size(D) - 1)\n    if copy_border is true (if false, respectively).\n\n    The padded size of each dimension D of the output is:\n\n    paddings(D, 0) + input.dim_size(D) + paddings(D, 1)",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "pad", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "mode", "type": "TFL_MirrorPaddingAttr" }
    ]
  },
  {
    "name": "tfl.mul",
    "summary": "Multiplication operator",
    "description": "Element-wise multiplication operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.multinomial",
    "summary": "Draws samples from a categorical distribution.",
    "description": "The generated values will have a categorical distribution based on the `logits`\nor unnormalized log-probabilities provided for all classes.",
    "inputs": [
      { "name": "logits", "type": "TFL_FpTensor" },
      { "name": "num_samples", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "out", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.neg",
    "summary": "Negation operator",
    "description": "Computes element-wise negation of input",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.no_value",
    "summary": "constant representing no value.",
    "description": "No value constant op.",
    "outputs": [
      { "name": "none_val", "type": "NoneType" }
    ],
    "attributes": [
      { "name": "value", "type": "UnitAttr" }
    ]
  },
  {
    "name": "tfl.non_max_suppression_v4",
    "summary": "Greedily selects a subset of bounding boxes in descending order of score,",
    "description": "pruning away boxes that have high intersection-over-union (IOU) overlap\nwith previously selected boxes.  Bounding boxes with score less than\n`score_threshold` are removed.  Bounding boxes are supplied as\n[y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any\ndiagonal pair of box corners and the coordinates can be provided as normalized\n(i.e., lying in the interval [0, 1]) or absolute.  Note that this algorithm\nis agnostic to where the origin is in the coordinate system and more\ngenerally is invariant to orthogonal transformations and translations\nof the coordinate system; thus translating or reflections of the coordinate\nsystem result in the same boxes being selected by the algorithm.\nThe output of this operation is a set of integers indexing into the input\ncollection of bounding boxes representing the selected boxes.  The bounding\nbox coordinates corresponding to the selected indices can then be obtained\nusing the `tf.gather operation`.  For example:\n  selected_indices = tf.image.non_max_suppression_v2(\n      boxes, scores, max_output_size, iou_threshold, score_threshold)\n  selected_boxes = tf.gather(boxes, selected_indices)",
    "inputs": [
      { "name": "boxes", "type": "TFL_FpTensor" },
      { "name": "scores", "type": "TFL_FpTensor" },
      { "name": "max_output_size", "type": "TFL_I32Tensor" },
      { "name": "iou_threshold", "type": "TFL_FpTensor" },
      { "name": "score_threshold", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "selected_indices", "type": "TFL_I32Tensor" },
      { "name": "valid_outputs", "type": "TFL_I32Tensor" }
    ]
  },
  {
    "name": "tfl.non_max_suppression_v5",
    "summary": "Greedily selects a subset of bounding boxes in descending order of score,",
    "description": "pruning away boxes that have high intersection-over-union (IOU) overlap\nwith previously selected boxes.  Bounding boxes with score less than\n`score_threshold` are removed.  Bounding boxes are supplied as\n[y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any\ndiagonal pair of box corners and the coordinates can be provided as normalized\n(i.e., lying in the interval [0, 1]) or absolute.  Note that this algorithm\nis agnostic to where the origin is in the coordinate system and more\ngenerally is invariant to orthogonal transformations and translations\nof the coordinate system; thus translating or reflections of the coordinate\nsystem result in the same boxes being selected by the algorithm.\nThe output of this operation is a set of integers indexing into the input\ncollection of bounding boxes representing the selected boxes.  The bounding\nbox coordinates corresponding to the selected indices can then be obtained\nusing the `tf.gather operation`.  For example:\n  selected_indices = tf.image.non_max_suppression_v2(\n      boxes, scores, max_output_size, iou_threshold, score_threshold)\n  selected_boxes = tf.gather(boxes, selected_indices)\nThis op also supports a Soft-NMS (with Gaussian weighting) mode (c.f.\nBodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score\nof other overlapping boxes instead of directly causing them to be pruned.\nTo enable this Soft-NMS mode, set the `soft_nms_sigma` parameter to be\nlarger than 0.",
    "inputs": [
      { "name": "boxes", "type": "TFL_FpTensor" },
      { "name": "scores", "type": "TFL_FpTensor" },
      { "name": "max_output_size", "type": "TFL_I32Tensor" },
      { "name": "iou_threshold", "type": "TFL_FpTensor" },
      { "name": "score_threshold", "type": "TFL_FpTensor" },
      { "name": "soft_nms_sigma", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "selected_indices", "type": "TFL_I32Tensor" },
      { "name": "selected_scores", "type": "TFL_FpTensor" },
      { "name": "valid_outputs", "type": "TFL_I32Tensor" }
    ]
  },
  {
    "name": "tfl.not_equal",
    "summary": "Not_equal operator",
    "description": "Element-wise not_equal operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ]
  },
  {
    "name": "tfl.NumericVerify",
    "summary": "Verifies the numericals of the two operands",
    "description": "The NumericVerify op is a debugging op to verify the numericals of the two\n    activations. It is a custom op in TFLite.\n    If log_if_failed is true, the NumericVerify op calculates statistics on\n    differences between float and quantized activations, output\n    logs, set differences to the output tensors, and throws an error if errors\n    above tolerance exist. If log_if_failed = false, then it doesn't care about\n    errors.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "ref", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_FpTensor" }
    ],
    "attributes": [
      { "name": "tolerance", "type": "DefaultValuedOptionalAttr" },
      { "name": "log_if_failed", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.one_hot",
    "summary": "OneHot operator",
    "description": "Returns a one-hot tensor.The locations represented by indices in `indices`\n    take value `on_value`, while all other locations take value `off_value`.\n\n    If the input `indices` is rank `N`, the output will have rank `N+1`,\n    The new axis is created at dimension `axis` (default: the new axis is\n    appended at the end).",
    "inputs": [
      { "name": "indices", "type": "TFL_TensorOf" },
      { "name": "depth", "type": "TFL_I32Tensor" },
      { "name": "on_value", "type": "TFL_TensorOf" },
      { "name": "off_value", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" }
    ]
  },
  {
    "name": "tfl.pack",
    "summary": "Packs a list of tensors along a dimension into one tensor",
    "description": "Packs a list of `values_count` rank-`R` tensors into one rank-`(R+1)`\n    tensor.\n\n    Packs the `values_count` tensors in `values` into a tensor with rank one\n    higher than each tensor in `values`, by packing them along the `axis`\n    dimension.\n\n    Given a list of tensors of shape `(A, B, C)`;\n\n    if `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`.\n    if `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`.\n    Etc.\n\n    For example:\n\n    ```\n    # 'x' is [1, 4]\n    # 'y' is [2, 5]\n    # 'z' is [3, 6]\n    pack([x, y, z]) => [[1, 4], [2, 5], [3, 6]]  # Pack along first dim.\n    pack([x, y, z], axis=1) => [[1, 2, 3], [4, 5, 6]]\n    ```\n\n    This is the opposite of `unpack`.",
    "inputs": [
      { "name": "values", "type": "TFL_VariadicTensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "values_count", "type": "ConfinedAttr" },
      { "name": "axis", "type": "I32Attr" }
    ]
  },
  {
    "name": "tfl.pad",
    "summary": "Padding operator",
    "description": "This operation pads a `input` with zeros according to the `paddings` you\n    specify. `paddings` is an integer tensor with shape `[Dn, 2]`, where n is\n    the rank of `input`. For each dimension D of `input`, `paddings[D, 0]`\n    indicates how many zeros to add before the contents of `input` in that\n    dimension, and `paddings[D, 1]` indicates how many zeros to add after the\n    contents of `input` in that dimension.\n\n    The padded size of each dimension D of the output is:\n\n      `paddings(D, 0) + input.dim_size(D) + paddings(D, 1)`\n\n    For example:\n\n    ```\n    # 't' is [[1, 1], [2, 2]]\n    # 'paddings' is [[1, 1], [2, 2]]\n    # rank of 't' is 2\n    pad(t, paddings) ==> [[0, 0, 0, 0, 0, 0]\n                          [0, 0, 1, 1, 0, 0]\n                          [0, 0, 2, 2, 0, 0]\n                          [0, 0, 0, 0, 0, 0]]\n    ```",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "padding", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "category": "Transform"
  },
  {
    "name": "tfl.padv2",
    "summary": "Padding operator v2",
    "description": "This operation pads a `input` according to the `paddings` and\n    `constant_values` you specify. `paddings` is an integer tensor with shape\n    `[Dn, 2]`, where n is the rank of `input`. For each dimension D of `input`,\n    `paddings[D, 0]` indicates how many zeros to add before the contents of\n    `input` in that dimension, and `paddings[D, 1]` indicates how many zeros to\n    add after the contents of `input` in that dimension. `constant_values` is a\n    scalar tensor of the same type as `input` that indicates the value to use\n    for padding `input`.\n\n    The padded size of each dimension D of the output is:\n\n      `paddings(D, 0) + input.dim_size(D) + paddings(D, 1)`\n\n    For example:\n\n    ```\n    # 't' is [[1, 1], [2, 2]]\n    # 'paddings' is [[1, 1], [2, 2]]\n    # rank of 't' is 2\n    pad(t, paddings) ==> [[0, 0, 0, 0, 0, 0]\n                          [0, 0, 1, 1, 0, 0]\n                          [0, 0, 2, 2, 0, 0]\n                          [0, 0, 0, 0, 0, 0]]\n    ```",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "padding", "type": "TFL_I32OrI64Tensor" },
      { "name": "constant_values", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.poly_call",
    "summary": "Poly call",
    "description": "Have multiple function bodies for the same computation. This allows a\n    program compiler/interpreter to choose one of the available options to\n    execute the program based on which one is most suitable for the target\n    backend.\n\n    input:  A list of input tensors whose types are T.\n    output: A list of output tensors whose types are T.\n\n    call:  Multiple regions, each of which encapsulates the same semantic\n           computation but in different forms.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ]
  },
  {
    "name": "tfl.pow",
    "summary": "Power operator",
    "description": "Element-wise power operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.prelu",
    "summary": "Parameterized Relu operator",
    "description": "Parameterized Relu operator\n      x -> x >= 0 ? x : (alpha * x)\n    where alpha is a trainable tensor.\n    input and alpha should be the same size as input or be broadcastable.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "alpha", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.pseudo_const",
    "summary": "Constant pseudo op.",
    "description": "Represents a constant value in TensorFlow Lite dialect. This is not an\n    actual operation and it will be lowered to buffer instead.\n\n    The op is allowed to have all the same type of attributes as tf.Const does\n    (e.g., opaque TF attributes are allowed).",
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "ElementsAttr" }
    ]
  },
  {
    "name": "tfl.pseudo_qconst",
    "summary": "Quantized constant pseudo op",
    "description": "Represents a quantized constant value in TensorFlow Lite dialect. This is\n    not an actual operation and it will be lowered to buffer instead. The\n    quantization parameters are stored as a type attribute in this constant.",
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "qtype", "type": "TensorTypeAttr" },
      { "name": "value", "type": "ElementsAttr" }
    ]
  },
  {
    "name": "tfl.pseudo_sparse_const",
    "summary": "Sparse constant pseudo op.",
    "description": "Represents a sparse constant value in TensorFlow Lite dialect. This is not\n    an actual operation and it will be lowered to buffer instead.",
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "ElementsAttr" },
      { "name": "s_param", "type": "SparsityParameterAttr" },
      { "name": "compressed_data", "type": "ElementsAttr" }
    ]
  },
  {
    "name": "tfl.pseudo_sparse_qconst",
    "summary": "Sparse quantized constant pseudo op",
    "description": "Represents a sparse quantized constant value in TensorFlow Lite dialect.\n    This is not an actual operation and it will be lowered to buffer instead.\n    The quantization parameters are stored as a type attribute in this constant.",
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "qtype", "type": "TensorTypeAttr" },
      { "name": "value", "type": "ElementsAttr" },
      { "name": "s_param", "type": "SparsityParameterAttr" },
      { "name": "compressed_data", "type": "ElementsAttr" }
    ]
  },
  {
    "name": "tfl.quantize",
    "summary": "Quantize operator",
    "description": "Converts floating point tensors to quantized integer tensors according to\n    the quantization parameters defined in the type attribute.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "qtype", "type": "TensorTypeAttr" }
    ]
  },
  {
    "name": "tfl.random_standard_normal",
    "summary": "Outputs random values from a normal distribution.",
    "description": "The generated values will have mean 0 and standard deviation 1.",
    "inputs": [
      { "name": "shape", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "out", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.random_uniform",
    "summary": "Outputs random values from a uniform distribution.",
    "description": "The generated values follow a uniform distribution in the range `[0, 1)`. The\nlower bound 0 is included in the range, while the upper bound 1 is excluded.",
    "inputs": [
      { "name": "shape", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "out", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "seed", "type": "DefaultValuedOptionalAttr" },
      { "name": "seed2", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.range",
    "summary": "Range operator",
    "description": "Returns a 1D tensor defined by a sequence from `start` to `limit` with\n    a given `delta`.",
    "inputs": [
      { "name": "start", "type": "TFL_TensorOf" },
      { "name": "limit", "type": "TFL_TensorOf" },
      { "name": "delta", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.rank",
    "summary": "Rank operator.",
    "description": "Returns the rank of a tensor.",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_IntTensor" }
    ]
  },
  {
    "name": "tfl.read_variable",
    "summary": "Reads variable value.",
    "description": "Read variable data identified by 'resource_id'.",
    "inputs": [
      { "name": "resource_id", "type": "TFL_ResourceTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.real",
    "summary": "Returns the real part of a complex number.",
    "description": "Given a tensor `input` of complex numbers, this operation returns a tensor of\ntype `float` that is the real part of each element in `input`. All elements in\n`input` must be complex numbers of the form \\\\(a + bj\\\\), where *a* is the real\n part returned by this operation and *b* is the imaginary part.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.reduce_all",
    "summary": "Computes the \"logical and\" of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "TFL_BoolTensor" },
      { "name": "reduction_indices", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.reduce_any",
    "summary": "Computes the \"logical or\" of elements across dimensions of a tensor.",
    "description": "Reduces `input` along the dimensions given in `axis`. Unless\n`keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n`axis`. If `keep_dims` is true, the reduced dimensions are\nretained with length 1.",
    "inputs": [
      { "name": "input", "type": "TFL_BoolTensor" },
      { "name": "reduction_indices", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_BoolTensor" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.reduce_max",
    "summary": "Max-reduction operator",
    "description": "Computes the max reduction along the specified axes",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "axes", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tfl.reduce_min",
    "summary": "Min-reduction operator",
    "description": "Computes the min reduction along the specified axes",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "axes", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tfl.reduce_prod",
    "summary": "Prod-reduction operator",
    "description": "Computes the product along the specified axes",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "axes", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tfl.relu",
    "summary": "Relu operator",
    "description": "Element-wise Relu operator\n      x -> max(0, x)",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ],
    "category": "Activation"
  },
  {
    "name": "tfl.relu_0_to_1",
    "summary": "Relu0To1 operator",
    "description": "Element-wise Relu0To1 operator\n      x -> max(0, min(1, x))",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.relu_n1_to_1",
    "summary": "Relu1 operator",
    "description": "Element-wise Relu1 operator\n      x -> max(-1, min(1, x))",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.relu6",
    "summary": "Relu6 operator",
    "description": "Element-wise Relu6 operator\n      x -> max(0, min(6, x))",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.reshape",
    "summary": "Reshape operator",
    "description": "Produces a tensor with the same values but different static shape defined\n    by the output type.",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "shape", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "category": "Shape"
  },
  {
    "name": "tfl.resize_bilinear",
    "summary": "ResizeBilinear Op",
    "description": "Resize `images` to `size` using bilinear interpolation.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "size", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "align_corners", "type": "BoolAttr" },
      { "name": "half_pixel_centers", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.resize_nearest_neighbor",
    "summary": "ResizeNearestNeighbor Op",
    "description": "Resize `images` to `size` using nearest neighbor interpolation.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "size", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "align_corners", "type": "BoolAttr" },
      { "name": "half_pixel_centers", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.reverse_sequence",
    "summary": "Reverses variable length slices.",
    "description": "This op first slices `input` along the dimension `batch_dim`, and for each\nslice `i`, reverses the first `seq_lengths[i]` elements along\nthe dimension `seq_dim`.\n\nThe elements of `seq_lengths` must obey `seq_lengths[i] <= input.dims[seq_dim]`,\nand `seq_lengths` must be a vector of length `input.dims[batch_dim]`.\n\nThe output slice `i` along dimension `batch_dim` is then given by input\nslice `i`, with the first `seq_lengths[i]` slices along dimension\n`seq_dim` reversed.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "seq_lengths", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "seq_dim", "type": "ConfinedAttr" },
      { "name": "batch_dim", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tfl.reverse_v2",
    "summary": "ReverseV2 Operator",
    "description": "Reverses specific dimensions of a tensor.\n\n    Given a tensor, and a int32/int64 tensor axis representing the set\n    of dimensions of tensor to reverse.\n    This operation reverses each dimension i for\n    which there exists j s.t. axis[j] == i.\n\n    Args:\n      tensor: A Tensor. Must be one of the following types:\n      uint8, int8, int16, int32, int64, float32, bool Up to 8-D.\n\n      axis: A Tensor. Must be one of the following types: int32, int64.\n      with only 1 element which is the axis index.\n      TODO: Add support for multiple elements.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "axis", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.rfft2d",
    "summary": "2D real-valued fast Fourier transform.",
    "description": "Computes the 2-dimensional discrete Fourier transform of a real-valued signal\nover the inner-most 2 dimensions of `input`.\n\nSince the DFT of a real signal is Hermitian-symmetric, `RFFT2D` only returns the\n`fft_length / 2 + 1` unique components of the FFT for the inner-most dimension\nof `output`: the zero-frequency term, followed by the `fft_length / 2`\npositive-frequency terms.\n\nAlong each axis `RFFT2D` is computed on, if `fft_length` is smaller than the\ncorresponding dimension of `input`, the dimension is cropped. If it is larger,\nthe dimension is padded with zeros.",
    "inputs": [
      { "name": "input", "type": "TFL_FpTensor" },
      { "name": "fft_length", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_Complex64Tensor" }
    ]
  },
  {
    "name": "tfl.right_shift",
    "summary": "Right Shift operator",
    "description": "Elementwise computes the bitwise right-shift of `lhs` by `rhs`.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.round",
    "summary": "Round operator",
    "description": "Rounds the values of a tensor to the nearest integer, element-wise.",
    "inputs": [
      { "name": "x", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_FpTensor" }
    ]
  },
  {
    "name": "tfl.rsqrt",
    "summary": "Reciprocal of square root operator",
    "description": "Computes element-wise reverse square root of input",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.scatter_nd",
    "summary": "Scatter_nd operator",
    "description": "Scatter `updates` into a new tensor according to `indices`",
    "inputs": [
      { "name": "indices", "type": "TFL_TensorOf" },
      { "name": "updates", "type": "TFL_TensorOf" },
      { "name": "shape", "type": "TFL_1DTensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.segment_sum",
    "summary": "SegmentSum operator",
    "description": "Computes the sum along segments of a tensor.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "segment_ids", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.select",
    "summary": "Select operator",
    "description": "Select values of 'x' if the corresponding value of 'condition' is true or\n    the value of 'y' if false. There are valid condition input sizes:\n\n    1. Either the same shape (in which case the select is elementwise), or\n    2. condition must be Rank 1 and match over the first dimension.",
    "inputs": [
      { "name": "condition", "type": "TFL_BoolTensor" },
      { "name": "x", "type": "TFL_TensorOf" },
      { "name": "y", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.select_v2",
    "summary": "SelectV2 operator",
    "description": "Select values of 'x' if the corresponding value of 'condition' is true or\n    the value of 'y' if false. There are valid condition input sizes:\n\n    1. Either the same shape (in which case the select is elementwise), or\n    2. Broadcastable shapes between 'condition', 'x' and 'y'.",
    "inputs": [
      { "name": "condition", "type": "TFL_BoolTensor" },
      { "name": "x", "type": "TFL_TensorOf" },
      { "name": "y", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.shape",
    "summary": "Shape operator",
    "description": "Returns the shape of a tensor.",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.sign",
    "summary": "Sign operation",
    "description": "Returns NaN if x is NaN, 0 if x is 0, -1 if x < 0 and 1 if x > 0.",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.sin",
    "summary": "Sine operator",
    "description": "Computes element-wise Sine of input",
    "inputs": [
      { "name": "x", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_FpTensor" }
    ]
  },
  {
    "name": "tfl.slice",
    "summary": "Return a slice from 'input'.",
    "description": "The output tensor is a tensor with dimensions described by 'size'\nwhose values are extracted from 'input' starting at the offsets in\n'begin'.\n\n`begin` is zero-based; `size` is one-based. If size[i] is -1, all remaining\nelements in dimension i are included in the slice. In other words, this is\nequivalent to setting:\n  size[i] = input.dim_size(i) - begin[i]\n\n*Requirements*:\n  0 <= begin[i] <= begin[i] + size[i] <= Di  for i in [0, n)",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "begin", "type": "TFL_I32OrI64Tensor" },
      { "name": "size", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "category": "Tensor"
  },
  {
    "name": "tfl.softmax",
    "summary": "Softmax operator",
    "description": "Computes element-wise softmax activations with the following formula\n\n      exp(input * beta) / tf.reduce_sum(exp(input * beta), dim)",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "beta", "type": "F32Attr" }
    ],
    "category": "Activation"
  },
  {
    "name": "tfl.space_to_batch_nd",
    "summary": "SpaceToBatchNd operator",
    "description": "This operation reshapes space dimensions into the \"batch\" dimension 0",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "block_shape", "type": "TFL_I32Tensor" },
      { "name": "paddings", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.space_to_depth",
    "summary": "SpaceToDepth operator",
    "description": "Rearranges blocks of spatial data, into depth. More specifically,\n    this op outputs a copy of the input tensor where values from the `height`\n    and `width` dimensions are moved to the `depth` dimension.\n    `block_size` indicates the input block size.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "block_size", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tfl.sparse_to_dense",
    "summary": "Converts a sparse representation into a dense tensor.",
    "description": "Builds an array `dense` with shape `output_shape` such that\n\n```\n# If sparse_indices is scalar\ndense[i] = (i == sparse_indices ? sparse_values : default_value)\n\n# If sparse_indices is a vector, then for each i\ndense[sparse_indices[i]] = sparse_values[i]\n\n# If sparse_indices is an n by d matrix, then for each i in [0, n)\ndense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]\n```\n\nAll other values in `dense` are set to `default_value`.  If `sparse_values` is a\nscalar, all sparse indices are set to this single value.\n\nIndices should be sorted in lexicographic order, and indices must not\ncontain any repeats. If `validate_indices` is true, these properties\nare checked during execution.",
    "inputs": [
      { "name": "sparse_indices", "type": "TFL_I32OrI64Tensor" },
      { "name": "output_shape", "type": "TFL_I32OrI64Tensor" },
      { "name": "sparse_values", "type": "TFL_TensorOf" },
      { "name": "default_value", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "dense", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.split",
    "summary": "Splits a tensor into `num_split` tensors along one dimension.",
    "description": "Splits the `value` tensor along `split_dim` into a number of sub-tensors\n    with same shape as the original one, except for `split_dim`. Same as\n    tf.Split.",
    "inputs": [
      { "name": "split_dim", "type": "TFL_TensorOf" },
      { "name": "value", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "outputs", "type": "TFL_VariadicTensorOf" }
    ],
    "attributes": [
      { "name": "num_splits", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tfl.split_v",
    "summary": "Splits a tensor into `num_split` tensors along one dimension.",
    "description": "Splits the `value` tensor along `split_dim` into a number of sub-tensors\n    with same shape as the original one, except for `split_dim`. The grouping\n    of the resultant sub-tensors is decided by `size-splits`. Same as tf.SplitV.",
    "inputs": [
      { "name": "value", "type": "TFL_TensorOf" },
      { "name": "size_splits", "type": "TFL_1DTensorOf" },
      { "name": "split_dim", "type": "TFL_0DTensorOf" }
    ],
    "outputs": [
      { "name": "outputs", "type": "TFL_VariadicTensorOf" }
    ],
    "attributes": [
      { "name": "num_splits", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tfl.sqrt",
    "summary": "Square root operator",
    "description": "Computes element-wise Square root of input",
    "inputs": [
      { "name": "x", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.square",
    "summary": "Square operator",
    "description": "Computes element-wise Square of input",
    "inputs": [
      { "name": "x", "type": "TFL_FpTensor" }
    ],
    "outputs": [
      { "name": "y", "type": "TFL_FpTensor" }
    ]
  },
  {
    "name": "tfl.squared_difference",
    "summary": "Squared difference operator",
    "description": "Element-wise squared difference operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.squeeze",
    "summary": "Removes dimensions of size 1 from the shape of a tensor.",
    "description": "Given a tensor `input`, this operation returns a tensor of the same type with\nall dimensions of size 1 removed. If you don't want to remove all size 1\ndimensions, you can remove specific size 1 dimensions by specifying\n`squeeze_dims`.\n\nFor example:\n\n```\n# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\nshape(squeeze(t)) ==> [2, 3]\n```\n\nOr, to remove specific size 1 dimensions:\n\n```\n# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\nshape(squeeze(t, [2, 4])) ==> [1, 2, 3, 1]\n```",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "squeeze_dims", "type": "ConfinedAttr" }
    ]
  },
  {
    "name": "tfl.strided_slice",
    "summary": "StridedSlice Op",
    "description": "Return a strided slice from `input`.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "begin", "type": "TFL_I32Tensor" },
      { "name": "end", "type": "TFL_I32Tensor" },
      { "name": "strides", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "begin_mask", "type": "I32Attr" },
      { "name": "end_mask", "type": "I32Attr" },
      { "name": "ellipsis_mask", "type": "I32Attr" },
      { "name": "new_axis_mask", "type": "I32Attr" },
      { "name": "shrink_axis_mask", "type": "I32Attr" },
      { "name": "offset", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tfl.sub",
    "summary": "Subtraction operator",
    "description": "Element-wise subtraction operation.",
    "inputs": [
      { "name": "lhs", "type": "TFL_TensorOf" },
      { "name": "rhs", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.sum",
    "summary": "Sum operator",
    "description": "Computes the sum reduction along the specified axes",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "axes", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "keep_dims", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tfl.svdf",
    "summary": "Single value decomposition filter operator",
    "description": "The SVDF op is a decomposition of a densely connected op into low rank\n    filters.\n    For details: https://research.google.com/pubs/pub43813.html\n                 https://arxiv.org/abs/1812.02802",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "feature_weights", "type": "TFL_TensorOf" },
      { "name": "time_weights", "type": "TFL_TensorOf" },
      { "name": "input_gate_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "activation_state", "type": "TFL_StatefulTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "rank", "type": "ConfinedAttr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "asymmetric_quantize_inputs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tfl.tanh",
    "summary": "Hyperbolic tangent operator",
    "description": "Computes element-wise Hyperbolic tangent of input",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "category": "Activation"
  },
  {
    "name": "tfl.tile",
    "summary": "Tile operator.",
    "description": "Constructs a tensor by tiling a given tensor.\n\n   This operation creates a new tensor by replicating input\n   multiples times. The output tensor's i'th dimension has\n   input.dims(i) * multiples[i] elements, and the values of input\n   are replicated multiples[i] times along the 'i'th dimension.\n   For example, tiling [a b c d] by [2] produces [a b c d a b c d].",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "multiples", "type": "TFL_I32OrI64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.topk_v2",
    "summary": "TopK operator",
    "description": "Returns the top `k` largest element along each last dimensional slice of\n    `input` and the indices of values within the last dimension of the input\n    tensor.\n\n    Results are always sorted in the descending order.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "k", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "values", "type": "TFL_TensorOf" },
      { "name": "indices", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.transpose",
    "summary": "Transpose operator",
    "description": "Returns the Transpose of x",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "perm", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "category": "Transform"
  },
  {
    "name": "tfl.transpose_conv",
    "summary": "Transpose convolution operator",
    "description": "Performs transpose convolution operation on input.",
    "inputs": [
      { "name": "output_shape", "type": "TFL_I32Tensor" },
      { "name": "weights", "type": "TFL_TensorOf" },
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "bias", "type": "TFL_TensorOfOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "padding", "type": "TFL_PaddingAttr" },
      { "name": "stride_h", "type": "ConfinedAttr" },
      { "name": "stride_w", "type": "ConfinedAttr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" }
    ]
  },
  {
    "name": "tfl.unidirectional_sequence_lstm",
    "summary": "Unidirectional sequence lstm operator",
    "description": "A recurrent neural network specified by an LSTM cell. This Op supports\n    unrolling the input along the time or batch dimensions, and\n    implements the following operation for\n    each element in the sequence s = 1...sequence_length:\n      outputs[s] = state = activation(LSTMOp(inputs[s]))\n\n    where LSTMOp is LSTM TF Lite Op and the “activation” is the function passed\n    as the “fused_activation_function” argument (if not “NONE”).",
    "inputs": [
      { "name": "input", "type": "TFL_FpTensor" },
      { "name": "input_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "input_to_forget_weights", "type": "TFL_TensorOf" },
      { "name": "input_to_cell_weights", "type": "TFL_TensorOf" },
      { "name": "input_to_output_weights", "type": "TFL_TensorOf" },
      { "name": "recurrent_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "recurrent_to_forget_weights", "type": "TFL_TensorOf" },
      { "name": "recurrent_to_cell_weights", "type": "TFL_TensorOf" },
      { "name": "recurrent_to_output_weights", "type": "TFL_TensorOf" },
      { "name": "cell_to_input_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "cell_to_forget_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "cell_to_output_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "input_gate_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "forget_gate_bias", "type": "TFL_FpTensor" },
      { "name": "cell_bias", "type": "TFL_FpTensor" },
      { "name": "output_gate_bias", "type": "TFL_FpTensor" },
      { "name": "projection_weights", "type": "TFL_TensorOfOrNone" },
      { "name": "projection_bias", "type": "TFL_TensorOfOrNone" },
      { "name": "input_activation_state", "type": "TFL_StatefulTensor" },
      { "name": "input_cell_state", "type": "TFL_StatefulTensor" },
      { "name": "input_layer_norm_coefficients", "type": "TFL_TensorOfOrNone" },
      { "name": "forget_layer_norm_coefficients", "type": "TFL_TensorOfOrNone" },
      { "name": "cell_layer_norm_coefficients", "type": "TFL_TensorOfOrNone" },
      { "name": "output_layer_norm_coefficients", "type": "TFL_TensorOfOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ],
    "attributes": [
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "cell_clip", "type": "ConfinedAttr" },
      { "name": "proj_clip", "type": "ConfinedAttr" },
      { "name": "time_major", "type": "BoolAttr" },
      { "name": "asymmetric_quantize_inputs", "type": "OptionalAttr" },
      { "name": "diagonal_recurrent_tensors", "type": "OptionalAttr" },
      { "name": "input_to_input_intermediate", "type": "OptionalAttr" },
      { "name": "input_to_forget_intermediate", "type": "OptionalAttr" },
      { "name": "input_to_cell_intermediate", "type": "OptionalAttr" },
      { "name": "input_to_output_intermediate", "type": "OptionalAttr" },
      { "name": "effective_hidden_scale_intermediate", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tfl.unidirectional_sequence_rnn",
    "summary": "Unidirectional sequence rnn operator",
    "description": "A recurrent neural network specified by an RNN cell. This Op takes in input\n    in a format {batch_size, seq_len, input_size} or\n    {seq_len, batch_size, input_size} if it's time-majored.\n\n    It implements the following operation for\n    each element in the sequence s = 1...sequence_length:\n      outputs[s] = state = activation(RNNOp(inputs[s]))\n\n    where RNNOp is RNNOp TF Lite Op and the “activation” is the function passed\n    as the “fused_activation_function” argument (if not “NONE”).",
    "inputs": [
      { "name": "input", "type": "TFL_FpTensor" },
      { "name": "input_to_input_weights", "type": "TFL_TensorOf" },
      { "name": "recurrent_to_input_weights", "type": "TFL_TensorOf" },
      { "name": "input_gate_bias", "type": "TFL_FpTensor" },
      { "name": "hidden_state", "type": "TFL_StatefulTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_FpTensor" }
    ],
    "attributes": [
      { "name": "time_major", "type": "BoolAttr" },
      { "name": "fused_activation_function", "type": "TFL_AFAttr" },
      { "name": "asymmetric_quantize_inputs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tfl.unique",
    "summary": "Unique Op.",
    "description": "This operation returns a tensor `output` containing all of the unique elements\nof `input` sorted in the same order that they occur in `input`. This operation\nalso returns a tensor `idx` the same size as `x` that contains the index of each\nvalue of `input` in the unique output `output`. In other words:",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" },
      { "name": "idx", "type": "TFL_I32OrI64Tensor" }
    ]
  },
  {
    "name": "tfl.unpack",
    "summary": "Unpacks a tensor along a dimension into multiple tensors",
    "description": "Unpacks a given dimension of a rank-`R` tensor into `num` rank-`(R-1)` tensors.\n\n    Unpacks `num` tensors from `value` by chipping it along the `axis` dimension.\n    For example, given a tensor of shape `(A, B, C, D)`;\n\n    If `axis == 0` then the i'th tensor in `output` is the slice `value[i, :, :, :]`\n      and each tensor in `output` will have shape `(B, C, D)`. (Note that the\n      dimension unpacked along is gone, unlike `split`).\n\n    If `axis == 1` then the i'th tensor in `output` is the slice `value[:, i, :, :]`\n      and each tensor in `output` will have shape `(A, C, D)`.\n    Etc.\n\n    This is the opposite of `pack`.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "outputs", "type": "TFL_VariadicTensorOf" }
    ],
    "attributes": [
      { "name": "num", "type": "ConfinedAttr" },
      { "name": "axis", "type": "I32Attr" }
    ]
  },
  {
    "name": "tfl.unsorted_segment_max",
    "summary": "UnsortedSegmentMax operator",
    "description": "Computes the maximum value along segments of a tensor such that\n    output[i] = max(data[j....]) where segment_ids[j...] = i\n    if the maximum is empty for a given segment ID i,\n    it outputs the smallest possible value for the specific numeric type,\n    output[i] = numeric_limits::lowest().\n    Note the values of segment_ids are always validated to be less than\n    num_segments and an error is thrown for out-of-bound indices.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "segment_ids", "type": "TFL_I32Tensor" },
      { "name": "num_segments", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.unsorted_segment_min",
    "summary": "UnsortedSegmentMin operator",
    "description": "Computes the minimum value along segments of a tensor such that\n    output[i] = min(data[j....]) where segment_ids[j...] = i\n    if the minimum is empty for a given segment ID i,\n    it outputs the largest possible value for the specific numeric type,\n    output[i] = numeric_limits::max().\n    Note the values of segment_ids are always validated to be less than\n    num_segments and an error is thrown for out-of-bound indices.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "segment_ids", "type": "TFL_I32Tensor" },
      { "name": "num_segments", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.unsorted_segment_prod",
    "summary": "UnsortedSegmentProd operator",
    "description": "Computes the product along segments of a tensor.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "segment_ids", "type": "TFL_I32Tensor" },
      { "name": "num_segments", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.unsorted_segment_sum",
    "summary": "UnsortedSegmentSum operator",
    "description": "From a tensor segmentation, computes the `output` resulting from\n    summing together elements mapped to the same segment_id. I.e. `output[i]` is\n    equal to the tensor sum of all elements from the input tensor mapped to\n    segment_id `i`. If no tensors are mapped to a particular included\n    segment_id, the output at that indice will be a zero tensor with the\n    appropriate shape. Note the values of segment_ids are always validated to be\n    less than num_segments and an error is thrown for out-of-bound indices",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" },
      { "name": "segment_ids", "type": "TFL_I32Tensor" },
      { "name": "num_segments", "type": "TFL_I32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfl.var_handle",
    "summary": "Returns a handle to a variable resource from its name.",
    "description": "Returns a handle for a variable resource from its name.\n    container: the container this variable is placed in.\n    shared_name: the name by which this variable is referred to.",
    "outputs": [
      { "name": "resource_handle", "type": "TFL_ResourceTensor" }
    ],
    "attributes": [
      { "name": "container", "type": "DefaultValuedStrAttr" },
      { "name": "shared_name", "type": "DefaultValuedStrAttr" }
    ]
  },
  {
    "name": "tfl.where",
    "summary": "Returns locations of nonzero / true values in a tensor.",
    "description": "This operation returns the coordinates of true elements in `condition`. The\ncoordinates are returned in a 2-D tensor where the first dimension (rows)\nrepresents the number of true elements, and the second dimension (columns)\nrepresents the coordinates of the true elements. Keep in mind, the shape of\nthe output tensor can vary depending on how many true values there are in\n`condition`. Indices are output in row-major order.",
    "inputs": [
      { "name": "condition", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "index", "type": "TFL_I64Tensor" }
    ]
  },
  {
    "name": "tfl.while",
    "summary": "While loop",
    "description": "output = input; while (cond(output)) { output = body(output) }\n\n    While loop where all values are passes through arguments with implicit\n    capture.\n\n    input: A list of input tensors whose types are T.\n    output: A list of output tensors whose types are T.\n    cond: A region that takes 'input' and returns a boolean scalar tensor.\n    body: A region that takes a list of tensors and returns another\n          list of tensors. Both lists have the same types.",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "is_stateless", "type": "DefaultValuedOptionalAttr" }
    ]
  },
  {
    "name": "tfl.yield",
    "summary": "Yield operation",
    "description": "The \"yield\" operation represents a return operation within the conditional\n    and body of structured control flow (e.g., while), and a terminator for ControlNodeOp.\n    The operation takes a variable number of operands and produces no results.\n    The operand number and types must match the signature of the region that contains the operation."
  },
  {
    "name": "tfl.zeros_like",
    "summary": "ZerosLike operator",
    "description": "Returns a tensor of zeros with the same shape and type as the input tensor.",
    "inputs": [
      { "name": "input", "type": "TFL_TensorOf" }
    ],
    "outputs": [
      { "name": "output", "type": "TFL_TensorOf" }
    ]
  },
  {
    "name": "tfr.build_list",
    "description": "The `build_list` operation builds a tensor list from a list of tensors, or\n   an tfr.attr from a list of scalars.\n\n    Example:\n\n    ```mlir\n    %3 = tfr.build_list(%2, %1, %0) :\n      (tfr.tensor, tfr.tensor, tfr.tensor) -> tfr.tensor_list\n    %3 = tfr.build_list(%2, %1, %0) : (i32, i32, i32) -> tfr.attr\n    ```",
    "inputs": [
      { "name": "tensors", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "out", "type": "TFR_allowedListResultType" }
    ]
  },
  {
    "name": "tfr.call",
    "description": "The `call` operation represents a direct call to a function that is within\n    the same symbol scope as the callee. The operands and result types of the\n    call must match the specified function type. The callee is encoded as a\n    symbol reference attribute named \"callee\".\n\n    Example:\n\n    ```mlir\n    %2 = tfr.call @my_add(%0, %1) : (tfr.tensor, f32) -> tfr.tensor_list\n    ```\n\n    Note that the operands of the `call` operation can only be with tfr.tensor,\n    tfr.tensor_list, tfr.attr and mlir float and integer types. The results of\n    the `call` operation can only be with tfr.tensor and tfr.tensor_list types.",
    "inputs": [
      { "name": "args", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "callee", "type": "FlatSymbolRefAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$callee `(` $args `)` attr-dict `:` functional-type($args, results)"
  },
  {
    "name": "tfr.cast",
    "description": "The `cast` operation converts the operand with built-in tensor type to\n    tfr.tensor type, or vice versa.\n\n    Example:\n\n    ```mlir\n    %1 = tfr.cast(%0) : tensor<f32> -> !tfr.tensor\n    %3 = tfr.cast(%1) : !tfr.tensor -> tensor<f32>\n    ```",
    "inputs": [
      { "name": "arg", "type": "TFR_singleTensorType" }
    ],
    "outputs": [
      { "name": "out", "type": "TFR_singleTensorType" }
    ]
  },
  {
    "name": "tfr.constant",
    "description": "The `attr` operation stores TF op's attribute, which doesn't support\n    arithmetic operations.\n\n    Example:\n\n    ```mlir\n    %1 = \"tfr.constant\"() { value: i32 } : () -> !tfr.attr\n    %2 = \"tfr.constant\"() { value: [i32, f32] } : () -> !tfr.attr\n    %3 = tfr.constant [i32, f32] -> !tfr.attr\n    %4 = tfr.constant f32 -> !tfr.attr\n    ```",
    "inputs": [
      { "name": "value", "type": "TFR_allowedConstValues" }
    ],
    "outputs": [
      { "name": "out", "type": "TFR_AttrType" }
    ],
    "assemblyFormat": "$value attr-dict `->` type($out)"
  },
  {
    "name": "tfr.constant_tensor",
    "description": "The `constant_tensor` operation converts the operand with non-built-in\n    tensor type to built-in tensor type or tfr.tensor type. If it is built-in\n    tensor type, the shape shouldn't be changed during the conversion.\n\n    Example:\n\n    ```mlir\n    %1 = tfr.constant_tensor(%0) : f32 -> tensor<f32>\n    %3 = tfr.constant_tensor(%2) : vector<1xf32> -> tensor<1xf32>\n    ```",
    "outputs": [
      { "name": "out", "type": "TFR_singleTensorType" }
    ],
    "attributes": [
      { "name": "arg", "type": "TFR_AllAttrTypes" }
    ]
  },
  {
    "name": "tfr.equal",
    "description": "The `equal` operation compares the values of the tfr.attr type arguments.\n    The operation returns an i1 boolean indicating if the two values are the\n    same.\n    Example:\n\n    ```mlir\n    %x = tfr.equal %lhs, %rhs -> i1\n    %x = \"tfr.equal\"(%lhs, %rhs) : (!tfr.attr, !tfr.attr) -> i1\n    ```",
    "outputs": [
      { "name": "result", "type": "BoolLike" }
    ],
    "attributes": [
      { "name": "lhs", "type": "TFR_AttrType" },
      { "name": "rhs", "type": "TFR_AttrType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `->` type($result)"
  },
  {
    "name": "tfr.func",
    "summary": "TFR Function defines a composition of other ops",
    "description": "Defines a function that can be used to decompose an TF function call to\n    the invocation of a set of other TF ops.\n\n    Syntax:\n\n    ```\n    op ::= `tfr.func` visibility? symbol-ref-id `(` argument-list `)` (`->`\n    function-result-list)? function-attributes? region\n    ```\n\n    Example:\n\n    ```mlir\n    tfr.func @foo(%arg0: !tfr.tensor, %arg1: !tfr.tensor_list<T>,\n                  %arg2: int {tfr.name=\"T\", tfr.default=1})\n        attributes {qux: \"quux\"} {\n      tfr.return\n    }\n    ```\n\n    Note the arguments are ordered by the following rule:\n      tfr.tensor > tfr.tensor_list > tfr.attr/i32/...,\n    and only one trfr.tensor_list argument is allowed.",
    "attributes": [
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "sym_name", "type": "StrAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tfr.get_element",
    "description": "The `get_element` operation extracts one tfr.tensor element from a\n    tfr.tensor_list.\n\n    Example:\n\n    ```mlir\n    %2 = tfr.get_element %1[%0] : (tfr.tensor, index) -> tfr.tensor\n    ```",
    "inputs": [
      { "name": "tensor_list", "type": "TFR_TensorListType" },
      { "name": "index", "type": "Index" }
    ],
    "outputs": [
      { "name": "out", "type": "TFR_TensorType" }
    ],
    "assemblyFormat": "$tensor_list `[` $index `]` attr-dict `:`\n      `(` type($tensor_list) `,` type($index) `)` `->` type($out)"
  },
  {
    "name": "tfr.get_element_type",
    "description": "The `get_element_type` operation gets the element type of a tfr.tensor and\n    returns !tfr.attr.\n\n    Example:\n\n    ```mlir\n    %1 = \"tfr.get_element_type\"(%0) : !tfr.tensor -> !tfr.attr\n    %1 = tfr.get_element_type %0 -> !tfr.attr\n    ```",
    "inputs": [
      { "name": "arg", "type": "TFR_TensorType" }
    ],
    "outputs": [
      { "name": "out", "type": "TFR_AttrType" }
    ],
    "assemblyFormat": "$arg attr-dict `->` type($out)"
  },
  {
    "name": "tfr.get_length",
    "description": "The `get_length` operation returns the number of tensors for a\n    tfr.tensor_list.\n\n    Example:\n\n    ```mlir\n    %2 = tfr.get_length(%1) : tfr.tensor -> index\n    %2 = tfr.get_length %1 -> index\n    ```",
    "inputs": [
      { "name": "tensor_list", "type": "TFR_TensorListType" }
    ],
    "outputs": [
      { "name": "out", "type": "Index" }
    ],
    "assemblyFormat": "$tensor_list attr-dict `->` type($out)"
  },
  {
    "name": "tfr.get_shape",
    "description": "The `get_shape` operation gets the shape of a tfr.tensor and returns\n    !shape.shape type.\n\n    Example:\n\n    ```mlir\n    %1 = \"tfr.get_shape\"(%0) : !tfr.tensor -> !shape.shape\n    %1 = tfr.get_shape %0 -> !shape.shape\n    ```",
    "inputs": [
      { "name": "arg", "type": "TFR_TensorType" }
    ],
    "outputs": [
      { "name": "out", "type": "Shape_ShapeType" }
    ],
    "assemblyFormat": "$arg attr-dict `->` type($out)"
  },
  {
    "name": "tfr.quant_act_range",
    "description": "The `quant_act_range` returns the a pair of integers to indicate the fixed\n   range for the fused activation `act` with the quantization defined by the\n   `scale` and `zero point`. Currently, the allowed activations are\n   `NONE`, `RELU`, `RELU6` and `RELU_N1_TO_1`.\n\n    Example:\n\n    ```mlir\n    %3, %4 = tfr.quant_act_range(%2, %1, %0) :\n        (tfr.attr, float, i64) -> (tfr.tensor, tfr.tensor)\n    ```",
    "inputs": [
      { "name": "scale", "type": "F32" },
      { "name": "zp", "type": "I64" }
    ],
    "outputs": [
      { "name": "min", "type": "TFR_TensorType" },
      { "name": "max", "type": "TFR_TensorType" }
    ],
    "attributes": [
      { "name": "act", "type": "TFR_AttrType" }
    ],
    "assemblyFormat": "`(` $act `,` $scale `,` $zp `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tfr.quant_qparam",
    "description": "The `quant_qparam` returns the quantization parameter of the input\n   tensors.\n\n    Example:\n\n    ```mlir\n    %3 = tfr.quant_qparam(%0) : (tfr.tensor) -> (float, tfr.tensor)\n    ```",
    "inputs": [
      { "name": "input", "type": "TFR_TensorType" }
    ],
    "outputs": [
      { "name": "scale", "type": "TFR_TensorType" },
      { "name": "zp", "type": "TFR_TensorType" }
    ],
    "assemblyFormat": "`(` $input `)` attr-dict `:` functional-type($input, results)"
  },
  {
    "name": "tfr.quant_raw_data",
    "description": "The `quant_raw_data` removes the quantization parameter from the intput\n   tensor(s).\n\n    Example:\n\n    ```mlir\n    %3 = tfr.quant_raw_data(%0) : (tfr.tensor) -> (tfr.tensor)\n    ```",
    "inputs": [
      { "name": "input", "type": "TFR_AllTensorTypes" }
    ],
    "outputs": [
      { "name": "output", "type": "TFR_AllTensorTypes" }
    ],
    "assemblyFormat": "`(` $input `)` attr-dict `:` functional-type($input, results)"
  },
  {
    "name": "tfr.quant_rescale",
    "description": "The `quant_rescale` rescales the elements of the integer tensor by the\n   floating-point rescale factor. This op needs to be legalized to the preferred\n   operations of the backends.\n\n    Example:\n\n    ```mlir\n    %3 = tfr.quant_rescale(%2, %1, %0) :\n        (tfr.tensor, tfr.tensor, i64) -> (tfr.tensor)\n    ```",
    "inputs": [
      { "name": "input", "type": "TFR_TensorType" },
      { "name": "scale", "type": "TFR_TensorType" },
      { "name": "zp", "type": "I64" }
    ],
    "outputs": [
      { "name": "output", "type": "TFR_TensorType" }
    ],
    "assemblyFormat": "`(` $input `,` $scale `,` $zp `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tfr.quant_scale_factor",
    "description": "The `quant_scale_factor` computes the effective scale factor according to the\n   output scale and input scales.\n\n    Example:\n\n    ```mlir\n    %3 = tfr.quant_scale_factor(%0) : (f32, tfr.tensor_list) -> (tfr.tensor)\n    ```",
    "inputs": [
      { "name": "out_scale", "type": "F32" },
      { "name": "in_scales", "type": "TFR_TensorListType" }
    ],
    "outputs": [
      { "name": "scale_factor", "type": "TFR_TensorType" }
    ],
    "assemblyFormat": "`(` $out_scale `,` $in_scales `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tfr.return",
    "description": "A terminator operation for regions that appear in the body of  `tfr.func`\n    functions. The operands to the `tfr.return` are the result values returned\n    by an invocation of the `tfr.func`.\n\n    Note that only the tfr.tensor and tfr.tensor_list can be returned.",
    "inputs": [
      { "name": "arguments", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($arguments^ `:` type($arguments))?"
  },
  {
    "name": "tile.abs",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.acos",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.acosh",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.add",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.argsort",
    "summary": "tensor sort index operation",
    "inputs": [
      { "name": "tensor", "type": "RankedTensorOf" },
      { "name": "direction", "type": "SortDirection" }
    ],
    "outputs": [
      { "name": "result", "type": "RankedTensorOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "IndexAttr" }
    ],
    "assemblyFormat": "$direction $tensor `[` $axis `]` attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.asin",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.asinh",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.assign",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.atan",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.atanh",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.bit_and",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.bit_not",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.bit_or",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.bit_shl",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.bit_shr",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.bit_xor",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.cast",
    "summary": "cast operation",
    "inputs": [
      { "name": "tensor", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$tensor attr-dict `:` functional-type($tensor, $result)"
  },
  {
    "name": "tile.ceil",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.cmp_eq",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseBool" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.cmp_ge",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseBool" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.cmp_gt",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseBool" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.cmp_le",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseBool" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.cmp_lt",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseBool" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.cmp_ne",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseBool" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.constant",
    "summary": "constant",
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "attributes": [
      { "name": "value", "type": "AnyAttr" }
    ],
    "assemblyFormat": "`(` $value `)` attr-dict `:` type($result)"
  },
  {
    "name": "tile.contract",
    "inputs": [
      { "name": "init", "type": "EltwiseAny" },
      { "name": "operands", "type": "Variadic" },
      { "name": "agg", "type": "AggregationKind" },
      { "name": "combo", "type": "CombinationKind" }
    ],
    "outputs": [
      { "name": "result", "type": "RankedTensorOf" }
    ],
    "attributes": [
      { "name": "sink", "type": "AffineMapAttr" },
      { "name": "srcs", "type": "AffineMapArrayAttr" },
      { "name": "cons", "type": "OptionalAttr" },
      { "name": "shape", "type": "OptionalAttr" },
      { "name": "lowerBounds", "type": "OptionalAttr" },
      { "name": "upperBounds", "type": "OptionalAttr" },
      { "name": "name", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tile.cos",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.cosh",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.div",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.erf",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.exp",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.floor",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.gather",
    "summary": "special gather operation",
    "inputs": [
      { "name": "tensor", "type": "RankedTensorOf" },
      { "name": "indices", "type": "RankedTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "RankedTensorOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "interpolationMode", "type": "DefaultValuedAttr" },
      { "name": "nearestMode", "type": "DefaultValuedAttr" },
      { "name": "cubeCoeff", "type": "DefaultValuedAttr" },
      { "name": "mode", "type": "DefaultValuedAttr" },
      { "name": "batchDims", "type": "DefaultValuedAttr" },
      { "name": "OutOfBoundsMode", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$tensor $indices attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.ident",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.index",
    "summary": "tensor index lookup operation",
    "outputs": [
      { "name": "result", "type": "EltwiseIndex" }
    ],
    "attributes": [
      { "name": "axis", "type": "IndexAttr" }
    ],
    "assemblyFormat": "$axis attr-dict `:` type($result)"
  },
  {
    "name": "tile.log",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.logical_and",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.logical_not",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.logical_or",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.logical_xor",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.max",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.min",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.mod",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.mul",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.neg",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.pow",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.pragma",
    "summary": "pragma operation",
    "inputs": [
      { "name": "tensor", "type": "AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "op", "type": "StrAttr" },
      { "name": "attrs", "type": "DictionaryAttr" }
    ],
    "assemblyFormat": "$tensor $op $attrs attr-dict `:` type($tensor)"
  },
  {
    "name": "tile.prng",
    "summary": "pseudorandom number generator",
    "inputs": [
      { "name": "state", "type": "RankedTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "RankedTensorOf" },
      { "name": "new_state", "type": "RankedTensorOf" }
    ],
    "assemblyFormat": "$state attr-dict `:` functional-type($state, results)"
  },
  {
    "name": "tile.relu",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.reshape",
    "summary": "tensor reshape operation",
    "inputs": [
      { "name": "tensor", "type": "RankedTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "RankedTensorOf" }
    ],
    "assemblyFormat": "$tensor attr-dict `:` functional-type($tensor, $result)"
  },
  {
    "name": "tile.round",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.scatter",
    "summary": "special scatter operation",
    "inputs": [
      { "name": "data", "type": "RankedTensorOf" },
      { "name": "indices", "type": "RankedTensorOf" },
      { "name": "updates", "type": "RankedTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "RankedTensorOf" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "mode", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$data $indices $updates attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.select",
    "summary": "conditional selection",
    "inputs": [
      { "name": "cond", "type": "EltwiseBool" },
      { "name": "tcase", "type": "EltwiseAny" },
      { "name": "fcase", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$cond `,` $tcase `,` $fcase attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.shape",
    "summary": "tensor shape operation",
    "inputs": [
      { "name": "tensor", "type": "RankedTensorOf" }
    ],
    "outputs": [
      { "name": "result", "type": "RankedTensorOf" }
    ],
    "assemblyFormat": "$tensor attr-dict `:` functional-type($tensor, $result)"
  },
  {
    "name": "tile.sign",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.sin",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.sinh",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.sqrt",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.sub",
    "inputs": [
      { "name": "lhs", "type": "EltwiseAny" },
      { "name": "rhs", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$lhs `,` $rhs attr-dict `:` functional-type(operands, $result)"
  },
  {
    "name": "tile.tan",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tile.tanh",
    "inputs": [
      { "name": "operand", "type": "EltwiseAny" }
    ],
    "outputs": [
      { "name": "result", "type": "EltwiseAny" }
    ],
    "assemblyFormat": "$operand attr-dict `:` functional-type($operand, $result)"
  },
  {
    "name": "tm_tensor.attention",
    "summary": "Attention operator",
    "description": "This operator takes in 3 to 4 tensors: query(Q), key(K), value(V), and an\n    optional mask(M) to compute the attention. These tensors must take on shapes\n    BxMxK1 for Q, BxK2xK1 for K, BxK2xN for V, and BxMxK2 for M. For all these\n    shapes, B represents the batch dimension, M represents sequence length, N\n    represents head dimension, and K1 and K2 are hidden dimensions.\n    Attention is defined as matmul(softmax(matmul(Q, transpose(K))+M), V) and\n    has shape BxMxN. Usually, this operator also performs scaling, masking and\n    dropout, but we leave that out of the current implementation.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict\n    `ins` `(` $inputs `:` type($inputs) `)`\n    `outs` `(` $outputs `:` type($outputs) `)`\n    (`->` type($result)^)?"
  },
  {
    "name": "tm_tensor.scan",
    "summary": "Scan operator",
    "description": "Computes the inclusive/exclusive scan along a given dimension.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "I64Attr" },
      { "name": "inclusive", "type": "BoolAttr" }
    ],
    "assemblyFormat": "`dimension` `(` $dimension `)`\n    `inclusive` `(` $inclusive `)`\n    attr-dict\n    `ins` `(` $inputs `:` type($inputs) `)`\n    `outs` `(` $outputs `:` type($outputs) `)`\n    $region (`->` type($results)^)?"
  },
  {
    "name": "tm_tensor.scatter",
    "summary": "Scatter operator",
    "description": "Based on XLA operation semantics, takes two `inputs` (`update` and\n    `indices`) and `outputs` value (`original`). The operation updates\n    the value at the slices specified by `indices` by combining the\n    current value with the value in `updates` using the computation\n    specified in `region`. The `region` specifies a binary operation\n    of signature (T, T) -> T, where `T` is the element-type of\n    `updates` (and `original`). The first argument correspond the\n    value to be updated (i.e. from `updates`), and the second the\n    current value (i.e. value from `original`).\n\n    The `indices` is a 2D tensor/memref type. The first dim is the number of\n    updates, and the second dim is index depth. The index depth should always be\n    static.\n\n    The first dim of `updates` and `indices` is identical, since they represent\n    the number of updates.\n\n    The rank of the `original`/`result` is at least\n    `index_depth + rank(%updates) - 1`. The first `index_depth` indices are\n    derived from `indices` and the shape of update value has the last\n    rank(%original) - index_depth values match %(originals) last dimensions,\n    with the previous dims extending from the index offsets.\n\n    The unique_indices attribute carries the information whether all the indices\n    are unique. If there are repeated indices, the first iteration loop will be\n    marked as reduction.\n\n    The shapes definition follows tensorflow operations execept that it force\n    batch dims to be 1D. See more information in\n      https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension_map", "type": "DenseI64ArrayAttr" },
      { "name": "unique_indices", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "attr-dict `unique_indices` `(` $unique_indices `)`\n    (`ins` `(` $inputs^ `:` type($inputs) `)`)?\n    `outs` `(` $outputs `:` type($outputs) `)`\n    $region (`->` type($results)^)?"
  },
  {
    "name": "tm_tensor.sort",
    "summary": "Sort operator",
    "description": "Based on XLA operation semantics, sorts the given `operands` at the given\n    `dimension` with the given `comparator`.\n\n    See https://www.tensorflow.org/xla/operation_semantics#sort.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "I64Attr" }
    ],
    "assemblyFormat": "attr-dict\n    `dimension` `(` $dimension `)`\n    (`ins` `(` $inputs^ `:` type($inputs) `)`)?\n    `outs` `(` $outputs `:` type($outputs) `)`\n    $region (`->` type($results)^)?"
  },
  {
    "name": "tm_tensor.topk",
    "summary": "Top-K operator",
    "description": "A Top-K operation for N-D tensors. Reduces the target dimension from the input\n    size N down to K elements based on the supplied binary region.\n\n    Accepts an N-D tensor input consisting of values and an optioanl N-D tensor\n    for indices of those values (i32 type). If input indices aren't provided, the\n    index mapping is inferred based on the k dim.  Both input values/indices\n    tensors and output values/indicies tensors must have the same shape. Top-K is\n    computed along the target dimension (from dimension()). Returns two output\n    tensors of values and the indicies of Top-K results. The output dimensions\n    must match the input save for the dimension that is reduced to K results.\n\n    Region accepts lhs=[next N input] and rhs=[exiting K output] and yeilds an\n    i1. If true, the two values are swapped:\n      - For Top-K compoarision: >\n      - For Min-K comparision: <\n    Note: when the two values are equal, the first occurence is always selected.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "outputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "I64Attr" }
    ],
    "assemblyFormat": "attr-dict\n    `dimension` `(` $dimension `)`\n    `ins` `(` $inputs `:` type($inputs) `)`\n    `outs` `(` $outputs `:` type($outputs) `)`\n    $region (`->` type($results)^)?"
  },
  {
    "name": "tm_tensor.yield",
    "summary": "TMTensor yield op",
    "description": "`tm_tensor.yield` is a special terminator operation for blocks inside\n    regions in `tm_tensor` ops.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "top.A16MatMul",
    "summary": "w8a16 / w4a16 matmul operator",
    "description": "The special matrix multiplication designed for LLM Linear Layer.\r\n    Weight is saved in int8 with f16 per-channel quant scale.\r\n\r\n    y_f16 = x_f16 x (quantized_w.to(f16) * scale_f16)",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "weight", "type": "AnyTensor" },
      { "name": "scale", "type": "AnyTensor" },
      { "name": "zp", "type": "AnyTensor" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "right_transpose", "type": "DefaultValuedAttr" },
      { "name": "q_group_size", "type": "DefaultValuedAttr" },
      { "name": "weight_bits", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Abs",
    "summary": "Abs operator",
    "description": "1.Op Introduction\r\n    computes the absolute value of each element in the input tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = abs(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.AdaptiveAvgPool",
    "summary": "pool operator",
    "description": "1.Op Introduction\r\n    This performs an  pooling over the given input tensor. A sliding\r\n    window of size given by <kernel size> is passed over the input tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n        {output}(N_i, C_j, p, q) = pool(input(N_i, C_j, start_h, start_w), kernel_size)\r\n    ```\r\n    start_h and start_w are the starting indices for the pooling window in the height and width dimensions.\r\n    pool can be any pooling function (max pooling, average pooling) applied over the region defined by the kernel size.\r\n    N is a batch size, C denotes a number of channels,\r\n    Shape:\r\n    - Input: (N, C_{in}, H_{in}, W_{in})\r\n    - Output: (N, C_{out}, H_{out}, W_{out})\r\n        ```math\r\n        H_{output} = {H_{in} + {padding}[0] + {padding}[2] - {kernel_shape}[0]} / {stride}[0] + 1\r\n        ```\r\n        ```math\r\n        W_{output} = {W_{in} + {padding}[1] + {padding}[3] - {kernel_shape}[1]} / {stride}[1] + 1\r\n        ```\r\n\r\n    3.activation and weight\r\n    input(act.): Variadic input tensor;\r\n\r\n    4.attribute\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    stride: the stride for the cross-correlation, a single number or a tuple.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    ceil_mode: whether to use ceiling or floor when calculating the output size.;\r\n    auto_pad: It can be set to different modes (e.g., SAME, VALID) to automatically calculate the necessary padding based on\r\n              the input size,kernel size, and stride.;\r\n    is_adaptive: whether the pooling operation is adaptive.\r\n                 If true, adjusts the kernel size based on the input size to produce a specified output size.\r\n    keepdims: whether to retain the dimensions of the input tensor in the output.\r\n               If true, will have the same number of dimensions as the input tensor.;\r\n    pad_value: whether to retain the dimensions of the input tensor in the output.\r\n                If true, will have the same number of dimensions as the input tensor.;\r\n    count_include_pad: whether to include the padded values in the pooling count.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;\r\n    first_round_mode: the rounding behavior applied to the scaled value before the offset is added.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "output_size", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "top.Add",
    "summary": "add operator",
    "description": "1.Op Introduction\r\n    Elementwise addition of input1 and input2. Axis of size 1 will be broadcast,\r\n    as necessary.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = ReLU((input1 + input2; dim))\r\n    ```\r\n    Axis of size 1 will be broadcast if necessary.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    coeff: It is an array and allows for scaling the output of the addition operation.;\r\n    is_scalar: whether the addition operation is performed with scalar values or tensor.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "coeff", "type": "OptionalAttr" },
      { "name": "is_scalar", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.AddConst",
    "summary": "Add Const operator",
    "description": "1.Op Introduction\r\n    The AddConst operator is designed to perform an element-wise addition of a constant value to an input tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = input + const_val\r\n    ```\r\n    Where input1, input2, ..., inputN are the input tensors.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    const_val: the constant value to be added to each element of the input tensor(positive, negative, or zero).;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    is_scalar: whether the addition operation is performed with scalar values or tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "const_val", "type": "F64Attr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "is_scalar", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Arange",
    "summary": "Arange operator",
    "description": "1.Op Introduction\r\n    gen by torch aten::arange.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = [x | x = start + n * step, n is an integer, and start ≤ x < end]\r\n    ```\r\n\r\n    3.activation and weight\r\n    start(act.): The starting value of the sequence. This can be a tensor or None. If None, it defaults to 0.;\r\n    end(act.): The exclusive upper limit of the sequence.;\r\n    step(act.): The increment between each value in the sequence. This can be a tensor or None. If None, it defaults to 1.;",
    "inputs": [
      { "name": "start", "type": "AnyTensorOrNone" },
      { "name": "end", "type": "AnyTensor" },
      { "name": "step", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Arccos",
    "summary": "Arccos operator",
    "description": "1.Op Introduction\r\n    Calculates the Arccos of the given input tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Arccos(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Arctanh",
    "summary": "Arctanh operator",
    "description": "1.Op Introduction\r\n    Calculates the Arctanh of the given input tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Arctanh(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Arg",
    "summary": "Arg operation",
    "description": "1.Op Introduction\r\n    Computes the indices of the min/max/ of the input tensor's element along the provided axis.\r\n\r\n    2.Math formula\r\n    ```math\r\n        maximum operation:\r\n            output_max[i_1, i_2, i_3,..., i_k] = arg max{j}(input[i_1, i_2,..., i_k, j])\r\n        minimum operation:\r\n            output_min[i_1, i_2, i_3,..., i_k] = arg min{j}(input[i_1, i_2,..., i_k, j])\r\n    ```\r\n    where, ( j ) represents the index along the specified axis.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    axis: the dimension of the input tensor.;\r\n    keepdims: whether to retain the dimensions of the input tensor in the output.\r\n               If true, will have the same number of dimensions as the input tensor.;\r\n    mode: the type of binary operation to be performed, addition, subtraction, or other types of binary operations.;\r\n    select_last_index: select the last index of the minimum or maximum value when multiple  along the specified axis.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "indices", "type": "AnyTensor" },
      { "name": "values", "type": "AnyTensorOrNone" }
    ],
    "attributes": [
      { "name": "axis", "type": "I64Attr" },
      { "name": "keepdims", "type": "BoolAttr" },
      { "name": "mode", "type": "ArgModeAttr" },
      { "name": "select_last_index", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Attention",
    "summary": "Attention operator",
    "description": "1.Op Introduction\r\n    Performs a multi head attention block. https://en.wikipedia.org/wiki/Attention_(machine_learning)\r\n    This block has Q_w, K_w,V_w, O_w and mask;\r\n    This operator utilizes query, key, and value weights (denoted as (Q_w), (K_w), and (V_w), respectively) to compute attention scores and\r\n    generate output representations.\r\n\r\n    2.Math formula\r\n    ```math\r\n        Attention(Q, K, V) = softmax(((Q x K^T) / \\sqrt{d_k}) + musk) x V;\r\n        head_i = Attention(Q x queries_weight, K x keys_weight, V x values_weight);\r\n        MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h) x out_weight + out_bias;\r\n        output = MultiHead(input x queries_weight + queries_bias, input x keys_weight + keys_bias, input x values_weight + values_bias).\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    keys(act.): The keys are derived from the input data and help the model determine which parts of the input are relevant for each query.;\r\n    values(act.): The values are the actual information that will be aggregated based on the attention scores computed from the queries and keys.;\r\n    queries_weight(w.): Queries are the features that the model uses to ask questions about the input data.;\r\n    queries_bias(w.): added to the query representations after the weight transformation.;\r\n    keys_weight(w.): This weight tensor transforms the input into key representations.;\r\n    keys_bias(w.): added to the key representations after the weight transformation, providing further adjustment.;\r\n    values_weight(w.): used to transform the input into value representations.;\r\n    values_bias(w.): added to the value representations after the weight transformation.;\r\n    out_weight(w.): used to transform the concatenated output of the attention heads into the final output representation.;\r\n    out_bias(w.): added to the output representation after the final weight transformation.;\r\n    musk(w.):  apply masking during the attention computation, Masks can prevent the model from attending to certain positions in the input.;\r\n\r\n\r\n    4.attribute\r\n    scale: a scaling factor applied to the attention scores before they are passed through the softmax function.;\r\n    head: the number of attention heads to use in the multi-head attention mechanism.;\r\n    dim: the size of the input features or the size of the query, key, and value vectors.;\r\n    scale_param: adjust the scaling factor for the attention scores, It allows for flexibility in tuning the attention mechanism.;\r\n    zp_param: the zero-point parameters for quantization,;\r\n    has_bias: whether the attention mechanism includes bias terms in its computations.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "keys", "type": "AnyTensor" },
      { "name": "values", "type": "AnyTensor" },
      { "name": "queries_weight", "type": "AnyTensor" },
      { "name": "queries_bias", "type": "AnyTensorOrNone" },
      { "name": "keys_weight", "type": "AnyTensor" },
      { "name": "keys_bias", "type": "AnyTensorOrNone" },
      { "name": "values_weight", "type": "AnyTensor" },
      { "name": "values_bias", "type": "AnyTensorOrNone" },
      { "name": "out_weight", "type": "AnyTensor" },
      { "name": "out_bias", "type": "AnyTensorOrNone" },
      { "name": "mask", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "scale", "type": "F64Attr" },
      { "name": "head", "type": "I64Attr" },
      { "name": "dim", "type": "DefaultValuedAttr" },
      { "name": "scale_param", "type": "DefaultValuedAttr" },
      { "name": "zp_param", "type": "DefaultValuedAttr" },
      { "name": "has_bias", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.AvgPool",
    "summary": "pool operator",
    "description": "1.Op Introduction\r\n    This performs an  pooling over the given input tensor. A sliding\r\n    window of size given by <kernel size> is passed over the input tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n        {output}(N_i, C_j, p, q) = pool(input(N_i, C_j, start_h, start_w), kernel_size)\r\n    ```\r\n    start_h and start_w are the starting indices for the pooling window in the height and width dimensions.\r\n    pool can be any pooling function (max pooling, average pooling) applied over the region defined by the kernel size.\r\n    N is a batch size, C denotes a number of channels,\r\n    Shape:\r\n    - Input: (N, C_{in}, H_{in}, W_{in})\r\n    - Output: (N, C_{out}, H_{out}, W_{out})\r\n        ```math\r\n        H_{output} = {H_{in} + {padding}[0] + {padding}[2] - {kernel_shape}[0]} / {stride}[0] + 1\r\n        ```\r\n        ```math\r\n        W_{output} = {W_{in} + {padding}[1] + {padding}[3] - {kernel_shape}[1]} / {stride}[1] + 1\r\n        ```\r\n\r\n    3.activation and weight\r\n    input(act.): Variadic input tensor;\r\n\r\n    4.attribute\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    stride: the stride for the cross-correlation, a single number or a tuple.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    ceil_mode: whether to use ceiling or floor when calculating the output size.;\r\n    auto_pad: It can be set to different modes (e.g., SAME, VALID) to automatically calculate the necessary padding based on\r\n              the input size,kernel size, and stride.;\r\n    is_adaptive: whether the pooling operation is adaptive.\r\n                 If true, adjusts the kernel size based on the input size to produce a specified output size.\r\n    keepdims: whether to retain the dimensions of the input tensor in the output.\r\n               If true, will have the same number of dimensions as the input tensor.;\r\n    pad_value: whether to retain the dimensions of the input tensor in the output.\r\n                If true, will have the same number of dimensions as the input tensor.;\r\n    count_include_pad: whether to include the padded values in the pooling count.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;\r\n    first_round_mode: the rounding behavior applied to the scaled value before the offset is added.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "ceil_mode", "type": "OptionalAttr" },
      { "name": "auto_pad", "type": "OptionalAttr" },
      { "name": "is_adaptive", "type": "DefaultValuedAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "pad_value", "type": "DefaultValuedAttr" },
      { "name": "count_include_pad", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "first_round_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.BatchNorm",
    "summary": "BatchNormalization operation",
    "description": "1.Op Introduction\r\n    Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension)\r\n    as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing\r\n    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = \\frac{input - \\mathrm{E}[input]}{ \\variance + \\epsilon} * \\gamma + \\beta\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    mean(w.): mean of input tensor in dim C;\r\n    variance(w.):  quantifies the spread of the input tensor values along the channel dimension (dimension C) for each mini-batch.;\r\n    gamma(w.): scalar;\r\n    beta(w.): scalar;\r\n    The mean and standard-deviation are calculated per-dimension over\r\n    the mini-batches and $$\\gamma$$ and $$\\beta$$ are learnable parameter vectors\r\n    of size C (where C is the input channel size).\r\n\r\n    4.attribute\r\n    epsilon;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "mean", "type": "AnyTensor" },
      { "name": "variance", "type": "AnyTensor" },
      { "name": "gamma", "type": "AnyTensorOrNone" },
      { "name": "beta", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.BatchNormBwd",
    "summary": "BatchNormalization backward operation",
    "description": "1.Op Introduction\r\n    Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs\r\n    with additional channel dimension) as described in the paper\r\n    Batch Normalization: Accelerating Deep Network Training by Reducing\r\n    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\r\n\r\n    2.Math formula\r\n    ```math\r\n        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\r\n    ```\r\n    The mean and standard-deviation are calculated per-dimension over\r\n    the mini-batches and $$\\gamma$$ and $$\\beta$$ are learnable parameter vectors\r\n    of size C (where C is the input channel size).\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    grad_out(w.): the gradient of the loss with respect to the output of the layer normalization.;\r\n\r\n    4.attributes\r\n    weight_opt: the optimal scaling for the normalized output.;\r\n    saved_mean: the mean of the input tensor calculated.;\r\n    saved_invstd: the inverse standard deviation of the input tensor calculated.;\r\n    epsilon;",
    "inputs": [
      { "name": "grad_out", "type": "AnyTensor" },
      { "name": "input", "type": "AnyTensor" },
      { "name": "weight_opt", "type": "AnyTensorOrNone" },
      { "name": "saved_mean", "type": "AnyTensorOrNone" },
      { "name": "saved_invstd", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "grad_in", "type": "AnyTensor" },
      { "name": "weight_grad", "type": "AnyTensorOrNone" },
      { "name": "bias_grad", "type": "AnyTensorOrNone" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.BatchNormTrain",
    "summary": "BatchNormalization train operation",
    "description": "1.Op Introduction\r\n    Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs\r\n    with additional channel dimension) as described in the paper\r\n    Batch Normalization: Accelerating Deep Network Training by Reducing\r\n    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\r\n\r\n    2.Math formula\r\n    ```math\r\n        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\r\n    ```\r\n    The mean and standard-deviation are calculated per-dimension over\r\n    the mini-batches and $$\\gamma$$ and $$\\beta$$ are learnable parameter vectors\r\n    of size C (where C is the input channel size).\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    mean(w.): mean values to subtract from each channel for normalization.;\r\n    variance(w.): adjust the predicted boxes during the training process.;\r\n\r\n    4.attributes\r\n    gamma;\r\n    beta;\r\n    epsilon;\r\n    momentum: hyperparameter;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "mean", "type": "AnyTensor" },
      { "name": "var", "type": "AnyTensor" },
      { "name": "gamma", "type": "AnyTensorOrNone" },
      { "name": "beta", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" },
      { "name": "mean_out", "type": "AnyTensor" },
      { "name": "saved_invstd", "type": "AnyTensor" },
      { "name": "running_mean", "type": "AnyTensor" },
      { "name": "running_var", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "momentum", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.BinaryConstShift",
    "summary": "Binary Const with shift operator",
    "description": "1.Op Introduction\r\n    The BinaryConstShift operator is a specialized tensor operation that combines binary arithmetic with constant scaling and shifting.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = saturation(input +/-/* scale >> -shift)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    scale: a scaling factor multiplies the input tensor.;\r\n    shift: a shift value applied to the quantized data before scaling.;\r\n    is_reverse: whether the subtraction operation is performed in reverse order.;\r\n    saturation: whether the output should be saturated.\r\n                When set true, the output will be clamped to a predefined range to prevent overflow or underflow during the operation.;\r\n    round_mode: It determines how values are rounded during the conversion from higher precision to lower precision.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "scale", "type": "SI32Attr" },
      { "name": "mode", "type": "BinaryShiftAttr" },
      { "name": "shift", "type": "SI32Attr" },
      { "name": "is_reverse", "type": "DefaultValuedAttr" },
      { "name": "saturation", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.BinaryShift",
    "summary": "Binary with shift operator",
    "description": "1.Op Introduction\r\n    The BinaryShift operator is designed to perform binary operations on two input tensors with an additional shift operation.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = saturation(input1 +/-/* input2 >> -shift)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input1(act.): input tensor;\r\n    input2(act.): input tensor;\r\n\r\n    4.attribute\r\n    shift: a shift value applied to the quantized data before scaling.;\r\n    mode: the type of binary operation to be performed, addition, subtraction, or other types of binary operations.;\r\n    is_reverse: whether the subtraction operation is performed in reverse order.;\r\n    saturation: whether the output should be saturated.\r\n                When set to true, the output will be clamped to a predefined range to prevent overflow or underflow during the operation.;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;",
    "inputs": [
      { "name": "input1", "type": "AnyTensor" },
      { "name": "input2", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "BinaryShiftAttr" },
      { "name": "shift", "type": "SI32Attr" },
      { "name": "is_reverse", "type": "DefaultValuedAttr" },
      { "name": "saturation", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Cast",
    "summary": "Cast operation",
    "description": "1.Op Introduction\r\n    quant::UniformQuantizedType cast to float type; or float type cast to quant::UniformQuantizedType\r\n\r\n    2.Math formula\r\n    Float(output) = Cast (input, dtype=UniformQuantizedType);\r\n    UniformQuantizedType(output) = Cast (input, dtype=Float);\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "round_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Ceil",
    "summary": "Ceil operator",
    "description": "1.Op Introduction\r\n    y = ceil(x)\r\n    rounds each element of the input tensor up to the nearest integer.\r\n\r\n     2.Math formula\r\n     ```math\r\n            output = ceil(input)\r\n     ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Clip",
    "summary": "Clip operator",
    "description": "1.Op Introduction\r\n    The operator limits the given input to a certain range.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = min      if input[i] < min;\r\n                        input[i] if input[i] >= min && input[i] <= max;\r\n                        max      if input[i] > max;\r\n\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    min: the minimum value that the elements of the input tensor can take.;\r\n    max: the maximum value that the elements of the input tensor can take.;",
    "inputs": [
      { "name": "inputs", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "min", "type": "F64Attr" },
      { "name": "max", "type": "F64Attr" }
    ]
  },
  {
    "name": "top.Compare",
    "summary": "Compare operation",
    "description": "1.Op Introduction\r\n    Returns the tensor resulted from performing the compare\r\n    operation elementwise on the input tensors A and B.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = 1 if lhs[i] mode rhs[i] is true\r\n                        0 otherwise\r\n    ```\r\n\r\n    3.activation and weight\r\n    lhs(act.): the first input tensor used as the left operand in the element-wise comparison.;\r\n    rhs(act.): the second input tensor used as the right operand in the element-wise comparison.;\r\n\r\n    4.attributes\r\n    mode: the type of comparison to be performed between the two input tensors.\r\n          mdoe include Equal, Not Equal, Less Than, Less Than or Equal, Greater Than and Greater Than or Equal;",
    "inputs": [
      { "name": "lhs", "type": "AnyTensor" },
      { "name": "rhs", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "CompareModeAttr" }
    ]
  },
  {
    "name": "top.CompareConst",
    "summary": "CompareConst operation",
    "description": "1.Op Introduction\r\n    Returns the tensor resulted from performing the compare\r\n    operation elementwise on the input tensors A and Const.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = 1 if input[i] mode const_val is true\r\n                        0 otherwise\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    mode: the type of comparison to be performed between the two input tensors.\r\n          mdoe include Equal, Not Equal, Less Than, Less Than or Equal, Greater Than and Greater Than or Equal;\r\n    const_val: specifies the constant value to be added to each element of the input tensor(positive, negative, or zero).;\r\n    inversed: whether the mask should be inverted.;\r\n    is_scalar: whether the addition operation is performed with scalar values or tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "CompareModeAttr" },
      { "name": "const_val", "type": "F64Attr" },
      { "name": "inversed", "type": "BoolAttr" },
      { "name": "is_scalar", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Concat",
    "summary": "Concat operator",
    "description": "1.Op Introduction\r\n    Concatenates the given sequence of seq tensors in the given dimension.\r\n    All tensors must either have the same shape (except in the concatenating dimension) or be empty.\r\n\r\n    2.Math formula\r\n    output = Concat(input1, input2, axis)\r\n           = input1[axis] + input2[axis];\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    axis: the dimension along which the input tensors will be concated together.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    round_mode: This determines how values are rounded during the conversion from higher precision to lower precision.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.ConstantFill",
    "summary": "constant fill operator",
    "description": "1.Op Introduction\r\n    fill the constant value\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = value * ones(shape(input))\r\n    ```\r\n    where, ones(shape(input)) generates a tensor of the same shape as the input tensor, filled with ones.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    value: the constant value that will fill the output tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "F64Attr" }
    ]
  },
  {
    "name": "top.Conv",
    "summary": "Convolution operator",
    "description": "1.Op Introduction\r\n    The Top_ConvOp is a convolution operator designed to perform convolution operations on input tensors.\r\n    The operation transforms an input tensor with shape ((N, C_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})) into\r\n    an output tensor with shape ((N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})).\r\n    The output is computed using learnable weights (filters) and optional biases,\r\n    following the mathematical formula for convolution, which incorporates parameters such as kernel size, strides, padding, and dilation.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output(N, C_{out}, H, H) = \\sum_{C_{in}} input(N, C_{in}, H + sH * kH, W + sW * kW) * filter(C_{in}, C_{out}, kH, kW) + bias(C_{out}) + bisa(C_{out})\r\n    ```\r\n    where, kH and kW are the height and width of the filter (kernel), sH and sW are the vertical and horizontal strides.\r\n            N is a batch size, C denotes a number of channels, H is a height of input, and W is width.\r\n    Shape:\r\n    - Input: (N, C_{in}, H_{in}, W_{in})\r\n    - Output: (N, C_{out}, H_{out}, W_{out})\r\n        ```math\r\n        H_{output} = {H_{in} + {padding}[0] + {padding}[2] - {dilation}[0] x ({kernel_size}[0] - 1) - 1} / {stride}[0] + 1\r\n        ```\r\n        ```math\r\n        W_{output} = {W_{in} + {padding}[1] + {padding}[3] - {dilation}[1] x ({kernel_size}[1] - 1) - 1} / {stride}[1] + 1\r\n        ```\r\n\r\n    3.activation and weight\r\n    input(act.): Variadic input tensor;\r\n    filter(w.): the learnable weights of the convolution 2d operation.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attribute\r\n    kernel_shape: the size of the convolution kernel (filter) as an array. ;\r\n    stride: the stride for the cross-correlation, a single number or a tuple.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    groups: (optional)Number of blocked connections from input channels to output channels. Default: 1.;\r\n    dilation: controls the spacing between the kernel points;\r\n    inserts: additional parameters that may be used for specific optimizations or configurations.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    dynweight_reorderd: whether the weights (filters) should be reordered dynamically.;\r\n    weight_is_coeff: whether the weights should be treated as coefficients.;\r\n    do_winograd: whether to use the Winograd algorithm for convolution,\r\n                 which can speed up the computation by reducing the number of multiplications needed.;\r\n    auto_pad: It can be set to different modes (e.g., SAME, VALID) to automatically calculate the necessary padding based on\r\n              the input size,kernel size, and stride.;\r\n    in_int4_scale: This attribute defines the scaling factor for input tensors represented in 4-bit integer format (INT4).;\r\n    in_int4_zp: The in_int4_zp attribute specifies the zero-point for the input tensors in INT4 format.;\r\n    out_int8_scale: This attribute defines the scaling factor for output tensors represented in 8-bit integer format (INT8).;\r\n    out_int8_zp: The out_int8_zp attribute specifies the zero-point for the output tensors in INT8 format.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "filter", "type": "AnyTensor" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "group", "type": "DefaultValuedAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "inserts", "type": "OptionalAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "dynweight_reorderd", "type": "DefaultValuedAttr" },
      { "name": "weight_is_coeff", "type": "DefaultValuedAttr" },
      { "name": "do_winograd", "type": "OptionalAttr" },
      { "name": "auto_pad", "type": "OptionalAttr" },
      { "name": "in_int4_scale", "type": "OptionalAttr" },
      { "name": "in_int4_zp", "type": "OptionalAttr" },
      { "name": "out_int8_scale", "type": "OptionalAttr" },
      { "name": "out_int8_zp", "type": "OptionalAttr" },
      { "name": "weight_bits", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "top.Convbwd",
    "summary": "convolution backward",
    "description": "1.Op Introduction\r\n    calculate grad_input,grad_weight,grad_bias of convolution operation.\r\n\r\n    2.Math formula\r\n    ```math\r\n        \\text{grad\\_input} = \\sum_{k=0}^{K-1} \\text{grad\\_out} \\ast \\text{kernel}_{k}\r\n        where \\( K \\) is the number of output channels and \\( \\ast \\) denotes the convolution operation.\r\n\r\n        \\text{grad\\_weight} = \\sum_{n=0}^{N-1} \\text{input}_{n} \\ast \\text{grad\\_out}\r\n        where \\( N \\) is the number of input channels.\r\n\r\n        \\text{grad\\_bias} = \\sum_{i=0}^{M-1} \\text{grad\\_out}_{i}\r\n        where \\( M \\) is the number of output channels.\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    grad_out(w.): the gradient of the loss with respect to the output of the layer normalization.;\r\n    kernel(w.): convolution kernel (filter) tensor.;\r\n\r\n    4.attributes\r\n    groups: Number of blocked connections from input channels to output channels. Default: 1.;\r\n    input_shape: The shape of the input tensor.;\r\n    grad_out_shape: The shape of the gradient output tensor.;\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    stride: the stride for the cross-correlation, a single number or a tuple.;\r\n    dilations: controls the spacing between the kernel points;\r\n    padding: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    inserts: selectively enabling or disabling the calculation of gradients;\r\n    grad_input_enable: whether to compute the gradient for the input tensor.;\r\n    grad_weight_enable: whether to compute the gradient for the weight tensor (kernel).;\r\n    grad_bias_enable: whether to compute the gradient for the bias term as well.;",
    "inputs": [
      { "name": "grad_out", "type": "AnyTensor" },
      { "name": "input", "type": "AnyTensor" },
      { "name": "kernel", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "grad_input", "type": "AnyTensorOrNone" },
      { "name": "grad_weight", "type": "AnyTensorOrNone" },
      { "name": "grad_bias", "type": "AnyTensorOrNone" }
    ],
    "attributes": [
      { "name": "groups", "type": "I64Attr" },
      { "name": "input_shape", "type": "I64ArrayAttr" },
      { "name": "grad_out_shape", "type": "I64ArrayAttr" },
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "stride", "type": "I64ArrayAttr" },
      { "name": "dilations", "type": "I64ArrayAttr" },
      { "name": "padding", "type": "I64ArrayAttr" },
      { "name": "inserts", "type": "I64ArrayAttr" },
      { "name": "grad_input_enable", "type": "BoolAttr" },
      { "name": "grad_weight_enable", "type": "BoolAttr" },
      { "name": "grad_bias_enable", "type": "BoolAttr" }
    ]
  },
  {
    "name": "top.ConvBwd_Weight",
    "summary": "Convolution Backward operator",
    "description": "1.Op Introduction\r\n    Gradient of Weight in Convolution Backward.\r\n\r\n    2.Math formula\r\n    ```math\r\n            \\frac{\\partial L}{\\partial W} = \\sum_{n=1}^{N} \\sum_{c=1}^{C_{in}} \\sum_{h=1}^{H} \\sum_{w=1}^{W} \\text{input}[n, c, h, w] \\cdot \\text{grad\\_out}[n, :, h', w']\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    grad_output(act.): the gradient of the loss with respect to the output.;\r\n    gradout_transpose(w.): The transposed gradient of the output tensor.;\r\n\r\n    4.attributes\r\n    groups: Number of blocked connections from input channels to output channels. Default: 1.;\r\n    input_shape: The shape of the input tensor.;\r\n    grad_out_shape: The shape of the gradient output tensor.;\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    stride: the stride for the cross-correlation, a single number or a tuple.;\r\n    dilations: controls the spacing between the kernel points;\r\n    padding: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    grad_bias_enable: whether to compute the gradient for the bias term as well.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "gradout", "type": "AnyTensor" },
      { "name": "gradout_transpose", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "groups", "type": "I64Attr" },
      { "name": "input_shape", "type": "I64ArrayAttr" },
      { "name": "grad_out_shape", "type": "I64ArrayAttr" },
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "stride", "type": "I64ArrayAttr" },
      { "name": "dilations", "type": "I64ArrayAttr" },
      { "name": "padding", "type": "I64ArrayAttr" },
      { "name": "grad_bias_enable", "type": "BoolAttr" }
    ]
  },
  {
    "name": "top.Copy",
    "summary": "Copy operator",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "shape", "type": "I64ArrayAttr" },
      { "name": "input_stride", "type": "I64ArrayAttr" },
      { "name": "output_stride", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "top.Correlation",
    "summary": "Custom operator correlation",
    "description": "Multiply the sliced left_feature and right_feature based on max_disp;\r\n  then perform a reduce operation;\r\n  and finally concatenate the results.\r\n\r\n  2.Math formula\r\n  for i in range(max_disp):\r\n    if i > 0:\r\n        output[:, i, :, i:] = (left_feature[:, :, :, i:] * right_feature[:, :, :, :-i]).mean(dim=1)\r\n    else:\r\n        output[:, i, :, :] = (left_feature * right_feature).mean(dim=1)\r\n\r\n  3.activation and weight\r\n  input(act.): input tensor;\r\n\r\n  4.attribute\r\n  max_disp: The number of slicing iterations, which is also the size of the output dimension C.\r\n  num_groups: The number of batch groups.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "max_disp", "type": "DefaultValuedAttr" },
      { "name": "num_groups", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Cos",
    "summary": "Cos operator",
    "description": "1.Op Introduction\r\n    Calculates the Cos of the given input tensor, element-wise.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = cos(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Cosh",
    "summary": "Cosh operator",
    "description": "1.Op Introduction\r\n    Calculates the Cosh of the given input tensor, element-wise.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = cosh(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Csc",
    "summary": "Color space convert for model's inputs",
    "description": "1.Op Introduction\r\n    Performs csc operation on inputs.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Csc(input, pixel_format, y_align, w_align, channel_align)\r\n    ```\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    pixel_format: required, pixel format type.;\r\n    y_align: width alignment of channel y.;\r\n    w_align: width alignment of channel uv.;\r\n    channel_align: alignment of channel.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "pixel_format", "type": "StrAttr" },
      { "name": "aligned", "type": "BoolAttr" },
      { "name": "y_align", "type": "I64Attr" },
      { "name": "w_align", "type": "I64Attr" },
      { "name": "channel_align", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.CumSum",
    "summary": "CumSum operator",
    "description": "1.Op Introduction\r\n    Returns the cumulative sum of elements of input in the dimension dim.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = \\sum{j=0, i}input[j]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    dim(w.): If set to 0, computed across rows, If set to 1, computed across columns.;\r\n\r\n    4.attributes\r\n    axis: the dimension of the input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "dim", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.Custom",
    "summary": "Custom operator",
    "description": "1.Op Introduction\r\n    Custom operator\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = CustomFunction(inputs, name, params)\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;\r\n\r\n    4.attributes\r\n    name: the name of the custom operation to be executed.;\r\n    params: A dictionary of parameters.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" },
      { "name": "params", "type": "DictArrayAttr" }
    ]
  },
  {
    "name": "top.Deconv",
    "summary": "Deconvolution operator",
    "description": "1.Op Introduction\r\n    Perform Deconvolution operation.\r\n\r\n    2.Math formula\r\n    The height and width of the output tensor can be calculated using the following formulas:\r\n    ```math\r\n            H_{out} = H_{in - 1} x stride[0] - 2 x pads[0] + H_k + output_padding[0]\r\n            W_{out} = W_{in - 1} x stride[1] - 2 x pads[1] + W_k + output_padding[1]\r\n    ```\r\n    The output tensor is computed as:\r\n    ```math\r\n            output(N, C_out, H_out, W_out) = \\sum(c_in) {\\sum(h_k) {\\sum(w_k){input(n, c_in, h_in, w_in) x filter(c_out, c_in, h_k, w_k)}}}\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    filter(w.): the learnable weights of the convolution 2d operation.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attributes\r\n    kernel_shape: the size of the convolution kernel (filter) as an array. ;\r\n    strides: the stride for the cross-correlation, a single number or a tuple.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    group: (optional)Number of blocked connections from input channels to output channels. Default: 1.;\r\n    dilations: controls the spacing between the kernel points;\r\n    output_padding: The value can be provided as a single integer or a tuple, allowing for different padding values for height and width.;\r\n    dynweight_reorderd: whether the weights (filters) should be reordered dynamically.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "filter", "type": "AnyTensor" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "group", "type": "DefaultValuedAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "output_padding", "type": "OptionalAttr" },
      { "name": "dynweight_reorderd", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.DeformConv2D",
    "summary": "Deformable Convolution Operator",
    "description": "1.Op Introduction\r\n    In the simplest case, the output value of the layer with input size\r\n    $$(N, C_{\\text{in}}, H, W)$$ and output $$(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$$\r\n    can be precisely described as:\r\n\r\n    2.Math formula\r\n        - Input: $$(N, C_{in}, H_{in}, W_{in})$$\r\n        - Output: $$(N, C_{out}, H_{out}, W_{out})$$ where\r\n            weight (Tensor): the learnable weights of the module of shape\r\n            $$(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},\r\n            \\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})$$\r\n\r\n            offset (Tensor): the learnable offsets of the module of shape\r\n            $$(\\text{N}, \\times{\\text{2}}{\\text{offset\\_groups}{\\text{kernel\\_size[0]}}{\\text{kernel\\_size[1]}}},\r\n            \\text{H_{\\text{out}}}, \\text{W_{\\text{out}}})$$\r\n\r\n            mask (Tensor): the learnable masks of the module of shape\r\n            $$(\\text{N}, \\times{\\text{offset\\_groups}{\\text{kernel\\_size[0]}}{\\text{kernel\\_size[1]}}},\r\n            \\text{H_{\\text{out}}}, \\text{W_{\\text{out}}})$$\r\n\r\n            bias (Tensor optional): the learnable bias of the module of shape (out_channels).\r\n\r\n          ```math\r\n              H_{out} = \\left\\lfloor\\frac{H_{in}  + \\text{padding}[0] + \\text{padding}[2] - \\text{dilation}[0]\r\n                        \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\r\n          ```\r\n          ```math\r\n              W_{out} = \\left\\lfloor\\frac{W_{in}  + \\text{padding}[1] + \\text{padding}[3] - \\text{dilation}[1]\r\n                        \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\r\n          ```\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    filter(w.): the learnable weights of the convolution 2d operation.;\r\n    offset(w.): the learnable offsets of the module of shape.;\r\n    mask(w.): the learnable masks of the module of shape.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attributes\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    strides: the stride for the cross-correlation, a single number or a tuple.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    group: (optional)Number of blocked connections from input channels to output channels. Default: 1.;\r\n    deform_group: (optional)Number of blocked connections from input channels to output channels. Default: 1.;\r\n    use_mask: whether use mask for input tensor.;\r\n    dilations: controls the spacing between the kernel points;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "filter", "type": "AnyTensor" },
      { "name": "offset", "type": "AnyTensor" },
      { "name": "mask", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "group", "type": "DefaultValuedAttr" },
      { "name": "deform_group", "type": "DefaultValuedAttr" },
      { "name": "use_mask", "type": "DefaultValuedAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.DepackRaw",
    "summary": "Postprocess for raw image.",
    "description": "1.Op Introduction\r\n    postprocess raw image from 4 channels to mixed pixel pattern,\r\n    remove padding first if padded before,\r\n    then for each channel depack to 2 * 2 image block.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Depack(RemovePadding(input, padding_h, padding_w), channel_order)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    padding_h: The height of the padding.;\r\n    padding_w: The width of the padding.;\r\n    white_level: The maximum intensity value for white in the image data.;\r\n    black_level: The minimum intensity value for black in the image data.;\r\n    channel_order: The order of color channels in the input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "padding_h", "type": "I64Attr" },
      { "name": "padding_w", "type": "I64Attr" },
      { "name": "white_level", "type": "F64Attr" },
      { "name": "black_level", "type": "F64Attr" },
      { "name": "channel_order", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "top.Depth2Space",
    "summary": "Depth2Space operator",
    "description": "1.Op Introduction\r\n    Refer to `https://github.com/onnx/onnx/blob/main/docs/Operators.md#depthtospace`\r\n    [n, c, h, w] => [n, c / (block_h * block_w), h * block_h, w * block_w];\r\n    if inversed, [n, c, h, w] => [n, c * block_h * block_w, h / block_h, w / block_w];\r\n    if DCR(depth-column-row), channel ordered by block_h * block_w * c;\r\n    else CRD(column-row-depth), channel ordered by c * block_h * block_w;\r\n    The format of input or output is NCHW or NHWC.\r\n\r\n    2.Math formula\r\n\r\n    (1)Standard Transformation:\r\n    Given an input tensor of shape ( (N, C, H, W) ):\r\n    The output tensor after applying the Depth2Space operation can be calculated as:\r\n    ```math\r\n        {output}(N_i, C_j', H_k, W_l) = input(N_i, C_j, k / block_h, l / block_w)\r\n    ```\r\n    where k / block_h and l / block_w are rounded down.\r\n\r\n    (2)Inverse Transformation:\r\n    Given an input tensor of shape ( (N, C, H, W) ):\r\n    The output tensor after applying the Depth2Space operation can be calculated as:\r\n    ```math\r\n        {output}(N_i, C_j, H_k, W_l) = input(N_i, C_j', (k x block_h + j / (C / (block_h x block_w))), (l x block_w + j % (C / (block_h x block_w))))\r\n    ```\r\n    where C / (block_h x block_w) is rounded down.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    block_h: The height of the blocks used to rearrange the depth into spatial dimensions.;\r\n    block_w: The width of the blocks used to rearrange the depth into spatial dimensions.;\r\n    is_CRD: whether the channel ordering is in Column-Row-Depth format.;\r\n    is_inversed: whether the channel ordering is in Column-Row-Depth format.;\r\n    in_is_NCHW: whether the input tensor is in NCHW format.;\r\n    out_is_NCHW: whether the output tensor should be in NCHW format.;\r\n    swap_cr: swaps the height and width dimensions in the output tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "block_h", "type": "I64Attr" },
      { "name": "block_w", "type": "I64Attr" },
      { "name": "is_CRD", "type": "BoolAttr" },
      { "name": "is_inversed", "type": "BoolAttr" },
      { "name": "in_is_NCHW", "type": "DefaultValuedAttr" },
      { "name": "out_is_NCHW", "type": "DefaultValuedAttr" },
      { "name": "swap_cr", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.DequantInt",
    "summary": "dequant operation",
    "description": "1.Op Introduction\r\n    Dequant 8 bit data to 32/16 bit data.\r\n\r\n    2.Math formula\r\n    32/16bit(output) = DequantIntOp((8bit(input), shift) x multiplier) ≪ lshift;\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    multiplier: Floating-point multiplication operations are usually converted to fixed-point multiplication operations.;\r\n    shift: a shift value applied to the quantized data before scaling.;\r\n    lshift: a left shift operation applied to the dequantized data after scaling.;\r\n    quant_mode: the mode or method used for quantization during the requantization operation.;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "multiplier", "type": "I64ArrayAttr" },
      { "name": "shift", "type": "I64ArrayAttr" },
      { "name": "lshift", "type": "DefaultValuedAttr" },
      { "name": "quant_mode", "type": "DequantModeAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.DequantizeLinear",
    "summary": "Linear dequantize operation",
    "description": "1.Op Introduction\r\n    DequantizeLinear(x) := (x - x_zero_point) * x_scale\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = (input - x_zero_point) * x_scale\r\n    ```\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    x_scale: convert the quantized values back original;\r\n    x_zero_point: adjust the zero point of quantized values, represents4 that corresponds to the original value of zero.;\r\n    axis: the dimension of the input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "x_scale", "type": "F64ArrayAttr" },
      { "name": "x_zero_point", "type": "I32ArrayAttr" },
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.DetectionOutput",
    "summary": "DetectionOutput operation",
    "description": "1.Op Introduction\r\n    takes the predicted bounding boxes, class scores, and other relevant information to produce the final detection results.\r\n\r\n    2.Math formula\r\n    ```math\r\n        1.Raw Detection Output\r\n            raw_output = {(b_i, c_i) | i = 1, 2,...,N}\r\n            b_i is the bounding box coordinates, c_i is the i-th confidence score.\r\n        2.Apply Confidence threshold\r\n            filtered_output = {(b_i, c_i) | c_i >= confidence_threshold}\r\n        3.Non-Maximum Suppression(NMS)\r\n            nms_output = NMS(filtered_output, nms_threshold)\r\n        4.Top K detections\r\n            output = top_k(nms_output, top_k)\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor;\r\n\r\n    4.attributes\r\n    num_classes: total number of classes, including the background class.;\r\n    background_label_id: background class, differentiate between detected objects and the background.;\r\n    nms_threshold: The threshold used for Non-Maximum Suppression (NMS).;\r\n    top_k: The maximum number of predictions to be considered for each image.;\r\n    code_type: the encoding type for the bounding box coordinates.;\r\n    keep_top_k: The number of top scoring detections to keep after applying NMS.;\r\n    confidence_threshold: The minimum confidence score required for a detection to be considered valid.;\r\n    share_location: whether the bounding box locations are shared across different classes.;\r\n    variance_encoded_in_target: whether the variance for bounding box predictions is encoded in the target.;\r\n    eta: adjusts the confidence scores during NMS.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "num_classes", "type": "I64Attr" },
      { "name": "background_label_id", "type": "DefaultValuedAttr" },
      { "name": "nms_threshold", "type": "F64Attr" },
      { "name": "top_k", "type": "I64Attr" },
      { "name": "code_type", "type": "DetectionOutputCodeTypeAttr" },
      { "name": "keep_top_k", "type": "I64Attr" },
      { "name": "confidence_threshold", "type": "F64Attr" },
      { "name": "share_location", "type": "DefaultValuedAttr" },
      { "name": "variance_encoded_in_target", "type": "DefaultValuedAttr" },
      { "name": "eta", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Div",
    "summary": "Div operator",
    "description": "1.Op Introduction\r\n    Performs element-wise binary division.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = \\frac{inputs[i]}{divisor}\r\n            if is_reverse == True;\r\n            output[i] = \\frac{divisor}{inputs[i]}\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;\r\n\r\n    4.attributes\r\n    is_reverse: whether the subtraction operation is performed in reverse order.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    is_scalar: whether the addition operation is performed with scalar values or tensor.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "is_reverse", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "is_scalar", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.DivConst",
    "summary": "Div Const operator",
    "description": "1.Op Introduction\r\n    The DivConst operator is designed to perform element-wise division of an input tensor by a constant value.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = input/const_val or const_val/input\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    const_val: the constant value to be added to each element of the input tensor(positive, negative, or zero).;\r\n    is_reverse: whether the subtraction operation is performed in reverse order.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    is_scalar: whether the addition operation is performed with scalar values or tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "const_val", "type": "F64Attr" },
      { "name": "is_reverse", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "is_scalar", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.DtypeCast",
    "summary": "Cast F32 to F16",
    "description": "1.Op Introduction\r\n    Cast F32 to F16\r\n\r\n    2.Math formula\r\n    FLOAT16(output) = DtypeCastOp (FLOAT32(input));\r\n\r\n    3.activation\r\n    input(act.): input tensor;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Einsum",
    "summary": "Einsum operator",
    "description": "1.Op Introduction\r\n    # https://pytorch.org/docs/1.13/generated/torch.einsum.html?highlight=einsum#torch.einsum\r\n    The Einsum operator implements Einstein summation notation, which provides a concise way to specify tensor operations.\r\n\r\n    2.Math formula\r\n    ```math\r\n        \\mathrm{Output}=\\sum_{i\\in I}A_iB_j\\ldots\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    mode: determines how the input tensors will be combined based on the specified subscripts. ;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "EinsumModeAttr" }
    ]
  },
  {
    "name": "top.Elu",
    "summary": "Elu operation",
    "description": "1.Op Introduction\r\n    Elu takes input data (Tensor<T>) and an argument alpha,\r\n    and produces one output data (Tensor<T>)\r\n    where the function f(x) = alpha * (e^x - 1) for x <= 0, f(x) = x for x > 0,\r\n    is applied to the data tensor elementwise.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = alpha * (e ^ input - 1) if input[i] < 0 && input[i] == 0\r\n                        input[i]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    alpha: scalar.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "alpha", "type": "F64Attr" }
    ]
  },
  {
    "name": "top.EmbDenseBwd",
    "summary": "EmbDenseBwd operation for train",
    "description": "1.Op Introduction\r\n    layer normalization\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = grad_output[indices]\r\n    ```\r\n\r\n    3.activation and weight\r\n    grad_output(act.): the gradient of the loss with respect to the output.;\r\n    indices(w.): the indices of the input tokens or items.;\r\n\r\n    4.attributes\r\n    num_weights: the total number of embedding weights.;",
    "inputs": [
      { "name": "grad_output", "type": "AnyTensor" },
      { "name": "indices", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "num_weights", "type": "SI32Attr" }
    ]
  },
  {
    "name": "top.Erf",
    "summary": "Erf operation",
    "description": "1.Op Introduction\r\n    Computes the error function of the given input tensor element-wise.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Erf(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Exp",
    "summary": "Exp operator",
    "description": "1.Op Introduction\r\n    Calculates the exponent of the given input tensor, element-wise.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = exp(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Expand",
    "summary": "Expand operator",
    "description": "1.Op Introduction\r\n    Broadcast the input tensor following the given shape and the broadcast rule.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i1, i2, i3,...in] = input[j1, j2, j3,...jn]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    shape: An array of the target shape to which the input tensor will be expanded.;\r\n    shapeT: An optional tensor that can be used to specify the shape dynamically.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "shapeT", "type": "Optional" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "shape", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.FAttention",
    "summary": "Flash Attention operator",
    "description": "Performs a two dimensional matrix multiplication. This allows both inputs to\r\n    be activations, rather than reserving weights as an attribute in the\r\n    FULLY_CONNECTED operator.",
    "inputs": [
      { "name": "queries", "type": "AnyTensor" },
      { "name": "keys", "type": "AnyTensor" },
      { "name": "values", "type": "AnyTensor" },
      { "name": "mask", "type": "AnyTensorOrNone" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "scale", "type": "F64Attr" },
      { "name": "batch", "type": "I64Attr" },
      { "name": "q_head", "type": "I64Attr" },
      { "name": "kv_head", "type": "I64Attr" },
      { "name": "dim", "type": "I64Attr" },
      { "name": "mq", "type": "I64Attr" },
      { "name": "mk", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.Flatten",
    "summary": "Flatten operation",
    "description": "1.Op Introduction\r\n    gen by torch aten::flatten or onnx, the flatten operation collapses the specified dimensions of the tensor into a single dimension,\r\n    effectively reducing the number of dimensions of the tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = FlattenOp(input, start_dim, end_dim)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    start_dim: the first dimension to flatten.;\r\n    end_dim: the last dimension to flatten.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "start_dim", "type": "DefaultValuedAttr" },
      { "name": "end_dim", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Floor",
    "summary": "Floor operation",
    "description": "1.Op Introduction\r\n    the Floor function rounds down each element of the input tensor to the nearest integer less than or equal to that element.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = floor(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "is_scalar", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.FrcnDetection",
    "summary": "Faster rcnn detection operator",
    "description": "1.Op Introduction\r\n    outputs the detected classes and their corresponding bounding boxes,\r\n    filtering results based on specified thresholds.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = FrcnDetection(inputs, class_num, obj_threshold, nms_threshold, keep_topk)\r\n    ```\r\n    3.activation and weight\r\n    inputs(act.): input tensor;\r\n\r\n    4.attributes\r\n    class_num: detection class num.;\r\n    obj_threshold: object threshold.;\r\n    nms_threshold: nms threshold.;\r\n    keep_topk: keep top k.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "class_num", "type": "I64Attr" },
      { "name": "obj_threshold", "type": "F64Attr" },
      { "name": "nms_threshold", "type": "F64Attr" },
      { "name": "keep_topk", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.Gather",
    "summary": "Gather operator",
    "description": "1.Op Introduction\r\n    Perform Gather operation on the given axis.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = input[indices]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    indices(w.): the indices of the elements to be gathered from the input tensor. ;\r\n\r\n    4.attributes\r\n    keepdims: whether to retain the dimensions of the input tensor in the output.\r\n               If true, will have the same number of dimensions as the input tensor.;\r\n    axis: the dimension of the input tensor.;\r\n    is_scalar: whether the addition operation is performed with scalar values or tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "indices", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "is_scalar", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.GatherElements",
    "summary": "GatherElements operator",
    "description": "1.Op Introduction\r\n    Perform GatherElements operation on the given axis.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i_1, i_2, i_3,...i_k] = input[i_1, i_2, i_3,..., indices[i_k]]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    indices(w.): the indices of the elements to be gathered from the input tensor. ;\r\n\r\n    4.attributes\r\n    axis: the dimension of the input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "indices", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.GatherND",
    "summary": "GatherND operator",
    "description": "1.Op Introduction\r\n    This operator is the inverse of ScatterND.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output_i = input[indices_i]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    indices(w.): which elements to gather from the input tensor.;\r\n\r\n    4.attributes\r\n    indice_dims: the number of dimensions in the indices tensor.;\r\n    batch_dims: the number of batch dimensions in the input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "indices", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "indice_dims", "type": "OptionalAttr" },
      { "name": "batch_dims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.GELU",
    "summary": "GELU operator,  0.5x * (1.0 + tf.erf(x / tf.sqrt(2.0)))",
    "description": "1.Op Introduction\r\n    An activation function based on Gaussian error function.;\r\n\r\n    2.Math formula\r\n    ```math\r\n        Y = 0.5*input * (1.0 + tf.erf(input / tf.sqrt(2.0)))\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;\r\n    approx_mode: include three mode.\r\n                  normal: This mode uses the exact mathematical form of the GELU function;\r\n                  tanh: This mode uses tanh to speed up computation;\r\n                  sigm: This mode uses sigmoid to speed up computation;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "approx_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.GridSampler",
    "summary": "GridSampler operation",
    "description": "1.Op Introduction\r\n    Given an input and a flow-field grid, computes the output\r\n    using input values and pixel locations from grid.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[N, C, H', W'] = input[C, grid[N, H', W', 1], grid[N, H', W', 0]]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    grid(w.): The flow-field grid tensor that defines the pixel locations for sampling.;\r\n\r\n    4.attributes\r\n    mode: the type of binary operation to be performed, addition, subtraction, or other types of binary operations.;\r\n    padding_mode: padding mode for outside grid values, Int attribute [0, 1, 2],\r\n                                representing 'zero' | 'boundary' | 'reflection't.;\r\n    align_corners: whether to align the corners of the input and output tensors.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "grid", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "I64Attr" },
      { "name": "padding_mode", "type": "I64Attr" },
      { "name": "align_corners", "type": "BoolAttr" }
    ]
  },
  {
    "name": "top.GroupNorm",
    "summary": "GroupNorm operation",
    "description": "1.Op Introduction\r\n    group normalization\r\n\r\n    2.Math formula\r\n    ```math\r\n            mean_g = 1 / ((C / num_groups) * H * W) \\sum{i=1, C/num_groups} \\sum{j=1, H} \\sum{k=1, W} input_{n,c,j,k}\r\n            var_g = 1 / ((C / num_groups) * H * W) \\sum{i=1, C/num_groups} \\sum{j=1, H} \\sum{k=1, W} (input_{n,c,j,k} - mean_g) ^ 2\r\n            output_{n,c,j,k} = weight * (input_{n,c,j,k} - mean_g) / sqrt(var_g + eps) + bias\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    weight(w.): weight tensor;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attributes\r\n    eps: a small constant added to the denominator during normalization to prevent division by zero.;\r\n    num_groups: the number of groups to divide the input channels into for normalization.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "weight", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "num_groups", "type": "I64Attr" },
      { "name": "eps", "type": "F64Attr" }
    ]
  },
  {
    "name": "top.GroupNormTrain",
    "summary": "GroupNorm operation",
    "description": "1.Op Introduction\r\n    group normalization\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = \\frac{input - mean}{\\sqrt{\\sigma^2 + eps}} \\cdot weight + bias\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    weight(w.): weight tensor.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attributes\r\n    eps: a small constant added to the denominator during normalization to prevent division by zero.;\r\n    num_groups: number of groups to divide the channels into for normalization.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "weight", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" },
      { "name": "mean", "type": "AnyTensor" },
      { "name": "rstd", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "num_groups", "type": "I64Attr" },
      { "name": "eps", "type": "F64Attr" }
    ]
  },
  {
    "name": "top.GRU",
    "summary": "GRU operator",
    "description": "1.Op Introduction\r\n    Perform RNN GRU operation.\r\n\r\n    2.Math formula\r\n    ```math\r\n        update gate(z_t):\r\n            z_t = Sigma(W_z · x_t + U_z ·h_(t-1) + b_z)\r\n        reset gate(r_t):\r\n            r_t = Sigma(W_r · x_t + U_r ·h_(t-1) + b_r)\r\n        Candidate Activation(h_t):\r\n            h_t = tanh(W_h · x_t + r_t \\odot (U_h · h_(t-1)) + b_h)\r\n        final output:\r\n            output = (1 - z_t) \\odot h_(t-1) + z_t \\odot h_t\r\n    ```\r\n    where, x_t is the input at time step (t), h_(t-1) is the hidden state from the previous time step.\r\n           W_z, W_r, W_h are the weight matrices for the input.\r\n           U_z, U_r, U_h are the weight matrices for the hidden state.\r\n           b_z, b_r, b_h are the bias vectors.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    filter(w.): the learnable weights of the convolution 2d operation.;\r\n    recurrence(w.): the previous hidden state influences the current hidden state.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n    initial_h(w.): the initial hidden state, which can be provided to start the GRU computation.;\r\n\r\n    4.attributes\r\n    hidden_size: the number of units in the GRU cell,;\r\n    bidirectional: whether the GRU should be bidirectional;\r\n    linear_before_reset: whether to apply a linear transformation to the input before applying the reset gate.;\r\n    batch_first: the input and output tensors are provided in the shape (batch_size, seq_length, input_size).;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "filter", "type": "AnyTensor" },
      { "name": "recurrence", "type": "AnyTensor" },
      { "name": "bias", "type": "AnyTensorOrNone" },
      { "name": "initial_h", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTensorOrNone" },
      { "name": "Y_h", "type": "AnyTensorOrNone" }
    ],
    "attributes": [
      { "name": "hidden_size", "type": "I64Attr" },
      { "name": "bidirectional", "type": "BoolAttr" },
      { "name": "linear_before_reset", "type": "DefaultValuedAttr" },
      { "name": "batch_first", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.HardSigmoid",
    "summary": "HardSigmoid operation",
    "description": "1.Op Introduction\r\n    a piecewise linear function to the input tensor element-wise.\r\n    hardsigmoid(x; alpha, beta) := min(max(alpha*x + beta, 0), 1).\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = min(max(alpha * input[i] + beta, 0), 1)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    alpha: scalar;\r\n    beta: scalar;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "alpha", "type": "F64Attr" },
      { "name": "beta", "type": "F64Attr" }
    ]
  },
  {
    "name": "top.HardSwish",
    "summary": "HardSwish operation",
    "description": "1.Op Introduction\r\n    hardswish(x) := x * hardsigmoid(x; 1/6, 0.5)\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = input[i] * min(max(1/6 * input[i] + 0.5, 0), 1)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.If",
    "summary": "if operation",
    "description": "1.Op Introduction\r\n    If conditional\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = then_branch if cond is true\r\n                     else_branch if cond is false\r\n    ```\r\n\r\n    3.activation and weight\r\n    cond(act.): which branch of execution to follow.;",
    "inputs": [
      { "name": "cond", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ]
  },
  {
    "name": "top.IndexPut",
    "summary": "Index_put_ operation",
    "description": "1.Op Introduction\r\n    update specific elements of an input tensor at given indices with new values.\r\n\r\n    2.Math formula\r\n    ```math\r\n        if accumulate\r\n            input[indices] += values\r\n        else\r\n            input[indices] = values\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    indices(w.): the indices of the elements in the input tensor that should be updated.;\r\n    values(w.): the new values that will replace the existing values in the input tensor at the specified indices.;\r\n\r\n    4.attributes\r\n    accumulate: whether the operation should accumulate values at the specified indices or replace them.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "indices", "type": "AnyTensor" },
      { "name": "values", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "accumulate", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Input",
    "summary": "Input operator",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "shape_tensor", "type": "OptionalAttr" },
      { "name": "do_preprocess", "type": "DefaultValuedAttr" },
      { "name": "pixel_format", "type": "OptionalAttr" },
      { "name": "channel_format", "type": "OptionalAttr" },
      { "name": "resize_dims", "type": "OptionalAttr" },
      { "name": "keep_aspect_ratio", "type": "OptionalAttr" },
      { "name": "keep_ratio_mode", "type": "OptionalAttr" },
      { "name": "pad_value", "type": "OptionalAttr" },
      { "name": "pad_type", "type": "OptionalAttr" },
      { "name": "scale", "type": "OptionalAttr" },
      { "name": "mean", "type": "OptionalAttr" },
      { "name": "customization_format", "type": "OptionalAttr" },
      { "name": "aligned", "type": "OptionalAttr" },
      { "name": "yuv_type", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "top.InstanceNorm",
    "summary": "Instance Norm operation",
    "description": "1.Op Introduction\r\n    instance normalization.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output[i] = weight * (input[i] - mean) / sprt(var + eps) + bias\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    weight(w.): weight tensor;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attributes\r\n    eps: a small constant added to the denominator during normalization to prevent division by zero.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "weight", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "eps", "type": "F64Attr" }
    ]
  },
  {
    "name": "top.Interp",
    "summary": "Interp operation",
    "description": "1.Op Introduction\r\n    Perform linear upsample on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n            H' = H x scale_h\r\n            W' = W x scale_w\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    target_shape(w.): the desired shape of the output tensor after interpolation.;\r\n\r\n    4.attributes\r\n    mode: the type of binary operation to be performed, addition, subtraction, or other types of binary operations.;\r\n    coord_mode: whether the coordinates are normalized (ranging from 0 to 1) or absolute (based on pixel indices).;\r\n    scale_h: the scaling factor for the height (number of rows) of the input tensor.;\r\n    scale_w: the scaling factor for the width (number of columns) of the input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "target_shape", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "InterpModeAttr" },
      { "name": "coord_mode", "type": "InterpCoordModeAttr" },
      { "name": "scale_h", "type": "DefaultValuedAttr" },
      { "name": "scale_w", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.LayerNorm",
    "summary": "LayerNorm operation",
    "description": "1.Op Introduction\r\n    layer normalization\r\n\r\n    2.Math formula\r\n    ```math\r\n        1.Normalization\r\n            mean = 1 / H \\sum{j=1, H} input[j]\r\n            var = 1 / H \\sum{j=1, H} (input[j] - mean) ^ 2\r\n        2.Layer Normalized Output\r\n            output[i] = weight * (input[i] - mean) / sprt(var + eps) + bias\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    weight(w.): weight tensor;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attributes\r\n    normalized_shape: the shape of the input tensor that will be normalized.;\r\n    axis: the dimension of the input tensor.;\r\n    eps: a small constant added to the denominator during normalization to prevent division by zero.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "weight", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "normalized_shape", "type": "I64ArrayAttr" },
      { "name": "axis", "type": "SI32Attr" },
      { "name": "eps", "type": "F64Attr" }
    ]
  },
  {
    "name": "top.LayerNormBwd",
    "summary": "LayerNorm operation for train",
    "description": "1.Op Introduction\r\n    layer normalization\r\n\r\n    2.Math formula\r\n    ```math\r\n        1.gradient input\r\n            grad_input = 1 / N(weight / sqrt(variance + eps) 1 / N \\sum{i=1, N}grad_out)\r\n                            - (input - mean) / N * \\sum(i=1, N)weight * grad_out / sqrt(variance + eps)\r\n        2.gradient weight\r\n            grad_weight = \\sum(i=1, N)grad_out * (input - mean) / sqrt(variance + eps)\r\n        3.gradient bias\r\n            grad_bias = \\sum(i=1, N)grad_out\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    grad_out(w.): the gradient of the loss with respect to the output of the layer normalization.;\r\n    mean(w.): mean values to subtract from each channel for normalization.;\r\n    variance(w.): adjust the predicted boxes during the training process.;\r\n    weight(w.): weight tensor.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attributes\r\n    normalized_shape:  the shape of the input tensor dimensions.;",
    "inputs": [
      { "name": "grad_out", "type": "AnyTensor" },
      { "name": "input", "type": "AnyTensor" },
      { "name": "mean", "type": "AnyTensor" },
      { "name": "variance", "type": "AnyTensor" },
      { "name": "weight", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "grad_input", "type": "AnyTensorOrNone" },
      { "name": "grad_weight", "type": "AnyTensorOrNone" },
      { "name": "grad_bias", "type": "AnyTensorOrNone" }
    ],
    "attributes": [
      { "name": "normalized_shape", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "top.LayerNormTrain",
    "summary": "LayerNorm operation for train",
    "description": "1.Op Introduction\r\n    layer normalization in train.\r\n\r\n    2.Math formula\r\n    ```math\r\n        1.Normalization\r\n            mean = 1 / H \\sum{j=1, H} input[j]\r\n            var = 1 / H \\sum{j=1, H} (input[j] - mean) ^ 2\r\n        2.Layer Normalized Output\r\n            output[i] = weight * (input[i] - mean) / sprt(var + eps) + bias\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    weight(w.): weight tensor.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attributes\r\n    normalized_shape: the shape of the input tensor dimensions.;\r\n    axis: the dimension of the input tensor.;\r\n    eps: a small constant added to the denominator during normalization to prevent division by zero.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "weight", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" },
      { "name": "mean", "type": "AnyTensor" },
      { "name": "variance", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "normalized_shape", "type": "I64ArrayAttr" },
      { "name": "axis", "type": "SI32Attr" },
      { "name": "eps", "type": "F64Attr" }
    ]
  },
  {
    "name": "top.LeakyRelu",
    "summary": "LeakyRelu operation",
    "description": "1.Op Introduction\r\n    LeakyRelu takes input data (Tensor<T>) and an argument alpha,\r\n    and produces one output data (Tensor<T>)\r\n    where the function f(x) = alpha * x for x < 0, f(x) = x for x >= 0,\r\n    is applied to the data tensor elementwise.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = alpha * input, if input < 0\r\n            output = input, if input >= 0\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    alpha: a scalar factor.;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "alpha", "type": "F64Attr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.List",
    "summary": "List operator",
    "description": "1.Op Introduction\r\n    gen by torch prim::ListConstruct, y = [a, b]\r\n    output shape is [1]\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = [input1, input2, ..., inputN]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): Variadic input tensor.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Log",
    "summary": "Log operator",
    "description": "1.Op Introduction\r\n    Calculates the natural log of the given input tensor, element-wise.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = ln(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.LogB",
    "summary": "LogB operator",
    "description": "1.Op Introduction\r\n    Calculates the log of the given input tensor to the base B, element-wise.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = ln(input) / ln(B)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "base", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.LogicalAnd",
    "summary": "logical and operation",
    "description": "1.Op Introduction\r\n    logical and operation between two variables\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = input1 and input2\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Loop",
    "summary": "Loop operation",
    "description": "Generic Looping construct. This loop has multiple termination conditions:\r\n\r\n  1. Trip count. Iteration count specified at runtime. Set by\r\n     specifying the input M. Optional. Set to empty string to omit.\r\n     Note that a static trip count (specified at graph construction time) can be\r\n     specified by passing in a constant node for input M.\r\n  2. Loop termination condition. This is an input to the op that determines\r\n     whether to run the first iteration and also a loop-carried dependency for\r\n     the body graph. The body graph must yield a value for the condition variable,\r\n     whether this input is provided or not.\r\n\r\n  This table summarizes the operating modes of this operator with equivalent\r\n  C-style code:\r\n\r\n      Operator inputs defined as (max_trip_count, condition_var).\r\n\r\n      input (\\\"\\\", \\\"\\\"):\r\n          for (int i=0; ; ++i) {\r\n            cond = ... // Note this value is ignored, but is required in the body\r\n          }\r\n\r\n      input (\\\"\\\", cond) // Note this is analogous to a while loop\r\n          bool cond = ...;\r\n          for (int i=0; cond; ++i) {\r\n            cond = ...;\r\n          }\r\n\r\n      input (\\\"\\\", 1) // Note this is analogous to a do-while loop\r\n          bool cond = true\r\n          for (int i=0; cond; ++i) {\r\n            cond = ...;\r\n          }\r\n\r\n      input (trip_count, \\\"\\\") // Note this is analogous to a for loop\r\n          int trip_count = ...\r\n          for (int i=0; i < trip_count; ++i) {\r\n            cond = ...; // ignored\r\n          }\r\n\r\n      input (trip_count, cond)\r\n          int trip_count = ...;\r\n          bool cond = ...;\r\n          for (int i=0; i < trip_count && cond; ++i) {\r\n            cond = ...;\r\n          }\r\n\r\n\r\n  *Sample usage - cond as well as trip count*\r\n\r\n      graph predict-net {\r\n        %a = Constant[value = <Scalar Tensor [3]>]()\r\n        %b = Constant[value = <Scalar Tensor [6]>]()\r\n        %keepgoing = Constant[value = <Scalar Tensor [1]>]()\r\n        %max_trip_count = Constant[value = <Scalar Tensor [10]>]()\r\n        %keepgoing_out, %b_out, %user_defined_vals = Loop[body = <graph body-net>](%max_trip_count, %keepgoing, %b)\r\n        return\r\n      }\r\n\r\n      graph body-net (\r\n        %i[INT32, scalar]           // iteration number\r\n        %keepgoing_in[BOOL, scalar] // incoming loop-termination-condition; not used\r\n        %b_in[INT32, scalar]        // incoming value of loop-carried-dependency b\r\n      ) {\r\n        %my_local = Add(%a, %b_in)\r\n        %b_out = Sub(%a, %b_in) // outgoing value of loop-carried-dependency b\r\n        %keepgoing_out = Greater(%my_local, %b_out) // outgoing loop-termination-condition\r\n        %user_defined_val = Add(%b_in, %b_in) // scan-output value to be accumulated\r\n        return %keepgoing_out, %b_out, %user_defined_val\r\n      }\r\n\r\n  *Sample equivalent C code*\r\n\r\n      {\r\n        /* User-defined code (enclosing scope) */\r\n        int a = 3, b = 6;\r\n        bool keepgoing = true; // Analogous to input cond\r\n        /* End user-defined code */\r\n\r\n        /* Implicitly-defined code */\r\n        const int max_trip_count = 10; // Analogous to input M\r\n        int user_defined_vals[]; // Imagine this is resizable\r\n        /* End implicitly-defined code */\r\n        /* initialize loop-carried variables and scan-output variables */\r\n        bool keepgoing_out = keepgoing\r\n        int b_out = b\r\n\r\n        for (int i=0; i < max_trip_count && keepgoing_out; ++i) {\r\n          /* Implicitly-defined code: bind actual parameter values\r\n             to formal parameter variables of loop-body */\r\n          bool keepgoing_in = keepgoing_out;\r\n          bool b_in = b_out;\r\n\r\n          /* User-defined code (loop body) */\r\n          int my_local = a + b_in; // Reading value \\\"a\\\" from the enclosing scope is fine\r\n          b_out = a - b_in;\r\n          keepgoing_out = my_local > b_out;\r\n          user_defined_val = b_in + b_in; // b_in and b_out are different variables\r\n          /* End user-defined code */\r\n\r\n          /* Implicitly defined-code */\r\n          user_defined_vals[i] = user_defined_val // accumulate scan-output values\r\n        }\r\n        // int t = my_local; // Can't do this. my_local is not accessible here.\r\n\r\n        // The values below are bound to the output variables of the loop and therefore accessible\r\n        // b_out; user_defined_vals; keepgoing_out;\r\n      }\r\n\r\n  There are several things of note in this code snippet:\r\n\r\n  1. Values from the enclosing scope (i.e. variable \\\"a\\\" here) are in scope and can\r\n     be referenced in the inputs of the loop.\r\n  2. Any values computed in the loop body that needs to be used in a subsequent\r\n     iteration or after the loop are modelled using a pair of variables in the loop-body,\r\n     consisting of an input variable (eg., b_in) and an output variable (eg., b_out).\r\n     These are referred to as loop-carried dependences. The loop operation node\r\n     supplies the input value of the input variable for the first iteration, and\r\n     returns the output value of the output variable produced by the final\r\n     iteration.\r\n  3. Scan_output variables are used to implicitly concatenate values computed across\r\n     all the iterations. In the above example, the value of user_defined_val computed\r\n     over all iterations are concatenated and returned as the value of user_defined_vals\r\n     after the loop.\r\n  4. Values created in the body cannot be accessed in the enclosing scope,\r\n     except using the mechanism described above.\r\n\r\n  Note that the semantics of this op support \\\"diagonal\\\" or \\\"wavefront\\\" execution.\r\n  (See Step 3 here for an example:\r\n  https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5/).\r\n  Frontends should emit multi-layer RNNs as a series of While operators (with\r\n  time being the inner looping dimension), with each successive layer consuming\r\n  the scan_outputs from the previous layer, possibly going through several\r\n  point-wise operators (e.g. dropout, residual connections, linear layer).\r\n\r\n  The input/output of subgraph (produced by loop node) matching is based on order instead of name. The implementation will figure out the names based on this order.",
    "inputs": [
      { "name": "M", "type": "AnyTypeOf" },
      { "name": "cond", "type": "AnyTypeOf" },
      { "name": "v_initial", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "v_final_and_scan_outputs", "type": "Variadic" }
    ]
  },
  {
    "name": "top.LRN",
    "summary": "Local Response Normalization",
    "description": "1.Op Introduction\r\n    It normalizes over local input regions. The local region is defined across the channels.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i, j, k] = \\frac{input[i, j, k]}{(bias + \\alpha \\sum_{c=\\max(0,k-\\text{size})}^{\\min(N-1,k+\\text{size})} input[i, j, c]^2)^{\\beta}}\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    size: how many neighboring channels are considered during the normalization process.;\r\n    alpha: a scaling factor;\r\n    beta: a scaling factor;\r\n    bias: A floating-point value added to the normalization denominator to prevent division by zero.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "size", "type": "I64Attr" },
      { "name": "alpha", "type": "DefaultValuedAttr" },
      { "name": "beta", "type": "DefaultValuedAttr" },
      { "name": "bias", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.LSTM",
    "summary": "LSTM operator",
    "description": "1.Op Introduction\r\n    Perform RNN LSTM operation.\r\n\r\n    2.Math formula\r\n    ```math\r\n        forget gate(f_t):\r\n            f_t = Sigma(W_f · x_t + U_f · h_(t-1) + b_f)\r\n        input gate(i_t):\r\n            i_t = Sigma(W_i · x_t + U_i · h_(t-1) + b_i)\r\n        Candidate cell state(C_t):\r\n            C_t = tanh(W_C · x_t + U_C · h_(t-1) + b_C)\r\n        cell state update(c_t):\r\n            c_t = f_t \\odot c_(t-1) + i_t \\odot C_t\r\n        output gate(o_t):\r\n            o_t = Sigma(W_o · x_t + U_o · h_(t-1) + b_o)\r\n        hidden state output(h_t):\r\n            h_t = o_t \\odot tanh(c_t)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    filter(w.): the learnable weights of the convolution 2d operation.;\r\n    recurrence(w.): the previous hidden state influences the current hidden state.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n    initial_h(w.): the initial hidden state, which can be provided to start the LSTM computation.;\r\n    initial_c(w.): the initial cell state, which can be provided to start the LSTM computation.;\r\n    cont(w.): control weights or additional context that may be provided to influence the LSTM's behavior.;\r\n\r\n    4.attributes\r\n    hidden_size: the number of units in the LSTM cell, ;\r\n    bidirectional: A boolean indicating whether the LSTM should be bidirectional;\r\n    batch_first: the input and output tensors are provided in the shape (batch_size, seq_length, input_size).;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "filter", "type": "AnyTensor" },
      { "name": "recurrence", "type": "AnyTensor" },
      { "name": "bias", "type": "AnyTensorOrNone" },
      { "name": "initial_h", "type": "AnyTensorOrNone" },
      { "name": "initial_c", "type": "AnyTensorOrNone" },
      { "name": "cont", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTensorOrNone" },
      { "name": "Y_h", "type": "AnyTensorOrNone" },
      { "name": "Y_c", "type": "AnyTensorOrNone" }
    ],
    "attributes": [
      { "name": "hidden_size", "type": "I64Attr" },
      { "name": "bidirectional", "type": "BoolAttr" },
      { "name": "batch_first", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Lut",
    "summary": "Lut operator",
    "description": "1.Op Introduction\r\n    lookup table in index [0-255], y[i] = table(x[i])\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = table(input[i])\r\n    ```\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    table(w.): map input values to corresponding output values.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "table", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.MaskedFill",
    "summary": "MaskedFill operation",
    "description": "1.Op Introduction\r\n    Return elements, either from X or Const, depending on condition.\r\n\r\n    2.Math formula\r\n    ```math\r\n                     brn                if inversed and cond=0\r\n            output = brn + const_val    if inversed and cond!=0\r\n                     brn + const_val    if !inversed and cond!=0\r\n                     brn                if !inversed and cond=0\r\n    ```\r\n        If inversed is true, the operation fills the elements of brn where cond is zero with const_val, while leaving other elements unchanged.\r\n        If inversed is false, the operation fills the elements of brn where cond is non-zero with const_val, while leaving other elements unchanged.\r\n\r\n    3.activation and weight\r\n    cond(act.): a tensor that serves as the condition for selecting elements from the true branch (tbrn) or the false branch (fbrn).;\r\n    brn(w.): the input tensor from which elements will be selected based on the condition provided by the cond tensor.;\r\n\r\n    4.attributes\r\n    inversed: whether the mask should be inverted.;\r\n    const_val: specifies the constant value to be added to each element of the input tensor(positive, negative, or zero).;",
    "inputs": [
      { "name": "cond", "type": "AnyTensor" },
      { "name": "brn", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "inversed", "type": "BoolAttr" },
      { "name": "const_val", "type": "F64Attr" }
    ]
  },
  {
    "name": "top.MaskRCNN_BboxPooler",
    "summary": "BBox_Pooler gen by PPL",
    "description": "1.Op Introduction\r\n    MaskRCNN_BBox_Pooler, the 1st ROIAlign in MaskRCNN.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = ROIAlign(feature map, rois_multi_batch)\r\n    ```\r\n\r\n    3.activation and weight\r\n    ptr_feat0(act.): Pointer to the feature map at level 0.;\r\n    ptr_feat1(act.): Pointer to the feature map at level 1.;\r\n    ptr_feat2(act.): Pointer to the feature map at level 2.;\r\n    ptr_feat3(act.): Pointer to the feature map at level 3.;\r\n    rois_multi_batch(w.): ROIs (Regions of Interest) for multiple batches.;\r\n\r\n    4.attributes\r\n    ROI_NUM_LEVELS: The number of levels in the ROI feature pyramid.;\r\n    ROI_H: The height of the pooled ROI features.;\r\n    ROI_W: The width of the pooled ROI features.;\r\n    CHANNEL_ROI: The number of channels in the pooled ROI features.;\r\n    ROI_SLICE: The number of slices or segments;\r\n    ROI_PH: The height of the ROI in the feature map.;\r\n    ROI_PW: The width of the ROI in the feature map.;\r\n    ROI_LEN: The length of the ROIs being processed.;",
    "inputs": [
      { "name": "ptr_feat0", "type": "AnyTensor" },
      { "name": "ptr_feat1", "type": "AnyTensor" },
      { "name": "ptr_feat2", "type": "AnyTensor" },
      { "name": "ptr_feat3", "type": "AnyTensor" },
      { "name": "rois_multi_batch", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "result_res", "type": "AnyTensor" },
      { "name": "result_rois", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "ROI_NUM_LEVELS", "type": "I64Attr" },
      { "name": "ROI_H", "type": "I64Attr" },
      { "name": "ROI_W", "type": "I64Attr" },
      { "name": "CHANNEL_ROI", "type": "I64Attr" },
      { "name": "ROI_SLICE", "type": "I64Attr" },
      { "name": "ROI_PH", "type": "I64Attr" },
      { "name": "ROI_PW", "type": "I64Attr" },
      { "name": "ROI_LEN", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.MaskRCNN_GetBboxB",
    "summary": "MaskRCNN GetBboxB gen by PPL",
    "description": "1.Op Introduction\r\n    Mask rcnn consist of three part: backbone to get proposals, bbox head to get bbox pred, mask_head to get mask pred, GetBboxB is final part in bbox head\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = \\text{NMS}\\Bigl( \\text{Decode}\\Bigl( \\text{ptr\\_rois},\\, \\text{ptr\\_bbox},\\, \\text{max\\_val},\\, \\text{scale\\_factor},\\, \\text{delta2bbox\\_means},\\, \\text{delta2bbox\\_stds} \\Bigr),\\, \\text{ptr\\_score},\\, \\text{threshold\\_score\\_eq},\\, \\text{nms\\_iou\\_thr} \\Bigr)\r\n    ```\r\n\r\n    3.activation and weight\r\n    ptr_rois(act.): candidate regions of interest for possible objects.;\r\n    ptr_bbox(act.): the bounding box predictions.;\r\n    ptr_score(act.): confidence scores associated with each proposal.;\r\n    max_val(w.): max scores the bbox predictions.;\r\n    scale_factor(w.): scale the decoded bounding box coordinates.;\r\n\r\n    4.attributes\r\n    threshold_score_eq: A threshold value used to filter out proposals with a low confidence score before applying NMS.;\r\n    wh_ratio_log: A logarithmic scaling factor, adjust the width-to-height ratio during decoding of bounding boxes.;\r\n    nms_iou_thr: IoU (Intersection over Union) threshold.;\r\n    delta2bbox_means: Mean values used to decode the bounding box regression.;\r\n    delta2bbox_stds_0: Standard deviation (first component) for scaling the decoded bbox values.;\r\n    delta2bbox_stds_1: Standard deviation (second component) for scaling the decoded bbox values.;\r\n    NUM_INDEXES: the number of indexes (or anchors).;\r\n    NUM_CLASSES: The total number of object classes.;\r\n    TOPK_ONNX_NMS: to select a fixed number of candidates.;\r\n    NUM_CLASSES_getBboxB: Number of classes used in this bounding box decoding step.;\r\n    MAX_NMS_LENGTH_GetBboxB: Maximum number of bounding box candidates.;\r\n    MAX_PER_IMG: The maximum number of detections allowed per image.;\r\n    MAX_PER_IMG_GetBboxB: maximum number of bounding boxes after the final processing.;",
    "inputs": [
      { "name": "ptr_rois", "type": "AnyTensor" },
      { "name": "ptr_bbox", "type": "AnyTensor" },
      { "name": "ptr_score", "type": "AnyTensor" },
      { "name": "max_val", "type": "AnyTensor" },
      { "name": "scale_factor", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "result_det_bboxes", "type": "AnyTensor" },
      { "name": "result_det_labels", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "threshold_score_eq", "type": "F64Attr" },
      { "name": "wh_ratio_log", "type": "F64Attr" },
      { "name": "nms_iou_thr", "type": "F64Attr" },
      { "name": "delta2bbox_means", "type": "F64Attr" },
      { "name": "delta2bbox_stds_0", "type": "F64Attr" },
      { "name": "delta2bbox_stds_1", "type": "F64Attr" },
      { "name": "NUM_INDEXES", "type": "I64Attr" },
      { "name": "NUM_CLASSES", "type": "I64Attr" },
      { "name": "TOPK_ONNX_NMS", "type": "I64Attr" },
      { "name": "NUM_CLASSES_getBboxB", "type": "I64Attr" },
      { "name": "MAX_NMS_LENGTH_GetBboxB", "type": "I64Attr" },
      { "name": "MAX_PER_IMG", "type": "I64Attr" },
      { "name": "MAX_PER_IMG_GetBboxB", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.MaskRCNN_MaskPooler",
    "summary": "Mask_Pooler gen by PPL",
    "description": "1.Op Introduction\r\n    MaskRCNN_Mask_Pooler, the 2st ROIAlign in MaskRCNN\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = \\text{ROIAlign}\\Bigl( \\{x_i\\}_{i=0}^3, \\, \\text{det\\_bboxes\\_multi\\_batch}, \\, \\text{det\\_labels\\_multi\\_batch}, \\, \\text{scale\\_factor}, \\, ROI\\_NUM\\_LEVELS, \\, ROI\\_H, \\, ROI\\_W, \\, CHANNEL\\_ROI, \\, ROI\\_SLICE, \\, ROI\\_PH, \\, ROI\\_PW, \\, ROI\\_LEN \\Bigr)\r\n    ```\r\n\r\n    3.activation and weight\r\n    x_0(act.): first level of the feature pyramid.;\r\n    x_1(act.): second level of the feature pyramid.;\r\n    x_2(act.): third level of the feature pyramid.;\r\n    x_3(act.): fourth level of the feature pyramid.;\r\n    det_bboxes_multi_batch(act.): detected bounding boxes over multiple batches.;\r\n    det_labels_multi_batch(act.): class labels associated with the detected bounding boxes across multiple batches.;\r\n    scale_factor(w.): scale the decoded bounding box coordinates.;\r\n\r\n    4.attributes\r\n    ROI_NUM_LEVELS: the number of feature levels (or pyramid levels) available for ROI pooling.;\r\n    ROI_H: target height of the pooled region for each ROI.;\r\n    ROI_W: target width of the pooled region for each ROI.;\r\n    CHANNEL_ROI: number of channels to be kept or considered when performing ROIAlign.;\r\n    ROI_SLICE: slicing strategy for ROIs, if the ROI needs to be segmented into sub-regions for finer pooling.;\r\n    ROI_PH: Padding height for the ROI.;\r\n    ROI_PW: Padding width for the ROI.;\r\n    ROI_LEN: total length (or area) of the ROI.;",
    "inputs": [
      { "name": "x_0", "type": "AnyTensor" },
      { "name": "x_1", "type": "AnyTensor" },
      { "name": "x_2", "type": "AnyTensor" },
      { "name": "x_3", "type": "AnyTensor" },
      { "name": "det_bboxes_multi_batch", "type": "AnyTensor" },
      { "name": "det_labels_multi_batch", "type": "AnyTensor" },
      { "name": "scale_factor", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "result_res", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "ROI_NUM_LEVELS", "type": "I64Attr" },
      { "name": "ROI_H", "type": "I64Attr" },
      { "name": "ROI_W", "type": "I64Attr" },
      { "name": "CHANNEL_ROI", "type": "I64Attr" },
      { "name": "ROI_SLICE", "type": "I64Attr" },
      { "name": "ROI_PH", "type": "I64Attr" },
      { "name": "ROI_PW", "type": "I64Attr" },
      { "name": "ROI_LEN", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.MaskRCNN_RPNGetBboxes",
    "summary": "RPN_get_bboxes gen by PPL",
    "description": "1.Op Introduction\r\n    MaskRCNN_RPN_get_bboxes, the sub-block with 1st NMS between RPN_head and 1st ROIAlign.\r\n\r\n    2.Math formula\r\n    ```math\r\n        1.Score Filtering\r\n            valid_indices_i = cls_scores_i > conf_threshold (for each level i)\r\n        2.Bounding Box Adjustment\r\n            adjusted_bboxes_i = anchors_i + bbox_preds_i * delta2bbox_std_i + delta2bbox_mean_i\r\n        3.IoU Calculation\r\n            iou = calculate_iou(adjusted_bboxes_i, ground_truth_boxes)\r\n        4.NMS Application\r\n            final_bboxes = nms(adjusted_bboxes_i[valid_indices_i], iou_threshold)\r\n        5.final output\r\n            output = concatenate(final_bboxes)\r\n    ```\r\n    3.activation and weight\r\n    cls_scores_0: the class scores0 for each anchor;\r\n    cls_scores_1: the class scores1 for each anchor;\r\n    cls_scores_2: the class scores2 for each anchor;\r\n    cls_scores_3: the class scores3 for each anchor;\r\n    cls_scores_4: the class scores4 for each anchor;\r\n    bbox_preds_0: the bounding box0 predictions;\r\n    bbox_preds_1: the bounding box1 predictions;\r\n    bbox_preds_2: the bounding box2 predictions;\r\n    bbox_preds_3: the bounding box3 predictions;\r\n    bbox_preds_4: the bounding box4 predictions;\r\n    max_shape: the maximum dimensions of the output bounding boxes.;\r\n    mlvl_anchors_0: the multi-level anchors0 used for generating bounding box proposals.;\r\n    mlvl_anchors_1: the multi-level anchors1 used for generating bounding box proposals.;\r\n    mlvl_anchors_2: the multi-level anchors2 used for generating bounding box proposals.;\r\n    mlvl_anchors_3: the multi-level anchors3 used for generating bounding box proposals.;\r\n    mlvl_anchors_4: the multi-level anchors4 used for generating bounding box proposals.;\r\n\r\n    4.attribute\r\n    delta2bbox_mean_0: the means used to normalize the bounding box0 deltas for the corresponding feature levels.;\r\n    delta2bbox_mean_1: the means used to normalize the bounding box1 deltas for the corresponding feature levels.;\r\n    delta2bbox_mean_2: the means used to normalize the bounding box2 deltas for the corresponding feature levels.;\r\n    delta2bbox_mean_3: the means used to normalize the bounding box3 deltas for the corresponding feature levels.;\r\n    delta2bbox_std_0: the standard deviations used to normalize the bounding box0 deltas for the corresponding feature levels.;\r\n    delta2bbox_std_1: the standard deviations used to normalize the bounding box1 deltas for the corresponding feature levels.;\r\n    delta2bbox_std_2: the standard deviations used to normalize the bounding box2 deltas for the corresponding feature levels.;\r\n    delta2bbox_std_3: the standard deviations used to normalize the bounding box3 deltas for the corresponding feature levels.;\r\n    delta2bbox_max_scalar_c: a scalar value;\r\n    iou_threshold: filtering out low-quality proposals during NMS.;\r\n    conf_threshold: a confidence score threshold.;\r\n    MAX_LENGTH_STATIC_STRECHED: the maximum length for the output list of bounding boxes after processing.;\r\n    NUM_INDEXES: the number of indexes.;\r\n    NUM_CLASSES: the number of classes.;\r\n    CHANNEL_RPN_BBOXES: the number of channels for the bounding box predictions.;\r\n    CHANNEL_RPN_SCORES: the number of channels used for the class score predictions.;\r\n    NMS_PRE: the number of proposals to be considered before NMS.;\r\n    HARDWARE_FACTOR_TOPK: how many top proposals to keep.;\r\n    NMS_MAX_LENGTH: the maximum number of boxes after NMS.;\r\n    TOPK_ONNX_NMS: the number of top proposals when using ONNX format for NMS.;\r\n    H_RPN_DYN_MAX: the maximum height for the dynamic RPN output.;\r\n    W_RPN_DYN_MAX: the maximum width for the dynamic RPN output.;\r\n    MAX_PER_IMG: the maximum number of proposals to be generated per image.;",
    "inputs": [
      { "name": "cls_scores_0", "type": "AnyTensor" },
      { "name": "cls_scores_1", "type": "AnyTensor" },
      { "name": "cls_scores_2", "type": "AnyTensor" },
      { "name": "cls_scores_3", "type": "AnyTensor" },
      { "name": "cls_scores_4", "type": "AnyTensor" },
      { "name": "bbox_preds_0", "type": "AnyTensor" },
      { "name": "bbox_preds_1", "type": "AnyTensor" },
      { "name": "bbox_preds_2", "type": "AnyTensor" },
      { "name": "bbox_preds_3", "type": "AnyTensor" },
      { "name": "bbox_preds_4", "type": "AnyTensor" },
      { "name": "max_shape", "type": "AnyTensor" },
      { "name": "mlvl_anchors_0", "type": "AnyTensor" },
      { "name": "mlvl_anchors_1", "type": "AnyTensor" },
      { "name": "mlvl_anchors_2", "type": "AnyTensor" },
      { "name": "mlvl_anchors_3", "type": "AnyTensor" },
      { "name": "mlvl_anchors_4", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "result_list", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "delta2bbox_mean_0", "type": "F64Attr" },
      { "name": "delta2bbox_mean_1", "type": "F64Attr" },
      { "name": "delta2bbox_mean_2", "type": "F64Attr" },
      { "name": "delta2bbox_mean_3", "type": "F64Attr" },
      { "name": "delta2bbox_std_0", "type": "F64Attr" },
      { "name": "delta2bbox_std_1", "type": "F64Attr" },
      { "name": "delta2bbox_std_2", "type": "F64Attr" },
      { "name": "delta2bbox_std_3", "type": "F64Attr" },
      { "name": "delta2bbox_max_scalar_c", "type": "F64Attr" },
      { "name": "iou_threshold", "type": "F64Attr" },
      { "name": "conf_threshold", "type": "F64Attr" },
      { "name": "MAX_LENGTH_STATIC_STRECHED", "type": "I64Attr" },
      { "name": "NUM_INDEXES", "type": "I64Attr" },
      { "name": "NUM_CLASSES", "type": "I64Attr" },
      { "name": "CHANNEL_RPN_BBOXES", "type": "I64Attr" },
      { "name": "CHANNEL_RPN_SCORES", "type": "I64Attr" },
      { "name": "NMS_PRE", "type": "I64Attr" },
      { "name": "HARDWARE_FACTOR_TOPK", "type": "I64Attr" },
      { "name": "NMS_MAX_LENGTH", "type": "I64Attr" },
      { "name": "TOPK_ONNX_NMS", "type": "I64Attr" },
      { "name": "H_RPN_DYN_MAX", "type": "I64Attr" },
      { "name": "W_RPN_DYN_MAX", "type": "I64Attr" },
      { "name": "MAX_PER_IMG", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.MatchTemplate",
    "summary": "opencv MatchTemplate operator",
    "description": "1.Op Introduction\r\n    Perform opencv MatchTemplate operation.\r\n\r\n    2.Math formula\r\n    ```math\r\n            R(x, y) = \\sum_{i=0}^{T_w-1} \\sum_{j=0}^{T_h-1} I(x+i, y+j) \\cdot T(i, j)\r\n    ```\r\n    where:\r\n    R(x, y) is the result of the match at position(x, y).\r\n    I is the input image.\r\n    T is the template image.\r\n    T_w and T_h are the width and height of the template.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    match(w.): the template image that will be matched against the input image.;\r\n\r\n    4.attributes\r\n    mode: the method of template matching to be used (e.g., correlation, squared difference, etc.).;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "match", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "MatchTemplateModeAttr" }
    ]
  },
  {
    "name": "top.MatMul",
    "summary": "matmul operator",
    "description": "1.Op Introduction\r\n    The MatMul operator performs two-dimensional matrix multiplication between two input tensors.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = input x right + bias\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): the first input tensor;\r\n    right(act.): the second input tensor;\r\n    bias(w.):  an optional tensor can be added to the result of the matrix multiplication. ;\r\n\r\n    4.attribute\r\n    right_transpose: whether to transpose the right input tensor before performing the multiplication.;\r\n    left_transpose: whether to transpose the input tensor before performing the multiplication.;\r\n    output_transpose: whether to transpose the output tensor after the multiplication.;\r\n    hdim_is_batch: whether the first dimension of the input tensor represents the batch size.;\r\n    keep_dims: whether to keep the dimensions of the output tensor the same as the input tensors.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    weight_bits: the bit-width for quantizing the weights.;\r\n    in_int4_scale: the scaling factor for input tensors that are quantized to 4 bits.;\r\n    in_int4_zp: the zero point for input tensors that are quantized to 4 bits.;\r\n    out_int8_scale:the scaling factor for the output tensor when quantized to 8 bits.;\r\n    out_int8_zp: This optional attribute specifies the zero point for the output tensor that is quantized to 8 bits.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "right", "type": "AnyTensor" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "right_transpose", "type": "DefaultValuedAttr" },
      { "name": "left_transpose", "type": "DefaultValuedAttr" },
      { "name": "output_transpose", "type": "DefaultValuedAttr" },
      { "name": "hdim_is_batch", "type": "DefaultValuedAttr" },
      { "name": "keep_dims", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "weight_bits", "type": "OptionalAttr" },
      { "name": "in_int4_scale", "type": "OptionalAttr" },
      { "name": "in_int4_zp", "type": "OptionalAttr" },
      { "name": "out_int8_scale", "type": "OptionalAttr" },
      { "name": "out_int8_zp", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "top.Max",
    "summary": "max operator",
    "description": "1.Op Introduction\r\n    Element-wise max of each of the input tensors. All inputs and outputs must have the same data type.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = max(input1,input2, ..., inputN)\r\n    ```\r\n    Where input1, input2, ..., inputN are the input tensors.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.MaxConst",
    "summary": "Max Const operator",
    "description": "1.Op Introduction\r\n    max of one input and one const.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = Max(input, const_val)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    const_val: the constant value to be added to each element of the input tensor(positive, negative, or zero).;\r\n    is_scalar: whether the addition operation is performed with scalar values or tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "const_val", "type": "F64Attr" },
      { "name": "is_scalar", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.MaxPool",
    "summary": "pool operator",
    "description": "1.Op Introduction\r\n    This performs an  pooling over the given input tensor. A sliding\r\n    window of size given by <kernel size> is passed over the input tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n        {output}(N_i, C_j, p, q) = pool(input(N_i, C_j, start_h, start_w), kernel_size)\r\n    ```\r\n    start_h and start_w are the starting indices for the pooling window in the height and width dimensions.\r\n    pool can be any pooling function (max pooling, average pooling) applied over the region defined by the kernel size.\r\n    N is a batch size, C denotes a number of channels,\r\n    Shape:\r\n    - Input: (N, C_{in}, H_{in}, W_{in})\r\n    - Output: (N, C_{out}, H_{out}, W_{out})\r\n        ```math\r\n        H_{output} = {H_{in} + {padding}[0] + {padding}[2] - {kernel_shape}[0]} / {stride}[0] + 1\r\n        ```\r\n        ```math\r\n        W_{output} = {W_{in} + {padding}[1] + {padding}[3] - {kernel_shape}[1]} / {stride}[1] + 1\r\n        ```\r\n\r\n    3.activation and weight\r\n    input(act.): Variadic input tensor;\r\n\r\n    4.attribute\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    stride: the stride for the cross-correlation, a single number or a tuple.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    ceil_mode: whether to use ceiling or floor when calculating the output size.;\r\n    auto_pad: It can be set to different modes (e.g., SAME, VALID) to automatically calculate the necessary padding based on\r\n              the input size,kernel size, and stride.;\r\n    is_adaptive: whether the pooling operation is adaptive.\r\n                 If true, adjusts the kernel size based on the input size to produce a specified output size.\r\n    keepdims: whether to retain the dimensions of the input tensor in the output.\r\n               If true, will have the same number of dimensions as the input tensor.;\r\n    pad_value: whether to retain the dimensions of the input tensor in the output.\r\n                If true, will have the same number of dimensions as the input tensor.;\r\n    count_include_pad: whether to include the padded values in the pooling count.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;\r\n    first_round_mode: the rounding behavior applied to the scaled value before the offset is added.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "ceil_mode", "type": "OptionalAttr" },
      { "name": "auto_pad", "type": "OptionalAttr" },
      { "name": "is_adaptive", "type": "DefaultValuedAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "pad_value", "type": "DefaultValuedAttr" },
      { "name": "count_include_pad", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "first_round_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.MaxPoolingIndicesBwd",
    "summary": "MaxPoolingIndicesBwd operator",
    "description": "1.Op Introduction\r\n    MaxPoolingIndicesBwd operator\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = grad_output[indices]\r\n    ```\r\n\r\n    3.activation and weight\r\n    grad_output(act.): the gradient of the loss with respect to the output.;\r\n    indices(w.): the indices of the input tokens or items.;\r\n\r\n    4.attributes\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    stride: the stride for the cross-correlation, a single number or a tuple.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    dilations: controls the spacing between the kernel points;\r\n    input_shape: The shape of the input tensor.;",
    "inputs": [
      { "name": "grad_output", "type": "AnyTensor" },
      { "name": "indices", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "grad_input", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "dilations", "type": "I64ArrayAttr" },
      { "name": "input_shape", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "top.MaxPoolWithMask",
    "summary": "pool operator",
    "description": "1.Op Introduction\r\n    This performs an  pooling over the given input tensor. A sliding\r\n    window of size given by <kernel size> is passed over the input tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n        {output}(N_i, C_j, p, q) = pool(input(N_i, C_j, start_h, start_w), kernel_size)\r\n    ```\r\n    start_h and start_w are the starting indices for the pooling window in the height and width dimensions.\r\n    pool can be any pooling function (max pooling, average pooling) applied over the region defined by the kernel size.\r\n    N is a batch size, C denotes a number of channels,\r\n    Shape:\r\n    - Input: (N, C_{in}, H_{in}, W_{in})\r\n    - Output: (N, C_{out}, H_{out}, W_{out})\r\n        ```math\r\n        H_{output} = {H_{in} + {padding}[0] + {padding}[2] - {kernel_shape}[0]} / {stride}[0] + 1\r\n        ```\r\n        ```math\r\n        W_{output} = {W_{in} + {padding}[1] + {padding}[3] - {kernel_shape}[1]} / {stride}[1] + 1\r\n        ```\r\n\r\n    3.activation and weight\r\n    input(act.): Variadic input tensor;\r\n\r\n    4.attribute\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    stride: the stride for the cross-correlation, a single number or a tuple.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    ceil_mode: whether to use ceiling or floor when calculating the output size.;\r\n    auto_pad: It can be set to different modes (e.g., SAME, VALID) to automatically calculate the necessary padding based on\r\n              the input size,kernel size, and stride.;\r\n    is_adaptive: whether the pooling operation is adaptive.\r\n                 If true, adjusts the kernel size based on the input size to produce a specified output size.\r\n    keepdims: whether to retain the dimensions of the input tensor in the output.\r\n               If true, will have the same number of dimensions as the input tensor.;\r\n    pad_value: whether to retain the dimensions of the input tensor in the output.\r\n                If true, will have the same number of dimensions as the input tensor.;\r\n    count_include_pad: whether to include the padded values in the pooling count.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;\r\n    first_round_mode: the rounding behavior applied to the scaled value before the offset is added.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" },
      { "name": "mask", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "ceil_mode", "type": "OptionalAttr" },
      { "name": "auto_pad", "type": "OptionalAttr" },
      { "name": "is_adaptive", "type": "DefaultValuedAttr" },
      { "name": "keepdims", "type": "DefaultValuedAttr" },
      { "name": "pad_value", "type": "DefaultValuedAttr" },
      { "name": "count_include_pad", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "first_round_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.MaxUnpool",
    "summary": "MaxUnpool operation",
    "description": "1.Op Introduction\r\n    Perform  MaxUnpool on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i, j] = Upsample(input[i / scale_h, j / scale_w])\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    mask(act.): the positions of the maximum values that were retained during the max pooling operation.;\r\n\r\n    4.attributes\r\n    scale_h: the scaling factor for the height (number of rows) of the input tensor.;\r\n    scale_w: the scaling factor for the width (number of columns) of the input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "mask", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "scale_h", "type": "I64Attr" },
      { "name": "scale_w", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.MeanRstd",
    "summary": "Compute Mean, Rstd, Running_mean, Running_var in batchnorm train op",
    "description": "1.Op Introduction\r\n    computes the mean, reverse standard deviation (Rstd), running mean, and running variance.\r\n\r\n    2.Math formula\r\n    ```math\r\n        mean(x),1/sqrt(var+eps),(1-momentum)*running_mean + momentum*mean,(1-momentum)*running_var + momentum*var\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    running_mean(w.): mean during running.;\r\n    running_var(w.): variances during running.;\r\n    weight(w.): weight tensor.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attributes\r\n    eps: a small constant added to the denominator during normalization to prevent division by zero.;\r\n    momentum: hyperparameter;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "running_mean", "type": "AnyTensor" },
      { "name": "running_var", "type": "AnyTensor" },
      { "name": "weight", "type": "AnyTensor" },
      { "name": "bias", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "mean", "type": "AnyTensor" },
      { "name": "rstd", "type": "AnyTensor" },
      { "name": "running_mean_update", "type": "AnyTensor" },
      { "name": "running_var_update", "type": "AnyTensor" },
      { "name": "scale", "type": "AnyTensor" },
      { "name": "bias_new", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "eps", "type": "F64Attr" },
      { "name": "momentum", "type": "F64Attr" }
    ]
  },
  {
    "name": "top.MeanStdScale",
    "summary": "MeanStdScale, it's for preprocess.",
    "description": "1.Op Introduction\r\n    for preprocess, do multiplier&rshift quantize.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = (input - mean) / std * scale + zero_points\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    quant_mode: the mode or method used for quantization during the requantization operation.;\r\n    customization_format: custom format for the input data.;\r\n    channel_order: The order of color channels in the input tensor.;\r\n    scale: each channel scale.;\r\n    mean: mean values to subtract from each channel for normalization.;\r\n    sign: if output is signed.;\r\n    std: standard deviation values for each channel.;\r\n    zero_points: zero point values for each channel.;\r\n    resize_dims: resize the input tensor' dimensions.;\r\n    rounding_mode: The rounding method to use during quantization.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "quant_mode", "type": "StrAttr" },
      { "name": "customization_format", "type": "StrAttr" },
      { "name": "channel_order", "type": "StrAttr" },
      { "name": "sign", "type": "DefaultValuedAttr" },
      { "name": "scale", "type": "F64ArrayAttr" },
      { "name": "std", "type": "F64ArrayAttr" },
      { "name": "mean", "type": "F64ArrayAttr" },
      { "name": "zero_points", "type": "F64ArrayAttr" },
      { "name": "resize_dims", "type": "I64ArrayAttr" },
      { "name": "rounding_mode", "type": "StrAttr" }
    ]
  },
  {
    "name": "top.MeshGrid",
    "summary": "MeshGrid operation",
    "description": "1.Op Introduction\r\n    torch mesh grid operation\r\n\r\n    2.Math formula\r\n    ```math\r\n            X[i] = x[i mod m] for i = 0, 1, ..., m · n-1\r\n            Y[i] = y[j // m] for j = 0, 1, ..., m · n-1\r\n    ```\r\n    where X, Y will have shape (m, n);\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    is_reverse: whether the subtraction operation is performed in reverse order.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "is_reverse", "type": "BoolAttr" }
    ]
  },
  {
    "name": "top.Min",
    "summary": "min operator",
    "description": "1.Op Introduction\r\n    Element-wise min of each of the input tensors. All inputs and outputs must have the same data type.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = min(input1,input2, ..., inputN)\r\n    ```\r\n    Where input1, input2, ..., inputN are the input tensors.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.MinConst",
    "summary": "Min Const operator",
    "description": "1.Op Introduction\r\n    min of one input and one const.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = Min(input, const_val)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    const_val: the constant value to be added to each element of the input tensor(positive, negative, or zero).;\r\n    is_scalar: whether the addition operation is performed with scalar values or tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "const_val", "type": "F64Attr" },
      { "name": "is_scalar", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Mish",
    "summary": "Mish operator",
    "description": "1.Op Introduction\r\n    Calculates the mish of the given input tensor, element-wise.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = input[i] x tanh(softplus(input[i]))\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Mmap2Rgbmap",
    "summary": "isp mmap2rgbmap.",
    "description": "isp mmap2rgbmap.",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Mod",
    "summary": "Mod operator",
    "description": "1.Op Introduction\r\n    a mathematical operation that calculates the remainder of the division of two numbers (or tensors) element-wise.\r\n\r\n    2.Math formula\r\n    ```math\r\n            f(x, y) = Mod(x, y)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Mul",
    "summary": "Mul operator",
    "description": "1.Op Introduction\r\n    Elementwise multiplication of input1 and input2. input1 and input2 are tensors.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = ReLU((input1 * input2))\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    is_scalar: whether the addition operation is performed with scalar values or tensor.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "is_scalar", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.MulConst",
    "summary": "Mul Const operator",
    "description": "1.Op Introduction\r\n    Elementwise mul of input1 and input2. Input2 is constant.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = input * const_val\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    const_val: the constant value to be added to each element of the input tensor(positive, negative, or zero).;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    is_scalar: whether the addition operation is performed with scalar values or tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "const_val", "type": "F64Attr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "is_scalar", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Nms",
    "summary": "NMS operator",
    "description": "1.Op Introduction\r\n    onnx nms\r\n    used to eliminate redundant overlapping bounding boxes by selecting only the most relevant ones based on their confidence scores.\r\n\r\n    2.Math formula\r\n    ```math\r\n        IOU(a, b) = Area(a \\cap b) / Area(a \\cup b)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): Variadic input tensor.;\r\n\r\n    4.attributes\r\n    center_point_box: whether the bounding boxes are defined by their center points.;\r\n    max_output_size: the maximum number of boxes to be output after NMS.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "center_point_box", "type": "I64Attr" },
      { "name": "max_output_size", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.None",
    "summary": "none operator",
    "description": "A none Op to return a NoneType."
  },
  {
    "name": "top.NonZero",
    "summary": "NonZero operation",
    "description": "1.Op Introduction\r\n    Returns the indices of the elements that are non-zero\r\n    (in row-major order - by dimension).\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = input[i1, i2, i3,...in] != 0\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    order: the order in which the non-zero indices should be returned.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "order", "type": "NonZeroOrderAttr" }
    ]
  },
  {
    "name": "top.Normalize",
    "summary": "Normalize operator",
    "description": "1.Op Introduction\r\n    Normalizes an array across batch and spatial dimensions.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = ((input - Mu) / (Sigma + Epsilon)) x scale\r\n    ```\r\n    where, Mu is the mean of the input values calculated across the specified dimensions.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    scale(w.): the scale weight tensor.;\r\n\r\n    4.attribute\r\n    across_spatial: determines whether the normalization is performed across the spatial dimensions of the input tensor.;\r\n    channel_shared: indicates whether the scaling weight tensor should be shared across channels.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "scale", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "across_spatial", "type": "DefaultValuedAttr" },
      { "name": "channel_shared", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Pack",
    "summary": "Pack operator",
    "description": "1.Op Introduction\r\n    Pack a list of tensors in the given dimension, All tensors must have the same shape.\r\n\r\n    2.Math formula\r\n    output = Pack(input1, input2, input3; axis)\r\n\r\n    3.activation and weight\r\n    input(act.): Variadic input tensor;\r\n\r\n    4.attribute\r\n    axis: It specifies the dimension along which the input tensors will be packed together.;\r\n    values_count: It indicates the number of tensors being packed.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "SI32Attr" },
      { "name": "values_count", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.Pad",
    "summary": "Pad operation",
    "description": "1.Op Introduction\r\n    This operation pads a tensor according to the paddings you specify.\r\n    paddings is an integer tensor with shape [2, n], where n is the rank of tensor.\r\n    For each dimension D of input, paddings[0, D] indicates how many values to add\r\n    before the contents of tensor in that dimension, and paddings[1, D] indicates\r\n    how many values to add after the contents of tensor in that dimension.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = input(padding, val, mode)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    paddingsT(act.):  the padding values for each dimension.;\r\n\r\n    4.attribute\r\n    paddings: defines how much padding to add before and after the contents of the input tensor for each dimension. ;\r\n    val: the value to be used for padding the input tensor. ;\r\n    mode: the padding mode include constant(Pads with a constant value); reflect(Pads with a reflection of the tensor values); replicate(Pads by replicating the edge values of the tensor).;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "paddingsT", "type": "Optional" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "paddings", "type": "I64ArrayAttr" },
      { "name": "val", "type": "DefaultValuedAttr" },
      { "name": "mode", "type": "PaddingModeAttr" }
    ]
  },
  {
    "name": "top.Permute",
    "summary": "Permute operator",
    "description": "1.Op Introduction\r\n    Perform permute on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output(...dim2, dim1, dim0) = PermuteOp(input(dim0, dim1, dim2...order))\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    order: An array of integers specifying the permutation order of the input tensor's dimensions.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "order", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "top.PixelNorm",
    "summary": "PixelNorm operation",
    "description": "1.Op Introduction\r\n    pixel normalization (normalize along c-axis)\r\n\r\n    2.Math formula\r\n    ```math\r\n            norm_{n, i, j} = sqrt(1 /C \\sum{c=1, C}input_{n, c, i, j} ^ 2) + eps\r\n            output_{n, c, i, j} = weight * input_{n, c, i, j} / norm_{n, i, j} + bias\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    weight(w.): weight tensor;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attributes\r\n    eps: a small constant added to the denominator during normalization to prevent division by zero.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "weight", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "eps", "type": "F64Attr" }
    ]
  },
  {
    "name": "top.PoolMask",
    "summary": "pool mask operator",
    "description": "1.Op Introduction\r\n    pooling mask on input\r\n\r\n    2.Math formula\r\n    ```math\r\n        {output}(N_i, C_j, p, q) = scale x pool(input(N_i, C_j, start_h, start_w), kernel_size)\r\n    ```\r\n    start_h and start_w are the starting indices for the pooling window in the height and width dimensions.\r\n    pool can be any pooling function (max pooling, average pooling) applied over the region defined by the kernel size.\r\n    N is a batch size, C denotes a number of channels,\r\n    Shape:\r\n    - Input: (N, C_{in}, H_{in}, W_{in})\r\n    - Output: (N, C_{out}, H_{out}, W_{out})\r\n        ```math\r\n        H_{output} = {H_{in} + {padding}[0] + {padding}[2] - {kernel_shape}[0]} / {stride}[0] + 1\r\n        ```\r\n        ```math\r\n        W_{output} = {W_{in} + {padding}[1] + {padding}[3] - {kernel_shape}[1]} / {stride}[1] + 1\r\n        ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    scale: a scaling factor is applied to the output of the pooling operation, can adjust the intensity or magnitude of the output mask.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "scale", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.Pow",
    "summary": "Pow operation",
    "description": "1.Op Introduction\r\n    computes the element-wise power of an input tensor raised to a specified exponent.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = input ^ n\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    exponent: the power to which each element of the input tensor will be raised.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "exponent", "type": "F64Attr" }
    ]
  },
  {
    "name": "top.Pow2",
    "summary": "Pow2 operation",
    "description": "1.Op Introduction\r\n    computes the result of raising a constant value ( n ) to the power of each element in the input tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = n ^ input\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    const_val: specifies the constant value to be added to each element of the input tensor(positive, negative, or zero).;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "const_val", "type": "F64Attr" }
    ]
  },
  {
    "name": "top.Pow3",
    "summary": "Pow3 operation",
    "description": "1.Op Introduction\r\n    computes the element-wise power of two input tensors.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = input1 ^ input2\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.PRelu",
    "summary": "PRelu operator",
    "description": "1.Op Introduction\r\n    Parametric Rectified Linear Unit is an activation function.\r\n\r\n    2.Math formula\r\n    ```math\r\n            f(x) = slope * x   for x < 0\r\n            f(x) = x           for x >= 0\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    slope(w.): the activation function for negative input values.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "slope", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Preprocess",
    "summary": "FusePreprcess, it's just a placeholder op.",
    "description": "1.Op Introduction\r\n    It may be divided to permute + slice + scale/scale_lut ops.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Scale(Slice(Permute(input,channel_order), resize_dims), scale) - mean.\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    quant_mode: the mode or method used for quantization during the requantization operation.;\r\n    customization_format: custom format for the input data.;\r\n    channel_order: The order of color channels in the input tensor.;\r\n    resize_dims: resize the input tensor' dimensions.;\r\n    scale: each channel scale.;\r\n    mean: mean values to subtract from each channel for normalization.;\r\n    sign: if output is signed.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "quant_mode", "type": "StrAttr" },
      { "name": "customization_format", "type": "StrAttr" },
      { "name": "channel_order", "type": "StrAttr" },
      { "name": "resize_dims", "type": "I64ArrayAttr" },
      { "name": "scale", "type": "F64ArrayAttr" },
      { "name": "mean", "type": "F64ArrayAttr" },
      { "name": "sign", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.PriorBox",
    "summary": "PriorBox operation",
    "description": "1.Op Introduction\r\n    Intended for use with MultiBox detection method to generate prior.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = input(x, y, w, h) i in [0, num_priors]\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor;\r\n\r\n    4.attributes\r\n    min_size: the minimum size of the prior boxes.;\r\n    max_size: the maximum size of the prior boxes.;\r\n    aspect_ratios: A list of aspect ratios for the generated prior boxes.;\r\n    variance: adjust the predicted boxes during the training process.;\r\n    clip: whether the prior boxes should be clipped to the image boundaries.;\r\n    step_h: The vertical step size for generating prior boxes.;\r\n    step_w: The horizontal step size for generating prior boxes.;\r\n    img_h: The height of the input image.;\r\n    img_w: The width of the input image.;\r\n    offset: A value used to offset the prior boxes from their calculated positions.;\r\n    num_priors: The number of prior boxes to be generated for each location in the feature map.;\r\n    use_default_aspect_ratio: whether to use a default aspect ratio (1.0);",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "min_size", "type": "F64ArrayAttr" },
      { "name": "max_size", "type": "F64ArrayAttr" },
      { "name": "aspect_ratios", "type": "F64ArrayAttr" },
      { "name": "variance", "type": "F64ArrayAttr" },
      { "name": "clip", "type": "DefaultValuedAttr" },
      { "name": "step_h", "type": "F64Attr" },
      { "name": "step_w", "type": "F64Attr" },
      { "name": "img_h", "type": "I64Attr" },
      { "name": "img_w", "type": "I64Attr" },
      { "name": "offset", "type": "DefaultValuedAttr" },
      { "name": "num_priors", "type": "I64Attr" },
      { "name": "use_default_aspect_ratio", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Proposal",
    "summary": "Proposal operator",
    "description": "1.Op Introduction\r\n    generate candidate bounding boxes primarily in object detection tasks.\r\n\r\n    2.Math formula\r\n    ```math\r\n        1.anchor box generation and regression\r\n            a_i = (x + \\delta(x), y + \\delta(y), w * e ^ \\delta(w), e ^ \\delta(h))\r\n        2.obj score\r\n            s_i = sigmoid(output_ModelPredict(a_i))\r\n        3.Final select output\r\n            output = NMS(s_i)\r\n    ```\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    net_input_h: net input height.;\r\n    net_input_w: net input width.;\r\n    feat_stride: anchor box stride size.;\r\n    anchor_base_size: anchor box base size.;\r\n    rpn_obj_threshold: obj threshold.;\r\n    rpn_nms_threshold: nms threshold for generate proposal boxes.;\r\n    rpn_nms_post_top_n: keep num boxes after nms.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "net_input_h", "type": "I64Attr" },
      { "name": "net_input_w", "type": "I64Attr" },
      { "name": "feat_stride", "type": "I64Attr" },
      { "name": "anchor_base_size", "type": "I64Attr" },
      { "name": "rpn_obj_threshold", "type": "F64Attr" },
      { "name": "rpn_nms_threshold", "type": "F64Attr" },
      { "name": "rpn_nms_post_top_n", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.QuantizeLinear",
    "summary": "Linear quantize operation",
    "description": "1.Op Introduction\r\n    QuantizeLinear(x) := saturate ((x / y_scale) + y_zero_point)\r\n\r\n    2.Math formula\r\n    ```math\r\n        output[i] = saturate((input[i] / y_scale[j]) + y_zero_point[j])\r\n    ```\r\n    i indexes the elements of the input tensor, and j corresponds to the specific dimension or channel.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    y_scale: Each element to a specific output channel or dimension, determining how much to scale the input values.;\r\n    y_zero_point: an array of zero points used in the quantization process.;\r\n    axis: the dimension of the input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "y_scale", "type": "F64ArrayAttr" },
      { "name": "y_zero_point", "type": "I32ArrayAttr" },
      { "name": "axis", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.RandnLike",
    "summary": "randn_like operator, y = randn_like(x)",
    "description": "1.Op Introduction\r\n    create a tensor with the same shape as input, and fill with value from normal distribution\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = randn(shape(input))\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    randn_data(w.): the characteristics of the normal distribution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "randn_data", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Range",
    "summary": "Range operator",
    "description": "1.Op Introduction\r\n    onnx range op.\r\n    generates a sequence of evenly spaced values within a specified range.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = [x | x = start + n * delta, n is an integer, and start ≤ x < limit]\r\n    ```\r\n\r\n    3.activation and weight\r\n    start(act.): The starting value of the sequence. This can be a tensor or None. If None, it defaults to 0.;\r\n    limit(w.): The exclusive upper limit of the sequence.;\r\n    delta(w.): The increment between each value;",
    "inputs": [
      { "name": "start", "type": "AnyTensorOrNone" },
      { "name": "limit", "type": "AnyTensor" },
      { "name": "delta", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Reciprocal",
    "summary": "Constant scalar divide tensor operator",
    "description": "1.Op Introduction\r\n    The Reciprocal operator is a tensor operation that performs division of a constant scalar value by an input tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = const_val / input\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    const_val: specifies the constant value to be added to each element of the input tensor(positive, negative, or zero).;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "const_val", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Reduce",
    "summary": "Reduce operation",
    "description": "1.Op Introduction\r\n    Computes the mean/max/prod/sum of the input tensor's element along the provided axes.\r\n\r\n    2.Math formula\r\n    ```math\r\n            1.Sum\r\n                output[i_1, i_2, i_3,..., i_k] = \\sum{j in A}input[i_1, i_2,..., j, ..., i_n]\r\n                where ( A ) is the set of axes to reduce.\r\n            2.Mean\r\n                output[i_1, i_2, i_3,..., i_k] = 1 / count (\\sum{j in A}input[i_1, i_2,..., j, ..., i_n])\r\n                where count is the number of elements being summed along the axes ( A ).\r\n            3.Max\r\n                output[i_1, i_2, i_3,..., i_k] = max{j in A}input[i_1, i_2,..., j, ..., i_n]\r\n            4.Product\r\n                output[i_1, i_2, i_3,..., i_k] = \\prod_{j=1}^m(input[i_1, i_2,..., i_k, j])\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    grid(w.): The flow-field grid tensor that defines the pixel locations for sampling.;\r\n\r\n    4.attributes\r\n    axes: the dimensions (axes) of the input tensor that should be squeezed (removed).;\r\n    keepdims: whether to retain the dimensions of the input tensor in the output.\r\n               If true, will have the same number of dimensions as the input tensor.;\r\n    mode: the type of binary operation to be performed, addition, subtraction, or other types of binary operations.;\r\n    is_scalar: whether the addition operation is performed with scalar values or tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "axes", "type": "I64ArrayAttr" },
      { "name": "keepdims", "type": "BoolAttr" },
      { "name": "mode", "type": "ReduceModeAttr" },
      { "name": "is_scalar", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Relu",
    "summary": "Relu operator",
    "description": "1.Op Introduction\r\n    ReLU with a scalar maximum value. if limit is zero, do not use upper limit.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = ReluOp(input) -> (0, 1)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "relu_limit", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Remainder",
    "summary": "Remainder operator",
    "description": "1.Op Introduction\r\n    computes the element-wise remainder of division between two tensors.\r\n\r\n    2.Math formula\r\n    ```math\r\n            quo = x / y;\r\n            floor_quo = floor(quo);\r\n            output = torch.remainder(x, y) = x - y * floor_quo.\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Repeat",
    "summary": "Repeat operator",
    "description": "1.Op Introduction\r\n    Perform aten::repeat operation on the given tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i_1, i_2, i_3,...i_k] = input[i_1 / r_1, i_2 / r_2, i_3 / r_3,..., i_k mod r_k]\r\n    ```\r\n    where r_j represents the corresponding value from the repeats tensor for dimension ( j ).\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    repeats(w.): the number of times to repeat the input tensor along each dimension.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "repeats", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.RequantFp",
    "summary": "requant operation",
    "description": "1.Op Introduction\r\n    Requant 32/16/8 bit data to int8 or uint8 data, by scale\r\n\r\n    2.Math formula\r\n    int8/uint8/fp8(output) = round(float32/float16/float8(input) x scale + offset);\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    scale: Scalar;\r\n    offset: Scalar;\r\n    quant_mode: the mode or method used for quantization during the requantization operation.;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;\r\n    first_round_mode: the rounding behavior applied to the scaled value before the offset is added.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "scale", "type": "F64ArrayAttr" },
      { "name": "offset", "type": "F64ArrayAttr" },
      { "name": "quant_mode", "type": "RequantModeAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "first_round_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.RequantInt",
    "summary": "requant operation",
    "description": "1.Op Introduction\r\n    Requant 32/16/8 bit data to int8 or uint8 data, by int multiplier and int shift;\r\n\r\n    2.Math formula\r\n    int8/uint8(output) = RequantIntOp (int32/16/8(input));\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    multiplier: Floating-point multiplication operations are usually converted to fixed-point multiplication operations.;\r\n    rshift: the number of bits to right-shift the quantized values.;\r\n    quant_mode: the mode or method used for quantization during the requantization operation.;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;\r\n    rq_axis: the axis along which the requantization operation is applied.;\r\n    fuse_rq: whether to fuse the requantization operation with a preceding operation (such as a convolution or activation function).;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "multiplier", "type": "I64ArrayAttr" },
      { "name": "rshift", "type": "I64ArrayAttr" },
      { "name": "quant_mode", "type": "RequantModeAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "rq_axis", "type": "DefaultValuedAttr" },
      { "name": "fuse_rq", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Reshape",
    "summary": "Reshape operation",
    "description": "1.Op Introduction\r\n    Returns a tensor with the same type/values as the input, with a new shape\r\n    specified by the shape argument. Reshape may operate on tensors of any rank.\r\n    No data conversion happens during a reshape operation.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = ReshapeOp(input, shape)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    shapeT(act.): an optional input tensor that specifies the desired shape for the output tensor.;\r\n\r\n    4.attribute\r\n    shape: 0: keep dim from input; -1: left dim from input.;\r\n    flatten_start_dim: the starting dimension from which to begin flattening the input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "shapeT", "type": "Optional" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "shape", "type": "OptionalAttr" },
      { "name": "flatten_start_dim", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.RetinaFaceDetection",
    "summary": "RetinaFaceDetection operator",
    "description": "1.Op Introduction\r\n    Perform retinaface detection on feature map\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = NMS(Filter(Detect(inputs), confidence_threshold), nms_threshold, keep_topk)\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor;\r\n\r\n    4.attributes\r\n    nms_threshold: nms threshold.;\r\n    confidence_threshold: classification confidence threshold.;\r\n    keep_topk: after nms, keep bbox num.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "nms_threshold", "type": "F64Attr" },
      { "name": "confidence_threshold", "type": "F64Attr" },
      { "name": "keep_topk", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.Reverse",
    "summary": "Reverse operation",
    "description": "1.Op Introduction\r\n    Reverse on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = ReverseOp(input, axis)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    axis: the dimension of reverse;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.RMSNorm",
    "summary": "RMSNorm operation",
    "description": "1.Op Introduction\r\n    A simplification of the original layer normalization (LayerNorm).\r\n    Only normalize the last dimension of tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = gamma * input / sqrt(mean(input ^ 2) + epsilon)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    gamma(w.): scalar.;\r\n\r\n    4.attributes\r\n    eps: a small constant added to the denominator during normalization to prevent division by zero.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "gamma", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "eps", "type": "F64Attr" },
      { "name": "weight_keep_f32", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.RoiAlign",
    "summary": "RoiAlign operator",
    "description": "1.Op Introduction\r\n    RoiAlign consumes an input tensor X and region of interests\r\n    (rois) to apply pooling across each RoI.\r\n\r\n    2.Math formula\r\n    ```math\r\n        1.ROI coordinate scaling[x1, y1, x2, y2]\r\n            x_scaled = x x spatial_scale\r\n            y_scaled = y x spatial_scale\r\n        2.Delineation of grid sub-areas\r\n            bin_height = (y2_scaled - y1_scaled) / output_height\r\n            bin_width  = (x2_scaled - x1_scaled) / output_width\r\n        3.align_corners -> true\r\n            x_grid = x1_scaled + (i + 0.5) x bin_width\r\n            y_grid = y1_scaled + (j + 0.5) x bin_height\r\n        output = RoiAlign(input, rois, output_height, output_width, sampling_ratio, spatial_scale, align_corners)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor(4D);\r\n    rois(w.): RoIs (Regions of Interest) to pool over.;\r\n          rois is 2-D input of shape (num_rois, 4) given as [[x1, y1, x2, y2], ...].;\r\n\r\n    4.attributes\r\n    mode: the type of comparison to be performed between the two input tensors.\r\n          mdoe include Equal, Not Equal, Less Than, Less Than or Equal, Greater Than and Greater Than or Equal;\r\n    output_height: the height of the output feature maps.;\r\n    output_width: the width of the output feature maps.;\r\n    sampling_ratio: the number of sampling points in each direction (height and width).;\r\n    spatial_scale: a scaling factor that maps the input coordinates (RoIs) to the input feature map's scale.;\r\n    align_corners: whether to align the corners of the input and output tensors.;\r\n    batch_indices: 1-D tensor with each element denoting the index of the corresponding image in the batch.",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "rois", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "RoiAlignModeAttr" },
      { "name": "output_height", "type": "I64Attr" },
      { "name": "output_width", "type": "I64Attr" },
      { "name": "sampling_ratio", "type": "I64Attr" },
      { "name": "spatial_scale", "type": "F64Attr" },
      { "name": "align_corners", "type": "BoolAttr" }
    ]
  },
  {
    "name": "top.RoiExtractor",
    "summary": "RoiExtractor operator",
    "description": "1.Op Introduction\r\n    RoiExtractor consumes an input tensor X and region of interests\r\n    (rois) to apply pooling across each RoI.\r\n\r\n    2.Math formula\r\n    ```math\r\n        1.ROI coordinate scaling[x1, y1, x2, y2]\r\n            x_scaled = x x spatial_scale\r\n            y_scaled = y x spatial_scale\r\n        2.Delineation of grid sub-areas\r\n            bin_height = (y2_scaled - y1_scaled) / output_height\r\n            bin_width  = (x2_scaled - x1_scaled) / output_width\r\n        3.align_corners -> true\r\n            x_grid = x1_scaled + (i + 0.5) x bin_width\r\n            y_grid = y1_scaled + (j + 0.5) x bin_height\r\n        output_i = RoiAlign(input, rois_i, output_height, output_width, sampling_ratio, spatial_scale, align_corners)\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor;\r\n    rois(w.): RoIs (Regions of Interest) to pool over.;\r\n          rois is 2-D input of shape (num_rois, 4) given as [[x1, y1, x2, y2], ...].;\r\n    target_lvls(w.): 1-D tensor with each element denoting the index of the corresponding image in the batch.;\r\n\r\n    4.attributes\r\n    mode: the type of comparison to be performed between the two input tensors.\r\n          mdoe include Equal, Not Equal, Less Than, Less Than or Equal, Greater Than and Greater Than or Equal;\r\n    num_levels: The number of levels in the feature pyramid.;\r\n    output_height: the height of the output feature maps.;\r\n    output_width: the width of the output feature maps.;\r\n    sampling_ratio: the number of sampling points in each direction (height and width).;\r\n    spatial_scale: a scaling factor that maps the input coordinates (RoIs) to the input feature map's scale.;\r\n    align_corners: whether to align the corners of the input and output tensors.;\r\n    is_static: whether the operation has a static shape.;",
    "inputs": [
      { "name": "rois", "type": "AnyTensor" },
      { "name": "target_lvls", "type": "AnyTensor" },
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "RoiAlignModeAttr" },
      { "name": "num_levels", "type": "I64Attr" },
      { "name": "output_height", "type": "I64Attr" },
      { "name": "output_width", "type": "I64Attr" },
      { "name": "sampling_ratio", "type": "I64Attr" },
      { "name": "spatial_scales", "type": "F64ArrayAttr" },
      { "name": "align_corners", "type": "BoolAttr" },
      { "name": "is_static", "type": "BoolAttr" }
    ]
  },
  {
    "name": "top.ROIPooling",
    "summary": "ROIPooling operator",
    "description": "1.Op Introduction\r\n    Max pooling on ROI.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output(pooled_h, pooled_w) = max_pooling(input(h, w) in ROI(spatial_scale))\r\n    ```\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    pooled_h: pooled output height.;\r\n    pooled_w: pooled output width.;\r\n    spatial_scale: adjust the ROI coordinates.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "pooled_h", "type": "I64Attr" },
      { "name": "pooled_w", "type": "I64Attr" },
      { "name": "spatial_scale", "type": "F64Attr" }
    ]
  },
  {
    "name": "top.Rope",
    "summary": "Rope operator",
    "description": "1.Op Introduction\r\n    The Rope operator is a specialized tensor operation designed for efficient computations involving multiple input tensors.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output=saturation((input1 x shift(input2, mul1_shift))⊕(input3 x shift(input2, mul2_shift))) + shift(input3, dd_shift)\r\n    ```\r\n    The operator ⊕ represents the addition of the two multiplicative results.\r\n    The function shift(input,shift_value) applies a shift to the input tensor based on the provided shift value.\r\n    The saturation function ensures that the output remains within a defined range, preventing overflow or underflow.\r\n\r\n    3.activation and weight\r\n    input1(act.): input tensor;\r\n    input2(act.): input tensor;\r\n    input3(act.): input tensor;\r\n\r\n    4.attribute\r\n    is_permute_optimize:whether to apply optimization for permuting the input tensors.;\r\n    mul1_round_mode: the rounding mode to be used for the first multiplication operation.;\r\n    mul2_round_mode: Similar to mul1_round_mode, this attribute defines the rounding mode for the second multiplication operation.;\r\n    add_round_mode: the rounding mode for the addition operation.;\r\n    mul1_shift: the number of bits to shift the result of the first multiplication.;\r\n    mul2_shift: Similar to mul1_shift, this attribute defines the number of bits to shift for the second multiplication operation.;\r\n    add_shift: the number of bits to shift the result of the addition operation.;\r\n    mul1_saturation: whether the output of the first multiplication should be saturated.\r\n                     When set to true, the result will be clamped to prevent overflow or underflow.;\r\n    mul2_saturation: Similar to mul1_saturation, this attribute specifies whether saturation should be applied to the second multiplication's output.;\r\n    add_saturation: whether to apply saturation to the output of the addition operation.;",
    "inputs": [
      { "name": "input1", "type": "AnyTensor" },
      { "name": "input2", "type": "AnyTensor" },
      { "name": "input3", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "is_permute_optimize", "type": "DefaultValuedAttr" },
      { "name": "mul1_round_mode", "type": "DefaultValuedAttr" },
      { "name": "mul2_round_mode", "type": "DefaultValuedAttr" },
      { "name": "add_round_mode", "type": "DefaultValuedAttr" },
      { "name": "mul1_shift", "type": "DefaultValuedAttr" },
      { "name": "mul2_shift", "type": "DefaultValuedAttr" },
      { "name": "add_shift", "type": "DefaultValuedAttr" },
      { "name": "mul1_saturation", "type": "DefaultValuedAttr" },
      { "name": "mul2_saturation", "type": "DefaultValuedAttr" },
      { "name": "add_saturation", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Round",
    "summary": "Round operator",
    "description": "1.Op Introduction\r\n    Round takes one input Tensor and rounds the values, element-wise,\r\n    meaning it finds the nearest integer for each value. In case of halfs,\r\n    the rule is to round them to the nearest even integer.\r\n    If input x is integral, +0, -0, NaN, or infinite, x itself is returned.\r\n    The output tensor has the same shape and type as the input.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Round(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Rsqrt",
    "summary": "Rsqrt Operator",
    "description": "1.Op Introduction\r\n    Reverse square root.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = \\frac{1}{\\sqrt{input}}\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Scale",
    "summary": "Scale operator",
    "description": "1.Op Introduction\r\n    Y = X * S + B,\r\n    where the shape of X/Y is [n, c, h, w] and the shape of S/B is [1, c, 1, 1].\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = input x scale + bias\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    scale(w.): scalar;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attributes\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "scale", "type": "AnyTensor" },
      { "name": "bias", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.ScaleDotProductAttention",
    "summary": "ScaleDotProductAttention operator (pytorch)",
    "description": "1.Op Introduction\r\n    Scale_Dot_Product_Attention Operation for pytorch.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = (softmax(Q * K^T) / sqrt(d_k) + mask) * V\r\n    ```\r\n\r\n    3.activation and weight\r\n    query(act.): queries input tensor.;\r\n    key(act.): keys input tensor.;\r\n    value(act.): values input tensor.;\r\n    mask(w.): the learnable masks of the module of shape.;\r\n\r\n    4.attributes\r\n    dropout_p: the dropout probability for attention weights.;\r\n    is_causal: whether the attention should be causal.;\r\n    scale: the scaling factor.;",
    "inputs": [
      { "name": "query", "type": "AnyTensor" },
      { "name": "key", "type": "AnyTensor" },
      { "name": "value", "type": "AnyTensor" },
      { "name": "mask", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "dropout_p", "type": "DefaultValuedAttr" },
      { "name": "is_causal", "type": "DefaultValuedAttr" },
      { "name": "scale", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.ScaleLut",
    "summary": "Scale by lut operator",
    "description": "1.Op Introduction\r\n    Performs scale on input, y = input * scale + bias.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = input * scale + bias\r\n    ```\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    scale: each channel scale.;\r\n    bias: each channel bias.;\r\n    sign: if output is signed.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "scale", "type": "F64ArrayAttr" },
      { "name": "bias", "type": "F64ArrayAttr" },
      { "name": "sign", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.ScatterElements",
    "summary": "ScatterElements op",
    "description": "1.Op Introduction\r\n    ScatterElements takes three inputs data, updates, and indices of the same rank r >= 1 and an optional attribute axis that\r\n    identifies an axis of data (by default, the outer-most axis, that is axis 0). The output of the operation is produced by\r\n    creating a copy of the input data, and then updating its value to values specified by updates at specific index\r\n    positions specified by indices. Its output shape is the same as the shape of data.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = ScatterElements(input[axis], updates, indices)\r\n    ```\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    indices(w.): Tensor of int32/int64 indices, of r >= 1 (same rank as input).\r\n             All index values are expected to be within bounds [-s, s-1] along axis of size s.\r\n    updates(w.): Tensor of rank r >=1 (same rank and shape as indices).\r\n\r\n    4.attributes\r\n    axis: the dimension of the input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "indices", "type": "AnyTensor" },
      { "name": "updates", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "I64Attr" },
      { "name": "reduction", "type": "DefaultValuedAttr" },
      { "name": "nc_can_split", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.ScatterND",
    "summary": "ScatterND operator",
    "description": "1.Op Introduction\r\n    The output of the operation is produced by creating a copy of the input data,\r\n    and then updating its value to values specified by updates at\r\n    specific index positions specified by indices.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = ScatterND(input_data[indices], updates, reduction)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input_data(act.): input tensor;\r\n    indices(w.): Tensor of rank q >= 1.;\r\n    updates(w.): Tensor of rank q + r - indices_shape[-1] - 1.;\r\n\r\n    4.attributes\r\n    reduction: Type of reduction to apply: none (0 default), add(1), sub(2), max(3), min(4), mul(5).;",
    "inputs": [
      { "name": "input_data", "type": "AnyTensor" },
      { "name": "indices", "type": "AnyTensor" },
      { "name": "updates", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "reduction", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.SelectiveScan",
    "summary": "2D Selective Scan Operator (specialized for VMamba)",
    "description": "1. Op Introduction\r\n    Performs structured state space modeling (SSM) on 2D image data using a bidirectional scanning mechanism.\r\n    Core component of the VMamba architecture that enables efficient long-range dependency modeling in visual data.\r\n\r\n    2. Math formula\r\n    For each scanning direction:\r\n    ```\r\n    h_t = δA_t ⊙ h_{t-1} + δB_t ⊙ u_t\r\n    y_t = c_t ⊙ h_t\r\n    ```\r\n    Final output:\r\n    ```\r\n    output = concat(y_forward, y_backward) + u ⊙ D\r\n    ```\r\n    code:\r\n\r\n    for i in range(L):\r\n        x_up = deltaA_up[:, :, i, :] * x_up + deltaB_u_up[:, :, i, :]\r\n        x_down = deltaA_down[:, :, L - 1 - i, :] * x_down + deltaB_u_down[:, :, L - 1 - i, :]\r\n        y_up[i, :, :] = x_up[0, :, :] * c_up[i, :, 0, :]\r\n        y_down[L - 1 - i, :, :] = x_down[0, :, :] * c_down[L - 1 - i, :, 0, :]\r\n\r\n    y = concat((y_up, y_down), dim=1)\r\n    out = y if D is None else y + u * D\r\n\r\n    3. Input parameters:\r\n    u: Input feature map tensor (after linear projection) [N, C, L, Batch]\r\n    c: State-to-output projection weights [N, C, L, Batch]\r\n    D: Residual connection weights (optional) [N, C]\r\n    δA: Discretized state transition matrix [N, C, L, Batch]\r\n    δB_u: Combined input projection and discretized control matrix [N, C, L, Batch]\r\n\r\n    4. Attributes:\r\n    direction: Bidirectional scanning scheme (forward & reverse)\r\n    time_dim: Sequence length dimension (L = H × W)\r\n    channel_split: Channel dimension partitioning factor (C → C//2)\r\n    residual: Whether to apply residual connection (D term)",
    "inputs": [
      { "name": "Cs", "type": "AnyTensor" },
      { "name": "deltaA", "type": "AnyTensor" },
      { "name": "deltaB_u", "type": "AnyTensor" },
      { "name": "us", "type": "AnyTensor" },
      { "name": "Ds", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Shape",
    "summary": "Shape operation",
    "description": "1.Op Introduction\r\n    Takes a tensor as input and outputs an 1D int tensor containing the shape of the input tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = shape(input[d1, d2,...,dn])\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    start(w.): the ending indices for slicing along each axis.;\r\n    step: the step sizes for slicing along each axis.;\r\n    end: the ending indices for slicing along each axis.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "start", "type": "OptionalAttr" },
      { "name": "end", "type": "OptionalAttr" },
      { "name": "step", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "top.ShuffleChannel",
    "summary": "ShuffleChannel operator",
    "description": "1.Op Introduction\r\n    Perform ShuffleChannel on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output(N, C, H, W) = input(N, Shuffle(C), H, W)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    group: An integer specifying the number of groups to divide the channels into.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "group", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.Sigmoid",
    "summary": "Exp operator,  scale * Sigmoid + bias",
    "description": "1.Op Introduction\r\n    Y = scale * Sigmoid(x) + bias\r\n    if log --> Y = Log(scale * Sigmoid(x) + bias)\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = scale * Sigmoid(input) + bias\r\n\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    scale: a scaling factor applied to the attention scores before they are passed through the softmax function.;\r\n    bias: added to the result of the matrix multiplication. ;\r\n    log: whether the output should be computed using the logarithm of the scaled sigmoid function. ;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "scale", "type": "DefaultValuedAttr" },
      { "name": "bias", "type": "DefaultValuedAttr" },
      { "name": "log", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Sign",
    "summary": "Sign Operator",
    "description": "1.Op Introduction\r\n    Calculate the sign of the given input tensor element-wise.\r\n    If input > 0, output 1. if input < 0, output -1. if input == 0,\r\n    output 0.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = SignOp(input) -> 0/1\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.SiLU",
    "summary": "SiLU operator,  y = x * Sigmoid(x)",
    "description": "1.Op Introduction\r\n    An activation function.\r\n    Smooth nonlinear transformation is provided to avoid the gradient disappearance problem of ReLU.;\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = input * Sigmoid(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Sin",
    "summary": "Sin operator",
    "description": "1.Op Introduction\r\n    Calculates the Sin of the given input tensor, element-wise.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = sin(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Sinh",
    "summary": "Sinh operator",
    "description": "1.Op Introduction\r\n    Calculates the Sinh of the given input tensor, element-wise.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = sinh(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Size",
    "summary": "Size operator",
    "description": "1.Op Introduction\r\n    gen by torch aten::size. The Size operation retrieves the size (or shape) of the given input tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = SizeOp(input) ->[dim0, dim1, dim2..., dimN]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    axis: the dimension of reverse;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "top.Slice",
    "summary": "Slice operator",
    "description": "1.Op Introduction\r\n    Slice Operation on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = input[offset[j] : ends[j] : steps[j]]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    offsetT(w.): the starting indices for each slice along the specified axes.;\r\n    endsT(w.): the ending indices for each slice along the specified axes.;\r\n    stepsT(w.): the step sizes for each slice along the specified axes.;\r\n\r\n    4.attribute\r\n    offset: An array of the starting indices for slicing along each axis.;\r\n    steps: An array of the step sizes for slicing along each axis.;\r\n    ends: An array of the ending indices for slicing along each axis.;\r\n    axes: An array of the axes along which to perform the slicing operation.;\r\n    hasparamConvert_axes: whether parameter conversion is needed for the specified axes.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "offsetT", "type": "AnyTensorOrNone" },
      { "name": "endsT", "type": "AnyTensorOrNone" },
      { "name": "stepsT", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "offset", "type": "I64ArrayAttr" },
      { "name": "steps", "type": "I64ArrayAttr" },
      { "name": "ends", "type": "I64ArrayAttr" },
      { "name": "axes", "type": "DefaultValuedAttr" },
      { "name": "hasparamConvert_axes", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.SliceAxis",
    "summary": "Slice operator on one axis",
    "description": "1.Op Introduction\r\n    Slice Operation on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = input[starts[j] + i * strides[j]]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    axis(w.): the dimension of the input tensor.;\r\n    start(w.): the ending indices for slicing along each axis.;\r\n    step: the step sizes for slicing along each axis.;\r\n    end: the ending indices for slicing along each axis.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "axis", "type": "AnyTensor" },
      { "name": "start", "type": "AnyTensor" },
      { "name": "step", "type": "AnyTensorOrNone" },
      { "name": "end", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Softmax",
    "summary": "Softmax operation",
    "description": "1.Op Introduction\r\n    Integrates some operations related to softmax.\r\n\r\n    2.Math formula\r\n    ```math\r\n            \\text{output}[i] = \\frac{e^{\\text{input}[i]}}{\\sum_{j} e^{\\text{input}[j]}}\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    axis: the dimension of the input tensor.;\r\n    log: when set to true, indicates that the output should be computed in log space.;\r\n    beta: scaling factor.;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "SI32Attr" },
      { "name": "log", "type": "DefaultValuedAttr" },
      { "name": "beta", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.SoftmaxBwd",
    "summary": "softmax backward operator",
    "description": "1.Op Introduction\r\n    Integrates some operations related to softmax backward.\r\n\r\n    2.Math formula\r\n    ```math\r\n            grad_input[i] = softmax(output)[i] * (grad_output[i] - \\sum{j}grad_output[j] * softmax(output)[j])\r\n    ```\r\n\r\n    3.activation and weight\r\n    grad_output(act.): the gradient of the loss with respect to the output.;\r\n    output(act.): output tensor.;\r\n\r\n    4.attributes\r\n    dim: If set to 0, computed across rows, If set to 1, computed across columns.;",
    "inputs": [
      { "name": "grad_output", "type": "AnyTensor" },
      { "name": "output", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "grad_input", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "dim", "type": "SI32Attr" }
    ]
  },
  {
    "name": "top.Softplus",
    "summary": "Softplus operation",
    "description": "1.Op Introduction\r\n    a smooth approximation of the ReLU (Rectified Linear Unit) activation function.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = ln(exp(input) + 1)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Softsign",
    "summary": "Softsign Operator",
    "description": "1.Op Introduction\r\n    The Softsign operation is an activation function that provides a smooth approximation of the sign function.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = input / (1 + |input|)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Sort",
    "summary": "Sort operation",
    "description": "1.Op Introduction\r\n    Integrates some operations related to Sort.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Sort(input, axis, descending)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    axis: the dimension of the input tensor.;\r\n    descending: the order of sorting.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "values", "type": "AnyTensorOrNone" },
      { "name": "indices", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "I64Attr" },
      { "name": "descending", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Split",
    "summary": "Split operator",
    "description": "1.Op Introduction\r\n    Split input tensor into a list of tensors.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = input[i * split_size: (i + 1) * split_size] for i = 0, 1, ... num - 1\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    axis: the dimension of split;\r\n    num: the number of equal parts to split the input tensor into along the specified axis.;\r\n    split_size: the exact sizes of each split along the specified axis.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "axis", "type": "SI32Attr" },
      { "name": "num", "type": "I64Attr" },
      { "name": "split_size", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "top.Sqrt",
    "summary": "Sqrt operation",
    "description": "1.Op Introduction\r\n    Computes the square root of the input tensor's element.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Sqrt(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Squeeze",
    "summary": "Squeeze operator",
    "description": "1.Op Introduction\r\n    The operator squeeze the input shapes by given axis.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = squeeze(input, axes)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    axes: the dimensions (axes) of the input tensor that should be squeezed (removed).;\r\n    is_scalar: whether the addition operation is performed with scalar values or tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "axes", "type": "I64ArrayAttr" },
      { "name": "is_scalar", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.StridedSlice",
    "summary": "Strided Slice operator",
    "description": "1.Op Introduction\r\n    Strided Slice Operation on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = input[starts[j] + i * strides[j]]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    starts(w.): the starting indices for each dimension of the input tensor.;\r\n    ends(w.): the ending indices for each dimension of the input tensor.;\r\n    strides(w.):  the stride values for each dimension, determining the step size between indices in the slicing operation.;\r\n\r\n    4.attribute\r\n    begin_mask: If set, the start index for that dimension is considered as 0.;\r\n    end_mask: If set, the end index for that dimension is considered as the size of the dimension.;\r\n    ellipsis_mask: whether allowing for the selection of all dimensions in between specified slices.;\r\n    new_axis_mask: which dimensions should be added as new axes in the output tensor.;\r\n    shrink_axis_mask: which dimensions should be removed from the output tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "starts", "type": "AnyTensor" },
      { "name": "ends", "type": "AnyTensor" },
      { "name": "strides", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "begin_mask", "type": "I64Attr" },
      { "name": "end_mask", "type": "I64Attr" },
      { "name": "ellipsis_mask", "type": "I64Attr" },
      { "name": "new_axis_mask", "type": "I64Attr" },
      { "name": "shrink_axis_mask", "type": "I64Attr" }
    ]
  },
  {
    "name": "top.Sub",
    "summary": "sub operator",
    "description": "1.Op Introduction\r\n    Elementwise subtraction of input1 and input2. Axis of size 1 will be broadcast,\r\n    as necessary.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = ReLU((input1 - input2; dim))\r\n    ```\r\n    Axis of size 1 will be broadcast if necessary.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    is_reverse: whether the subtraction operation is performed in reverse order.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    coeff: It is an array and allows for scaling the output of the addition operation.;\r\n    is_scalar: whether the addition operation is performed with scalar values or tensor.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "is_reverse", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "coeff", "type": "OptionalAttr" },
      { "name": "is_scalar", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.SubConst",
    "summary": "Sub Const operator",
    "description": "1.Op Introduction\r\n    Elementwise subtraction of input1 and input2. Input1 or Input2 is constant.\r\n    as necessary.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = input - const_val or const_val - input\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    const_val: the constant value to be added to each element of the input tensor(positive, negative, or zero).;\r\n    is_reverse: This boolean attribute indicates whether the subtraction operation is performed in reverse order.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    is_scalar: whether the addition operation is performed with scalar values or tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "const_val", "type": "F64Attr" },
      { "name": "is_reverse", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "is_scalar", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.SwapChannel",
    "summary": "swap channel operator, normally RGB <=> BGR",
    "description": "1.Op Introduction\r\n    Swap Channel on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output(h, w, c) = input(h, w, channel_order)\r\n    ```\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    channel_order: channel swap order.;\r\n    quant: a QuantParam struct attributes.;\r\n    name: name for calibration, comparing, or debug.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "channel_order", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "top.SwapDimInner",
    "summary": "if offset is not 0, split there and swap first part and second part of it",
    "description": "1.Op Introduction\r\n    a dimension-swapping operation based on a specified offset.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = SwapDimInner(input, offset)\r\n    ```\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    offset: the position at which the input tensor is split.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "offset", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "top.Swish",
    "summary": "Swish operation",
    "description": "1.Op Introduction\r\n    hardswish(x) := x * sigmoid(x * beta)\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = input[i] * sigmoid(input[i] * beta)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    beta: scalar;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "beta", "type": "F64Attr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Tan",
    "summary": "Tan operator",
    "description": "1.Op Introduction\r\n    Calculates the tan of the given input tensor, element-wise.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Tan(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Tanh",
    "summary": "Tanh operator",
    "description": "1.Op Introduction\r\n    Calculates the tanh of the given input tensor, element-wise.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Tan(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "round_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Tile",
    "summary": "Tile operator",
    "description": "1.Op Introduction\r\n    Perform Tile operation on the given tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i_1, i_2, i_3,...i_k] = input[i_1 mod d_1, i_2 mod d_2, i_3 mod d_3,..., i_k mod d_k]\r\n    ```\r\n    where d_j represents the corresponding dimension size of the input tensor after tiling.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    tileT(w.): how many times to replicate the input tensor along each dimension.;\r\n\r\n    4.attributes\r\n    tile: the number of times to replicate the input tensor along each dimension.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "tileT", "type": "Optional" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "tile", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "top.TopK",
    "summary": "TopK operation",
    "description": "1.Op Introduction\r\n    Integrates some operations related to topk.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output_values, output_indices = TopK(input, K, axis, largest, sorted)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    axis: the dimension of the input tensor.;\r\n    K: how many of the largest (or smallest, depending on the largest attribute) values will be returned. defaults is -1;\r\n    largest: whether to retrieve the largest or smallest values.;\r\n    sorted: whether the output values should be sorted in descending order (if largest is true) or ascending order (if largest is false).;\r\n    kT: provide a specific tensor for K values. This allows for dynamic specification of K.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "kT", "type": "Optional" }
    ],
    "outputs": [
      { "name": "values", "type": "AnyTensorOrNone" },
      { "name": "indices", "type": "AnyTensorOrNone" }
    ],
    "attributes": [
      { "name": "axis", "type": "I64Attr" },
      { "name": "K", "type": "DefaultValuedAttr" },
      { "name": "largest", "type": "DefaultValuedAttr" },
      { "name": "sorted", "type": "DefaultValuedAttr" },
      { "name": "replace_topk_indices", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Transpose",
    "summary": "Transpose operator",
    "description": "1.Op Introduction\r\n    Transpose on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output(dim1, dim0) = TransposeOp(input(dim0, dim1))\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    dim0: the first dimension of input tensor.;\r\n    dim1: the second dimension of input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "dim0", "type": "SI32Attr" },
      { "name": "dim1", "type": "SI32Attr" }
    ]
  },
  {
    "name": "top.Trilu",
    "summary": "Trilu operation",
    "description": "1.Op Introduction\r\n    Returns the upper or lower triangular part of input.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Triu(input, diagonal) if upper = 1\r\n            output = Tril(input, diagonal) if upper = 0\r\n    ```\r\n    where, Triu() return the upper triangular part of the input tensor.\r\n           Tril() return the lower triangular part of the input tensor.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    upper: whether to extract the upper or lower triangular part of the input tensor.;\r\n    diagonal: 0 refers to the main diagonal, positive values indicate diagonals above the main diagonal,\r\n              and negative values indicate diagonals below the main diagonal.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "upper", "type": "SI32Attr" },
      { "name": "diagonal", "type": "SI32Attr" }
    ]
  },
  {
    "name": "top.Tuple",
    "summary": "Tuple operator",
    "description": "1.Op Introduction\r\n    gen by torch prim::TupleConstruct, y = (a, b)\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = TupleOp(input1, input2)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Unpack",
    "summary": "Unpack operator",
    "description": "1.Op Introduction\r\n    Unpack a tensor to list of tensors in the given dimension.\r\n\r\n    2.Math formula\r\n    output1, output2, output3 = UnPack(input; axis)\r\n\r\n    3.activation and weight\r\n    input(act.): Variadic input tensor;\r\n\r\n    4.attribute\r\n    axis: It specifies the dimension along which the input tensors will be packed together.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "axis", "type": "SI32Attr" }
    ]
  },
  {
    "name": "top.Unsqueeze",
    "summary": "Unsqueeze operator",
    "description": "1.Op Introduction\r\n    The operator unsqueeze the input shapes by given axis.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = unsqueeze(input, axes)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    axes: the dimensions (axes) of the input tensor that should be squeezed (removed).;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "axes", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "top.UnTuple",
    "summary": "UnTuple operator",
    "description": "1.Op Introduction\r\n    gen by torch prim::TupleUnpack, a, b = y\r\n\r\n    2.Math formula\r\n    ```math\r\n        output1, output2 = UnTupleOp(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ]
  },
  {
    "name": "top.Upsample",
    "summary": "Upsample operation",
    "description": "1.Op Introduction\r\n    Perform nearest upsample on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i, j] = Upsample(input[i / scale_h, j / scale_w])\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    scale_h: the scaling factor for the height (number of rows) of the input tensor.;\r\n    scale_w: the scaling factor for the width (number of columns) of the input tensor.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "scale_h", "type": "I64Attr" },
      { "name": "scale_w", "type": "I64Attr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Variance",
    "summary": "Compute Variance operator",
    "description": "1.Op Introduction\r\n    variance\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = \\frac{1}{N} \\sum_{i=1}^{N} (input_i - mean)^2\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    reduce_list: A list of dimensions along which to compute the variance.;\r\n    correction: correction factor.;\r\n    keep_dims: whether to keep the dimensions of the output tensor the same as the input tensors.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "reduce_list", "type": "I64ArrayAttr" },
      { "name": "correction", "type": "F64Attr" },
      { "name": "keep_dims", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.View",
    "summary": "View operation",
    "description": "1.Op Introduction\r\n    gen by torch aten::view, the view operation allows for changing the shape of the tensor without altering its data.\r\n    0: keep dim from input\r\n    -1: left dim from input\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = ViewOp(input, shape)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    shape(w.): 0: keep dim from input; -1: left dim from input.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "shape", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "top.Weight",
    "summary": "weight operator",
    "description": "If `inline_bytes` is not defined or `inline_bytes` is a null string:\r\n      Load weight from a file. The file should be a valid .npz format file.\r\n      This Op does not take any input, and the location captures the tensor name.\r\n      The Output is an n-dimensional tensor whose type matches\r\n      the tensor type in the .npz file.\r\n    Else:\r\n      Load weight from `inline_bytes`.",
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "scale", "type": "OptionalAttr" },
      { "name": "store_mode", "type": "OptionalAttr" },
      { "name": "allow_split", "type": "OptionalAttr" },
      { "name": "indices_idx", "type": "OptionalAttr" },
      { "name": "indices_slice", "type": "OptionalAttr" },
      { "name": "do_compress", "type": "OptionalAttr" },
      { "name": "bias0", "type": "OptionalAttr" },
      { "name": "bias1", "type": "OptionalAttr" },
      { "name": "is_signed", "type": "OptionalAttr" },
      { "name": "zero_guard", "type": "OptionalAttr" },
      { "name": "inline_bytes", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "top.WeightReorder",
    "summary": "WeightReorder operator",
    "description": "1.Op Introduction\r\n    reorder Weight.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Reorder(input, reorder_mode)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    reorder_mode: rearranging the weight tensor, such as sorting, shuffling, or applying a specific permutation.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "reorder_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Where",
    "summary": "Where operation",
    "description": "1.Op Introduction\r\n    Return elements, either from X or Y, depending on condition.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = tbrn if condition else fbrn\r\n    ```\r\n\r\n    3.activation and weight\r\n    cond(act.): a tensor that serves as the condition for selecting elements from the true branch (tbrn) or the false branch (fbrn).;\r\n    tbrn(w.): the tensor that will be selected when the condition is true.;\r\n    fbrn(w.): the tensor that will be selected when the condition is false.;\r\n\r\n    4.attributes\r\n    x_is_const: the tensor for the true branch (tbrn) is a constant.;\r\n    y_is_const: the tensor for the false branch (fbrn) is a constant.;\r\n    x_const_val: the constant value to be used for the true branch if tbrn is not provided or is constant.;\r\n    y_const_val: the constant value to be used for the false branch if fbrn is not provided or is constant.;",
    "inputs": [
      { "name": "cond", "type": "AnyTensor" },
      { "name": "tbrn", "type": "AnyTensorOrNone" },
      { "name": "fbrn", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "x_is_const", "type": "DefaultValuedAttr" },
      { "name": "y_is_const", "type": "DefaultValuedAttr" },
      { "name": "x_const_val", "type": "DefaultValuedAttr" },
      { "name": "y_const_val", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Yield",
    "summary": "Yield operation",
    "description": "1.Op Introduction\r\n    The `top.Yield` operation represents a return operation within an subgraph.\r\n    The operation takes variable number of operands and produces no results.\r\n    This operation is not part of the standard and was added to assist tpu-mlr.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Yield(operands)\r\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ]
  },
  {
    "name": "top.YoloDetection",
    "summary": "YoloDetection operator",
    "description": "1.Op Introduction\r\n    Perform yolo detection on feature map.\r\n\r\n    2.Math formula\r\n    ```math\r\n        1.Feature Map output\r\n            raw_predictions = {(b_i, c_i, p_i) | i = 1, 2,...,N}\r\n            b_i is the bounding box coordinates, c_i is the i-th class score, p_i is the i-th obj score.\r\n        2.Apply Objectness Threshold\r\n            filtered_predictions = {(b_i, c_i, p_i) | p_i >= obj_threshold}\r\n        3.Non-Maximum Suppression(NMS)\r\n            nms_output = NMS(filtered_predictions, nms_threshold)\r\n        4.Top K Detections\r\n            output = top_k(nms_output, keep_topk)\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor;\r\n\r\n    4.attributes\r\n    net_input_h: The height of the input image.;\r\n    net_input_w: The width of the input image;\r\n    nms_threshold: The threshold used for Non-Maximum Suppression (NMS).;\r\n    obj_threshold: The minimum confidence score required for an object detection to be considered valid.;\r\n    keep_topk: The maximum number of detections to keep after applying NMS.;\r\n    anchors: A list of anchor box dimensions.;\r\n    version: The version of the YOLO model being used.;\r\n    class_num: The number of classes that the YOLO model can predict.;\r\n    num_boxes: The number of bounding boxes that the model predicts for each grid cell in the feature map.;\r\n    agnostic_nms: whether to use class-agnostic NMS.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "net_input_h", "type": "I64Attr" },
      { "name": "net_input_w", "type": "I64Attr" },
      { "name": "nms_threshold", "type": "F64Attr" },
      { "name": "obj_threshold", "type": "F64Attr" },
      { "name": "keep_topk", "type": "I64Attr" },
      { "name": "anchors", "type": "F64ArrayAttr" },
      { "name": "version", "type": "YoloVersionAttr" },
      { "name": "class_num", "type": "DefaultValuedAttr" },
      { "name": "num_boxes", "type": "DefaultValuedAttr" },
      { "name": "agnostic_nms", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "top.Yuv2rgbFormula",
    "summary": "Yuv2rgb formula operator",
    "description": "1.Op Introduction\r\n    Yuv2rgb formula Operator.\r\n\r\n    2.Math formula\r\n    ```math\r\n            (R) = (Y + 1.402 x (V - 128))\r\n            (G) = (Y - 0.344136 x (U - 128) - 0.714136 x (V - 128))\r\n            (B) = (Y + 1.772 x (U - 128))\r\n\r\n    ```\r\n\r\n    3.activation and weight\r\n    YUV(act.): input tensor.;\r\n\r\n    4.attributes\r\n    src_format: the source format of the input YUV data.;\r\n    dst_format: the desired destination format for the output RGB data.;\r\n    image_format: how the YUV data should be processed and how the output RGB data should be structured.;\r\n    formula_mode: the mode of the conversion formula used for the YUV to RGB transformation. ;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;",
    "inputs": [
      { "name": "YUV", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "src_format", "type": "UI32Attr" },
      { "name": "dst_format", "type": "UI32Attr" },
      { "name": "image_format", "type": "ImageOutFormatAttr" },
      { "name": "formula_mode", "type": "Yuv2rgbFormulaAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "torch_c.from_builtin_tensor",
    "summary": "Convert a `tensor` to a `!torch.vtensor`",
    "description": "This op only operates on ValueTensorType, to avoid conflating conversions\n    between value-semantic and non-value-semantic types.",
    "inputs": [
      { "name": "operand", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_ValueTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` qualified(type($operand)) `->` qualified(type($result))"
  },
  {
    "name": "torch_c.from_f64",
    "summary": "Convert an `f64` to a `!torch.float`",
    "description": "This op is primarily useful as a materialization during dialect conversion.",
    "inputs": [
      { "name": "operand", "type": "F64" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ],
    "assemblyFormat": "$operand attr-dict"
  },
  {
    "name": "torch_c.from_i1",
    "summary": "Convert an `i1` to a `!torch.bool`",
    "description": "This op is primarily useful as a materialization during dialect conversion.",
    "inputs": [
      { "name": "operand", "type": "I1" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ],
    "assemblyFormat": "$operand attr-dict"
  },
  {
    "name": "torch_c.from_i64",
    "summary": "Convert an `i64` to a `!torch.int`",
    "description": "This op is primarily useful as a materialization during dialect conversion.",
    "inputs": [
      { "name": "operand", "type": "I64" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ],
    "assemblyFormat": "$operand attr-dict"
  },
  {
    "name": "torch_c.generator_to_i64",
    "summary": "Convert a `Generator` to a `i64`",
    "description": "This op is primarily useful as a materialization during dialect conversion.",
    "inputs": [
      { "name": "operand", "type": "Torch_GeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "I64" }
    ],
    "assemblyFormat": "$operand attr-dict"
  },
  {
    "name": "torch_c.get_next_seed",
    "summary": "Get the next global seed",
    "description": "This op is for getting the next global seed for RNG",
    "outputs": [
      { "name": "result", "type": "I64" }
    ],
    "assemblyFormat": "attr-dict `:` `(``)` `->` qualified(type($result))"
  },
  {
    "name": "torch_c.i64_to_generator",
    "summary": "Convert an `i64` to an `Generator`",
    "description": "This op is primarily useful as a materialization during dialect conversion.",
    "inputs": [
      { "name": "operand", "type": "I64" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_GeneratorType" }
    ],
    "assemblyFormat": "$operand attr-dict"
  },
  {
    "name": "torch_c.to_builtin_tensor",
    "summary": "Convert a `!torch.vtensor` to a `tensor`",
    "description": "This op only operates on ValueTensorType, to avoid conflating conversions\n    between value-semantic and non-value-semantic types.",
    "inputs": [
      { "name": "operand", "type": "Torch_ValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTensor" }
    ],
    "assemblyFormat": "$operand attr-dict `:` qualified(type($operand)) `->` qualified(type($result))"
  },
  {
    "name": "torch_c.to_f64",
    "summary": "Convert a `!torch.float` to an `f64`",
    "description": "This op is primarily useful as a materialization during dialect conversion.",
    "inputs": [
      { "name": "operand", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "F64" }
    ],
    "assemblyFormat": "$operand attr-dict"
  },
  {
    "name": "torch_c.to_i1",
    "summary": "Convert a `!torch.bool` to an `i1`",
    "description": "This op is primarily useful as a materialization during dialect conversion.",
    "inputs": [
      { "name": "operand", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "I1" }
    ],
    "assemblyFormat": "$operand attr-dict"
  },
  {
    "name": "torch_c.to_i64",
    "summary": "Convert a `!torch.int` to an `i64`",
    "description": "This op is primarily useful as a materialization during dialect conversion.",
    "inputs": [
      { "name": "operand", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "I64" }
    ],
    "assemblyFormat": "$operand attr-dict"
  },
  {
    "name": "torch.aten.__and__.bool",
    "summary": "Generated op for `aten::__and__.bool : (bool, bool) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_BoolType" },
      { "name": "b", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.__and__.Scalar",
    "summary": "Generated op for `aten::__and__.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.__and__.Tensor",
    "summary": "Generated op for `aten::__and__.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.__contains__.int_list",
    "summary": "Generated op for `aten::__contains__.int_list : (int[], int) -> (bool)`",
    "inputs": [
      { "name": "l", "type": "AnyTorchListOfTorchIntType" },
      { "name": "item", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.__contains__.str",
    "summary": "Generated op for `aten::__contains__.str : (Dict(str, t), str) -> (bool)`",
    "inputs": [
      { "name": "dict", "type": "Torch_DictType" },
      { "name": "key", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.__contains__.str_list",
    "summary": "Generated op for `aten::__contains__.str_list : (str[], str) -> (bool)`",
    "inputs": [
      { "name": "l", "type": "AnyTorchListOfTorchStringType" },
      { "name": "item", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.__derive_index",
    "summary": "Generated op for `aten::__derive_index : (int, int, int) -> (int)`",
    "inputs": [
      { "name": "index", "type": "Torch_IntType" },
      { "name": "start", "type": "Torch_IntType" },
      { "name": "step", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.__getitem__.Dict_str",
    "summary": "Generated op for `aten::__getitem__.Dict_str : (Dict(str, t), str) -> (t)`",
    "inputs": [
      { "name": "self", "type": "Torch_DictType" },
      { "name": "key", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ]
  },
  {
    "name": "torch.aten.__getitem__.t",
    "summary": "Generated op for `aten::__getitem__.t : (t[], int) -> (t)`",
    "inputs": [
      { "name": "list", "type": "AnyTorchListType" },
      { "name": "idx", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ]
  },
  {
    "name": "torch.aten.__interpolate.size_list_scale_list",
    "summary": "Generated op for `aten::__interpolate.size_list_scale_list : (Tensor, int[]?, float[]?, str, bool?, bool?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "scale_factor", "type": "AnyTorchOptionalListOfTorchFloatType" },
      { "name": "mode", "type": "Torch_StringType" },
      { "name": "align_corners", "type": "AnyTorchOptionalBoolType" },
      { "name": "recompute_scale_factor", "type": "AnyTorchOptionalBoolType" },
      { "name": "antialias", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.__is__",
    "summary": "Generated op for `aten::__is__ : (t1, t2) -> (bool)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchType" },
      { "name": "obj", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.__isnot__",
    "summary": "Generated op for `aten::__isnot__ : (t1, t2) -> (bool)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchType" },
      { "name": "obj", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.__lshift__.Scalar",
    "summary": "Generated op for `aten::__lshift__.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.__not__",
    "summary": "Generated op for `aten::__not__ : (bool) -> (bool)`",
    "inputs": [
      { "name": "self", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.__or__.bool",
    "summary": "Generated op for `aten::__or__.bool : (bool, bool) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_BoolType" },
      { "name": "b", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.__or__.Tensor",
    "summary": "Generated op for `aten::__or__.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.__range_length",
    "summary": "Generated op for `aten::__range_length : (int, int, int) -> (int)`",
    "inputs": [
      { "name": "lo", "type": "Torch_IntType" },
      { "name": "hi", "type": "Torch_IntType" },
      { "name": "step", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.__rshift__.Scalar",
    "summary": "Generated op for `aten::__rshift__.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._adaptive_avg_pool2d",
    "summary": "Generated op for `aten::_adaptive_avg_pool2d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._adaptive_avg_pool2d_backward",
    "summary": "Generated op for `aten::_adaptive_avg_pool2d_backward : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._adaptive_avg_pool3d",
    "summary": "Generated op for `aten::_adaptive_avg_pool3d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._adaptive_avg_pool3d_backward",
    "summary": "Generated op for `aten::_adaptive_avg_pool3d_backward : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._assert_scalar",
    "summary": "Generated op for `aten::_assert_scalar : (Scalar, str) -> ()`",
    "inputs": [
      { "name": "self", "type": "AnyTorchScalarType" },
      { "name": "assert_msg", "type": "Torch_StringType" }
    ]
  },
  {
    "name": "torch.aten._assert_tensor_metadata",
    "summary": "Generated op for `aten::_assert_tensor_metadata : (Tensor, int[]?, int[]?, int?, Device?, int?) -> ()`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" }
    ]
  },
  {
    "name": "torch.aten._cast_Float",
    "summary": "Generated op for `aten::_cast_Float : (Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "non_blocking", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._cast_Long",
    "summary": "Generated op for `aten::_cast_Long : (Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "non_blocking", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._convolution",
    "summary": "Generated op for `aten::_convolution : (Tensor, Tensor, Tensor?, int[], int[], int[], bool, int[], int, bool, bool, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "transposed", "type": "Torch_BoolType" },
      { "name": "output_padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" },
      { "name": "benchmark", "type": "Torch_BoolType" },
      { "name": "deterministic", "type": "Torch_BoolType" },
      { "name": "cudnn_enabled", "type": "Torch_BoolType" },
      { "name": "allow_tf32", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._convolution.deprecated",
    "summary": "Generated op for `aten::_convolution.deprecated : (Tensor, Tensor, Tensor?, int[], int[], int[], bool, int[], int, bool, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "transposed", "type": "Torch_BoolType" },
      { "name": "output_padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" },
      { "name": "benchmark", "type": "Torch_BoolType" },
      { "name": "deterministic", "type": "Torch_BoolType" },
      { "name": "cudnn_enabled", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._embedding_bag",
    "summary": "Generated op for `aten::_embedding_bag : (Tensor, Tensor, Tensor, bool, int, bool, Tensor?, bool, int) -> (Tensor, Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchTensorType" },
      { "name": "offsets", "type": "AnyTorchTensorType" },
      { "name": "scale_grad_by_freq", "type": "Torch_BoolType" },
      { "name": "mode", "type": "Torch_IntType" },
      { "name": "sparse", "type": "Torch_BoolType" },
      { "name": "per_sample_weights", "type": "AnyTorchOptionalTensorType" },
      { "name": "include_last_offset", "type": "Torch_BoolType" },
      { "name": "padding_idx", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" },
      { "name": "result3", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._fake_quantize_per_tensor_affine_cachemask_tensor_qparams",
    "summary": "Generated op for `aten::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams : (Tensor, Tensor, Tensor, Tensor, int, int) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "AnyTorchTensorType" },
      { "name": "zero_point", "type": "AnyTorchTensorType" },
      { "name": "fake_quant_enabled", "type": "AnyTorchTensorType" },
      { "name": "quant_min", "type": "Torch_IntType" },
      { "name": "quant_max", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTorchOptionalTensorType" },
      { "name": "mask", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._index_put_impl",
    "summary": "Generated op for `aten::_index_put_impl : (Tensor, Tensor?[], Tensor, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchListOfOptionalTensorType" },
      { "name": "values", "type": "AnyTorchTensorType" },
      { "name": "accumulate", "type": "Torch_BoolType" },
      { "name": "unsafe", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._index_put_impl_",
    "summary": "Generated op for `aten::_index_put_impl_ : (Tensor, Tensor?[], Tensor, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "indices", "type": "AnyTorchListOfOptionalNonValueTensorType" },
      { "name": "values", "type": "Torch_NonValueTensorType" },
      { "name": "accumulate", "type": "Torch_BoolType" },
      { "name": "unsafe", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten._int_mm",
    "summary": "Generated op for `aten::_int_mm : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mat2", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._linalg_det",
    "summary": "Generated op for `aten::_linalg_det : (Tensor) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "A", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" },
      { "name": "LU", "type": "AnyTorchOptionalTensorType" },
      { "name": "pivots", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._log_softmax",
    "summary": "Generated op for `aten::_log_softmax : (Tensor, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "half_to_float", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._log_softmax_backward_data",
    "summary": "Generated op for `aten::_log_softmax_backward_data : (Tensor, Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "output", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "input_dtype", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._make_per_channel_quantized_tensor",
    "summary": "Generated op for `aten::_make_per_channel_quantized_tensor : (Tensor, Tensor, Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "AnyTorchTensorType" },
      { "name": "zero_point", "type": "AnyTorchTensorType" },
      { "name": "axis", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._make_per_tensor_quantized_tensor",
    "summary": "Generated op for `aten::_make_per_tensor_quantized_tensor : (Tensor, float, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "Torch_FloatType" },
      { "name": "zero_point", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._reshape_alias",
    "summary": "Generated op for `aten::_reshape_alias : (Tensor, int[], int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._reshape_alias_copy",
    "summary": "Generated op for `aten::_reshape_alias_copy : (Tensor, int[], int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._safe_softmax",
    "summary": "Generated op for `aten::_safe_softmax : (Tensor, int, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._set_item.str",
    "summary": "Generated op for `aten::_set_item.str : (Dict(str, t), str, t) -> ()`",
    "inputs": [
      { "name": "l", "type": "Torch_DictType" },
      { "name": "idx", "type": "Torch_StringType" },
      { "name": "v", "type": "AnyTorchType" }
    ]
  },
  {
    "name": "torch.aten._set_item.t",
    "summary": "Generated op for `aten::_set_item.t : (t[], int, t) -> (t[])`",
    "inputs": [
      { "name": "l", "type": "AnyTorchListType" },
      { "name": "idx", "type": "Torch_IntType" },
      { "name": "el", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListType" }
    ]
  },
  {
    "name": "torch.aten._shape_as_tensor",
    "summary": "Generated op for `aten::_shape_as_tensor : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._softmax",
    "summary": "Generated op for `aten::_softmax : (Tensor, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "half_to_float", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._softmax_backward_data",
    "summary": "Generated op for `aten::_softmax_backward_data : (Tensor, Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "output", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "input_dtype", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._to_copy",
    "summary": "Generated op for `aten::_to_copy : (Tensor, int?, int?, Device?, bool?, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "non_blocking", "type": "Torch_BoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._trilinear",
    "summary": "Generated op for `aten::_trilinear : (Tensor, Tensor, Tensor, int[], int[], int[], int[], int) -> (Tensor)`",
    "inputs": [
      { "name": "i1", "type": "AnyTorchTensorType" },
      { "name": "i2", "type": "AnyTorchTensorType" },
      { "name": "i3", "type": "AnyTorchTensorType" },
      { "name": "expand1", "type": "AnyTorchListOfTorchIntType" },
      { "name": "expand2", "type": "AnyTorchListOfTorchIntType" },
      { "name": "expand3", "type": "AnyTorchListOfTorchIntType" },
      { "name": "sumdim", "type": "AnyTorchListOfTorchIntType" },
      { "name": "unroll_dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._unsafe_index_put.hacked_twin",
    "summary": "Generated op for `aten::_unsafe_index_put.hacked_twin : (Tensor, Tensor[], Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchListOfTensorType" },
      { "name": "values", "type": "AnyTorchTensorType" },
      { "name": "accumulate", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._unsafe_view",
    "summary": "Generated op for `aten::_unsafe_view : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten._weight_norm_interface",
    "summary": "Generated op for `aten::_weight_norm_interface : (Tensor, Tensor, int) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "v", "type": "AnyTorchTensorType" },
      { "name": "g", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.abs",
    "summary": "Generated op for `aten::abs : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.abs_",
    "summary": "Generated op for `aten::abs_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.acos",
    "summary": "Generated op for `aten::acos : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.acos_",
    "summary": "Generated op for `aten::acos_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.acosh",
    "summary": "Generated op for `aten::acosh : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.acosh_",
    "summary": "Generated op for `aten::acosh_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.adaptive_avg_pool1d",
    "summary": "Generated op for `aten::adaptive_avg_pool1d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.adaptive_avg_pool2d",
    "summary": "Generated op for `aten::adaptive_avg_pool2d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.adaptive_avg_pool3d",
    "summary": "Generated op for `aten::adaptive_avg_pool3d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.adaptive_max_pool1d",
    "summary": "Generated op for `aten::adaptive_max_pool1d : (Tensor, int[]) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.adaptive_max_pool2d",
    "summary": "Generated op for `aten::adaptive_max_pool2d : (Tensor, int[]) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.adaptive_max_pool3d",
    "summary": "Generated op for `aten::adaptive_max_pool3d : (Tensor, int[]) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.add",
    "summary": "Generated op for `aten::add : (Scalar, Scalar) -> (Scalar)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" },
      { "name": "b", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchScalarType" }
    ]
  },
  {
    "name": "torch.aten.add_.Scalar",
    "summary": "Generated op for `aten::add_.Scalar : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.add_.Tensor",
    "summary": "Generated op for `aten::add_.Tensor : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.add.float_int",
    "summary": "Generated op for `aten::add.float_int : (float, int) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.add.int",
    "summary": "Generated op for `aten::add.int : (int, int) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.add.Scalar",
    "summary": "Generated op for `aten::add.Scalar : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.add.str",
    "summary": "Generated op for `aten::add.str : (str, str) -> (str)`",
    "inputs": [
      { "name": "a", "type": "Torch_StringType" },
      { "name": "b", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_StringType" }
    ]
  },
  {
    "name": "torch.aten.add.t",
    "summary": "Generated op for `aten::add.t : (t[], t[]) -> (t[])`",
    "inputs": [
      { "name": "a", "type": "AnyTorchListType" },
      { "name": "b", "type": "AnyTorchListType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListType" }
    ]
  },
  {
    "name": "torch.aten.add.Tensor",
    "summary": "Generated op for `aten::add.Tensor : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.addcdiv",
    "summary": "Generated op for `aten::addcdiv : (Tensor, Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "tensor1", "type": "AnyTorchTensorType" },
      { "name": "tensor2", "type": "AnyTorchTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.addcdiv_",
    "summary": "Generated op for `aten::addcdiv_ : (Tensor, Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "tensor1", "type": "Torch_NonValueTensorType" },
      { "name": "tensor2", "type": "Torch_NonValueTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.addcmul",
    "summary": "Generated op for `aten::addcmul : (Tensor, Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "tensor1", "type": "AnyTorchTensorType" },
      { "name": "tensor2", "type": "AnyTorchTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.addcmul_",
    "summary": "Generated op for `aten::addcmul_ : (Tensor, Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "tensor1", "type": "Torch_NonValueTensorType" },
      { "name": "tensor2", "type": "Torch_NonValueTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.addmm",
    "summary": "Generated op for `aten::addmm : (Tensor, Tensor, Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mat1", "type": "AnyTorchTensorType" },
      { "name": "mat2", "type": "AnyTorchTensorType" },
      { "name": "beta", "type": "AnyTorchScalarType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.alias",
    "summary": "Generated op for `aten::alias : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.alias_copy",
    "summary": "Generated op for `aten::alias_copy : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.all",
    "summary": "Generated op for `aten::all : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.all.bool",
    "summary": "Generated op for `aten::all.bool : (bool[]) -> (bool)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchListOfTorchBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.all.dim",
    "summary": "Generated op for `aten::all.dim : (Tensor, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.amax",
    "summary": "Generated op for `aten::amax : (Tensor, int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.amin",
    "summary": "Generated op for `aten::amin : (Tensor, int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.aminmax",
    "summary": "Generated op for `aten::aminmax : (Tensor, int?, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "min", "type": "AnyTorchOptionalTensorType" },
      { "name": "max", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.any",
    "summary": "Generated op for `aten::any : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.any.bool",
    "summary": "Generated op for `aten::any.bool : (bool[]) -> (bool)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchListOfTorchBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.any.dim",
    "summary": "Generated op for `aten::any.dim : (Tensor, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.any.dims",
    "summary": "Generated op for `aten::any.dims : (Tensor, int[]?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.append.t",
    "summary": "Generated op for `aten::append.t : (t[], t) -> (t[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchListType" },
      { "name": "el", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListType" }
    ]
  },
  {
    "name": "torch.aten.arange",
    "summary": "Generated op for `aten::arange : (Scalar, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "end", "type": "AnyTorchScalarType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.arange.start",
    "summary": "Generated op for `aten::arange.start : (Scalar, Scalar, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "start", "type": "AnyTorchScalarType" },
      { "name": "end", "type": "AnyTorchScalarType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.arange.start_out",
    "summary": "Generated op for `aten::arange.start_out : (Scalar, Scalar, Scalar, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "start", "type": "AnyTorchScalarType" },
      { "name": "end", "type": "AnyTorchScalarType" },
      { "name": "step", "type": "AnyTorchScalarType" },
      { "name": "out", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.arange.start_step",
    "summary": "Generated op for `aten::arange.start_step : (Scalar, Scalar, Scalar, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "start", "type": "AnyTorchScalarType" },
      { "name": "end", "type": "AnyTorchScalarType" },
      { "name": "step", "type": "AnyTorchScalarType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.argmax",
    "summary": "Generated op for `aten::argmax : (Tensor, int?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.argmin",
    "summary": "Generated op for `aten::argmin : (Tensor, int?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.argsort",
    "summary": "Generated op for `aten::argsort : (Tensor, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "descending", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.as_strided",
    "summary": "Generated op for `aten::as_strided : (Tensor, int[], int[], int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "storage_offset", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.as_strided_copy",
    "summary": "Generated op for `aten::as_strided_copy : (Tensor, int[], int[], int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "storage_offset", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.as_strided_scatter",
    "summary": "Generated op for `aten::as_strided_scatter : (Tensor, Tensor, int[], int[], int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "storage_offset", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.asin",
    "summary": "Generated op for `aten::asin : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.asin_",
    "summary": "Generated op for `aten::asin_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.asinh",
    "summary": "Generated op for `aten::asinh : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.asinh_",
    "summary": "Generated op for `aten::asinh_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.atan",
    "summary": "Generated op for `aten::atan : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.atan_",
    "summary": "Generated op for `aten::atan_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.atan2",
    "summary": "Generated op for `aten::atan2 : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.atan2_",
    "summary": "Generated op for `aten::atan2_ : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.atanh",
    "summary": "Generated op for `aten::atanh : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.atanh_",
    "summary": "Generated op for `aten::atanh_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.atleast_1d",
    "summary": "Generated op for `aten::atleast_1d : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.atleast_2d",
    "summary": "Generated op for `aten::atleast_2d : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.avg_pool1d",
    "summary": "Generated op for `aten::avg_pool1d : (Tensor, int[], int[], int[], bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" },
      { "name": "count_include_pad", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.avg_pool2d",
    "summary": "Generated op for `aten::avg_pool2d : (Tensor, int[], int[], int[], bool, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" },
      { "name": "count_include_pad", "type": "Torch_BoolType" },
      { "name": "divisor_override", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.avg_pool2d_backward",
    "summary": "Generated op for `aten::avg_pool2d_backward : (Tensor, Tensor, int[], int[], int[], bool, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" },
      { "name": "count_include_pad", "type": "Torch_BoolType" },
      { "name": "divisor_override", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.avg_pool3d",
    "summary": "Generated op for `aten::avg_pool3d : (Tensor, int[], int[], int[], bool, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" },
      { "name": "count_include_pad", "type": "Torch_BoolType" },
      { "name": "divisor_override", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.avg_pool3d_backward",
    "summary": "Generated op for `aten::avg_pool3d_backward : (Tensor, Tensor, int[], int[], int[], bool, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" },
      { "name": "count_include_pad", "type": "Torch_BoolType" },
      { "name": "divisor_override", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.baddbmm",
    "summary": "Generated op for `aten::baddbmm : (Tensor, Tensor, Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "batch1", "type": "AnyTorchTensorType" },
      { "name": "batch2", "type": "AnyTorchTensorType" },
      { "name": "beta", "type": "AnyTorchScalarType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.baddbmm_",
    "summary": "Generated op for `aten::baddbmm_ : (Tensor, Tensor, Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "batch1", "type": "Torch_NonValueTensorType" },
      { "name": "batch2", "type": "Torch_NonValueTensorType" },
      { "name": "beta", "type": "AnyTorchScalarType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.batch_norm",
    "summary": "Generated op for `aten::batch_norm : (Tensor, Tensor?, Tensor?, Tensor?, Tensor?, bool, float, float, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "running_mean", "type": "AnyTorchOptionalTensorType" },
      { "name": "running_var", "type": "AnyTorchOptionalTensorType" },
      { "name": "training", "type": "Torch_BoolType" },
      { "name": "momentum", "type": "Torch_FloatType" },
      { "name": "eps", "type": "Torch_FloatType" },
      { "name": "cudnn_enabled", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bernoulli",
    "summary": "Generated op for `aten::bernoulli : (Tensor, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bernoulli_.float",
    "summary": "Generated op for `aten::bernoulli_.float : (Tensor, float, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "Torch_FloatType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bernoulli_.Tensor",
    "summary": "Generated op for `aten::bernoulli_.Tensor : (Tensor, Tensor, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "p", "type": "Torch_NonValueTensorType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.bernoulli.p",
    "summary": "Generated op for `aten::bernoulli.p : (Tensor, float, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "Torch_FloatType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bernoulli.Tensor",
    "summary": "Generated op for `aten::bernoulli.Tensor : (Tensor, Tensor, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "AnyTorchTensorType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.binary_cross_entropy",
    "summary": "Generated op for `aten::binary_cross_entropy : (Tensor, Tensor, Tensor?, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "reduction", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.binary_cross_entropy_backward",
    "summary": "Generated op for `aten::binary_cross_entropy_backward : (Tensor, Tensor, Tensor, Tensor?, int) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "reduction", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.binary_cross_entropy_with_logits",
    "summary": "Generated op for `aten::binary_cross_entropy_with_logits : (Tensor, Tensor, Tensor?, Tensor?, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "pos_weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "reduction", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bincount",
    "summary": "Generated op for `aten::bincount : (Tensor, Tensor?, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "weights", "type": "AnyTorchOptionalTensorType" },
      { "name": "minlength", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_and_.Scalar",
    "summary": "Generated op for `aten::bitwise_and_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_and_.Tensor",
    "summary": "Generated op for `aten::bitwise_and_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_and.Scalar",
    "summary": "Generated op for `aten::bitwise_and.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_and.Tensor",
    "summary": "Generated op for `aten::bitwise_and.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_left_shift_.Tensor",
    "summary": "Generated op for `aten::bitwise_left_shift_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_left_shift.Tensor",
    "summary": "Generated op for `aten::bitwise_left_shift.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_not",
    "summary": "Generated op for `aten::bitwise_not : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_not_",
    "summary": "Generated op for `aten::bitwise_not_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_or_.Tensor",
    "summary": "Generated op for `aten::bitwise_or_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_or.Tensor",
    "summary": "Generated op for `aten::bitwise_or.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_right_shift_.Tensor",
    "summary": "Generated op for `aten::bitwise_right_shift_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_right_shift.Tensor",
    "summary": "Generated op for `aten::bitwise_right_shift.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_xor_.Tensor",
    "summary": "Generated op for `aten::bitwise_xor_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.bitwise_xor.Tensor",
    "summary": "Generated op for `aten::bitwise_xor.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bmm",
    "summary": "Generated op for `aten::bmm : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mat2", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.Bool.float",
    "summary": "Generated op for `aten::Bool.float : (float) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.Bool.int",
    "summary": "Generated op for `aten::Bool.int : (int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.Bool.Tensor",
    "summary": "Generated op for `aten::Bool.Tensor : (Tensor) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.broadcast_tensors",
    "summary": "Generated op for `aten::broadcast_tensors : (Tensor[]) -> (Tensor[])`",
    "inputs": [
      { "name": "tensors", "type": "AnyTorchListOfTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.broadcast_to",
    "summary": "Generated op for `aten::broadcast_to : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.bucketize.Tensor",
    "summary": "Generated op for `aten::bucketize.Tensor : (Tensor, Tensor, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "boundaries", "type": "AnyTorchTensorType" },
      { "name": "out_int32", "type": "Torch_BoolType" },
      { "name": "right", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cat",
    "summary": "Generated op for `aten::cat : (Tensor[], int) -> (Tensor)`",
    "inputs": [
      { "name": "tensors", "type": "AnyTorchListOfTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ceil",
    "summary": "Generated op for `aten::ceil : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ceil_",
    "summary": "Generated op for `aten::ceil_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.ceil.float",
    "summary": "Generated op for `aten::ceil.float : (float) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.ceil.Scalar",
    "summary": "Generated op for `aten::ceil.Scalar : (Scalar) -> (Scalar)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchScalarType" }
    ]
  },
  {
    "name": "torch.aten.celu",
    "summary": "Generated op for `aten::celu : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.celu_",
    "summary": "Generated op for `aten::celu_ : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.channel_shuffle",
    "summary": "Generated op for `aten::channel_shuffle : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "groups", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.chunk",
    "summary": "Generated op for `aten::chunk : (Tensor, int, int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "chunks", "type": "Torch_IntType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp",
    "summary": "Generated op for `aten::clamp : (Tensor, Scalar?, Scalar?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "min", "type": "AnyTorchOptionalScalarType" },
      { "name": "max", "type": "AnyTorchOptionalScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_",
    "summary": "Generated op for `aten::clamp_ : (Tensor, Scalar?, Scalar?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "min", "type": "AnyTorchOptionalScalarType" },
      { "name": "max", "type": "AnyTorchOptionalScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_.Tensor",
    "summary": "Generated op for `aten::clamp_.Tensor : (Tensor, Tensor?, Tensor?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "min", "type": "AnyTorchOptionalNonValueTensorType" },
      { "name": "max", "type": "AnyTorchOptionalNonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_max",
    "summary": "Generated op for `aten::clamp_max : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "max", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_max_",
    "summary": "Generated op for `aten::clamp_max_ : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "max", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_max_.Tensor",
    "summary": "Generated op for `aten::clamp_max_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "max", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_max.Tensor",
    "summary": "Generated op for `aten::clamp_max.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "max", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_min",
    "summary": "Generated op for `aten::clamp_min : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "min", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_min_",
    "summary": "Generated op for `aten::clamp_min_ : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "min", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_min_.Tensor",
    "summary": "Generated op for `aten::clamp_min_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "min", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp_min.Tensor",
    "summary": "Generated op for `aten::clamp_min.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "min", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.clamp.Tensor",
    "summary": "Generated op for `aten::clamp.Tensor : (Tensor, Tensor?, Tensor?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "min", "type": "AnyTorchOptionalTensorType" },
      { "name": "max", "type": "AnyTorchOptionalTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.clone",
    "summary": "Generated op for `aten::clone : (Tensor, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.col2im",
    "summary": "Generated op for `aten::col2im : (Tensor, int[], int[], int[], int[], int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.column_stack",
    "summary": "Generated op for `aten::column_stack : (Tensor[]) -> (Tensor)`",
    "inputs": [
      { "name": "tensors", "type": "AnyTorchListOfTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.complex",
    "summary": "Generated op for `aten::complex : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "real", "type": "AnyTorchTensorType" },
      { "name": "imag", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.constant_pad_nd",
    "summary": "Generated op for `aten::constant_pad_nd : (Tensor, int[], Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "pad", "type": "AnyTorchListOfTorchIntType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.contiguous",
    "summary": "Generated op for `aten::contiguous : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "memory_format", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv_tbc",
    "summary": "Generated op for `aten::conv_tbc : (Tensor, Tensor, Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchTensorType" },
      { "name": "pad", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv_tbc_backward",
    "summary": "Generated op for `aten::conv_tbc_backward : (Tensor, Tensor, Tensor, Tensor, int) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchTensorType" },
      { "name": "pad", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv_transpose1d",
    "summary": "Generated op for `aten::conv_transpose1d : (Tensor, Tensor, Tensor?, int[], int[], int[], int, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "output_padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv_transpose2d.input",
    "summary": "Generated op for `aten::conv_transpose2d.input : (Tensor, Tensor, Tensor?, int[], int[], int[], int, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "output_padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv_transpose3d.input",
    "summary": "Generated op for `aten::conv_transpose3d.input : (Tensor, Tensor, Tensor?, int[], int[], int[], int, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "output_padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv1d",
    "summary": "Generated op for `aten::conv1d : (Tensor, Tensor, Tensor?, int[], int[], int[], int) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv1d.padding",
    "summary": "Generated op for `aten::conv1d.padding : (Tensor, Tensor, Tensor?, int[], str, int[], int) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "Torch_StringType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv2d",
    "summary": "Generated op for `aten::conv2d : (Tensor, Tensor, Tensor?, int[], int[], int[], int) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ],
    "category": "Layer"
  },
  {
    "name": "torch.aten.conv2d.padding",
    "summary": "Generated op for `aten::conv2d.padding : (Tensor, Tensor, Tensor?, int[], str, int[], int) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "Torch_StringType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.conv3d",
    "summary": "Generated op for `aten::conv3d : (Tensor, Tensor, Tensor?, int[], int[], int[], int) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ],
    "category": "Layer"
  },
  {
    "name": "torch.aten.conv3d.padding",
    "summary": "Generated op for `aten::conv3d.padding : (Tensor, Tensor, Tensor?, int[], str, int[], int) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "Torch_StringType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.convolution",
    "summary": "Generated op for `aten::convolution : (Tensor, Tensor, Tensor?, int[], int[], int[], bool, int[], int) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "transposed", "type": "Torch_BoolType" },
      { "name": "output_padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ],
    "category": "Layer"
  },
  {
    "name": "torch.aten.convolution_backward",
    "summary": "Generated op for `aten::convolution_backward : (Tensor, Tensor, Tensor, int[]?, int[], int[], int[], bool, int[], int, bool[]) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias_sizes", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "transposed", "type": "Torch_BoolType" },
      { "name": "output_padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "groups", "type": "Torch_IntType" },
      { "name": "output_mask", "type": "AnyTorchListOfTorchBoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.copy",
    "summary": "Generated op for `aten::copy : (Tensor, Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" },
      { "name": "non_blocking", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.copy_",
    "summary": "Generated op for `aten::copy_ : (Tensor, Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "src", "type": "Torch_NonValueTensorType" },
      { "name": "non_blocking", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.copysign_.Tensor",
    "summary": "Generated op for `aten::copysign_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.copysign.Tensor",
    "summary": "Generated op for `aten::copysign.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cos",
    "summary": "Generated op for `aten::cos : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cos_",
    "summary": "Generated op for `aten::cos_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.cosh",
    "summary": "Generated op for `aten::cosh : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cosh_",
    "summary": "Generated op for `aten::cosh_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.cosine_embedding_loss",
    "summary": "Generated op for `aten::cosine_embedding_loss : (Tensor, Tensor, Tensor, float, int) -> (Tensor)`",
    "inputs": [
      { "name": "input1", "type": "AnyTorchTensorType" },
      { "name": "input2", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "margin", "type": "Torch_FloatType" },
      { "name": "reduction", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cosine_similarity",
    "summary": "Generated op for `aten::cosine_similarity : (Tensor, Tensor, int, float) -> (Tensor)`",
    "inputs": [
      { "name": "x1", "type": "AnyTorchTensorType" },
      { "name": "x2", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "eps", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.count_nonzero",
    "summary": "Generated op for `aten::count_nonzero : (Tensor, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.count_nonzero.dim_IntList",
    "summary": "Generated op for `aten::count_nonzero.dim_IntList : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cpu",
    "summary": "Generated op for `aten::cpu : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cross_entropy_loss",
    "summary": "Generated op for `aten::cross_entropy_loss : (Tensor, Tensor, Tensor?, int, int, float) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "reduction", "type": "Torch_IntType" },
      { "name": "ignore_index", "type": "Torch_IntType" },
      { "name": "label_smoothing", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cuda",
    "summary": "Generated op for `aten::cuda : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cumprod",
    "summary": "Generated op for `aten::cumprod : (Tensor, int, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.cumsum",
    "summary": "Generated op for `aten::cumsum : (Tensor, int, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.deg2rad",
    "summary": "Generated op for `aten::deg2rad : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.Delete.Dict_str",
    "summary": "Generated op for `aten::Delete.Dict_str : (Dict(str, t), str) -> ()`",
    "inputs": [
      { "name": "self", "type": "Torch_DictType" },
      { "name": "key", "type": "Torch_StringType" }
    ]
  },
  {
    "name": "torch.aten.dequantize.self",
    "summary": "Generated op for `aten::dequantize.self : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.dequantize.tensor",
    "summary": "Generated op for `aten::dequantize.tensor : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "qtensor", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.detach",
    "summary": "Generated op for `aten::detach : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.detach_copy",
    "summary": "Generated op for `aten::detach_copy : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.device.with_index",
    "summary": "Generated op for `aten::device.with_index : (str, int) -> (Device)`",
    "inputs": [
      { "name": "type", "type": "Torch_StringType" },
      { "name": "index", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_DeviceType" }
    ]
  },
  {
    "name": "torch.aten.diag_embed",
    "summary": "Generated op for `aten::diag_embed : (Tensor, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "offset", "type": "Torch_IntType" },
      { "name": "dim1", "type": "Torch_IntType" },
      { "name": "dim2", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.diagonal",
    "summary": "Generated op for `aten::diagonal : (Tensor, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "offset", "type": "Torch_IntType" },
      { "name": "dim1", "type": "Torch_IntType" },
      { "name": "dim2", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.diagonal_copy",
    "summary": "Generated op for `aten::diagonal_copy : (Tensor, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "offset", "type": "Torch_IntType" },
      { "name": "dim1", "type": "Torch_IntType" },
      { "name": "dim2", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.diagonal_scatter",
    "summary": "Generated op for `aten::diagonal_scatter : (Tensor, Tensor, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" },
      { "name": "offset", "type": "Torch_IntType" },
      { "name": "dim1", "type": "Torch_IntType" },
      { "name": "dim2", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.dim",
    "summary": "Generated op for `aten::dim : (Tensor) -> (int)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.div",
    "summary": "Generated op for `aten::div : (Scalar, Scalar) -> (float)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" },
      { "name": "b", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.div_.Scalar",
    "summary": "Generated op for `aten::div_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.div_.Scalar_mode",
    "summary": "Generated op for `aten::div_.Scalar_mode : (Tensor, Scalar, str?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" },
      { "name": "rounding_mode", "type": "AnyTorchOptionalStringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.div_.Tensor",
    "summary": "Generated op for `aten::div_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.div_.Tensor_mode",
    "summary": "Generated op for `aten::div_.Tensor_mode : (Tensor, Tensor, str?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" },
      { "name": "rounding_mode", "type": "AnyTorchOptionalStringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.div.float",
    "summary": "Generated op for `aten::div.float : (float, float) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.div.int",
    "summary": "Generated op for `aten::div.int : (int, int) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.div.Scalar",
    "summary": "Generated op for `aten::div.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.div.Scalar_mode",
    "summary": "Generated op for `aten::div.Scalar_mode : (Tensor, Scalar, str?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" },
      { "name": "rounding_mode", "type": "AnyTorchOptionalStringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.div.Tensor",
    "summary": "Generated op for `aten::div.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.div.Tensor_mode",
    "summary": "Generated op for `aten::div.Tensor_mode : (Tensor, Tensor, str?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" },
      { "name": "rounding_mode", "type": "AnyTorchOptionalStringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.dot",
    "summary": "Generated op for `aten::dot : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "tensor", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.dropout",
    "summary": "Generated op for `aten::dropout : (Tensor, float, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "Torch_FloatType" },
      { "name": "train", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.dropout_",
    "summary": "Generated op for `aten::dropout_ : (Tensor, float, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "p", "type": "Torch_FloatType" },
      { "name": "train", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.einsum",
    "summary": "Generated op for `aten::einsum : (str, Tensor[], int[]?) -> (Tensor)`",
    "inputs": [
      { "name": "equation", "type": "Torch_StringType" },
      { "name": "tensors", "type": "AnyTorchListOfTensorType" },
      { "name": "path", "type": "AnyTorchOptionalListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.elu",
    "summary": "Generated op for `aten::elu : (Tensor, Scalar, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" },
      { "name": "scale", "type": "AnyTorchScalarType" },
      { "name": "input_scale", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.elu_",
    "summary": "Generated op for `aten::elu_ : (Tensor, Scalar, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" },
      { "name": "scale", "type": "AnyTorchScalarType" },
      { "name": "input_scale", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.elu_backward",
    "summary": "Generated op for `aten::elu_backward : (Tensor, Scalar, Scalar, Scalar, bool, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" },
      { "name": "scale", "type": "AnyTorchScalarType" },
      { "name": "input_scale", "type": "AnyTorchScalarType" },
      { "name": "is_result", "type": "Torch_BoolType" },
      { "name": "self_or_result", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.embedding",
    "summary": "Generated op for `aten::embedding : (Tensor, Tensor, int, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchTensorType" },
      { "name": "padding_idx", "type": "Torch_IntType" },
      { "name": "scale_grad_by_freq", "type": "Torch_BoolType" },
      { "name": "sparse", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.embedding_bag.padding_idx",
    "summary": "Generated op for `aten::embedding_bag.padding_idx : (Tensor, Tensor, Tensor, bool, int, bool, Tensor?, bool, int?) -> (Tensor, Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchTensorType" },
      { "name": "offsets", "type": "AnyTorchTensorType" },
      { "name": "scale_grad_by_freq", "type": "Torch_BoolType" },
      { "name": "mode", "type": "Torch_IntType" },
      { "name": "sparse", "type": "Torch_BoolType" },
      { "name": "per_sample_weights", "type": "AnyTorchOptionalTensorType" },
      { "name": "include_last_offset", "type": "Torch_BoolType" },
      { "name": "padding_idx", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" },
      { "name": "result3", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.embedding_dense_backward",
    "summary": "Generated op for `aten::embedding_dense_backward : (Tensor, Tensor, int, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchTensorType" },
      { "name": "num_weights", "type": "Torch_IntType" },
      { "name": "padding_idx", "type": "Torch_IntType" },
      { "name": "scale_grad_by_freq", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.empty_like",
    "summary": "Generated op for `aten::empty_like : (Tensor, int?, int?, Device?, bool?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.empty_strided",
    "summary": "Generated op for `aten::empty_strided : (int[], int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.empty.memory_format",
    "summary": "Generated op for `aten::empty.memory_format : (int[], int?, int?, Device?, bool?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.eq_.Scalar",
    "summary": "Generated op for `aten::eq_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.eq_.Tensor",
    "summary": "Generated op for `aten::eq_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.eq.bool",
    "summary": "Generated op for `aten::eq.bool : (bool, bool) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_BoolType" },
      { "name": "b", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.eq.device",
    "summary": "Generated op for `aten::eq.device : (Device, Device) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_DeviceType" },
      { "name": "b", "type": "Torch_DeviceType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.eq.float",
    "summary": "Generated op for `aten::eq.float : (float, float) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.eq.int",
    "summary": "Generated op for `aten::eq.int : (int, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.eq.int_list",
    "summary": "Generated op for `aten::eq.int_list : (int[], int[]) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchListOfTorchIntType" },
      { "name": "b", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.eq.Scalar",
    "summary": "Generated op for `aten::eq.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.eq.str",
    "summary": "Generated op for `aten::eq.str : (str, str) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_StringType" },
      { "name": "b", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.eq.Tensor",
    "summary": "Generated op for `aten::eq.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.erf",
    "summary": "Generated op for `aten::erf : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.erf_",
    "summary": "Generated op for `aten::erf_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.erfinv",
    "summary": "Generated op for `aten::erfinv : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.erfinv_",
    "summary": "Generated op for `aten::erfinv_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.exp",
    "summary": "Generated op for `aten::exp : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.exp_",
    "summary": "Generated op for `aten::exp_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.exp2",
    "summary": "Generated op for `aten::exp2 : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.exp2_",
    "summary": "Generated op for `aten::exp2_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.expand",
    "summary": "Generated op for `aten::expand : (Tensor, int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "implicit", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.expand_as",
    "summary": "Generated op for `aten::expand_as : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.expand_copy",
    "summary": "Generated op for `aten::expand_copy : (Tensor, int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "implicit", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.expm1",
    "summary": "Generated op for `aten::expm1 : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.expm1_",
    "summary": "Generated op for `aten::expm1_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.exponential",
    "summary": "Generated op for `aten::exponential : (Tensor, float, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "lambd", "type": "Torch_FloatType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.eye",
    "summary": "Generated op for `aten::eye : (int, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "n", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.eye.m",
    "summary": "Generated op for `aten::eye.m : (int, int, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "n", "type": "Torch_IntType" },
      { "name": "m", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fake_quantize_per_channel_affine",
    "summary": "Generated op for `aten::fake_quantize_per_channel_affine : (Tensor, Tensor, Tensor, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "AnyTorchTensorType" },
      { "name": "zero_point", "type": "AnyTorchTensorType" },
      { "name": "axis", "type": "Torch_IntType" },
      { "name": "quant_min", "type": "Torch_IntType" },
      { "name": "quant_max", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fake_quantize_per_channel_affine_cachemask",
    "summary": "Generated op for `aten::fake_quantize_per_channel_affine_cachemask : (Tensor, Tensor, Tensor, int, int, int) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "AnyTorchTensorType" },
      { "name": "zero_point", "type": "AnyTorchTensorType" },
      { "name": "axis", "type": "Torch_IntType" },
      { "name": "quant_min", "type": "Torch_IntType" },
      { "name": "quant_max", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTorchOptionalTensorType" },
      { "name": "mask", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fake_quantize_per_tensor_affine",
    "summary": "Generated op for `aten::fake_quantize_per_tensor_affine : (Tensor, float, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "Torch_FloatType" },
      { "name": "zero_point", "type": "Torch_IntType" },
      { "name": "quant_min", "type": "Torch_IntType" },
      { "name": "quant_max", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fake_quantize_per_tensor_affine_cachemask",
    "summary": "Generated op for `aten::fake_quantize_per_tensor_affine_cachemask : (Tensor, float, int, int, int) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "Torch_FloatType" },
      { "name": "zero_point", "type": "Torch_IntType" },
      { "name": "quant_min", "type": "Torch_IntType" },
      { "name": "quant_max", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTorchOptionalTensorType" },
      { "name": "mask", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fake_quantize_per_tensor_affine.tensor_qparams",
    "summary": "Generated op for `aten::fake_quantize_per_tensor_affine.tensor_qparams : (Tensor, Tensor, Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "AnyTorchTensorType" },
      { "name": "zero_point", "type": "AnyTorchTensorType" },
      { "name": "quant_min", "type": "Torch_IntType" },
      { "name": "quant_max", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fft_fft",
    "summary": "Generated op for `aten::fft_fft : (Tensor, int?, int, str?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "n", "type": "AnyTorchOptionalIntType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "norm", "type": "AnyTorchOptionalStringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fft_ifft",
    "summary": "Generated op for `aten::fft_ifft : (Tensor, int?, int, str?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "n", "type": "AnyTorchOptionalIntType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "norm", "type": "AnyTorchOptionalStringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fft_rfft",
    "summary": "Generated op for `aten::fft_rfft : (Tensor, int?, int, str?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "n", "type": "AnyTorchOptionalIntType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "norm", "type": "AnyTorchOptionalStringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fill_.Scalar",
    "summary": "Generated op for `aten::fill_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.fill_.Tensor",
    "summary": "Generated op for `aten::fill_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "value", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.fill.Scalar",
    "summary": "Generated op for `aten::fill.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fill.Tensor",
    "summary": "Generated op for `aten::fill.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "value", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fix",
    "summary": "Generated op for `aten::fix : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fix_",
    "summary": "Generated op for `aten::fix_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.flatten.using_ints",
    "summary": "Generated op for `aten::flatten.using_ints : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "start_dim", "type": "Torch_IntType" },
      { "name": "end_dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.flip",
    "summary": "Generated op for `aten::flip : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dims", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fliplr",
    "summary": "Generated op for `aten::fliplr : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.flipud",
    "summary": "Generated op for `aten::flipud : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.float_power.Tensor_Tensor",
    "summary": "Generated op for `aten::float_power.Tensor_Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "exponent", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.Float.Scalar",
    "summary": "Generated op for `aten::Float.Scalar : (Scalar) -> (float)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.Float.str",
    "summary": "Generated op for `aten::Float.str : (str) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.Float.Tensor",
    "summary": "Generated op for `aten::Float.Tensor : (Tensor) -> (float)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.FloatImplicit",
    "summary": "Generated op for `aten::FloatImplicit : (Tensor) -> (float)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.floor",
    "summary": "Generated op for `aten::floor : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.floor_",
    "summary": "Generated op for `aten::floor_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.floor_divide",
    "summary": "Generated op for `aten::floor_divide : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.floor_divide.Scalar",
    "summary": "Generated op for `aten::floor_divide.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.floordiv.int",
    "summary": "Generated op for `aten::floordiv.int : (int, int) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.fmax",
    "summary": "Generated op for `aten::fmax : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fmin",
    "summary": "Generated op for `aten::fmin : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fmod_.Scalar",
    "summary": "Generated op for `aten::fmod_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.fmod.Scalar",
    "summary": "Generated op for `aten::fmod.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.fmod.Tensor",
    "summary": "Generated op for `aten::fmod.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.format",
    "summary": "Generated op for `aten::format : (...) -> (str)`",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_StringType" }
    ],
    "assemblyFormat": "`(` $operands `)` attr-dict `:` qualified(type($operands)) `->` qualified(type($result))"
  },
  {
    "name": "torch.aten.frac",
    "summary": "Generated op for `aten::frac : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.frac_",
    "summary": "Generated op for `aten::frac_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.frobenius_norm.dim",
    "summary": "Generated op for `aten::frobenius_norm.dim : (Tensor, int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.full",
    "summary": "Generated op for `aten::full : (int[], Scalar, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "fill_value", "type": "AnyTorchScalarType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.full_like",
    "summary": "Generated op for `aten::full_like : (Tensor, Scalar, int?, int?, Device?, bool?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "fill_value", "type": "AnyTorchScalarType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.gather",
    "summary": "Generated op for `aten::gather : (Tensor, int, Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "AnyTorchTensorType" },
      { "name": "sparse_grad", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ],
    "category": "Tensor"
  },
  {
    "name": "torch.aten.ge_.Scalar",
    "summary": "Generated op for `aten::ge_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.ge_.Tensor",
    "summary": "Generated op for `aten::ge_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.ge.float",
    "summary": "Generated op for `aten::ge.float : (float, float) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.ge.float_int",
    "summary": "Generated op for `aten::ge.float_int : (float, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.ge.int",
    "summary": "Generated op for `aten::ge.int : (int, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.ge.Scalar",
    "summary": "Generated op for `aten::ge.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ge.Tensor",
    "summary": "Generated op for `aten::ge.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.gelu",
    "summary": "Generated op for `aten::gelu : (Tensor, str) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "approximate", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.gelu_backward",
    "summary": "Generated op for `aten::gelu_backward : (Tensor, Tensor, str) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "approximate", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.get.default_str",
    "summary": "Generated op for `aten::get.default_str : (Dict(str, t), str, t) -> (t)`",
    "inputs": [
      { "name": "self", "type": "Torch_DictType" },
      { "name": "key", "type": "Torch_StringType" },
      { "name": "default_value", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ]
  },
  {
    "name": "torch.aten.glu",
    "summary": "Generated op for `aten::glu : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.grid_sampler",
    "summary": "Generated op for `aten::grid_sampler : (Tensor, Tensor, int, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "grid", "type": "AnyTorchTensorType" },
      { "name": "interpolation_mode", "type": "Torch_IntType" },
      { "name": "padding_mode", "type": "Torch_IntType" },
      { "name": "align_corners", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.group_norm",
    "summary": "Generated op for `aten::group_norm : (Tensor, int, Tensor?, Tensor?, float, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "num_groups", "type": "Torch_IntType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "eps", "type": "Torch_FloatType" },
      { "name": "cudnn_enabled", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.gt_.Scalar",
    "summary": "Generated op for `aten::gt_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.gt_.Tensor",
    "summary": "Generated op for `aten::gt_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.gt.float",
    "summary": "Generated op for `aten::gt.float : (float, float) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.gt.float_int",
    "summary": "Generated op for `aten::gt.float_int : (float, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.gt.int",
    "summary": "Generated op for `aten::gt.int : (int, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.gt.Scalar",
    "summary": "Generated op for `aten::gt.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.gt.Tensor",
    "summary": "Generated op for `aten::gt.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.hann_window.periodic",
    "summary": "Generated op for `aten::hann_window.periodic : (int, bool, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "window_length", "type": "Torch_IntType" },
      { "name": "periodic", "type": "Torch_BoolType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.hardshrink",
    "summary": "Generated op for `aten::hardshrink : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "lambd", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.hardsigmoid",
    "summary": "Generated op for `aten::hardsigmoid : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.hardsigmoid_",
    "summary": "Generated op for `aten::hardsigmoid_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.hardswish",
    "summary": "Generated op for `aten::hardswish : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.hardswish_",
    "summary": "Generated op for `aten::hardswish_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.hardtanh",
    "summary": "Generated op for `aten::hardtanh : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "min_val", "type": "AnyTorchScalarType" },
      { "name": "max_val", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.hardtanh_",
    "summary": "Generated op for `aten::hardtanh_ : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "min_val", "type": "AnyTorchScalarType" },
      { "name": "max_val", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.hardtanh_backward",
    "summary": "Generated op for `aten::hardtanh_backward : (Tensor, Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "min_val", "type": "AnyTorchScalarType" },
      { "name": "max_val", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.heaviside",
    "summary": "Generated op for `aten::heaviside : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "values", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.heaviside_",
    "summary": "Generated op for `aten::heaviside_ : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "values", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.hstack",
    "summary": "Generated op for `aten::hstack : (Tensor[]) -> (Tensor)`",
    "inputs": [
      { "name": "tensors", "type": "AnyTorchListOfTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.im2col",
    "summary": "Generated op for `aten::im2col : (Tensor, int[], int[], int[], int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.imag",
    "summary": "Generated op for `aten::imag : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.index_put",
    "summary": "Generated op for `aten::index_put : (Tensor, Tensor?[], Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchListOfOptionalTensorType" },
      { "name": "values", "type": "AnyTorchTensorType" },
      { "name": "accumulate", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.index_put_",
    "summary": "Generated op for `aten::index_put_ : (Tensor, Tensor?[], Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "indices", "type": "AnyTorchListOfOptionalNonValueTensorType" },
      { "name": "values", "type": "Torch_NonValueTensorType" },
      { "name": "accumulate", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.index_put_.hacked_twin",
    "summary": "Generated op for `aten::index_put_.hacked_twin : (Tensor, Tensor[], Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "indices", "type": "AnyTorchListOfNonValueTensorType" },
      { "name": "values", "type": "Torch_NonValueTensorType" },
      { "name": "accumulate", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.index_put.hacked_twin",
    "summary": "Generated op for `aten::index_put.hacked_twin : (Tensor, Tensor[], Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchListOfTensorType" },
      { "name": "values", "type": "AnyTorchTensorType" },
      { "name": "accumulate", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.index_select",
    "summary": "Generated op for `aten::index_select : (Tensor, int, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.index.Tensor",
    "summary": "Generated op for `aten::index.Tensor : (Tensor, Tensor?[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchListOfOptionalTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.index.Tensor_hacked_twin",
    "summary": "Generated op for `aten::index.Tensor_hacked_twin : (Tensor, Tensor[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchListOfTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.insert.t",
    "summary": "Generated op for `aten::insert.t : (t[], int, t) -> ()`",
    "inputs": [
      { "name": "self", "type": "AnyTorchListType" },
      { "name": "idx", "type": "Torch_IntType" },
      { "name": "el", "type": "AnyTorchType" }
    ]
  },
  {
    "name": "torch.aten.instance_norm",
    "summary": "Generated op for `aten::instance_norm : (Tensor, Tensor?, Tensor?, Tensor?, Tensor?, bool, float, float, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "running_mean", "type": "AnyTorchOptionalTensorType" },
      { "name": "running_var", "type": "AnyTorchOptionalTensorType" },
      { "name": "use_input_stats", "type": "Torch_BoolType" },
      { "name": "momentum", "type": "Torch_FloatType" },
      { "name": "eps", "type": "Torch_FloatType" },
      { "name": "cudnn_enabled", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.int_repr",
    "summary": "Generated op for `aten::int_repr : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.Int.bool",
    "summary": "Generated op for `aten::Int.bool : (bool) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.Int.float",
    "summary": "Generated op for `aten::Int.float : (float) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.Int.Scalar",
    "summary": "Generated op for `aten::Int.Scalar : (Scalar) -> (int)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.Int.Tensor",
    "summary": "Generated op for `aten::Int.Tensor : (Tensor) -> (int)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.IntImplicit",
    "summary": "Generated op for `aten::IntImplicit : (Tensor) -> (int)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.is_floating_point",
    "summary": "Generated op for `aten::is_floating_point : (Tensor) -> (bool)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.isclose",
    "summary": "Generated op for `aten::isclose : (Tensor, Tensor, float, float, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" },
      { "name": "rtol", "type": "Torch_FloatType" },
      { "name": "atol", "type": "Torch_FloatType" },
      { "name": "equal_nan", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.isfinite",
    "summary": "Generated op for `aten::isfinite : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.isinf",
    "summary": "Generated op for `aten::isinf : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.isnan",
    "summary": "Generated op for `aten::isnan : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.isneginf",
    "summary": "Generated op for `aten::isneginf : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.isposinf",
    "summary": "Generated op for `aten::isposinf : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.item",
    "summary": "Generated op for `aten::item : (Tensor) -> (Scalar)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchScalarType" }
    ]
  },
  {
    "name": "torch.aten.join",
    "summary": "Generated op for `aten::join : (str, str[]) -> (str)`",
    "inputs": [
      { "name": "self", "type": "Torch_StringType" },
      { "name": "values", "type": "AnyTorchListOfTorchStringType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_StringType" }
    ]
  },
  {
    "name": "torch.aten.keys.str",
    "summary": "Generated op for `aten::keys.str : (Dict(str, t)) -> (str[])`",
    "inputs": [
      { "name": "self", "type": "Torch_DictType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTorchStringType" }
    ]
  },
  {
    "name": "torch.aten.kl_div",
    "summary": "Generated op for `aten::kl_div : (Tensor, Tensor, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "reduction", "type": "Torch_IntType" },
      { "name": "log_target", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.kthvalue",
    "summary": "Generated op for `aten::kthvalue : (Tensor, int, int, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "k", "type": "Torch_IntType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "values", "type": "AnyTorchOptionalTensorType" },
      { "name": "indices", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.l1_loss",
    "summary": "Generated op for `aten::l1_loss : (Tensor, Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "reduction", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.layer_norm",
    "summary": "Generated op for `aten::layer_norm : (Tensor, int[], Tensor?, Tensor?, float, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "normalized_shape", "type": "AnyTorchListOfTorchIntType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "eps", "type": "Torch_FloatType" },
      { "name": "cudnn_enable", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ldexp.Tensor",
    "summary": "Generated op for `aten::ldexp.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.le_.Scalar",
    "summary": "Generated op for `aten::le_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.le_.Tensor",
    "summary": "Generated op for `aten::le_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.le.int",
    "summary": "Generated op for `aten::le.int : (int, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.le.Scalar",
    "summary": "Generated op for `aten::le.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.le.Tensor",
    "summary": "Generated op for `aten::le.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.leaky_relu",
    "summary": "Generated op for `aten::leaky_relu : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "negative_slope", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.leaky_relu_",
    "summary": "Generated op for `aten::leaky_relu_ : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "negative_slope", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.leaky_relu_backward",
    "summary": "Generated op for `aten::leaky_relu_backward : (Tensor, Tensor, Scalar, bool) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "negative_slope", "type": "AnyTorchScalarType" },
      { "name": "self_is_result", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.len.str",
    "summary": "Generated op for `aten::len.str : (str) -> (int)`",
    "inputs": [
      { "name": "s", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.len.t",
    "summary": "Generated op for `aten::len.t : (t[]) -> (int)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchListType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.len.Tensor",
    "summary": "Generated op for `aten::len.Tensor : (Tensor) -> (int)`",
    "inputs": [
      { "name": "t", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.lerp_.Scalar",
    "summary": "Generated op for `aten::lerp_.Scalar : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "end", "type": "Torch_NonValueTensorType" },
      { "name": "weight", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.lerp_.Tensor",
    "summary": "Generated op for `aten::lerp_.Tensor : (Tensor, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "end", "type": "Torch_NonValueTensorType" },
      { "name": "weight", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.lerp.Scalar",
    "summary": "Generated op for `aten::lerp.Scalar : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "end", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.lerp.Tensor",
    "summary": "Generated op for `aten::lerp.Tensor : (Tensor, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "end", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.lift_fresh_copy",
    "summary": "Generated op for `aten::lift_fresh_copy : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.linalg_cross",
    "summary": "Generated op for `aten::linalg_cross : (Tensor, Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.linalg_det",
    "summary": "Generated op for `aten::linalg_det : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "A", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.linalg_norm",
    "summary": "Generated op for `aten::linalg_norm : (Tensor, Scalar?, int[]?, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "ord", "type": "AnyTorchOptionalScalarType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.linalg_qr",
    "summary": "Generated op for `aten::linalg_qr : (Tensor, str) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "A", "type": "AnyTorchTensorType" },
      { "name": "mode", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "Q", "type": "AnyTorchOptionalTensorType" },
      { "name": "R", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.linalg_slogdet",
    "summary": "Generated op for `aten::linalg_slogdet : (Tensor) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "A", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "sign", "type": "AnyTorchOptionalTensorType" },
      { "name": "logabsdet", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.linalg_vector_norm",
    "summary": "Generated op for `aten::linalg_vector_norm : (Tensor, Scalar, int[]?, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "ord", "type": "AnyTorchScalarType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.linear",
    "summary": "Generated op for `aten::linear : (Tensor, Tensor, Tensor?) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.linspace",
    "summary": "Generated op for `aten::linspace : (Scalar, Scalar, int, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "start", "type": "AnyTorchScalarType" },
      { "name": "end", "type": "AnyTorchScalarType" },
      { "name": "steps", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.list.t",
    "summary": "Generated op for `aten::list.t : (t[]) -> (t[])`",
    "inputs": [
      { "name": "l", "type": "AnyTorchListType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListType" }
    ]
  },
  {
    "name": "torch.aten.log",
    "summary": "Generated op for `aten::log : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.log_",
    "summary": "Generated op for `aten::log_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.log_sigmoid",
    "summary": "Generated op for `aten::log_sigmoid : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.log_sigmoid_backward",
    "summary": "Generated op for `aten::log_sigmoid_backward : (Tensor, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "buffer", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.log_sigmoid_forward",
    "summary": "Generated op for `aten::log_sigmoid_forward : (Tensor) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTorchOptionalTensorType" },
      { "name": "buffer", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.log_softmax.int",
    "summary": "Generated op for `aten::log_softmax.int : (Tensor, int, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.log.int",
    "summary": "Generated op for `aten::log.int : (int) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.log10",
    "summary": "Generated op for `aten::log10 : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.log10_",
    "summary": "Generated op for `aten::log10_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.log1p",
    "summary": "Generated op for `aten::log1p : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.log1p_",
    "summary": "Generated op for `aten::log1p_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.log2",
    "summary": "Generated op for `aten::log2 : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.log2_",
    "summary": "Generated op for `aten::log2_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.logaddexp",
    "summary": "Generated op for `aten::logaddexp : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.logaddexp2",
    "summary": "Generated op for `aten::logaddexp2 : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.logcumsumexp",
    "summary": "Generated op for `aten::logcumsumexp : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.logical_and",
    "summary": "Generated op for `aten::logical_and : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.logical_and_",
    "summary": "Generated op for `aten::logical_and_ : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.logical_not",
    "summary": "Generated op for `aten::logical_not : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.logical_not_",
    "summary": "Generated op for `aten::logical_not_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.logical_or",
    "summary": "Generated op for `aten::logical_or : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.logical_or_",
    "summary": "Generated op for `aten::logical_or_ : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.logical_xor",
    "summary": "Generated op for `aten::logical_xor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.logical_xor_",
    "summary": "Generated op for `aten::logical_xor_ : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.logit",
    "summary": "Generated op for `aten::logit : (Tensor, float?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "eps", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.logit_",
    "summary": "Generated op for `aten::logit_ : (Tensor, float?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "eps", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.logsumexp",
    "summary": "Generated op for `aten::logsumexp : (Tensor, int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.lt_.Scalar",
    "summary": "Generated op for `aten::lt_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.lt_.Tensor",
    "summary": "Generated op for `aten::lt_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.lt.float",
    "summary": "Generated op for `aten::lt.float : (float, float) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.lt.float_int",
    "summary": "Generated op for `aten::lt.float_int : (float, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.lt.int",
    "summary": "Generated op for `aten::lt.int : (int, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.lt.Scalar",
    "summary": "Generated op for `aten::lt.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.lt.Tensor",
    "summary": "Generated op for `aten::lt.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.masked_fill_.Scalar",
    "summary": "Generated op for `aten::masked_fill_.Scalar : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "mask", "type": "Torch_NonValueTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.masked_fill_.Tensor",
    "summary": "Generated op for `aten::masked_fill_.Tensor : (Tensor, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "mask", "type": "Torch_NonValueTensorType" },
      { "name": "value", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.masked_fill.Scalar",
    "summary": "Generated op for `aten::masked_fill.Scalar : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mask", "type": "AnyTorchTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.masked_fill.Tensor",
    "summary": "Generated op for `aten::masked_fill.Tensor : (Tensor, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mask", "type": "AnyTorchTensorType" },
      { "name": "value", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.masked_scatter",
    "summary": "Generated op for `aten::masked_scatter : (Tensor, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mask", "type": "AnyTorchTensorType" },
      { "name": "source", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.masked_scatter_",
    "summary": "Generated op for `aten::masked_scatter_ : (Tensor, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "mask", "type": "Torch_NonValueTensorType" },
      { "name": "source", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.masked_select",
    "summary": "Generated op for `aten::masked_select : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mask", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.matmul",
    "summary": "Generated op for `aten::matmul : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ],
    "category": "Layer"
  },
  {
    "name": "torch.aten.max",
    "summary": "Generated op for `aten::max : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_pool1d",
    "summary": "Generated op for `aten::max_pool1d : (Tensor, int[], int[], int[], int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_pool1d_with_indices",
    "summary": "Generated op for `aten::max_pool1d_with_indices : (Tensor, int[], int[], int[], int[], bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_pool2d",
    "summary": "Generated op for `aten::max_pool2d : (Tensor, int[], int[], int[], int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_pool2d_with_indices",
    "summary": "Generated op for `aten::max_pool2d_with_indices : (Tensor, int[], int[], int[], int[], bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_pool2d_with_indices_backward",
    "summary": "Generated op for `aten::max_pool2d_with_indices_backward : (Tensor, Tensor, int[], int[], int[], int[], bool, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" },
      { "name": "indices", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_pool3d",
    "summary": "Generated op for `aten::max_pool3d : (Tensor, int[], int[], int[], int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_pool3d_with_indices",
    "summary": "Generated op for `aten::max_pool3d_with_indices : (Tensor, int[], int[], int[], int[], bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_pool3d_with_indices_backward",
    "summary": "Generated op for `aten::max_pool3d_with_indices_backward : (Tensor, Tensor, int[], int[], int[], int[], bool, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "kernel_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dilation", "type": "AnyTorchListOfTorchIntType" },
      { "name": "ceil_mode", "type": "Torch_BoolType" },
      { "name": "indices", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_unpool2d",
    "summary": "Generated op for `aten::max_unpool2d : (Tensor, Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max_unpool3d",
    "summary": "Generated op for `aten::max_unpool3d : (Tensor, Tensor, int[], int[], int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "indices", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max.dim",
    "summary": "Generated op for `aten::max.dim : (Tensor, int, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "values", "type": "AnyTorchOptionalTensorType" },
      { "name": "indices", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.max.other",
    "summary": "Generated op for `aten::max.other : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.maximum",
    "summary": "Generated op for `aten::maximum : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mean",
    "summary": "Generated op for `aten::mean : (Tensor, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mean.dim",
    "summary": "Generated op for `aten::mean.dim : (Tensor, int[]?, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.meshgrid",
    "summary": "Generated op for `aten::meshgrid : (Tensor[]) -> (Tensor[])`",
    "inputs": [
      { "name": "tensors", "type": "AnyTorchListOfTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.meshgrid.indexing",
    "summary": "Generated op for `aten::meshgrid.indexing : (Tensor[], str) -> (Tensor[])`",
    "inputs": [
      { "name": "tensors", "type": "AnyTorchListOfTensorType" },
      { "name": "indexing", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.min",
    "summary": "Generated op for `aten::min : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.min.dim",
    "summary": "Generated op for `aten::min.dim : (Tensor, int, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "values", "type": "AnyTorchOptionalTensorType" },
      { "name": "indices", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.min.other",
    "summary": "Generated op for `aten::min.other : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.minimum",
    "summary": "Generated op for `aten::minimum : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mish",
    "summary": "Generated op for `aten::mish : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mm",
    "summary": "Generated op for `aten::mm : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mat2", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.movedim.int",
    "summary": "Generated op for `aten::movedim.int : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "source", "type": "Torch_IntType" },
      { "name": "destination", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mse_loss",
    "summary": "Generated op for `aten::mse_loss : (Tensor, Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "reduction", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mse_loss_backward",
    "summary": "Generated op for `aten::mse_loss_backward : (Tensor, Tensor, Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "reduction", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mul",
    "summary": "Generated op for `aten::mul : (Scalar, Scalar) -> (Scalar)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" },
      { "name": "b", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchScalarType" }
    ]
  },
  {
    "name": "torch.aten.mul_.Scalar",
    "summary": "Generated op for `aten::mul_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.mul_.Tensor",
    "summary": "Generated op for `aten::mul_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.mul.float",
    "summary": "Generated op for `aten::mul.float : (float, float) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.mul.float_int",
    "summary": "Generated op for `aten::mul.float_int : (float, int) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.mul.int",
    "summary": "Generated op for `aten::mul.int : (int, int) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.mul.int_float",
    "summary": "Generated op for `aten::mul.int_float : (int, float) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.mul.left_t",
    "summary": "Generated op for `aten::mul.left_t : (t[], int) -> (t[])`",
    "inputs": [
      { "name": "l", "type": "AnyTorchListType" },
      { "name": "n", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListType" }
    ]
  },
  {
    "name": "torch.aten.mul.Scalar",
    "summary": "Generated op for `aten::mul.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mul.Tensor",
    "summary": "Generated op for `aten::mul.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.multinomial",
    "summary": "Generated op for `aten::multinomial : (Tensor, int, bool, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "num_samples", "type": "Torch_IntType" },
      { "name": "replacement", "type": "Torch_BoolType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.mv",
    "summary": "Generated op for `aten::mv : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "vec", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.nan_to_num",
    "summary": "Generated op for `aten::nan_to_num : (Tensor, float?, float?, float?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "nan", "type": "AnyTorchOptionalFloatType" },
      { "name": "posinf", "type": "AnyTorchOptionalFloatType" },
      { "name": "neginf", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.narrow",
    "summary": "Generated op for `aten::narrow : (Tensor, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "start", "type": "Torch_IntType" },
      { "name": "length", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.narrow.Tensor",
    "summary": "Generated op for `aten::narrow.Tensor : (Tensor, int, Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "start", "type": "AnyTorchTensorType" },
      { "name": "length", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.native_batch_norm",
    "summary": "Generated op for `aten::native_batch_norm : (Tensor, Tensor?, Tensor?, Tensor?, Tensor?, bool, float, float) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "running_mean", "type": "AnyTorchOptionalTensorType" },
      { "name": "running_var", "type": "AnyTorchOptionalTensorType" },
      { "name": "training", "type": "Torch_BoolType" },
      { "name": "momentum", "type": "Torch_FloatType" },
      { "name": "eps", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.native_batch_norm_backward",
    "summary": "Generated op for `aten::native_batch_norm_backward : (Tensor, Tensor, Tensor?, Tensor?, Tensor?, Tensor?, Tensor?, bool, float, bool[]) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "grad_out", "type": "AnyTorchTensorType" },
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "running_mean", "type": "AnyTorchOptionalTensorType" },
      { "name": "running_var", "type": "AnyTorchOptionalTensorType" },
      { "name": "save_mean", "type": "AnyTorchOptionalTensorType" },
      { "name": "save_invstd", "type": "AnyTorchOptionalTensorType" },
      { "name": "train", "type": "Torch_BoolType" },
      { "name": "eps", "type": "Torch_FloatType" },
      { "name": "output_mask", "type": "AnyTorchListOfTorchBoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.native_dropout",
    "summary": "Generated op for `aten::native_dropout : (Tensor, float, bool?) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "Torch_FloatType" },
      { "name": "train", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.native_dropout_backward",
    "summary": "Generated op for `aten::native_dropout_backward : (Tensor, Tensor, float) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "mask", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.native_group_norm",
    "summary": "Generated op for `aten::native_group_norm : (Tensor, Tensor?, Tensor?, int, int, int, int, float) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "N", "type": "Torch_IntType" },
      { "name": "C", "type": "Torch_IntType" },
      { "name": "HxW", "type": "Torch_IntType" },
      { "name": "group", "type": "Torch_IntType" },
      { "name": "eps", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.native_group_norm_backward",
    "summary": "Generated op for `aten::native_group_norm_backward : (Tensor, Tensor, Tensor, Tensor, Tensor?, int, int, int, int, bool[]) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "grad_out", "type": "AnyTorchTensorType" },
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "mean", "type": "AnyTorchTensorType" },
      { "name": "rstd", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "N", "type": "Torch_IntType" },
      { "name": "C", "type": "Torch_IntType" },
      { "name": "HxW", "type": "Torch_IntType" },
      { "name": "group", "type": "Torch_IntType" },
      { "name": "output_mask", "type": "AnyTorchListOfTorchBoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.native_layer_norm",
    "summary": "Generated op for `aten::native_layer_norm : (Tensor, int[], Tensor?, Tensor?, float) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "normalized_shape", "type": "AnyTorchListOfTorchIntType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "eps", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.native_layer_norm_backward",
    "summary": "Generated op for `aten::native_layer_norm_backward : (Tensor, Tensor, int[], Tensor, Tensor, Tensor?, Tensor?, bool[]) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "grad_out", "type": "AnyTorchTensorType" },
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "normalized_shape", "type": "AnyTorchListOfTorchIntType" },
      { "name": "mean", "type": "AnyTorchTensorType" },
      { "name": "rstd", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "bias", "type": "AnyTorchOptionalTensorType" },
      { "name": "output_mask", "type": "AnyTorchListOfTorchBoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ne_.Scalar",
    "summary": "Generated op for `aten::ne_.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.ne_.Tensor",
    "summary": "Generated op for `aten::ne_.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.ne.bool",
    "summary": "Generated op for `aten::ne.bool : (bool, bool) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_BoolType" },
      { "name": "b", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.ne.float_int",
    "summary": "Generated op for `aten::ne.float_int : (float, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.ne.int",
    "summary": "Generated op for `aten::ne.int : (int, int) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.ne.int_list",
    "summary": "Generated op for `aten::ne.int_list : (int[], int[]) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchListOfTorchIntType" },
      { "name": "b", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.ne.Scalar",
    "summary": "Generated op for `aten::ne.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ne.str",
    "summary": "Generated op for `aten::ne.str : (str, str) -> (bool)`",
    "inputs": [
      { "name": "a", "type": "Torch_StringType" },
      { "name": "b", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.ne.Tensor",
    "summary": "Generated op for `aten::ne.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.neg",
    "summary": "Generated op for `aten::neg : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.neg_",
    "summary": "Generated op for `aten::neg_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.neg.float",
    "summary": "Generated op for `aten::neg.float : (float) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.neg.int",
    "summary": "Generated op for `aten::neg.int : (int) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.new_empty",
    "summary": "Generated op for `aten::new_empty : (Tensor, int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.new_empty_strided",
    "summary": "Generated op for `aten::new_empty_strided : (Tensor, int[], int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "stride", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.new_full",
    "summary": "Generated op for `aten::new_full : (Tensor, int[], Scalar, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "fill_value", "type": "AnyTorchScalarType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.new_ones",
    "summary": "Generated op for `aten::new_ones : (Tensor, int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.new_zeros",
    "summary": "Generated op for `aten::new_zeros : (Tensor, int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.nll_loss_backward",
    "summary": "Generated op for `aten::nll_loss_backward : (Tensor, Tensor, Tensor, Tensor?, int, int, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "reduction", "type": "Torch_IntType" },
      { "name": "ignore_index", "type": "Torch_IntType" },
      { "name": "total_weight", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.nll_loss_forward",
    "summary": "Generated op for `aten::nll_loss_forward : (Tensor, Tensor, Tensor?, int, int) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "reduction", "type": "Torch_IntType" },
      { "name": "ignore_index", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTorchOptionalTensorType" },
      { "name": "total_weight", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.nll_loss2d_backward",
    "summary": "Generated op for `aten::nll_loss2d_backward : (Tensor, Tensor, Tensor, Tensor?, int, int, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "reduction", "type": "Torch_IntType" },
      { "name": "ignore_index", "type": "Torch_IntType" },
      { "name": "total_weight", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.nll_loss2d_forward",
    "summary": "Generated op for `aten::nll_loss2d_forward : (Tensor, Tensor, Tensor?, int, int) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "reduction", "type": "Torch_IntType" },
      { "name": "ignore_index", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTorchOptionalTensorType" },
      { "name": "total_weight", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.nonzero",
    "summary": "Generated op for `aten::nonzero : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.nonzero_numpy",
    "summary": "Generated op for `aten::nonzero_numpy : (Tensor) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.nonzero_static",
    "summary": "Generated op for `aten::nonzero_static : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "Torch_IntType" },
      { "name": "fill_value", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.norm.Scalar",
    "summary": "Generated op for `aten::norm.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.norm.ScalarOpt_dim",
    "summary": "Generated op for `aten::norm.ScalarOpt_dim : (Tensor, Scalar?, int[], bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "AnyTorchOptionalScalarType" },
      { "name": "dim", "type": "AnyTorchListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.normal_functional",
    "summary": "Generated op for `aten::normal_functional : (Tensor, float, float, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "mean", "type": "Torch_FloatType" },
      { "name": "std", "type": "Torch_FloatType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.numel",
    "summary": "Generated op for `aten::numel : (Tensor) -> (int)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.numpy_T",
    "summary": "Generated op for `aten::numpy_T : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.one_hot",
    "summary": "Generated op for `aten::one_hot : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "num_classes", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ones",
    "summary": "Generated op for `aten::ones : (int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ones_like",
    "summary": "Generated op for `aten::ones_like : (Tensor, int?, int?, Device?, bool?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.outer",
    "summary": "Generated op for `aten::outer : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "vec2", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.pad",
    "summary": "Generated op for `aten::pad : (Tensor, int[], str, float?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "pad", "type": "AnyTorchListOfTorchIntType" },
      { "name": "mode", "type": "Torch_StringType" },
      { "name": "value", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ],
    "category": "Transform"
  },
  {
    "name": "torch.aten.permute",
    "summary": "Generated op for `aten::permute : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dims", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.permute_copy",
    "summary": "Generated op for `aten::permute_copy : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dims", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.pixel_shuffle",
    "summary": "Generated op for `aten::pixel_shuffle : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "upscale_factor", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.pixel_unshuffle",
    "summary": "Generated op for `aten::pixel_unshuffle : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "downscale_factor", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.poisson_nll_loss",
    "summary": "Generated op for `aten::poisson_nll_loss : (Tensor, Tensor, bool, bool, float, int) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "target", "type": "AnyTorchTensorType" },
      { "name": "log_input", "type": "Torch_BoolType" },
      { "name": "full", "type": "Torch_BoolType" },
      { "name": "eps", "type": "Torch_FloatType" },
      { "name": "reduction", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.polar",
    "summary": "Generated op for `aten::polar : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "abs", "type": "AnyTorchTensorType" },
      { "name": "angle", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.pow.int_float",
    "summary": "Generated op for `aten::pow.int_float : (int, float) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.pow.Scalar",
    "summary": "Generated op for `aten::pow.Scalar : (Scalar, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchScalarType" },
      { "name": "exponent", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.pow.Tensor_Scalar",
    "summary": "Generated op for `aten::pow.Tensor_Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "exponent", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.pow.Tensor_Tensor",
    "summary": "Generated op for `aten::pow.Tensor_Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "exponent", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.prelu",
    "summary": "Generated op for `aten::prelu : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.prod",
    "summary": "Generated op for `aten::prod : (Tensor, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.prod.dim_int",
    "summary": "Generated op for `aten::prod.dim_int : (Tensor, int, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "keepdim", "type": "Torch_BoolType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.quantize_per_channel",
    "summary": "Generated op for `aten::quantize_per_channel : (Tensor, Tensor, Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scales", "type": "AnyTorchTensorType" },
      { "name": "zero_points", "type": "AnyTorchTensorType" },
      { "name": "axis", "type": "Torch_IntType" },
      { "name": "dtype", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.quantize_per_tensor",
    "summary": "Generated op for `aten::quantize_per_tensor : (Tensor, float, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "Torch_FloatType" },
      { "name": "zero_point", "type": "Torch_IntType" },
      { "name": "dtype", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rad2deg",
    "summary": "Generated op for `aten::rad2deg : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rand",
    "summary": "Generated op for `aten::rand : (int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rand_like",
    "summary": "Generated op for `aten::rand_like : (Tensor, int?, int?, Device?, bool?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.randint",
    "summary": "Generated op for `aten::randint : (int, int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "high", "type": "Torch_IntType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.randint.low",
    "summary": "Generated op for `aten::randint.low : (int, int, int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "low", "type": "Torch_IntType" },
      { "name": "high", "type": "Torch_IntType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.randn",
    "summary": "Generated op for `aten::randn : (int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.randn_like",
    "summary": "Generated op for `aten::randn_like : (Tensor, int?, int?, Device?, bool?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.randn.generator",
    "summary": "Generated op for `aten::randn.generator : (int[], Generator?, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.random",
    "summary": "Generated op for `aten::random : (Tensor, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.random.from",
    "summary": "Generated op for `aten::random.from : (Tensor, int, int?, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "from", "type": "Torch_IntType" },
      { "name": "to", "type": "AnyTorchOptionalIntType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.real",
    "summary": "Generated op for `aten::real : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.reciprocal",
    "summary": "Generated op for `aten::reciprocal : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.reciprocal_",
    "summary": "Generated op for `aten::reciprocal_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.reflection_pad1d",
    "summary": "Generated op for `aten::reflection_pad1d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.reflection_pad2d",
    "summary": "Generated op for `aten::reflection_pad2d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.reflection_pad3d",
    "summary": "Generated op for `aten::reflection_pad3d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.relu",
    "summary": "Generated op for `aten::relu : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ],
    "category": "Activation"
  },
  {
    "name": "torch.aten.relu_",
    "summary": "Generated op for `aten::relu_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.relu6",
    "summary": "Generated op for `aten::relu6 : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.relu6_",
    "summary": "Generated op for `aten::relu6_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.remainder.int",
    "summary": "Generated op for `aten::remainder.int : (int, int) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.remainder.Scalar",
    "summary": "Generated op for `aten::remainder.Scalar : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.remainder.Tensor",
    "summary": "Generated op for `aten::remainder.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.renorm",
    "summary": "Generated op for `aten::renorm : (Tensor, Scalar, int, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "AnyTorchScalarType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "maxnorm", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.repeat",
    "summary": "Generated op for `aten::repeat : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "repeats", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.repeat_interleave.self_int",
    "summary": "Generated op for `aten::repeat_interleave.self_int : (Tensor, int, int?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "repeats", "type": "Torch_IntType" },
      { "name": "dim", "type": "AnyTorchOptionalIntType" },
      { "name": "output_size", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.replication_pad1d",
    "summary": "Generated op for `aten::replication_pad1d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.replication_pad2d",
    "summary": "Generated op for `aten::replication_pad2d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.replication_pad3d",
    "summary": "Generated op for `aten::replication_pad3d : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "padding", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.reshape",
    "summary": "Generated op for `aten::reshape : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "shape", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ],
    "category": "Shape"
  },
  {
    "name": "torch.aten.reshape_as",
    "summary": "Generated op for `aten::reshape_as : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.resize",
    "summary": "Generated op for `aten::resize : (Tensor, int[], int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.resize_",
    "summary": "Generated op for `aten::resize_ : (Tensor, int[], int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rms_norm",
    "summary": "Generated op for `aten::rms_norm : (Tensor, int[], Tensor?, float?) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "normalized_shape", "type": "AnyTorchListOfTorchIntType" },
      { "name": "weight", "type": "AnyTorchOptionalTensorType" },
      { "name": "eps", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.roll",
    "summary": "Generated op for `aten::roll : (Tensor, int[], int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "shifts", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dims", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rot90",
    "summary": "Generated op for `aten::rot90 : (Tensor, int, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "k", "type": "Torch_IntType" },
      { "name": "dims", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.round",
    "summary": "Generated op for `aten::round : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.round_",
    "summary": "Generated op for `aten::round_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.round_.decimals",
    "summary": "Generated op for `aten::round_.decimals : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "decimals", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.round.decimals",
    "summary": "Generated op for `aten::round.decimals : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "decimals", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rrelu",
    "summary": "Generated op for `aten::rrelu : (Tensor, Scalar, Scalar, bool, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "lower", "type": "AnyTorchScalarType" },
      { "name": "upper", "type": "AnyTorchScalarType" },
      { "name": "training", "type": "Torch_BoolType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rrelu_",
    "summary": "Generated op for `aten::rrelu_ : (Tensor, Scalar, Scalar, bool, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "lower", "type": "AnyTorchScalarType" },
      { "name": "upper", "type": "AnyTorchScalarType" },
      { "name": "training", "type": "Torch_BoolType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.rrelu_with_noise",
    "summary": "Generated op for `aten::rrelu_with_noise : (Tensor, Tensor, Scalar, Scalar, bool, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "noise", "type": "AnyTorchTensorType" },
      { "name": "lower", "type": "AnyTorchScalarType" },
      { "name": "upper", "type": "AnyTorchScalarType" },
      { "name": "training", "type": "Torch_BoolType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rrelu_with_noise_",
    "summary": "Generated op for `aten::rrelu_with_noise_ : (Tensor, Tensor, Scalar, Scalar, bool, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "noise", "type": "Torch_NonValueTensorType" },
      { "name": "lower", "type": "AnyTorchScalarType" },
      { "name": "upper", "type": "AnyTorchScalarType" },
      { "name": "training", "type": "Torch_BoolType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.rrelu_with_noise_backward",
    "summary": "Generated op for `aten::rrelu_with_noise_backward : (Tensor, Tensor, Tensor, Scalar, Scalar, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "noise", "type": "AnyTorchTensorType" },
      { "name": "lower", "type": "AnyTorchScalarType" },
      { "name": "upper", "type": "AnyTorchScalarType" },
      { "name": "training", "type": "Torch_BoolType" },
      { "name": "self_is_result", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rrelu_with_noise_functional",
    "summary": "Generated op for `aten::rrelu_with_noise_functional : (Tensor, Tensor, Scalar, Scalar, bool, Generator?) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "noise", "type": "AnyTorchTensorType" },
      { "name": "lower", "type": "AnyTorchScalarType" },
      { "name": "upper", "type": "AnyTorchScalarType" },
      { "name": "training", "type": "Torch_BoolType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "noise_out", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rsqrt",
    "summary": "Generated op for `aten::rsqrt : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.rsqrt_",
    "summary": "Generated op for `aten::rsqrt_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.rsub.Scalar",
    "summary": "Generated op for `aten::rsub.Scalar : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.scalar_tensor",
    "summary": "Generated op for `aten::scalar_tensor : (Scalar, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "s", "type": "AnyTorchScalarType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.ScalarImplicit",
    "summary": "Generated op for `aten::ScalarImplicit : (Tensor) -> (Scalar)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchScalarType" }
    ]
  },
  {
    "name": "torch.aten.scaled_dot_product_attention",
    "summary": "Generated op for `aten::scaled_dot_product_attention : (Tensor, Tensor, Tensor, Tensor?, float, bool, float?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "query", "type": "AnyTorchTensorType" },
      { "name": "key", "type": "AnyTorchTensorType" },
      { "name": "value", "type": "AnyTorchTensorType" },
      { "name": "attn_mask", "type": "AnyTorchOptionalTensorType" },
      { "name": "dropout_p", "type": "Torch_FloatType" },
      { "name": "is_causal", "type": "Torch_BoolType" },
      { "name": "scale", "type": "AnyTorchOptionalFloatType" },
      { "name": "enable_gqa", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter_.src",
    "summary": "Generated op for `aten::scatter_.src : (Tensor, int, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "Torch_NonValueTensorType" },
      { "name": "src", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter_.value",
    "summary": "Generated op for `aten::scatter_.value : (Tensor, int, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "Torch_NonValueTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter_add",
    "summary": "Generated op for `aten::scatter_add : (Tensor, int, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter_add_",
    "summary": "Generated op for `aten::scatter_add_ : (Tensor, int, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "Torch_NonValueTensorType" },
      { "name": "src", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter_reduce_.two",
    "summary": "Generated op for `aten::scatter_reduce_.two : (Tensor, int, Tensor, Tensor, str, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "Torch_NonValueTensorType" },
      { "name": "src", "type": "Torch_NonValueTensorType" },
      { "name": "reduce", "type": "Torch_StringType" },
      { "name": "include_self", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter_reduce.two",
    "summary": "Generated op for `aten::scatter_reduce.two : (Tensor, int, Tensor, Tensor, str, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" },
      { "name": "reduce", "type": "Torch_StringType" },
      { "name": "include_self", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter.reduce",
    "summary": "Generated op for `aten::scatter.reduce : (Tensor, int, Tensor, Tensor, str) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" },
      { "name": "reduce", "type": "Torch_StringType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter.src",
    "summary": "Generated op for `aten::scatter.src : (Tensor, int, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.scatter.value",
    "summary": "Generated op for `aten::scatter.value : (Tensor, int, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "AnyTorchTensorType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.select_copy.int",
    "summary": "Generated op for `aten::select_copy.int : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.select_scatter",
    "summary": "Generated op for `aten::select_scatter : (Tensor, Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.select.int",
    "summary": "Generated op for `aten::select.int : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "index", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.selu",
    "summary": "Generated op for `aten::selu : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.selu_",
    "summary": "Generated op for `aten::selu_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.sgn",
    "summary": "Generated op for `aten::sgn : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sgn_",
    "summary": "Generated op for `aten::sgn_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.sigmoid",
    "summary": "Generated op for `aten::sigmoid : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ],
    "category": "Activation"
  },
  {
    "name": "torch.aten.sigmoid_",
    "summary": "Generated op for `aten::sigmoid_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.sigmoid_backward",
    "summary": "Generated op for `aten::sigmoid_backward : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "output", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sign",
    "summary": "Generated op for `aten::sign : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sign_",
    "summary": "Generated op for `aten::sign_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.signbit",
    "summary": "Generated op for `aten::signbit : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.silu",
    "summary": "Generated op for `aten::silu : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.silu_",
    "summary": "Generated op for `aten::silu_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.sin",
    "summary": "Generated op for `aten::sin : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sin_",
    "summary": "Generated op for `aten::sin_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.sinh",
    "summary": "Generated op for `aten::sinh : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sinh_",
    "summary": "Generated op for `aten::sinh_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.size",
    "summary": "Generated op for `aten::size : (Tensor) -> (int[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTorchIntType" }
    ]
  },
  {
    "name": "torch.aten.size.int",
    "summary": "Generated op for `aten::size.int : (Tensor, int) -> (int)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.slice_copy.Tensor",
    "summary": "Generated op for `aten::slice_copy.Tensor : (Tensor, int, int?, int?, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "start", "type": "AnyTorchOptionalIntType" },
      { "name": "end", "type": "AnyTorchOptionalIntType" },
      { "name": "step", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.slice_scatter",
    "summary": "Generated op for `aten::slice_scatter : (Tensor, Tensor, int, int?, int?, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "src", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "start", "type": "AnyTorchOptionalIntType" },
      { "name": "end", "type": "AnyTorchOptionalIntType" },
      { "name": "step", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.slice.t",
    "summary": "Generated op for `aten::slice.t : (t[], int?, int?, int) -> (t[])`",
    "inputs": [
      { "name": "l", "type": "AnyTorchListType" },
      { "name": "start", "type": "AnyTorchOptionalIntType" },
      { "name": "end", "type": "AnyTorchOptionalIntType" },
      { "name": "step", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListType" }
    ]
  },
  {
    "name": "torch.aten.slice.Tensor",
    "summary": "Generated op for `aten::slice.Tensor : (Tensor, int, int?, int?, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "start", "type": "AnyTorchOptionalIntType" },
      { "name": "end", "type": "AnyTorchOptionalIntType" },
      { "name": "step", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.softmax.int",
    "summary": "Generated op for `aten::softmax.int : (Tensor, int, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.softplus",
    "summary": "Generated op for `aten::softplus : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "beta", "type": "AnyTorchScalarType" },
      { "name": "threshold", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.softshrink",
    "summary": "Generated op for `aten::softshrink : (Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "lambd", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sort",
    "summary": "Generated op for `aten::sort : (Tensor, int, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "descending", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "values", "type": "AnyTorchOptionalTensorType" },
      { "name": "indices", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sort.int",
    "summary": "Generated op for `aten::sort.int : (int[], bool) -> ()`",
    "inputs": [
      { "name": "self", "type": "AnyTorchListOfTorchIntType" },
      { "name": "reverse", "type": "Torch_BoolType" }
    ]
  },
  {
    "name": "torch.aten.special_expm1",
    "summary": "Generated op for `aten::special_expm1 : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.split_copy.Tensor",
    "summary": "Generated op for `aten::split_copy.Tensor : (Tensor, int, int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "split_size", "type": "Torch_IntType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.split_with_sizes",
    "summary": "Generated op for `aten::split_with_sizes : (Tensor, int[], int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "split_sizes", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.split_with_sizes_copy",
    "summary": "Generated op for `aten::split_with_sizes_copy : (Tensor, int[], int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "split_sizes", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.split.sizes",
    "summary": "Generated op for `aten::split.sizes : (Tensor, int[], int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "split_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.split.Tensor",
    "summary": "Generated op for `aten::split.Tensor : (Tensor, int, int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "split_size", "type": "Torch_IntType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.sqrt",
    "summary": "Generated op for `aten::sqrt : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sqrt_",
    "summary": "Generated op for `aten::sqrt_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.sqrt.int",
    "summary": "Generated op for `aten::sqrt.int : (int) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.square",
    "summary": "Generated op for `aten::square : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.square_",
    "summary": "Generated op for `aten::square_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.squeeze",
    "summary": "Generated op for `aten::squeeze : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.squeeze_copy",
    "summary": "Generated op for `aten::squeeze_copy : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.squeeze_copy.dim",
    "summary": "Generated op for `aten::squeeze_copy.dim : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.squeeze.dim",
    "summary": "Generated op for `aten::squeeze.dim : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.stack",
    "summary": "Generated op for `aten::stack : (Tensor[], int) -> (Tensor)`",
    "inputs": [
      { "name": "tensors", "type": "AnyTorchListOfTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.std",
    "summary": "Generated op for `aten::std : (Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "unbiased", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.std.correction",
    "summary": "Generated op for `aten::std.correction : (Tensor, int[]?, Scalar?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "correction", "type": "AnyTorchOptionalScalarType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.std.dim",
    "summary": "Generated op for `aten::std.dim : (Tensor, int[]?, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "unbiased", "type": "Torch_BoolType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.stft",
    "summary": "Generated op for `aten::stft : (Tensor, int, int?, int?, Tensor?, bool, bool?, bool?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "n_fft", "type": "Torch_IntType" },
      { "name": "hop_length", "type": "AnyTorchOptionalIntType" },
      { "name": "win_length", "type": "AnyTorchOptionalIntType" },
      { "name": "window", "type": "AnyTorchOptionalTensorType" },
      { "name": "normalized", "type": "Torch_BoolType" },
      { "name": "onesided", "type": "AnyTorchOptionalBoolType" },
      { "name": "return_complex", "type": "AnyTorchOptionalBoolType" },
      { "name": "align_to_window", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.stft.center",
    "summary": "Generated op for `aten::stft.center : (Tensor, int, int?, int?, Tensor?, bool, str, bool, bool?, bool?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "n_fft", "type": "Torch_IntType" },
      { "name": "hop_length", "type": "AnyTorchOptionalIntType" },
      { "name": "win_length", "type": "AnyTorchOptionalIntType" },
      { "name": "window", "type": "AnyTorchOptionalTensorType" },
      { "name": "center", "type": "Torch_BoolType" },
      { "name": "pad_mode", "type": "Torch_StringType" },
      { "name": "normalized", "type": "Torch_BoolType" },
      { "name": "onesided", "type": "AnyTorchOptionalBoolType" },
      { "name": "return_complex", "type": "AnyTorchOptionalBoolType" },
      { "name": "align_to_window", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.str",
    "summary": "Generated op for `aten::str : (t) -> (str)`",
    "inputs": [
      { "name": "elem", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_StringType" }
    ]
  },
  {
    "name": "torch.aten.sub",
    "summary": "Generated op for `aten::sub : (Scalar, Scalar) -> (Scalar)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" },
      { "name": "b", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchScalarType" }
    ]
  },
  {
    "name": "torch.aten.sub_.Scalar",
    "summary": "Generated op for `aten::sub_.Scalar : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.sub_.Tensor",
    "summary": "Generated op for `aten::sub_.Tensor : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "other", "type": "Torch_NonValueTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.sub.float",
    "summary": "Generated op for `aten::sub.float : (float, float) -> (float)`",
    "inputs": [
      { "name": "a", "type": "Torch_FloatType" },
      { "name": "b", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ]
  },
  {
    "name": "torch.aten.sub.int",
    "summary": "Generated op for `aten::sub.int : (int, int) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.sub.Scalar",
    "summary": "Generated op for `aten::sub.Scalar : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sub.Tensor",
    "summary": "Generated op for `aten::sub.Tensor : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" },
      { "name": "alpha", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sum",
    "summary": "Generated op for `aten::sum : (Tensor, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sum.dim_IntList",
    "summary": "Generated op for `aten::sum.dim_IntList : (Tensor, int[]?, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "keepdim", "type": "Torch_BoolType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.sym_constrain_range",
    "summary": "Generated op for `aten::sym_constrain_range : (Scalar, int?, int?) -> ()`",
    "inputs": [
      { "name": "size", "type": "AnyTorchScalarType" },
      { "name": "min", "type": "AnyTorchOptionalIntType" },
      { "name": "max", "type": "AnyTorchOptionalIntType" }
    ]
  },
  {
    "name": "torch.aten.sym_constrain_range_for_size",
    "summary": "Generated op for `aten::sym_constrain_range_for_size : (Scalar, int?, int?) -> ()`",
    "inputs": [
      { "name": "size", "type": "AnyTorchScalarType" },
      { "name": "min", "type": "AnyTorchOptionalIntType" },
      { "name": "max", "type": "AnyTorchOptionalIntType" }
    ]
  },
  {
    "name": "torch.aten.t",
    "summary": "Generated op for `aten::t : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.t_copy",
    "summary": "Generated op for `aten::t_copy : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tan",
    "summary": "Generated op for `aten::tan : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tan_",
    "summary": "Generated op for `aten::tan_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.tanh",
    "summary": "Generated op for `aten::tanh : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ],
    "category": "Activation"
  },
  {
    "name": "torch.aten.tanh_",
    "summary": "Generated op for `aten::tanh_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.tanh_backward",
    "summary": "Generated op for `aten::tanh_backward : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "output", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tensor",
    "summary": "Generated op for `aten::tensor : (t[], int?, Device?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "data", "type": "AnyTorchListType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "requires_grad", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tensor_split.sections",
    "summary": "Generated op for `aten::tensor_split.sections : (Tensor, int, int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "sections", "type": "Torch_IntType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.tensor.bool",
    "summary": "Generated op for `aten::tensor.bool : (bool, int?, Device?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "t", "type": "Torch_BoolType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "requires_grad", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tensor.float",
    "summary": "Generated op for `aten::tensor.float : (float, int?, Device?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "t", "type": "Torch_FloatType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "requires_grad", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tensor.int",
    "summary": "Generated op for `aten::tensor.int : (int, int?, Device?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "t", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "requires_grad", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.threshold",
    "summary": "Generated op for `aten::threshold : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "threshold", "type": "AnyTorchScalarType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.threshold_",
    "summary": "Generated op for `aten::threshold_ : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "threshold", "type": "AnyTorchScalarType" },
      { "name": "value", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.threshold_backward",
    "summary": "Generated op for `aten::threshold_backward : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "threshold", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tile",
    "summary": "Generated op for `aten::tile : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dims", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.to.device",
    "summary": "Generated op for `aten::to.device : (Tensor, Device, int, bool, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "device", "type": "Torch_DeviceType" },
      { "name": "dtype", "type": "Torch_IntType" },
      { "name": "non_blocking", "type": "Torch_BoolType" },
      { "name": "copy", "type": "Torch_BoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.to.dtype",
    "summary": "Generated op for `aten::to.dtype : (Tensor, int, bool, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "Torch_IntType" },
      { "name": "non_blocking", "type": "Torch_BoolType" },
      { "name": "copy", "type": "Torch_BoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.to.dtype_layout",
    "summary": "Generated op for `aten::to.dtype_layout : (Tensor, int?, int?, Device?, bool?, bool, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "non_blocking", "type": "Torch_BoolType" },
      { "name": "copy", "type": "Torch_BoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.to.other",
    "summary": "Generated op for `aten::to.other : (Tensor, Tensor, bool, bool, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" },
      { "name": "non_blocking", "type": "Torch_BoolType" },
      { "name": "copy", "type": "Torch_BoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.to.prim_Device",
    "summary": "Generated op for `aten::to.prim_Device : (Tensor, Device?, int?, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "non_blocking", "type": "Torch_BoolType" },
      { "name": "copy", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.topk",
    "summary": "Generated op for `aten::topk : (Tensor, int, int, bool, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "k", "type": "Torch_IntType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "largest", "type": "Torch_BoolType" },
      { "name": "sorted", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "values", "type": "AnyTorchOptionalTensorType" },
      { "name": "indices", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.trace",
    "summary": "Generated op for `aten::trace : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.transpose_copy.int",
    "summary": "Generated op for `aten::transpose_copy.int : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim0", "type": "Torch_IntType" },
      { "name": "dim1", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.transpose.int",
    "summary": "Generated op for `aten::transpose.int : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim0", "type": "Torch_IntType" },
      { "name": "dim1", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tril",
    "summary": "Generated op for `aten::tril : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "diagonal", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.tril_",
    "summary": "Generated op for `aten::tril_ : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "diagonal", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.tril_indices",
    "summary": "Generated op for `aten::tril_indices : (int, int, int, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "row", "type": "Torch_IntType" },
      { "name": "col", "type": "Torch_IntType" },
      { "name": "offset", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.triu",
    "summary": "Generated op for `aten::triu : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "diagonal", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.triu_",
    "summary": "Generated op for `aten::triu_ : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "diagonal", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.triu_indices",
    "summary": "Generated op for `aten::triu_indices : (int, int, int, int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "row", "type": "Torch_IntType" },
      { "name": "col", "type": "Torch_IntType" },
      { "name": "offset", "type": "Torch_IntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.trunc",
    "summary": "Generated op for `aten::trunc : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.trunc_",
    "summary": "Generated op for `aten::trunc_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.type_as",
    "summary": "Generated op for `aten::type_as : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.unbind_copy.int",
    "summary": "Generated op for `aten::unbind_copy.int : (Tensor, int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.unbind.int",
    "summary": "Generated op for `aten::unbind.int : (Tensor, int) -> (Tensor[])`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListOfTensorType" }
    ]
  },
  {
    "name": "torch.aten.unflatten.int",
    "summary": "Generated op for `aten::unflatten.int : (Tensor, int, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "sizes", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.unfold",
    "summary": "Generated op for `aten::unfold : (Tensor, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dimension", "type": "Torch_IntType" },
      { "name": "size", "type": "Torch_IntType" },
      { "name": "step", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.unfold_copy",
    "summary": "Generated op for `aten::unfold_copy : (Tensor, int, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dimension", "type": "Torch_IntType" },
      { "name": "size", "type": "Torch_IntType" },
      { "name": "step", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.uniform",
    "summary": "Generated op for `aten::uniform : (Tensor, float, float, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "from", "type": "Torch_FloatType" },
      { "name": "to", "type": "Torch_FloatType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.uniform_",
    "summary": "Generated op for `aten::uniform_ : (Tensor, float, float, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "from", "type": "Torch_FloatType" },
      { "name": "to", "type": "Torch_FloatType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.unique_consecutive",
    "summary": "Generated op for `aten::unique_consecutive : (Tensor, bool, bool, int?) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "return_inverse", "type": "Torch_BoolType" },
      { "name": "return_counts", "type": "Torch_BoolType" },
      { "name": "dim", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.unique_dim",
    "summary": "Generated op for `aten::unique_dim : (Tensor, int, bool, bool, bool) -> (Tensor, Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "sorted", "type": "Torch_BoolType" },
      { "name": "return_inverse", "type": "Torch_BoolType" },
      { "name": "return_counts", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" },
      { "name": "result2", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.unsqueeze",
    "summary": "Generated op for `aten::unsqueeze : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.unsqueeze_",
    "summary": "Generated op for `aten::unsqueeze_ : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.unsqueeze_copy",
    "summary": "Generated op for `aten::unsqueeze_copy : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.upsample_bilinear2d",
    "summary": "Generated op for `aten::upsample_bilinear2d : (Tensor, int[], bool, float?, float?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "align_corners", "type": "Torch_BoolType" },
      { "name": "scales_h", "type": "AnyTorchOptionalFloatType" },
      { "name": "scales_w", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.upsample_bilinear2d.vec",
    "summary": "Generated op for `aten::upsample_bilinear2d.vec : (Tensor, int[]?, bool, float[]?) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "align_corners", "type": "Torch_BoolType" },
      { "name": "scale_factors", "type": "AnyTorchOptionalListOfTorchFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.upsample_nearest1d",
    "summary": "Generated op for `aten::upsample_nearest1d : (Tensor, int[], float?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "scales", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.upsample_nearest1d.vec",
    "summary": "Generated op for `aten::upsample_nearest1d.vec : (Tensor, int[]?, float[]?) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "scale_factors", "type": "AnyTorchOptionalListOfTorchFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.upsample_nearest2d",
    "summary": "Generated op for `aten::upsample_nearest2d : (Tensor, int[], float?, float?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "scales_h", "type": "AnyTorchOptionalFloatType" },
      { "name": "scales_w", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.upsample_nearest2d_backward",
    "summary": "Generated op for `aten::upsample_nearest2d_backward : (Tensor, int[], int[], float?, float?) -> (Tensor)`",
    "inputs": [
      { "name": "grad_output", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "input_size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "scales_h", "type": "AnyTorchOptionalFloatType" },
      { "name": "scales_w", "type": "AnyTorchOptionalFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.upsample_nearest2d.vec",
    "summary": "Generated op for `aten::upsample_nearest2d.vec : (Tensor, int[]?, float[]?) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "output_size", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "scale_factors", "type": "AnyTorchOptionalListOfTorchFloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.var",
    "summary": "Generated op for `aten::var : (Tensor, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "unbiased", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.var_mean",
    "summary": "Generated op for `aten::var_mean : (Tensor, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "unbiased", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.var_mean.correction",
    "summary": "Generated op for `aten::var_mean.correction : (Tensor, int[]?, Scalar?, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "correction", "type": "AnyTorchOptionalScalarType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.var_mean.dim",
    "summary": "Generated op for `aten::var_mean.dim : (Tensor, int[]?, bool, bool) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "unbiased", "type": "Torch_BoolType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.var.correction",
    "summary": "Generated op for `aten::var.correction : (Tensor, int[]?, Scalar?, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "correction", "type": "AnyTorchOptionalScalarType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.var.dim",
    "summary": "Generated op for `aten::var.dim : (Tensor, int[]?, bool, bool) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "unbiased", "type": "Torch_BoolType" },
      { "name": "keepdim", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.view",
    "summary": "Generated op for `aten::view : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.view_as_complex",
    "summary": "Generated op for `aten::view_as_complex : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.view_as_real",
    "summary": "Generated op for `aten::view_as_real : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.view_copy",
    "summary": "Generated op for `aten::view_copy : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "size", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.view_copy.dtype",
    "summary": "Generated op for `aten::view_copy.dtype : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.view.dtype",
    "summary": "Generated op for `aten::view.dtype : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.warn",
    "summary": "Generated op for `aten::warn : (str, int) -> ()`",
    "inputs": [
      { "name": "message", "type": "Torch_StringType" },
      { "name": "stacklevel", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.aten.where.Scalar",
    "summary": "Generated op for `aten::where.Scalar : (Tensor, Scalar, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "condition", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchScalarType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.where.ScalarOther",
    "summary": "Generated op for `aten::where.ScalarOther : (Tensor, Tensor, Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "condition", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.where.ScalarSelf",
    "summary": "Generated op for `aten::where.ScalarSelf : (Tensor, Scalar, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "condition", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchScalarType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.where.self",
    "summary": "Generated op for `aten::where.self : (Tensor, Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "condition", "type": "AnyTorchTensorType" },
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.xlogy.Tensor",
    "summary": "Generated op for `aten::xlogy.Tensor : (Tensor, Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "other", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.zero",
    "summary": "Generated op for `aten::zero : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.zero_",
    "summary": "Generated op for `aten::zero_ : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalNonValueTensorType" }
    ]
  },
  {
    "name": "torch.aten.zeros",
    "summary": "Generated op for `aten::zeros : (int[], int?, int?, Device?, bool?) -> (Tensor)`",
    "inputs": [
      { "name": "size", "type": "AnyTorchListOfTorchIntType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.aten.zeros_like",
    "summary": "Generated op for `aten::zeros_like : (Tensor, int?, int?, Device?, bool?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "AnyTorchOptionalIntType" },
      { "name": "layout", "type": "AnyTorchOptionalIntType" },
      { "name": "device", "type": "AnyTorchOptionalDeviceType" },
      { "name": "pin_memory", "type": "AnyTorchOptionalBoolType" },
      { "name": "memory_format", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.attr",
    "summary": "Declare an attribute of a torch.class_type",
    "description": "This op declaratively specifies that torch.nn.Module's of the parent\n    torch.class_type must have an attribute `name` of type `type`.\n\n    If `private` is present, it indicates that the value of this attribute\n    cannot be accessed externally.",
    "attributes": [
      { "name": "name", "type": "StrAttr" },
      { "name": "type", "type": "TypeAttr" },
      { "name": "isPrivate", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`private` $isPrivate^)? $name `:` $type attr-dict"
  },
  {
    "name": "torch.bind_symbolic_shape",
    "summary": "Binds shape expressions to tensors using an affine map indexed by shape symbols",
    "description": "The `torch.bind_symbolic_shape` operation binds shape expressions\n    useful to compute the dynamic dimensions of a tensor. It takes a\n    variadic of SSA symbols that map 1:1 to the local symbols declared\n    in the affine map. The affine map contains a list of affine shape\n    expressions for each dim where the terminals are from the declared\n    symbols.\n\n    Example:\n    ```\n    torch.bind_symbolic_shape %arg0, [%0, %1], affine_map<()[s0, s1] -> (s0, s1, 3)> : !torch.vtensor<[?,?,3],f32>\n    torch.bind_symbolic_shape %out0, [%0, %1, %2], affine_map<()[s0, s1, s2] -> (s0, s1 * 2 + s2, 3)> : !torch.vtensor<[?,?,3],f32>\n    ```",
    "inputs": [
      { "name": "operand", "type": "Torch_ValueTensorType" },
      { "name": "shape_symbols", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "shape_expressions", "type": "Builtin_AffineMapAttr" }
    ]
  },
  {
    "name": "torch.class_type",
    "summary": "Constructs a torch.ClassType",
    "description": "Declares a class type. Class types are the types used to describe\n    TorchScript `torch.nn.Module`'s. The terminology \"class type\" is for\n    consistency with TorchScript (a better name in our context might be\n    \"nn module subtype\"). The `syn_name` of this op is the same string\n    as in the `!torch.nn.Module<\"...\">` type.\n\n    Example:\n\n    ```mlir\n    // A simple empty torch.class_type, with corresponding torch.nn_module.\n    torch.class_type @empty {}\n    %submodule = torch.nn_module {} : !torch.nn.Module<\"empty\">\n\n    // A class type with many members.\n    torch.class_type @test {\n      torch.attr \"b\" : !torch.bool\n      torch.attr \"i\" : !torch.int\n      torch.attr \"f\" : !torch.float\n      torch.attr \"t\" : !torch.tensor\n      torch.attr \"submodule\" : !torch.nn.Module<\"empty\">\n      torch.method \"method\", @f\n    }\n    torch.nn_module {\n      // These must match the order and names in the `torch.class_type`.\n      torch.slot \"b\", %bool_true : !torch.bool\n      torch.slot \"i\", %int3 : !torch.int\n      torch.slot \"f\", %float : !torch.float\n      torch.slot \"t\", %t : !torch.tensor\n      torch.slot \"submodule\", %submodule : !torch.nn.Module<\"empty\">\n    } : !torch.nn.Module<\"test\">\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "$sym_name $region attr-dict"
  },
  {
    "name": "torch.class_type_terminator",
    "summary": "Implicit terminator for torch.class_type",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "torch.constant.bool",
    "summary": "Materialize a constant `bool` value.",
    "outputs": [
      { "name": "result", "type": "Torch_BoolType" }
    ],
    "attributes": [
      { "name": "value", "type": "I1Attr" }
    ],
    "assemblyFormat": "$value attr-dict"
  },
  {
    "name": "torch.constant.device",
    "summary": "Materialize a constant Device value.",
    "outputs": [
      { "name": "result", "type": "Torch_DeviceType" }
    ],
    "attributes": [
      { "name": "value", "type": "StrAttr" }
    ],
    "assemblyFormat": "$value attr-dict"
  },
  {
    "name": "torch.constant.float",
    "summary": "Materialize a constant `float` value.",
    "description": "Note: TorchScript represents `float` as 64-bit floating point values.\n\n    TODO: Add a `!torch.float` type.",
    "outputs": [
      { "name": "result", "type": "Torch_FloatType" }
    ],
    "attributes": [
      { "name": "value", "type": "F64Attr" }
    ],
    "assemblyFormat": "$value attr-dict"
  },
  {
    "name": "torch.constant.int",
    "summary": "Materialize a constant `int` value.",
    "description": "Note: TorchScript represents integers as 64-bit signed values, unlike\n    Python where they are arbitrary precision.",
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ],
    "attributes": [
      { "name": "value", "type": "I64Attr" }
    ]
  },
  {
    "name": "torch.constant.none",
    "summary": "Get the singleton None value.",
    "description": "Not to be confused with the `mlir::NoneType`. Be careful to use\n    `Torch::NoneType` to avoid namespace ambiguity.",
    "outputs": [
      { "name": "result", "type": "Torch_NoneType" }
    ],
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "torch.constant.number",
    "summary": "Materialize a constant `number` value.",
    "description": "This op is used as a workaround to the fact that the constant\n    materialization in MLIR must materialize a constant with a single op.\n    To materialize ops with a static `!torch.number` type, we must use this op,\n    even though we statically know if it is an integer or a float.\n\n    Note: This op unconditionally canonicalizes to\n    `torch.constant.{float,int}` + `torch.derefine`",
    "outputs": [
      { "name": "result", "type": "Torch_NumberType" }
    ],
    "attributes": [
      { "name": "value", "type": "AnyAttrOf" }
    ]
  },
  {
    "name": "torch.constant.str",
    "summary": "Materialize a constant str value.",
    "description": "Note: Strings in Python (and TorchScript) are immutable.",
    "outputs": [
      { "name": "result", "type": "Torch_StringType" }
    ],
    "attributes": [
      { "name": "value", "type": "StrAttr" }
    ],
    "assemblyFormat": "$value attr-dict"
  },
  {
    "name": "torch.copy.to_tensor",
    "summary": "Create a !torch.tensor with the same contents as the operand",
    "description": "This op is used to convert from !torch.vtensor to !torch.tensor.\n    It does so by allocating a new !torch.tensor and filling it with\n    the contents of the operand.\n\n    However, this op *does not* allow adding/removing static information about\n    sizes/dtype. For that, use `torch.tensor_static_info_cast`.\n\n    This op does not have the AllowsTypeRefinement trait because the operand\n    and result types are coupled. Only places that know how to simultaneously\n    update both types should be changing the type of this op.",
    "inputs": [
      { "name": "operand", "type": "Torch_ValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_NonValueTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` qualified(type($result))"
  },
  {
    "name": "torch.copy.to_vtensor",
    "summary": "Create a !torch.vtensor with the same contents as the operand",
    "description": "This op is used to convert from !torch.tensor to !torch.vtensor.\n\n    However, this op *does not* allow adding/removing static information about\n    sizes/dtype. For that, use `torch.tensor_static_info_cast`.\n\n    This op does not have the AllowsTypeRefinement trait because the operand\n    and result types are coupled. Only places that know how to simultaneously\n    update both types should be changing the type of this op.",
    "inputs": [
      { "name": "operand", "type": "Torch_NonValueTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_ValueTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` qualified(type($result))"
  },
  {
    "name": "torch.derefine",
    "summary": "De-refine a type",
    "description": "In terms of IR structure, TorchScript allows types to vary in many\n    circumstances where MLIR requires pointer-identical types. In particular,\n    it is valid to pass any subtype in place of a type. For example, if an\n    `Optional[int]` is required somewhere in the IR, it is legal to pass a\n    value of just `int` (but not the other way around; see\n    `torch.prim.unchecked_cast`). In effect, every *use* can have a different\n    type.\n\n    This op bridges that impedance mismatch. This op allows casting a value\n    from one type to a type that it is a subtype of to model this behavior.\n    This op uses the TorchScript notion of subtype, which matches the\n    Python notion of subtype presented in PEP 483.",
    "inputs": [
      { "name": "operand", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` qualified(type($operand)) `to` qualified(type($result))"
  },
  {
    "name": "torch.dtype.calculate",
    "summary": "Dtype calculation encapsulation op",
    "description": "The `torch.dtype.calculate` op captures a dtype calculation\n    (in the region `calculation`) which calculates the dtypes for\n    the set of values yielded by the `body` region.\n\n    The `calculation` region yields a `!torch.int` for each\n    value yielded by the `body` region.\n\n    Conceptually, the `calculation` region executes first, then `body`\n    region. So the `calculation` region can also contain arbitrary\n    assertions or side-effecting code which guard the validity of the execution\n    of the body (typically by terminating the program with a\n    torch.prim.RaiseException op).\n\n    The program has undefined behavior if the values yielded by the `body`\n    region do not have the dtypes yielded by the `calculation` region.",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$body `dtypes` $calculation attr-dict `:` type($results)"
  },
  {
    "name": "torch.dtype.calculate.yield",
    "summary": "yield-like terminator for torch.dtype.calculate",
    "description": "This op terminates the `body` region of a `torch.dtype.calculate` op.",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($results^ `:` type($results))?"
  },
  {
    "name": "torch.dtype.calculate.yield.dtypes",
    "summary": "yield-like terminator for torch.dtype.calculate shape region",
    "description": "This op terminates the `dtypeCalculation` region of a\n    `torch.dtype.calculate` op.",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($results^ `:` type($results))?"
  },
  {
    "name": "torch.global_slot",
    "summary": "A slot with global storage",
    "description": "Represents a slot with global storage. The slot semantics are the same\n    as Python's: getting or setting a slot is done by object identity.\n\n    The `typeBound` is a type that the contained type is a subtype of.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "typeBound", "type": "TypeAttr" }
    ],
    "assemblyFormat": "($sym_visibility^)? $sym_name attr-dict `:` $typeBound"
  },
  {
    "name": "torch.global_slot.get",
    "summary": "Get the value stored in a torch.global_slot",
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ],
    "attributes": [
      { "name": "slot", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$slot attr-dict `:` qualified(type($result))"
  },
  {
    "name": "torch.global_slot.init",
    "summary": "yield-like terminator for torch.initialize.global_slotsr region",
    "description": "The operand to this op becomes the initial value of the parent\n    torch.global_slot.",
    "inputs": [
      { "name": "initialValue", "type": "AnyTorchType" }
    ],
    "assemblyFormat": "$initialValue attr-dict `:` qualified(type($initialValue))"
  },
  {
    "name": "torch.global_slot.module_initializer",
    "summary": "Module initializer for all `torch.global_slot` ops",
    "description": "Initializer function that runs once at program startup to initialize\n    all `torch.global_slot` ops in the module.\n\n    The only ops that should be in the module initializer should be ops\n    generated by the IValue importer. This set avoids the need to define\n    the behavior in case of certain kinds of side effects in the initializer\n    (except for the side effect of updating the torch.global_slot ops with the\n    `torch.initialize.global_slots` op).",
    "assemblyFormat": "$initializer attr-dict"
  },
  {
    "name": "torch.global_slot.set",
    "summary": "Set the value stored in a torch.global_slot",
    "inputs": [
      { "name": "value", "type": "AnyTorchType" }
    ],
    "attributes": [
      { "name": "slot", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$slot `=` $value attr-dict `:` qualified(type($value))"
  },
  {
    "name": "torch.initialize.global_slots",
    "summary": "Terminator for torch.global_slot.module_initializer region",
    "description": "Atomically updates the value of all the global slots named in `slotSymNames`\n    with the corresponding values provided in `initialValues`.",
    "inputs": [
      { "name": "initialValues", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "slotSymNames", "type": "SymbolRefArrayAttr" }
    ]
  },
  {
    "name": "torch.linear_params.create",
    "summary": "Create a `!torch.LinearParams`",
    "inputs": [
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_LinearParamsType" }
    ],
    "assemblyFormat": "$weight (`,` $bias^)? attr-dict `:` qualified(type($weight)) (`,` qualified(type($bias))^)?"
  },
  {
    "name": "torch.method",
    "summary": "Declare a method of a torch.class_type",
    "description": "This op declaratively specifies that the parent torch.class_type has a\n    method `name` which calls `function`. `function` is an unbound function.\n    That is, it explicitly takes the torch.nn.Module as a parameter (no implicit\n    \"self\" object).\n\n    If `private` is present, it indicates that external calls cannot be made\n    to this method.",
    "attributes": [
      { "name": "name", "type": "StrAttr" },
      { "name": "function", "type": "FlatSymbolRefAttr" },
      { "name": "isPrivate", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`private` $isPrivate^)? $name `,` $function attr-dict"
  },
  {
    "name": "torch.nn_module",
    "summary": "Constructs a torch.nn.Module",
    "description": "This op is used to represent a torch.nn.Module when importing a\n    graph of Python objects.\n\n    This op returns a new torch.nn.Module as an SSA value, with a set of\n    declaratively specified properties.\n\n    Example:\n\n    ```mlir\n    %2 = torch.nn_module {\n      torch.slot \"b\", %bool_true : !torch.bool\n      torch.slot \"i\", %int3 : !torch.int\n      torch.slot \"f\", %float : !torch.float\n      torch.slot \"t\", %t : !torch.tensor\n      torch.slot \"submodule\", %1 : !torch.nn.Module\n    } : !torch.nn.Module<\"my_class_name\">\n    ```\n\n    This op is tightly coupled to the `torch.class_type` op named in the\n    `!torch.nn.Module<\"my_class_name\">` type. Each slot must match precisely\n    with the corresponding `torch.attr` in the `torch.class_type`.\n    See the documentation for `torch.class_type` for information.",
    "outputs": [
      { "name": "result", "type": "Torch_NnModuleType" }
    ],
    "assemblyFormat": "$region attr-dict `:` qualified(type($result))"
  },
  {
    "name": "torch.nn_module_terminator",
    "summary": "Implicit terminator for torch.nn_module",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "torch.onnx.rotary_embedding",
    "summary": "`rotary_embedding op : (Tensor, Tensor, Tensor, Tensor, int, int, int, int, float) -> (Tensor)`",
    "description": "The `torch.onnx.rotary_embedding` operation is an op which is used\n    specifically for supporting the Onnx's Rotary Embedding op. The\n    reason for this is that the Onnx ops can't be directly lowered to\n    Linalg and we have to map them to a legal Torch Dialect op, hence\n    this op is used for that purpose.",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "position_ids", "type": "AnyTorchTensorType" },
      { "name": "cos_cache", "type": "AnyTorchTensorType" },
      { "name": "sin_cache", "type": "AnyTorchTensorType" },
      { "name": "interleaved", "type": "Torch_IntType" },
      { "name": "is_packed_batching", "type": "Torch_IntType" },
      { "name": "num_heads", "type": "Torch_IntType" },
      { "name": "rotary_embedding_dim", "type": "Torch_IntType" },
      { "name": "scale", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchTensorType" }
    ]
  },
  {
    "name": "torch.operator",
    "summary": "Opaque torch operator",
    "description": "Represents an invocation of a `torch::jit::Operator` for which we don't\n    have a registered MLIR operation.\n\n    The `name` attribute contains the name that the MLIR op would have\n    (excluding `torch.`) if we did have it registered, which allows easy\n    cross referencing with `JITOperatorRegistryDump.txt`.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$name `(` $operands `)` attr-dict `:` functional-type($operands, $results) $regions"
  },
  {
    "name": "torch.operator_terminator",
    "summary": "Implicit terminator for torch.operator",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "$operands attr-dict `:` type($operands)"
  },
  {
    "name": "torch.overwrite.tensor.contents",
    "summary": "Ovewrite the contents of tensor with values from another.",
    "description": "Replaces the contents of `overwritten` with corresponding values from\n    `value`.\n\n    Immediately after this op has completed, indexing `overwritten` will result\n    in identical values as indexing into `value`. Of course, later ops\n    might mutate `overwritten`, so this relationship need not hold for the\n    entire program. This op only updates the tensor data (not metadata).\n    In other words, it cannot change the (dynamic) shape of the overwritten tensor.\n\n    This op does not have the AllowsTypeRefinement trait because the types of the\n    two operands are coupled. Only places that know how to simultaneously update\n    both types should be changing the type of this op.",
    "inputs": [
      { "name": "value", "type": "Torch_ValueTensorType" },
      { "name": "overwritten", "type": "Torch_NonValueTensorType" }
    ],
    "assemblyFormat": "$value `overwrites` $overwritten attr-dict\n      `:` qualified(type($value)) `,` qualified(type($overwritten))"
  },
  {
    "name": "torch.per_tensor_affine.create",
    "summary": "Create a per-tensor-affine quantized tensor",
    "description": "Create a quantized tensor.\n\n    Quantization formula is:\n    ```\n    Q(x, scale, zero_point) = round(x/scale + zero_point)\n    ```\n\n    See:\n    https://pytorch.org/docs/stable/quantization.html#quantized-tensors",
    "inputs": [
      { "name": "int_repr", "type": "AnyTorchTensorType" },
      { "name": "scale", "type": "Torch_FloatType" },
      { "name": "offset", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchTensorType" }
    ],
    "assemblyFormat": "$int_repr `,` $scale `,` $offset attr-dict\n    `:` qualified(type($int_repr)) `,` qualified(type($scale)) `,` qualified(type($offset)) `->` qualified(type($result))"
  },
  {
    "name": "torch.prim.abs.Scalar",
    "summary": "Generated op for `prim::abs.Scalar : (Scalar) -> (Scalar)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchScalarType" }
    ]
  },
  {
    "name": "torch.prim.CallMethod",
    "summary": "TorchScript prim::CallMethod op",
    "inputs": [
      { "name": "receiver", "type": "Torch_NnModuleType" },
      { "name": "methodOperands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$receiver `[` $name `]` `(` $methodOperands `)` attr-dict `:` qualified(type($receiver)) `,` functional-type($methodOperands, $result)"
  },
  {
    "name": "torch.prim.CreateObject",
    "summary": "TorchScript prim::CreateObject op",
    "outputs": [
      { "name": "result", "type": "Torch_NnModuleType" }
    ],
    "assemblyFormat": "attr-dict qualified(type($result))"
  },
  {
    "name": "torch.prim.device",
    "summary": "Generated op for `prim::device : (Tensor) -> (Device)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_DeviceType" }
    ]
  },
  {
    "name": "torch.prim.DictConstruct",
    "summary": "TorchScript prim::DictConstruct op",
    "inputs": [
      { "name": "keys", "type": "Variadic" },
      { "name": "values", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_DictType" }
    ],
    "assemblyFormat": "`keys` `(` ($keys^ `:` qualified(type($keys)))? `)` `values` `(` ($values^ `:` qualified(type($values)))? `)` attr-dict `->` qualified(type($result))"
  },
  {
    "name": "torch.prim.dtype",
    "summary": "Generated op for `prim::dtype : (Tensor) -> (int)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.prim.Enter",
    "summary": "enter operation",
    "description": "This op represents a prim::Enter node in the Python object graph.",
    "inputs": [
      { "name": "inp", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_NoneType" }
    ],
    "assemblyFormat": "$inp attr-dict `:` qualified(type($inp))"
  },
  {
    "name": "torch.prim.Exit",
    "summary": "exit operation",
    "description": "This op represents a prim::Exit node in the Python object graph.",
    "inputs": [
      { "name": "inp", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchTensorType" }
    ],
    "assemblyFormat": "$inp attr-dict `:` qualified(type($inp)) `->` qualified(type($result))"
  },
  {
    "name": "torch.prim.GetAttr",
    "summary": "TorchScript prim::GetAttr op",
    "inputs": [
      { "name": "receiver", "type": "Torch_NnModuleType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$receiver `[` $name `]` attr-dict `:` qualified(type($receiver)) `->` qualified(type($result))"
  },
  {
    "name": "torch.prim.If",
    "summary": "TorchScript prim::If op",
    "description": "This op (together with prim.If.yield) define a conditional control flow\n    construct. It is analogous to `scf.if` for MLIR folks that are familiar\n    with that. The main differences from that op are:\n\n    - `!torch.bool` condition value.\n    - The \"else\" region is always present. This is reflective of invariants of\n      the TorchScript IR.\n    - No special prettiness for the \"no yielded values\" case. These are\n      interesting for modeling mostly-non-SSA programs, but TorchScript IR\n      is already in SSA form.\n\n    See: https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/OVERVIEW.md#if",
    "inputs": [
      { "name": "condition", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "torch.prim.If.yield",
    "summary": "yield-like terminator for torch.prim.If",
    "description": "Does not correspond to any torch prim op directly (the way that they model\n    blocks has a built-in notion of yield-like terminator).",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($results^ `:` qualified(type($results)))?"
  },
  {
    "name": "torch.prim.layout",
    "summary": "Generated op for `prim::layout : (Tensor) -> (int)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.prim.ListConstruct",
    "summary": "TorchScript prim::ListConstruct op",
    "inputs": [
      { "name": "elements", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchListType" }
    ],
    "assemblyFormat": "$elements attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "torch.prim.ListUnpack",
    "summary": "TorchScript prim::ListUnpack op",
    "inputs": [
      { "name": "operand", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$operand attr-dict `:` qualified(type($operand)) `->` qualified(type($results))"
  },
  {
    "name": "torch.prim.Load",
    "summary": "load operation",
    "description": "This op represents a prim::Load node in the Python object graph.",
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$name attr-dict `:` qualified(type($result))"
  },
  {
    "name": "torch.prim.Loop",
    "summary": "TorchScript prim::Loop op",
    "description": "This op (together with prim.Loop.condition) define a looping construct\n    that combines `for` and `while` behavior.\n\n    See: https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/OVERVIEW.md#loops",
    "inputs": [
      { "name": "maxTripCount", "type": "Torch_IntType" },
      { "name": "initialCondition", "type": "Torch_BoolType" },
      { "name": "iterArgsInit", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$maxTripCount `,` $initialCondition `,` `init` `(` $iterArgsInit `)` $region\n    attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "torch.prim.Loop.condition",
    "summary": "yield-like terminator for torch.prim.Loop",
    "description": "Does not correspond to any torch prim op directly (the way that they model\n    blocks has a built-in notion of yield-like terminator).",
    "inputs": [
      { "name": "shouldContinue", "type": "Torch_BoolType" },
      { "name": "iterArgs", "type": "Variadic" }
    ],
    "assemblyFormat": "$shouldContinue `,`\n    `iter` `(` ($iterArgs^ `:` qualified(type($iterArgs)))? `)` attr-dict"
  },
  {
    "name": "torch.prim.max.int",
    "summary": "Generated op for `prim::max.int : (int, int) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.prim.max.self_int",
    "summary": "Generated op for `prim::max.self_int : (int[]) -> (int)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.prim.min.int",
    "summary": "Generated op for `prim::min.int : (int, int) -> (int)`",
    "inputs": [
      { "name": "a", "type": "Torch_IntType" },
      { "name": "b", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.prim.min.self_int",
    "summary": "Generated op for `prim::min.self_int : (int[]) -> (int)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ]
  },
  {
    "name": "torch.prim.NumToTensor.Scalar",
    "summary": "Generated op for `prim::NumToTensor.Scalar : (Scalar) -> (Tensor)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchScalarType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prim.Print",
    "summary": "Generated op for `prim::Print : (...) -> ()`",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "`(` $operands `)` attr-dict `:` qualified(type($operands))"
  },
  {
    "name": "torch.prim.RaiseException",
    "summary": "Generated op for `prim::RaiseException : (str, str?) -> ()`",
    "inputs": [
      { "name": "msg", "type": "Torch_StringType" },
      { "name": "cls", "type": "AnyTorchOptionalStringType" }
    ]
  },
  {
    "name": "torch.prim.SetAttr",
    "summary": "TorchScript prim::SetAttr op",
    "inputs": [
      { "name": "receiver", "type": "Torch_NnModuleType" },
      { "name": "value", "type": "AnyTorchType" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$receiver `[` $name `]` `=` $value attr-dict `:` qualified(type($receiver)) `,` qualified(type($value))"
  },
  {
    "name": "torch.prim.Store",
    "summary": "store operation",
    "description": "This op represents a prim::Store node in the Python object graph.",
    "inputs": [
      { "name": "value", "type": "AnyTorchType" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$name `,` $value attr-dict `:` qualified(type($value))"
  },
  {
    "name": "torch.prim.tolist",
    "summary": "Generated op for `prim::tolist : (...) -> (...)`",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "`(` $operands `)` attr-dict `:` qualified(type($operands)) `->` qualified(type($results))"
  },
  {
    "name": "torch.prim.TupleConstruct",
    "summary": "TorchScript prim::TupleConstruct op",
    "description": "Note: This op does not allow trivial type refinement, because the\n    operand types and the result types must be in correspondence.",
    "inputs": [
      { "name": "elements", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_TupleType" }
    ],
    "assemblyFormat": "$elements attr-dict `:` qualified(type($elements)) `->` qualified(type($result))"
  },
  {
    "name": "torch.prim.TupleIndex",
    "summary": "Generated op for `prim::TupleIndex : (Any, int) -> (Any)`",
    "inputs": [
      { "name": "tup", "type": "AnyTorchType" },
      { "name": "i", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ]
  },
  {
    "name": "torch.prim.TupleUnpack",
    "summary": "Generated op for `prim::TupleUnpack : (Any) -> (...)`",
    "inputs": [
      { "name": "tup", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$tup attr-dict `:` qualified(type($tup)) `->` qualified(type($results))"
  },
  {
    "name": "torch.prim.unchecked_cast",
    "summary": "Generated op for `prim::unchecked_cast : (t) -> (t)`",
    "inputs": [
      { "name": "x", "type": "AnyTorchType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ]
  },
  {
    "name": "torch.prim.Uninitialized",
    "summary": "Generated op for `prim::Uninitialized : () -> (Any)`",
    "outputs": [
      { "name": "result", "type": "AnyTorchType" }
    ]
  },
  {
    "name": "torch.prims.collapse",
    "summary": "Generated op for `prims::collapse : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" },
      { "name": "start", "type": "Torch_IntType" },
      { "name": "end", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prims.convert_element_type",
    "summary": "Generated op for `prims::convert_element_type : (Tensor, int) -> (Tensor)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" },
      { "name": "dtype", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prims.iota",
    "summary": "Generated op for `prims::iota : (int, int, int, int, Device, bool) -> (Tensor)`",
    "inputs": [
      { "name": "length", "type": "Torch_IntType" },
      { "name": "start", "type": "Torch_IntType" },
      { "name": "step", "type": "Torch_IntType" },
      { "name": "dtype", "type": "Torch_IntType" },
      { "name": "device", "type": "Torch_DeviceType" },
      { "name": "requires_grad", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prims.split_dim",
    "summary": "Generated op for `prims::split_dim : (Tensor, int, int) -> (Tensor)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" },
      { "name": "dim", "type": "Torch_IntType" },
      { "name": "outer_length", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prims.sqrt",
    "summary": "Generated op for `prims::sqrt : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prims.squeeze",
    "summary": "Generated op for `prims::squeeze : (Tensor, int[]) -> (Tensor)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" },
      { "name": "dimensions", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prims.sum",
    "summary": "Generated op for `prims::sum : (Tensor, int[]?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "inp", "type": "AnyTorchTensorType" },
      { "name": "dims", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "output_dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prims.var",
    "summary": "Generated op for `prims::var : (Tensor, int[]?, float?, int?) -> (Tensor)`",
    "inputs": [
      { "name": "inp", "type": "AnyTorchTensorType" },
      { "name": "dims", "type": "AnyTorchOptionalListOfTorchIntType" },
      { "name": "correction", "type": "AnyTorchOptionalFloatType" },
      { "name": "output_dtype", "type": "AnyTorchOptionalIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.prims.view_of",
    "summary": "Generated op for `prims::view_of : (Tensor) -> (Tensor)`",
    "inputs": [
      { "name": "a", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.promote_dtypes",
    "summary": "`promote_dtypes op : (int?[], int[]) -> (int)`",
    "description": "This op is generated when the python function\n    `__torch_mlir_internal_promote_dtypes` is used in a dtype refinement\n    function. It represents the type promotion logic used by PyTorch to\n    determine result types.\n\n    The first argument is a list of optional ranks for each of the inputs\n    being used for promotion. The ranks are optional to allow representing\n    `Scalar` inputs, which follow their own set of promotion rules.\n\n    The second argument is a list of dtypes for each of the inputs being used\n    for promotion.\n\n    The order of the values in each list must be the same. In other words,\n    the ith rank and the ith dtype must be from the same Scalar/Tensor.\n\n    It is an error to call this op with empty lists or lists of different size.",
    "inputs": [
      { "name": "ranks", "type": "AnyTorchListOfOptionalIntType" },
      { "name": "dtypes", "type": "AnyTorchListOfTorchIntType" }
    ],
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ],
    "assemblyFormat": "$ranks `,` $dtypes attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "torch.quantized.linear",
    "summary": "Generated op for `quantized::linear : (Tensor, __torch__.torch.classes.quantized.LinearPackedParamsBase, float, int) -> (Tensor)`",
    "inputs": [
      { "name": "X", "type": "AnyTorchTensorType" },
      { "name": "W_prepack", "type": "Torch_LinearParamsType" },
      { "name": "Y_scale_i", "type": "Torch_FloatType" },
      { "name": "Y_zero_point_i", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.runtime.assert",
    "summary": "Runtime Assertion",
    "inputs": [
      { "name": "condition", "type": "Torch_BoolType" }
    ],
    "attributes": [
      { "name": "message", "type": "StrAttr" }
    ],
    "assemblyFormat": "$condition `,` $message attr-dict"
  },
  {
    "name": "torch.shape.calculate",
    "summary": "Shape calculation encapsulation op",
    "description": "The `torch.shape.calculate` op captures a shape calculation\n    (in the region `calculation`) which calculates the shapes for\n    the set of values yielded by the `body` region.\n\n    The `calculation` region yields a `!torch.list<int>` for each\n    value yielded by the `body` region.\n\n    Conceptually, the `calculation` region executes first, then `body`\n    region. So the `calculation` region can also contain arbitrary\n    assertions or side-effecting code which guard the validity of the execution\n    of the body (typically by terminating the program with a\n    torch.prim.RaiseException op).\n\n    The program has undefined behavior if the values yielded by the `body`\n    region do not have the shapes yielded by the `calculation` region.",
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "$body `shapes` $calculation attr-dict `:` type($results)"
  },
  {
    "name": "torch.shape.calculate.yield",
    "summary": "yield-like terminator for torch.shape.calculate",
    "description": "This op terminates the `body` region of a `torch.shape.calculate` op.",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($results^ `:` type($results))?"
  },
  {
    "name": "torch.shape.calculate.yield.shapes",
    "summary": "yield-like terminator for torch.shape.calculate shape region",
    "description": "This op terminates the `shapeCalculation` region of a\n    `torch.shape.calculate` op.",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($results^ `:` type($results))?"
  },
  {
    "name": "torch.slot",
    "summary": "Define the value of a slot of a torch.nn.Module",
    "description": "This op specifies that the initial value of the slot `name` of the\n    parent torch.nn_module should be `value`, which is allowed to be an\n    arbitrary Torch-compatible SSA value, including other !torch.nn.Module's.",
    "inputs": [
      { "name": "value", "type": "AnyTorchType" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$name `,` $value attr-dict `:` qualified(type($value))"
  },
  {
    "name": "torch.symbolic_int",
    "summary": "Symbolic int representing a dynamic dimension",
    "description": "The `torch.symbolic_int` operation captures a dynamic dimension on the\n    global function arguments as exported by TorchDynamo (torch.export).\n    It associates the shape symbols (i.e. \"s0\", \"s1\") with the\n    global SSA values (i.e. `%0`, `%1`) that is then referenced\n    to bind shapes on op results.\n\n    Additionally, the operation annotates `min_val` and `max_val` attributes\n    denoting the range constraints for the dynamic dimension. This may be\n    useful for modeling runtime shape guards, or compile-time optimizations\n    based on the shape bounds (min, opt, max) on results of ops / regions.\n\n    Example:\n    ```\n    %0 = torch.symbolic_int \"s0\" {min_val = 5, max_val = 10} : !torch.int\n    %1 = torch.symbolic_int \"s1\" {min_val = 2, max_val = 20} : !torch.int\n    ```\n\n    In this case, we see that `s0` has the range [5, 10] and `s1` has the\n    range [2, 20]. When unspecified, the range constraints feeding in from\n    TorchDynamo default to [0, INT_MAX] (or [2, INT_MAX] in older PyTorch\n    releases). In either case, the interpretation (as specified by TorchDynamo)\n    is that the dynamic dimension is assumed to be not 0 or 1. This is not a\n    bug, and does not necessarily mean that the exported program will not work\n    for dimensions 0 or 1. For an in-depth discussion of this topic, see\n    [The 0/1 Specialization Problem](https://docs.google.com/document/d/16VPOa3d-Liikf48teAOmxLc92rgvJdfosIy-yoT38Io/edit?fbclid=IwAR3HNwmmexcitV0pbZm_x1a4ykdXZ9th_eJWK-3hBtVgKnrkmemz6Pm5jRQ#heading=h.ez923tomjvyk).",
    "outputs": [
      { "name": "result", "type": "Torch_IntType" }
    ],
    "attributes": [
      { "name": "symbol_name", "type": "StrAttr" },
      { "name": "min_val", "type": "I64Attr" },
      { "name": "max_val", "type": "I64Attr" }
    ],
    "assemblyFormat": "$symbol_name ` ` `{` `min_val` `=` $min_val `,` `max_val` `=` $max_val `}` attr-dict `:` type($result)"
  },
  {
    "name": "torch.tensor_static_info_cast",
    "summary": "Adds/removes static information from a tensor type.",
    "description": "This op does not imply any runtime code. Semantically it is an identity\n    function. However, it statically annotates (or erases) shape and dtype\n    information from a tensor type.\n\n    This op *cannot* be used to add/remove value semantics from a tensor.\n    For converting between the value-semantic and non-value-semantic domains,\n    use `torch.copy.to_tensor` and `torch.copy.from_tensor`. This op is kept\n    separate to prevent canonicalizations from accidentally dropping static\n    information. In most cases, after running the `torch-refine-types` pass,\n    this op becomes a no-op (the pass will incorporate the static information\n    into other ops that allow type refinement).",
    "inputs": [
      { "name": "operand", "type": "AnyTorchTensorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchTensorType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` qualified(type($operand)) `to` qualified(type($result))"
  },
  {
    "name": "torch.tensor.literal",
    "summary": "Create a value of !torch.tensor type from a literal",
    "description": "Example:\n    ```\n    %0 = torch.tensor.literal(dense<0.0> : tensor<3x5xf32>) : !torch.tensor\n    %1 = torch.tensor.literal(dense<0.0> : tensor<3xf32>) : !torch.tensor<[3],f32>\n    ```\n\n    This op covers a typical frontend use case of creating a type-erased\n    `!torch.tensor`. Inside the compiler, we decompose it into\n    `torch.vtensor.literal` which is easier to analyze and transform.\n\n    Note: This op is not called \"constant\" because the created tensor is not\n    \"constant\" in any meaning of that word.",
    "outputs": [
      { "name": "result", "type": "Torch_NonValueTensorType" }
    ],
    "attributes": [
      { "name": "value", "type": "ElementsAttr" }
    ],
    "assemblyFormat": "`(` $value `)` attr-dict `:` qualified(type($result))"
  },
  {
    "name": "torch.torchvision.deform_conv2d",
    "summary": "Generated op for `torchvision::deform_conv2d : (Tensor, Tensor, Tensor, Tensor, Tensor, int, int, int, int, int, int, int, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "weight", "type": "AnyTorchTensorType" },
      { "name": "offset", "type": "AnyTorchTensorType" },
      { "name": "mask", "type": "AnyTorchTensorType" },
      { "name": "bias", "type": "AnyTorchTensorType" },
      { "name": "stride_h", "type": "Torch_IntType" },
      { "name": "stride_w", "type": "Torch_IntType" },
      { "name": "pad_h", "type": "Torch_IntType" },
      { "name": "pad_w", "type": "Torch_IntType" },
      { "name": "dilation_h", "type": "Torch_IntType" },
      { "name": "dilation_w", "type": "Torch_IntType" },
      { "name": "groups", "type": "Torch_IntType" },
      { "name": "offset_groups", "type": "Torch_IntType" },
      { "name": "use_mask", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.torchvision.nms",
    "summary": "Generated op for `torchvision::nms : (Tensor, Tensor, float) -> (Tensor)`",
    "inputs": [
      { "name": "dets", "type": "AnyTorchTensorType" },
      { "name": "scores", "type": "AnyTorchTensorType" },
      { "name": "iou_threshold", "type": "Torch_FloatType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.torchvision.roi_align",
    "summary": "Generated op for `torchvision::roi_align : (Tensor, Tensor, float, int, int, int, bool) -> (Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "rois", "type": "AnyTorchTensorType" },
      { "name": "spatial_scale", "type": "Torch_FloatType" },
      { "name": "pooled_height", "type": "Torch_IntType" },
      { "name": "pooled_width", "type": "Torch_IntType" },
      { "name": "sampling_ratio", "type": "Torch_IntType" },
      { "name": "aligned", "type": "Torch_BoolType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.torchvision.roi_pool",
    "summary": "Generated op for `torchvision::roi_pool : (Tensor, Tensor, float, int, int) -> (Tensor, Tensor)`",
    "inputs": [
      { "name": "input", "type": "AnyTorchTensorType" },
      { "name": "rois", "type": "AnyTorchTensorType" },
      { "name": "spatial_scale", "type": "Torch_FloatType" },
      { "name": "pooled_height", "type": "Torch_IntType" },
      { "name": "pooled_width", "type": "Torch_IntType" }
    ],
    "outputs": [
      { "name": "result0", "type": "AnyTorchOptionalTensorType" },
      { "name": "result1", "type": "AnyTorchOptionalTensorType" }
    ]
  },
  {
    "name": "torch.valsem.aten.bernoulli.float",
    "summary": "`bernoulli.float op : (Tensor, float, Generator?) -> (Tensor)`",
    "inputs": [
      { "name": "self", "type": "AnyTorchTensorType" },
      { "name": "p", "type": "Torch_FloatType" },
      { "name": "generator", "type": "AnyTorchOptionalGeneratorType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTorchTensorType" }
    ],
    "assemblyFormat": "$self `,` $p `,` $generator attr-dict `:` type($self) `,` type($p) `,` type($generator) `->` type($result)"
  },
  {
    "name": "torch.vtensor.literal",
    "summary": "Create a value of !torch.vtensor type from a literal",
    "description": "Example:\n    ```\n    %0 = torch.vtensor.literal(dense<0.0> : tensor<3x5xf32>) : !torch.vtensor<[3,5],f32>\n    %1 = torch.vtensor.literal(dense<0.0> : tensor<3xf32>) : !torch.vtensor<[3],f32>\n    ```\n\n    Unlike `torch.tensor.literal`, which covers a typical frontend use case\n    and allows type refinement, this op always has a maximally resolved type\n    (which is always possible, because it is created from a literal). This\n    has a stronger set of invariants that better fit the needs of the\n    compiler internals.",
    "outputs": [
      { "name": "result", "type": "Torch_ValueTensorType" }
    ],
    "attributes": [
      { "name": "value", "type": "ElementsAttr" }
    ],
    "assemblyFormat": "`(` $value `)` attr-dict `:` qualified(type($result))"
  },
  {
    "name": "tosa.abs",
    "summary": "Elementwise abs operator.",
    "description": "Elementwise absolute value operation.\n\n    Example:\n\n    ```mlir\n    %output = tosa.abs(%input1) : (tensor<21x3xf32>) -> tensor<21x3xf32>\n    ```",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.add",
    "summary": "Elementwise addition operator.",
    "description": "Elementwise addition of input1 and input2. Axis of size 1 will be broadcast,\n    as necessary. Rank of input tensors must match.\n\n    Example:\n\n    ```mlir\n    // Elementwise addition.\n    %out = tosa.add %input1, %input2 : tensor<12x6xf32>, tensor<12x6xf32> -> tensor<12x6xf32>\n\n    // Elementwise addition with broadcasting.\n    %out = tosa.add %input1, %input2 : tensor<12x6xsi32>, tensor<1x1xsi32> -> tensor<12x6xsi32>\n    ```",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.apply_scale",
    "summary": "Rescale scalar operator for Tosa tensor operators",
    "description": "Applies rescaling for fixed point values. This behavior is replicated in\n    multiple quantized operations (mul, convolution, rescale, matmul, pooling).\n\n    The commonplace implementation is to use i64 operations to avoid integer\n    overflow with target specific implementations can use native operations to\n    avoid wider than necessary types.",
    "inputs": [
      { "name": "value", "type": "Tosa_IntLike" },
      { "name": "multiplier", "type": "Tosa_IntLike" },
      { "name": "shift", "type": "Tosa_Int8Like" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_IntLike" }
    ],
    "attributes": [
      { "name": "rounding_mode", "type": "Tosa_RoundingModeAttr" }
    ]
  },
  {
    "name": "tosa.argmax",
    "summary": "Perform argmax on the input.",
    "description": "This returns the index with the largest value across the given axis of the\n    input tensor. If multiple locations have equal values, returns the first\n    match along the search axis.",
    "inputs": [
      { "name": "input", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" },
      { "name": "nan_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tosa.arithmetic_right_shift",
    "summary": "Elementwise Arithmetic Right Shift.",
    "description": "Elementwise arithmetic right shift of input1 by the amount specified in\n    input2. Axis of size 1 will be broadcast, as necessary. Rank of input tensors\n    must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "round", "type": "BoolAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.avg_pool2d",
    "summary": "Performs average pooling on the input.",
    "description": "This performs an average pooling over the given input tensor. A sliding\n    window of size given by <kernel size> is passed over the input tensor, with\n    the mean value being placed in the output tensor. When calculating the\n    average, only the number of valid input tensor values, but not padding, are\n    used to calculate the divisor.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor4D" },
      { "name": "input_zp", "type": "Tosa_ScalarIntOrFloatTensor" },
      { "name": "output_zp", "type": "Tosa_ScalarIntOrFloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor4D" }
    ],
    "attributes": [
      { "name": "kernel", "type": "Tosa_IntArrayAttr2" },
      { "name": "stride", "type": "Tosa_IntArrayAttr2" },
      { "name": "pad", "type": "Tosa_IntArrayAttr4" },
      { "name": "acc_type", "type": "TypeAttrOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.bitwise_and",
    "summary": "Bitwise AND operator.",
    "description": "Elementwise bitwise AND of input1 and input2. Axis of size 1\n    will be broadcast as necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.bitwise_not",
    "summary": "Bitwise NOT operator.",
    "description": "Elementwise bitwise NOT of input tensor.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.bitwise_or",
    "summary": "Bitwise OR operator.",
    "description": "Elementwise bitwise OR of input1 and input2. Axis of size 1 will be\n    broadcast as necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.bitwise_xor",
    "summary": "Bitwise XOR operator.",
    "description": "Elementwise bitwise XOR of input1 and input2. Axis of size 1 will be\n    broadcast as necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.cast",
    "summary": "Cast operation.",
    "description": "Casts a tensor from one data type to another.\n    * This table is showing the supported conversions from the TOSA Specification.\n    * The MLIR dialect here can be used to represent other conversions.\n\n    | Mode                     | Input   | Output  |\n    |--------------------------|---------|---------|\n    | fp16 to fp32             | float16 | float32 |\n    | fp16 to int 16           | float16 | int16   |\n    | fp16 to int 32           | float16 | int32   |\n    | fp16 to int 8            | float16 | int8    |\n    | fp32 to fp16             | float32 | float16 |\n    | fp32 to int 16           | float32 | int16   |\n    | fp32 to int 32           | float32 | int32   |\n    | fp32 to int 8            | float32 | int8    |\n    | int 16 to fp16           | int16   | float16 |\n    | int 16 to fp32           | int16   | float32 |\n    | int 32 to fp16           | int32   | float16 |\n    | int 32 to fp32           | int32   | float32 |\n    | int 8 to fp16            | int8    | float16 |\n    | int 8 to fp32            | int8    | float32 |\n    | bool to int 16           | Boolean | int16   |\n    | bool to int 32           | Boolean | int32   |\n    | bool to int 8            | Boolean | int8    |\n    | int 16 to bool           | int16   | Boolean |\n    | int 16 to int 32         | int16   | int32   |\n    | int 16 to int 8          | int16   | int8    |\n    | int 32 to bool           | int32   | Boolean |\n    | int 32 to int 16         | int32   | int16   |\n    | int 32 to int 8          | int32   | int8    |\n    | int 8 to bool            | int8    | Boolean |\n    | int 8 to int 16          | int8    | int16   |\n    | int 8 to int 32          | int8    | int32   |\n    | bf16 to fp32             | bf16    | float32 |\n    | bf16 to int 16           | bf16    | int16   |\n    | bf16 to int 32           | bf16    | int32   |\n    | bf16 to int 8            | bf16    | int8    |\n    | fp32 to bf16             | float32 | bf16    |\n    | int 16 to bf16           | int16   | bf16    |\n    | int 32 to bf16           | int32   | bf16    |\n    | int 8 to bf16            | int8    | bf16    |\n    | bf16 to fp8e4m3          | bf16    | fp8e4m3 |\n    | fp8e4m3 to bf16          | fp8e4m3 | bf16    |\n    | bf16 to fp8e5m2          | bf16    | fp8e5m2 |\n    | fp8e5m2 to bf16          | fp8e5m2 | bf16    |\n    | fp16 to fp8e4m3          | float16 | fp8e4m3 |\n    | fp32 to fp8e4m3          | float32 | fp8e4m3 |\n    | fp8e4m3 to fp16          | fp8e4m3 | float16 |\n    | fp8e4m3 to fp32          | fp8e4m3 | float32 |\n    | fp16 to fp8e5m2          | float16 | fp8e5m2 |\n    | fp32 to fp8e5m2          | float32 | fp8e5m2 |\n    | fp8e5m2 to fp16          | fp8e5m2 | float16 |\n    | fp8e5m2 to fp32          | fp8e5m2 | float32 |",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.cast_from_block_scaled",
    "summary": "Apply scales from a scale tensor to the values in a value tensor",
    "description": "Apply the scales from a scale tensor to the values in a value tensor, casting\n    the result to the output type. The block dimension must be the last dimension\n    of the tensor.",
    "inputs": [
      { "name": "input_data", "type": "Tosa_MXFPDataTensorAtLeast1D" },
      { "name": "input_scale", "type": "Tosa_MXFPScaleTensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output_data", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "block_size", "type": "Tosa_BlockSizeAttr" }
    ]
  },
  {
    "name": "tosa.cast_to_block_scaled",
    "summary": "Calculate scale tensor values per block, output to separate scale and data tensors.",
    "description": "Calculate a scale value per block of input values and use that to calculate\n    scaled data values from an input tensor. The output tensors are cast to the\n    specified scale and value types. The block dimension will be the last dimension\n    of the tensor.",
    "inputs": [
      { "name": "input_data", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output_data", "type": "Tosa_MXFPDataTensorAtLeast1D" },
      { "name": "output_scale", "type": "Tosa_MXFPScaleTensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "block_size", "type": "Tosa_BlockSizeAttr" }
    ]
  },
  {
    "name": "tosa.ceil",
    "summary": "Elementwise ceil operator.",
    "description": "Elementwise ceiling operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.clamp",
    "summary": "Computes clamp(features, min, max).",
    "description": "Clamp to an arbitrary minimum and maximum value.\n    Maximum and minimum values are specified as values in the range of the\n    input type.\n    No zero point subtraction is done to the values, thus to clamp to the zero\n    point value, the zero point itself should be supplied as the minimum value.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "min_val", "type": "Tosa_IntOrFloatAttr" },
      { "name": "max_val", "type": "Tosa_IntOrFloatAttr" },
      { "name": "nan_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tosa.clz",
    "summary": "Elementwise count leading zero operator.",
    "description": "Elementwise count leading zeros operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.concat",
    "summary": "Concatenates tensors along one dimension.",
    "description": "Concatenate a list of tensors along a given axis.\n    No data conversion happens during a concat operation.",
    "inputs": [
      { "name": "input1", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.cond_if",
    "summary": "Conditional if operator.",
    "description": "Evaluates a Boolean condition and then takes one of two distinct execution\n    paths. This implements the semantic If-then-else structure.",
    "inputs": [
      { "name": "condition", "type": "Tosa_I1Tensor" },
      { "name": "input_list", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output_list", "type": "Variadic" }
    ]
  },
  {
    "name": "tosa.const",
    "summary": "Constant operator.",
    "description": "A node containing constant data for use as the input to an operation. May\n    hold data in any of the supported data formats.\n\n    Example:\n\n    ```mlir\n    // Generic form\n    %out = \"tosa.const\"() {values = dense<0> : tensor<2x3xi32>} : () -> tensor<2x3xi32>\n    ```",
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "values", "type": "ElementsAttr" }
    ]
  },
  {
    "name": "tosa.const_shape",
    "summary": "Constant Shape operator.",
    "description": "A node containing a constant shape.\n\n    Example:\n\n    ```mlir\n    // Generic form\n    %out = \"tosa.const_shape\"() {values = dense<0> : tensor<4xindex>} : () -> !tosa.shape<4>\n    ```",
    "outputs": [
      { "name": "output", "type": "Tosa_Shape" }
    ],
    "attributes": [
      { "name": "values", "type": "IndexElementsAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.conv2d",
    "summary": "2D Convolution operator.",
    "description": "Performs a 2D convolution over the given tensor input, using the weight\n    tensor. Implementations may choose to skip calculation of multiplies in\n    the padding area.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor4D" },
      { "name": "weight", "type": "Tosa_Tensor4D" },
      { "name": "bias", "type": "Tosa_Tensor1D" },
      { "name": "input_zp", "type": "Tosa_ScalarIntOrFloatTensor" },
      { "name": "weight_zp", "type": "Tosa_ScalarIntOrFloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor4D" }
    ],
    "attributes": [
      { "name": "pad", "type": "Tosa_IntArrayAttr4" },
      { "name": "stride", "type": "Tosa_IntArrayAttr2" },
      { "name": "dilation", "type": "Tosa_IntArrayAttr2" },
      { "name": "acc_type", "type": "TypeAttrOf" },
      { "name": "local_bound", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Layer"
  },
  {
    "name": "tosa.conv3d",
    "summary": "3D Convolution operator.",
    "description": "Performs a 3D convolution over the given input tensor. Implementations\n    may choose to skip calculation of multiplies in the padding area.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor5D" },
      { "name": "weight", "type": "Tosa_Tensor5D" },
      { "name": "bias", "type": "Tosa_Tensor1D" },
      { "name": "input_zp", "type": "Tosa_ScalarIntOrFloatTensor" },
      { "name": "weight_zp", "type": "Tosa_ScalarIntOrFloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor5D" }
    ],
    "attributes": [
      { "name": "pad", "type": "Tosa_IntArrayAttr6" },
      { "name": "stride", "type": "Tosa_IntArrayAttr3" },
      { "name": "dilation", "type": "Tosa_IntArrayAttr3" },
      { "name": "acc_type", "type": "TypeAttrOf" },
      { "name": "local_bound", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Layer"
  },
  {
    "name": "tosa.cos",
    "summary": "Elementwise cos operator.",
    "description": "Elementwise cosine operation for values given in radians.",
    "inputs": [
      { "name": "input1", "type": "Tosa_FloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_FloatTensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.custom",
    "summary": "Custom operator wrapper for Tosa",
    "description": "Hardware implementing TOSA may choose to add additional custom operators\n    that are not expressed in the existing TOSA operations. These operators are\n    not expected to be portable across TOSA implementations. The input and\n    output signatures must be expressed in the corresponding TOSA node.\n\n    `operator_name` is a string that tells the backend which custom operator is\n    being called.\n\n    `domain_name` is a string identifier which can help avoid name collisions on\n    the identifier field.\n\n    `implementation_attrs` is a string which is a backend and identifier specific\n    set of attributes to the custom operator.\n\n    `input_list` is the set of tensor inputs to the custom operator.\n\n    `output_list` is the list of tensors returned by the operator. The number of operators\n    is backend specific.\n\n    Example:\n\n    ```mlir\n    %out = tosa.custom %in {domain_name = \"tosa_mlir_test\", operator_name =\n           \"custom_test\", implementation_attrs = \"\"}: (tensor<10xi32>) ->\n           (tensor<10xi32>)\n    ```",
    "inputs": [
      { "name": "input_list", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output_list", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "operator_name", "type": "StrAttr" },
      { "name": "domain_name", "type": "StrAttr" },
      { "name": "implementation_attrs", "type": "StrAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.depthwise_conv2d",
    "summary": "Depthwise 2D Convolution operator.",
    "description": "Performs 2D convolutions separately over each channel of the given tensor\n    input, using the weight tensor. Implementations may choose to skip\n    calculation of multiplies in the padding area.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor4D" },
      { "name": "weight", "type": "Tosa_Tensor4D" },
      { "name": "bias", "type": "Tosa_Tensor1D" },
      { "name": "input_zp", "type": "Tosa_ScalarIntOrFloatTensor" },
      { "name": "weight_zp", "type": "Tosa_ScalarIntOrFloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor4D" }
    ],
    "attributes": [
      { "name": "pad", "type": "Tosa_IntArrayAttr4" },
      { "name": "stride", "type": "Tosa_IntArrayAttr2" },
      { "name": "dilation", "type": "Tosa_IntArrayAttr2" },
      { "name": "acc_type", "type": "TypeAttrOf" },
      { "name": "local_bound", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.equal",
    "summary": "Returns the truth value of (input1 == input2) element-wise.",
    "description": "Elementwise comparison operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_I1Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.erf",
    "summary": "Computes gauss error function of input.",
    "description": "Gauss error function: $ erf(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} dt $\n    For quantized integer data types, the TABLE operator should be used instead\n    with the following definition. The ERF table has 513 entries each of\n    16-bit precision and covering the input range -4.0 to +4.0 in steps of 1/64.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.exp",
    "summary": "Elementwise exp operator.",
    "description": "Elementwise e to the x operation",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.fft2d",
    "summary": "Performs FFT2D operation on the input.",
    "description": "Performs a batched complex 2D Fast Fourier Transform over the input. The\n    complex input values are constructed from the corresponding values in the\n    input_real and input_imag tensors. The resulting values in the output are\n    split into the output_real and output_imag tensors. No normalization is\n    applied on either the forward or inverse versions of the operation.\n\n    Example:\n\n    ```mlir\n     %output_real, %output_imag = tosa.fft2d %input_real, %input_imag : (tensor<8x9xf32>, tensor<8x9xf32>) -> (tensor<8x9xf32>, tensor<8x9xf32>)\n    ```",
    "inputs": [
      { "name": "input_real", "type": "Tosa_Tensor3D" },
      { "name": "input_imag", "type": "Tosa_Tensor3D" }
    ],
    "outputs": [
      { "name": "output_real", "type": "Tosa_Tensor3D" },
      { "name": "output_imag", "type": "Tosa_Tensor3D" }
    ],
    "attributes": [
      { "name": "inverse", "type": "BoolAttr" },
      { "name": "local_bound", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.floor",
    "summary": "Elementwise floor operator.",
    "description": "Elementwise floor operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.gather",
    "summary": "Gather operation.",
    "description": "Generate a tensor for which each element in the output is a subtensor of the\n    values tensor based on the indices. N is the number of batches, W the number\n    of indices in each batch, K the range of each index and C the number data\n    channels for each index.",
    "inputs": [
      { "name": "values", "type": "Tosa_Tensor3D" },
      { "name": "indices", "type": "Tosa_Int32Tensor2D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor3D" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Tensor"
  },
  {
    "name": "tosa.greater",
    "summary": "Returns the truth value of (input1 > input2) element-wise.",
    "description": "Elementwise greater than comparison operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_I1Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.greater_equal",
    "summary": "Returns the truth value of (input1 >= input2) element-wise.",
    "description": "Elementwise comparison operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_I1Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.identity",
    "summary": "Identity operator.",
    "description": "Returns a tensor with the same shape, type, and contents as the input.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.intdiv",
    "summary": "Integer divide operator.",
    "description": "Elementwise integer divide of input1 by input2. Axis of size 1 will be\n    broadcast as necessary. Rank of input tensors must match. The result of the\n    divide is truncated towards zero. Expected use is for operations on\n    non-scaled integers. Floating point divide should use RECIPROCAL and MUL.\n    Quantized integer divide should use TABLE (for 1/x) and MUL.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Int32Tensor" },
      { "name": "input2", "type": "Tosa_Int32Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Int32Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.log",
    "summary": "Elementwise log operator.",
    "description": "Elementwise natural logarithm operation",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.logical_and",
    "summary": "Returns the truth value of input1 AND input2 element-wise.",
    "description": "Elementwise logical AND of input1 and input2. Axis of size 1 will be\n    broadcast, as necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_I1Tensor" },
      { "name": "input2", "type": "Tosa_I1Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_I1Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.logical_left_shift",
    "summary": "Elementwise Logical Left Shift.",
    "description": "Elementwise logical left-shift of input1 by the amount specified in input2.\n    Axis of size 1 will be broadcast, as necessary.\n    Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.logical_not",
    "summary": "Returns the truth value of NOT input1 element-wise.",
    "description": "Elementwise logical NOT of input.",
    "inputs": [
      { "name": "input1", "type": "Tosa_I1Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_I1Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.logical_or",
    "summary": "Returns the truth value of x OR y element-wise.",
    "description": "Elementwise logical OR of input1 and input2. Axis of size 1 will be\n    broadcast as necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_I1Tensor" },
      { "name": "input2", "type": "Tosa_I1Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_I1Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.logical_right_shift",
    "summary": "Elementwise Logical Right Shift.",
    "description": "Elementwise logical right shift of input1 by the amount specified in input2.\n    Axis of size 1 will be broadcast, as necessary. Rank of input tensors must\n    match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.logical_xor",
    "summary": "Returns the truth value of input1 XOR input2 element-wise.",
    "description": "Elementwise logical XOR of input1 and input2. Axis of size 1 will be\n    broadcast as necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_I1Tensor" },
      { "name": "input2", "type": "Tosa_I1Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_I1Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.matmul",
    "summary": "Matrix multiplication operator.",
    "description": "Performs two dimensional matrix multiplications.",
    "inputs": [
      { "name": "a", "type": "Tosa_Tensor3D" },
      { "name": "b", "type": "Tosa_Tensor3D" },
      { "name": "a_zp", "type": "Tosa_ScalarIntOrFloatTensor" },
      { "name": "b_zp", "type": "Tosa_ScalarIntOrFloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor3D" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Layer"
  },
  {
    "name": "tosa.matmul_t_block_scaled",
    "summary": "Performs two dimensional matrix multiplications using block scaled tensors.",
    "description": "Performs two dimensional matrix multiplications using block scaled tensors. The block\n    dimension is always the the last dimension of the tensor, so the result is effectively\n    a matrix multiply of A by the transposed B matrix. If the N dimension of input B is of\n    size 1, the B matrix will be broadcast.",
    "inputs": [
      { "name": "a_data", "type": "Tosa_MXFPDataTensor3D" },
      { "name": "a_scale", "type": "Tosa_MXFPScaleTensor3D" },
      { "name": "b_data", "type": "Tosa_MXFPDataTensor3D" },
      { "name": "b_scale", "type": "Tosa_MXFPScaleTensor3D" }
    ],
    "outputs": [
      { "name": "output_data", "type": "Tosa_Tensor3D" }
    ],
    "attributes": [
      { "name": "block_size", "type": "Tosa_BlockSizeAttr" }
    ]
  },
  {
    "name": "tosa.max_pool2d",
    "summary": "Performs max pooling on the input.",
    "description": "This performs a max pooling over the given input tensor. A sliding window of\n    size given by <kernel size> is passed over the input tensor, with the\n    maximum value being placed in the\n    output tensor.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor4D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor4D" }
    ],
    "attributes": [
      { "name": "kernel", "type": "Tosa_IntArrayAttr2" },
      { "name": "stride", "type": "Tosa_IntArrayAttr2" },
      { "name": "pad", "type": "Tosa_IntArrayAttr4" },
      { "name": "nan_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tosa.maximum",
    "summary": "Elementwise Maximum.",
    "description": "Elementwise max of input1 and input2. Axis of size 1 will be broadcast, as\n    necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "nan_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tosa.minimum",
    "summary": "Elementwise Minimum.",
    "description": "Elementwise minimum of input1 and input2. Axis of size 1\n    will be broadcast, as necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "nan_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tosa.mul",
    "summary": "Multiplication operator.",
    "description": "Elementwise multiplication (Hadamard product) of input1 and input2.\n    Axis of size 1 will be broadcast, as necessary. Rank of input tensors must\n    match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" },
      { "name": "shift", "type": "Tosa_ScalarInt8Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.negate",
    "summary": "Elementwise negate operator.",
    "description": "Elementwise negation operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input1_zp", "type": "Tosa_ScalarIntOrFloatTensor" },
      { "name": "output_zp", "type": "Tosa_ScalarIntOrFloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.pad",
    "summary": "Pads a tensor with value specified.",
    "description": "Pads a tensor along the borders of each dimension with a supplied value.\n    Returns a new tensor with the padding included. The pad_const value includes\n    the zero point if the tensor uses a zero point.\n\n    Example:\n\n    ```mlir\n    %pad_const = \"tosa.const\"() {values = dense<3.14> : tensor<1xf32>} : () -> tensor<1xf32>\n    %padding = tosa.const_shape {values = dense<[1, 2, 3, 4]> : tensor<4xindex>} : () -> !tosa.shape<4>\n    tosa.pad %arg0, %padding, %pad_const: (tensor<1x2xf32>, !tosa.shape<4>, tensor<1xf32>)  -> (tensor<4x9xf32>)\n    ```\n\n    Example 2:\n\n    ```mlir\n    %pad_const = \"tosa.const\"() {values = dense<3.14> : tensor<1xf32>} : () -> tensor<1xf32>\n    %padding = tosa.const_shape {values = dense<[-1, 2, 3, 4]> : tensor<4xindex>} : () -> !tosa.shape<4>\n    tosa.pad %arg0, %padding, %pad_const : (tensor<1x2xf32>, !tosa.shape<4>, tensor<1xf32>)  -> (tensor<?x9xf32>)\n    ```",
    "inputs": [
      { "name": "input1", "type": "Tosa_TensorAtLeast1D" },
      { "name": "padding", "type": "Tosa_Shape" },
      { "name": "pad_const", "type": "Tosa_ScalarTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Transform"
  },
  {
    "name": "tosa.pow",
    "summary": "Computes the power of one value to another.",
    "description": "Elementwise input1 value raised to the power of input2.\n    Axis of size 1 will be broadcast, as necessary. Rank of input tensors must\n    match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.reciprocal",
    "summary": "Elementwise reciprocal operator.",
    "description": "Elementwise reciprocal operation. For integer operation, a TABLE should be\n    used with the appropriate ranges.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.reduce_all",
    "summary": "Reduce All operator.",
    "description": "Reduce a tensor along the given axis with a logical AND operation.",
    "inputs": [
      { "name": "input", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.reduce_any",
    "summary": "Reduce Any operator.",
    "description": "Reduce a tensor along the given axis with a logical OR operation.",
    "inputs": [
      { "name": "input", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.reduce_max",
    "summary": "Reduce Max operator.",
    "description": "Reduce a tensor along the given axis with a maximum operation.",
    "inputs": [
      { "name": "input", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" },
      { "name": "nan_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tosa.reduce_min",
    "summary": "Reduce Min operator.",
    "description": "Reduce a tensor along the given axis with a minimum operation.",
    "inputs": [
      { "name": "input", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" },
      { "name": "nan_mode", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tosa.reduce_product",
    "summary": "Reduce Product operator.",
    "description": "Reduce a tensor along the given axis by computing the product of the axis.",
    "inputs": [
      { "name": "input", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.reduce_sum",
    "summary": "Reduce Sum operator.",
    "description": "Reduce a tensor along the given axis by computing the sum of the axis.",
    "inputs": [
      { "name": "input", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.rescale",
    "summary": "Tosa rescale operator.",
    "description": "RESCALE is defined using an integer multiply, add, and shift.\n\n    Rescale supports two precisions of multiplier: 16-bit and 32-bit. The 32-bit multiplier\n    version supports two rounding modes to enable simpler lowering of existing frameworks\n    that use two stage rounding. All arithmetic is designed so that it does not overflow a\n    64-bit accumulator and that the result fits in 32 bits. In particular, a 48-bit value\n    cannot be scaled with the 32-bit multiplier because the accumulator would need to have\n    80 bits.\n\n    The shift and value range are limited to allow a variety of implementations. The limit\n    of 62 on shift allows the shift to be decomposed as two right shifts of 31.\n\n    Supported rescalings:\n    * This table is showing the supported conversions from the TOSA Specification.\n    * The MLIR dialect here can be used to represent other conversions.\n\n    | Mode                   | Input | Output | Unsigned input | Unsigned output |\n    |------------------------|-------|--------|----------------|-----------------|\n    | signed 16 to 16        | int16 | int16  |  false         |  false          |\n    | signed 16 to 32        | int16 | int32  |  false         |  false          |\n    | signed 16 to 8         | int16 | int8   |  false         |  false          |\n    | signed 32 to 16        | int32 | int16  |  false         |  false          |\n    | signed 32 to 32        | int32 | int32  |  false         |  false          |\n    | signed 32 to 8         | int32 | int8   |  false         |  false          |\n    | signed 8 to 16         | int8  | int16  |  false         |  false          |\n    | signed 8 to 32         | int8  | int32  |  false         |  false          |\n    | signed 8 to 8          | int8  | int8   |  false         |  false          |\n    | signed 48 to 16        | int48 | int16  |  false         |  false          |\n    | signed 48 to 32        | int48 | int32  |  false         |  false          |\n    | signed 48 to 8         | int48 | int8   |  false         |  false          |\n    | unsigned 8 to signed 8 | uint8 | int8   |  true          |  false          |\n    | signed 8 to unsigned 8 | int8  | uint8  |  false         |  true           |",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor" },
      { "name": "multiplier", "type": "Tosa_1DInt16Or32Tensor" },
      { "name": "shift", "type": "Tosa_1DInt8Tensor" },
      { "name": "input_zp", "type": "Tosa_ScalarIntOrFloatTensor" },
      { "name": "output_zp", "type": "Tosa_ScalarIntOrFloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "scale32", "type": "BoolAttr" },
      { "name": "rounding_mode", "type": "Tosa_RoundingModeAttr" },
      { "name": "per_channel", "type": "BoolAttr" },
      { "name": "input_unsigned", "type": "BoolAttr" },
      { "name": "output_unsigned", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tosa.reshape",
    "summary": "Reshape operator.",
    "description": "Returns a tensor with the same type/values as the input, with a new shape\n    specified by the shape argument. Reshape may operate on tensors of any rank.\n    No data conversion happens during a reshape operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "shape", "type": "Tosa_Shape" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Shape"
  },
  {
    "name": "tosa.resize",
    "summary": "Resize operation, supports various resize/upsample modes.",
    "description": "Resizes a tensor. Resize is only allowed in the H and W dimensions.\n\n    The height dimension is scaled by factor (scale_y_n/scale_y_d). The width\n    dimension is scaled by factor (scale_x_n/scale_x_d).\n\n    The NEAREST_NEIGHBOR mode returns the value of the input tensor closest to\n    the calculated sample position for both floating-point and integer data\n    formats.\n\n    Floating-point BILINEAR mode returns a bilinearly interpolated output value\n    based on the four closest input sample positions.\n\n    For integer BILINEAR interpolation mode, the output value must be scaled by\n    1/(scale_y_n * scale_x_n) in a following operation to complete the\n    interpolation (for example with a RESCALE operator).\n\n    The output dimensions can be derived from the input dimensions by inverting\n    the scale as described in the pseudocode. The [border_y, border_x] values\n    adjust the output size to allow fractional sampling beyond integer input\n    position (IH - 1,IW - 1).\n\n    The limit MAX_SCALE is applied to each scale ratio after reduction of the\n    ratio. Individual scale numerator and denominator values are allowed to be\n    larger than MAX_SCALE.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor4D" },
      { "name": "scale", "type": "Rank4TosaShape" },
      { "name": "offset", "type": "Rank2TosaShape" },
      { "name": "border", "type": "Rank2TosaShape" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor4D" }
    ],
    "attributes": [
      { "name": "mode", "type": "Tosa_ResizeModeAttr" }
    ]
  },
  {
    "name": "tosa.reverse",
    "summary": "Reverse operator.",
    "description": "Returns a tensor with the same type/values as the input, with the data\n    reversed along the given axis. No data conversion happens during a reverse\n    operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "axis", "type": "I32Attr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Transform"
  },
  {
    "name": "tosa.rfft2d",
    "summary": "Performs RFFT2D operation on the input.",
    "description": "Performs a batched 2D real-valued Fast Fourier Transform over the input where\n    the input tensor consists of real values producing complex valued output. The\n    complex output values will be split into the output_real and output_imag\n    tensor arguments. RFFT2D takes advantage of Hermitian symmetry to only\n    calculate the first half of the final output axis. Implementations may choose\n    to skip calculation of the imaginary values at (0,0), (0,W/2), (H/2,0), and\n    (H/2, W/2). If the calculation is skipped, the result at that location must be\n    zero.\n\n    Example:\n\n    ```mlir\n     %ouput_real, %output_imag = tosa.rfft2d %input_real : (tensor<8x16xf32>) -> (tensor<8x9xf32>, tensor<8x9xf32>)\n    ```",
    "inputs": [
      { "name": "input_real", "type": "Tosa_Tensor3D" }
    ],
    "outputs": [
      { "name": "output_real", "type": "Tosa_Tensor3D" },
      { "name": "output_imag", "type": "Tosa_Tensor3D" }
    ],
    "attributes": [
      { "name": "local_bound", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.rsqrt",
    "summary": "Elementwise 1/sqrt operator.",
    "description": "Elementwise reciprocal square root operation. For integer operation, a TABLE\n    should be used with the appropriate ranges.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.scatter",
    "summary": "Scatter operation.",
    "description": "The values_out tensor is set to the values_in tensor with data modified as\n    follows: data from the input tensor is inserted at the positions specified\n    by the indices tensor. N is the number of batches, W the number of indices\n    in each batch, K the range of each index and C the number data channels for\n    each index. It is not permitted to repeat the same output index within a\n    single SCATTER operation and so each output index occurs at most once. It\n    follows that K >= W. In use cases that require multiple updates to the same\n    output position, these must be decomposed into multiple SCATTER operations.",
    "inputs": [
      { "name": "values_in", "type": "Tosa_Tensor3D" },
      { "name": "indices", "type": "Tosa_Int32Tensor2D" },
      { "name": "input", "type": "Tosa_Tensor3D" }
    ],
    "outputs": [
      { "name": "values_out", "type": "Tosa_Tensor3D" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Tensor"
  },
  {
    "name": "tosa.select",
    "summary": "Elementwise select operator.",
    "description": "Elementwise select of the output based on a condition.",
    "inputs": [
      { "name": "input1", "type": "Tosa_I1Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" },
      { "name": "input3", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.sigmoid",
    "summary": "Computes elementwise sigmoid of input.",
    "description": "Applies the sigmoid logistic function to each element of the input tensor:\n    $ sigmoid(x) = \\frac{1}{1 + e^{-x}} $.\n\n    For quantized integer data types, the TABLE operator should be used instead.\n    Each implementation may choose an appropriate TABLE given the scale and zero\n    point of the input data. Eight or sixteen bit precision tables may be used\n    based on the input tensor to the sigmoid function.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Activation"
  },
  {
    "name": "tosa.sin",
    "summary": "Elementwise sin operator.",
    "description": "Elementwise sine operation for values given in radians.",
    "inputs": [
      { "name": "input1", "type": "Tosa_FloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_FloatTensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.slice",
    "summary": "Slice operator.",
    "description": "Extracts a slice of input1, beginning at the start coordinates,\n    and extending for size elements in each direction.\n    No data conversion happens during a slice operation.",
    "inputs": [
      { "name": "input1", "type": "Tosa_TensorAtLeast1D" },
      { "name": "start", "type": "Tosa_Shape" },
      { "name": "size", "type": "Tosa_Shape" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Tensor"
  },
  {
    "name": "tosa.sub",
    "summary": "Elementwise subtraction operator.",
    "description": "Elementwise subtraction of input1 and input2. Axis of size 1 will be\n    broadcast as necessary. Rank of input tensors must match.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "input2", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.table",
    "summary": "Table lookup operator.",
    "description": "Table lookup operation. For int8_t TABLE operation, perform a 256 entry\n    table lookup returning an int8_t value. For int16_t tables, the int16_t\n    input is treated as a fixed-point 9.7 value. The most significant 9 bits\n    are used to index into the table. The fractional 7 bits are used to\n    interpolate based on table[index] and table[index+1]. For int16_t inputs,\n    the TABLE operator returns a 16.7 interpolated value in an int32_t. This\n    value can then be input to the RESCALE operator to scale to the required\n    output data type. Note that int16_t table has 513 values to handle\n    table[index+1] when index=511.\n\n    An int16_t to int16_t table lookup can be constructed in TOSA as follows:\n    * Use the TABLE operator to produce a fixed point 16.7 interpolated result\n    * Use RESCALE (in_t=int32_t, out_t=int16_t, scale=1<<14, shift=21) to\n      scale the output to int16_t range (or alternate scale as required)",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" },
      { "name": "table", "type": "Tosa_Tensor1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.tanh",
    "summary": "Computes elementwise hyperbolic tangent of input.",
    "description": "Parameterized hyperbolic tangent: $ tanh(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}} $.\n\n    For quantized integer data types, the TABLE operator should be used instead.\n    Each implementation may choose an appropriate TABLE given the scale and zero\n    point of the input data. Eight or sixteen bit precision tables may be used\n    based on the input tensor to the tanh function.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Activation"
  },
  {
    "name": "tosa.tile",
    "summary": "Tile operator.",
    "description": "Replicates input1 multiples times along each dimension.",
    "inputs": [
      { "name": "input1", "type": "Tosa_TensorAtLeast1D" },
      { "name": "multiples", "type": "Tosa_Shape" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.transpose",
    "summary": "Transpose operator.",
    "description": "Permutes the dimensions of the input tensor input1 based on the perms\n    argument. Each value in the perms list must be a valid dimension of the\n    input tensor and may not be repeated.",
    "inputs": [
      { "name": "input1", "type": "Tosa_TensorAtLeast1D" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_TensorAtLeast1D" }
    ],
    "attributes": [
      { "name": "perms", "type": "DenseI32ArrayAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)",
    "category": "Transform"
  },
  {
    "name": "tosa.transpose_conv2d",
    "summary": "Transpose 2D Convolution operator.",
    "description": "Performs a 2D transposed convolution over the given tensor input, using the\n    weights tensor. Implementations may choose to skip calculation of multiplies\n    by zero at fractional input positions.",
    "inputs": [
      { "name": "input", "type": "Tosa_Tensor4D" },
      { "name": "weight", "type": "Tosa_Tensor4D" },
      { "name": "bias", "type": "Tosa_Tensor1D" },
      { "name": "input_zp", "type": "Tosa_ScalarIntOrFloatTensor" },
      { "name": "weight_zp", "type": "Tosa_ScalarIntOrFloatTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Tosa_Tensor4D" }
    ],
    "attributes": [
      { "name": "out_pad", "type": "Tosa_IntArrayAttr4" },
      { "name": "stride", "type": "Tosa_IntArrayAttr2" },
      { "name": "acc_type", "type": "TypeAttrOf" },
      { "name": "local_bound", "type": "DefaultValuedOptionalAttr" }
    ],
    "assemblyFormat": "operands attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "tosa.variable",
    "summary": "Defines a variable",
    "description": "Defines a new TOSA variable. This is a persistent mutable value across multiple\n    TOSA graph invocations. Modifications are expressed using read/write semantics.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "var_shape", "type": "IndexElementsAttr" },
      { "name": "type", "type": "TypeAttr" },
      { "name": "initial_value", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$sym_name\n    attr-dict\n    custom<VariableOpTypeOrInitialValue>($var_shape, $type, $initial_value)"
  },
  {
    "name": "tosa.variable_read",
    "summary": "read_buffer operator",
    "description": "Reads the value from a pseudo-buffer resource holding a persistent mutable tensor.",
    "outputs": [
      { "name": "output1", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "$name attr-dict `:` type($output1)"
  },
  {
    "name": "tosa.variable_write",
    "summary": "write_buffer operator",
    "description": "Assigns a value to the pseudo-buffer resource holding a persistent mutable tensor.",
    "inputs": [
      { "name": "input1", "type": "Tosa_Tensor" }
    ],
    "attributes": [
      { "name": "name", "type": "SymbolNameAttr" }
    ],
    "assemblyFormat": "$name attr-dict `,` $input1 `:` type($input1)"
  },
  {
    "name": "tosa.while_loop",
    "summary": "output = input; While (Cond(output)) {output = Body(output)}",
    "description": "Generates and evaluates a Boolean condition and either executes a loop body\n    or exits the loop. This action is performed repeatedly after\n    updating and re-evaluating the Boolean condition every iteration. This\n    implements the semantic foreach or while iterative loop structure.",
    "inputs": [
      { "name": "input_list", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output_list", "type": "Variadic" }
    ]
  },
  {
    "name": "tosa.yield",
    "summary": "yield operator",
    "description": "return operation within the conditional and body of\n    structured control flow. Operation takes variadic operands\n    but produces no results of its own.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "assemblyFormat": "$inputs attr-dict `:` type($inputs)"
  },
  {
    "name": "toy.add",
    "summary": "element-wise addition operation",
    "description": "The \"add\" operation performs element-wise addition between two tensors.\n    The shapes of the tensor operands are expected to match.",
    "inputs": [
      { "name": "lhs", "type": "F64Tensor" },
      { "name": "rhs", "type": "F64Tensor" }
    ]
  },
  {
    "name": "toy.cast",
    "summary": "shape cast operation",
    "description": "The \"cast\" operation converts a tensor from one type to an equivalent type\n    without changing any data elements. The source and destination types must\n    both be tensor types with the same element type. If both are ranked, then\n    shape is required to match. The operation is invalid if converting to a\n    mismatching constant dimension.",
    "inputs": [
      { "name": "input", "type": "F64Tensor" }
    ],
    "outputs": [
      { "name": "output", "type": "F64Tensor" }
    ],
    "assemblyFormat": "$input attr-dict `:` type($input) `to` type($output)"
  },
  {
    "name": "toy.constant",
    "summary": "constant",
    "description": "Constant operation turns a literal into an SSA value. The data is attached\n    to the operation as an attribute. For example:\n\n    ```mlir\n      %0 = toy.constant dense<[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]>\n                        : tensor<2x3xf64>\n    ```",
    "attributes": [
      { "name": "value", "type": "F64ElementsAttr" }
    ]
  },
  {
    "name": "toy.func",
    "summary": "user defined function operation",
    "description": "The \"toy.func\" operation represents a user defined function. These are\n    callable SSA-region operations that contain toy computations.\n\n    Example:\n\n    ```mlir\n    toy.func @main() {\n      %0 = toy.constant dense<5.500000e+00> : tensor<f64>\n      %1 = toy.reshape(%0 : tensor<f64>) to tensor<2x2xf64>\n      toy.print %1 : tensor<2x2xf64>\n      toy.return\n    }\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "toy.generic_call",
    "summary": "generic call operation",
    "description": "Generic calls represent calls to a user defined function that needs to\n    be specialized for the shape of its arguments. The callee name is attached\n    as a symbol reference via an attribute. The arguments list must match the\n    arguments expected by the callee. For example:\n\n    ```mlir\n     %4 = toy.generic_call @my_func(%1, %3)\n           : (tensor<2x3xf64>, tensor<2x3xf64>) -> tensor<*xf64>\n    ```\n\n    This is only valid if a function named \"my_func\" exists and takes two\n    arguments.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "callee", "type": "FlatSymbolRefAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$callee `(` $inputs `)` attr-dict `:` functional-type($inputs, results)"
  },
  {
    "name": "toy.mul",
    "summary": "element-wise multiplication operation",
    "description": "The \"mul\" operation performs element-wise multiplication between two\n    tensors. The shapes of the tensor operands are expected to match.",
    "inputs": [
      { "name": "lhs", "type": "F64Tensor" },
      { "name": "rhs", "type": "F64Tensor" }
    ]
  },
  {
    "name": "toy.print",
    "summary": "print operation",
    "description": "The \"print\" builtin operation prints a given input tensor, and produces\n    no results.",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$input attr-dict `:` type($input)"
  },
  {
    "name": "toy.reshape",
    "summary": "tensor reshape operation",
    "description": "Reshape operation is transforming its input tensor into a new tensor with\n    the same number of elements but different shapes. For example:\n\n    ```mlir\n       %0 = toy.reshape (%arg1 : tensor<10xf64>) to tensor<5x2xf64>\n    ```",
    "inputs": [
      { "name": "input", "type": "F64Tensor" }
    ],
    "assemblyFormat": "`(` $input `:` type($input) `)` attr-dict `to` type(results)"
  },
  {
    "name": "toy.return",
    "summary": "return operation",
    "description": "The \"return\" operation represents a return operation within a function.\n    The operation takes an optional operand and produces no results.\n    The operand type must match the signature of the function that contains\n    the operation. For example:\n\n    ```mlir\n      toy.func @foo() -> tensor<2xf64> {\n        ...\n        toy.return %0 : tensor<2xf64>\n      }\n    ```",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "assemblyFormat": "($input^ `:` type($input))? attr-dict"
  },
  {
    "name": "toy.struct_access",
    "summary": "struct access",
    "description": "Access the Nth element of a value returning a struct type.",
    "inputs": [
      { "name": "input", "type": "Toy_StructType" }
    ],
    "outputs": [
      { "name": "output", "type": "Toy_Type" }
    ],
    "attributes": [
      { "name": "index", "type": "I64Attr" }
    ],
    "assemblyFormat": "$input `[` $index `]` attr-dict `:` type($input) `->` type($output)"
  },
  {
    "name": "toy.struct_constant",
    "summary": "struct constant",
    "description": "Constant operation turns a literal struct value into an SSA value. The data\n    is attached to the operation as an attribute. The struct constant is encoded\n    as an array of other constant values. For example:\n\n    ```mlir\n      %0 = toy.struct_constant [\n        dense<[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]> : tensor<2x3xf64>\n      ] : !toy.struct<tensor<*xf64>>\n    ```",
    "outputs": [
      { "name": "output", "type": "Toy_StructType" }
    ],
    "attributes": [
      { "name": "value", "type": "ArrayAttr" }
    ],
    "assemblyFormat": "$value attr-dict `:` type($output)"
  },
  {
    "name": "toy.transpose",
    "summary": "transpose operation",
    "inputs": [
      { "name": "input", "type": "F64Tensor" }
    ],
    "assemblyFormat": "`(` $input `:` type($input) `)` attr-dict `to` type(results)"
  },
  {
    "name": "tpu.A16MatMul",
    "summary": "w8a16 / w4a16 matmul operator",
    "description": "1.Op Introduction\r\n    The special matrix multiplication designed for LLM Linear Layer.\r\n    Weight is saved in int8 with f16 per-channel quant scale.\r\n\r\n    2.Math formula\r\n    ```math\r\n            y_f16 = x_f16 x (quantized_w.to(f16) * scale_f16)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    weight(w.): weight tensor.;\r\n    scale(w.): scalar.;\r\n    zp(w.): zero points for weight quant.;\r\n    bias(w.): an optional tensor can be added to the result of the matrix multiplication. ;\r\n\r\n    4.attributes\r\n    weight_bits: the bit-width used to represent the weight values.;\r\n    sign: if output is signed.;\r\n    w_transpose: whether the weight tensor should be transposed;\r\n    q_group_size: the group size for per-group quantization.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "weight", "type": "AnyRankedTensor" },
      { "name": "scale", "type": "AnyTensorOrNone" },
      { "name": "zp", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "weight_bits", "type": "I64Attr" },
      { "name": "sign", "type": "DefaultValuedAttr" },
      { "name": "w_transpose", "type": "DefaultValuedAttr" },
      { "name": "q_group_size", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Active",
    "summary": "Active operator",
    "description": "1.Op Introduction\r\n    The operator for activation function.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Active(input),for example:Relu, Silu...\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    mode: the specific activation mode or function to be applied by the operator.;\r\n    coeffs: an array of float64 values as coefficients for activation functions.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "Tpu_ActiveModeAttr" },
      { "name": "coeffs", "type": "OptionalAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Add",
    "summary": "add operator",
    "description": "1.Op Introduction\r\n    Elementwise addition of input1 and input2. Axis of size 1 will be broadcast,\r\n    as necessary.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = ReLU((input1 + input2; dim))\r\n    ```\r\n    Axis of size 1 will be broadcast if necessary.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    coeff: It is an array and allows for scaling the output of the addition operation.;\r\n    // early stride param\r\n    do_early_stride: whether to apply early stride optimization during the addition operation.;\r\n    early_stride_h: the height of the early stride.;\r\n    early_stride_w: the width of the early stride.;\r\n    // quant param\r\n    multipliers: an array of multipliers used for quantization, It allows for scaling the input values before the addition operation.;\r\n    rshifts: an array of right shift values corresponding to each input tensor.;\r\n    f8_scales: scaling factors for FP8 (8-bit floating point) quantization.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "coeff", "type": "OptionalAttr" },
      { "name": "do_early_stride", "type": "OptionalAttr" },
      { "name": "early_stride_h", "type": "OptionalAttr" },
      { "name": "early_stride_w", "type": "OptionalAttr" },
      { "name": "multipliers", "type": "OptionalAttr" },
      { "name": "rshifts", "type": "OptionalAttr" },
      { "name": "f8_scales", "type": "OptionalAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.AddConst",
    "summary": "add const operator",
    "description": "1.Op Introduction\r\n    Elementwise add of input1 and input2. Input2 is constant.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = input + const_val\r\n    ```\r\n    Where input1, input2, ..., inputN are the input tensors.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    const_val: the constant value to be added to each element of the input tensor(positive, negative, or zero).;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    // quant param\r\n    multiplier: Floating-point multiplication operations are usually converted to fixed-point multiplication operations.;\r\n    rshift: the number of bits to right-shift the quantized values.;\r\n    f8_scale: the scaling factor for FP8 (8-bit floating point) quantization.;\r\n    ginfo: contains layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "const_val", "type": "F64Attr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "multiplier", "type": "DefaultValuedAttr" },
      { "name": "rshift", "type": "DefaultValuedAttr" },
      { "name": "f8_scale", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Arg",
    "summary": "Arg operator",
    "description": "1.Op Introduction\r\n    Computes the indices of the min/max/ of the input tensor's element along the provided axis.\r\n\r\n    2.Math formula\r\n    ```math\r\n        maximum operation:\r\n            output_max[i_1, i_2, i_3,..., i_k] = arg max{j}(input[i_1, i_2,..., i_k, j])\r\n        minimum operation:\r\n            output_min[i_1, i_2, i_3,..., i_k] = arg min{j}(input[i_1, i_2,..., i_k, j])\r\n    ```\r\n    where, ( j ) represents the index along the specified axis.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    axis: the dimension of the input tensor.;\r\n    keepdims: whether to retain the dimensions of the input tensor in the output.\r\n               If true, will have the same number of dimensions as the input tensor.;\r\n    mode: the type of binary operation to be performed, addition, subtraction, or other types of binary operations.;\r\n    select_last_index: select the last index of the minimum or maximum value when multiple  along the specified axis.;\r\n    use_int_input: choose if use i32/i16 as input, due to BF16 chip arch limit.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "indices", "type": "AnyRankedTensor" },
      { "name": "values", "type": "AnyTensorOrNone" }
    ],
    "attributes": [
      { "name": "axis", "type": "I64Attr" },
      { "name": "keepdims", "type": "BoolAttr" },
      { "name": "mode", "type": "ArgModeAttr" },
      { "name": "select_last_index", "type": "DefaultValuedAttr" },
      { "name": "use_int_input", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Attention",
    "summary": "Attention operator",
    "description": "1.Op Introduction\r\n    Performs a multi head attention block. https://en.wikipedia.org/wiki/Attention_(machine_learning)\r\n    This block has Q_w, K_w,V_w, O_w and mask\r\n\r\n    2.Math formula\r\n    ```math\r\n        Attention(Q, K, V) = softmax(((Q x K^T) / \\sqrt{d_k}) + musk) x V;\r\n        head_i = Attention(Q x queries_weight, K x keys_weight, V x values_weight);\r\n        MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h) x out_weight + out_bias;\r\n        output = MultiHead(input x queries_weight + queries_bias, input x keys_weight + keys_bias, input x values_weight + values_bias).\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    keys(act.): The keys are derived from the input data and help the model determine which parts of the input are relevant for each query.;\r\n    values(act.): The values are the actual information that will be aggregated based on the attention scores computed from the queries and keys.;\r\n    queries_weight(w.): Queries are the features that the model uses to ask questions about the input data.;\r\n    queries_bias(w.): added to the query representations after the weight transformation.;\r\n    keys_weight(w.): This weight tensor transforms the input into key representations.;\r\n    keys_bias(w.): added to the key representations after the weight transformation, providing further adjustment.;\r\n    values_weight(w.): used to transform the input into value representations.;\r\n    values_bias(w.): added to the value representations after the weight transformation.;\r\n    out_weight(w.): used to transform the concatenated output of the attention heads into the final output representation.;\r\n    out_bias(w.): added to the output representation after the final weight transformation.;\r\n    musk(w.):  apply masking during the attention computation, Masks can prevent the model from attending to certain positions in the input.;\r\n    table(w.): additional computations or transformations during the softmax operation.;\r\n\r\n    4.attribute\r\n    quant_param: used to reduce the precision of the numbers used in computations.;\r\n    scale: a scaling factor applied to the attention scores before they are passed through the softmax function.;\r\n    head: the number of attention heads to use in the multi-head attention mechanism.;\r\n    dim: the size of the input features or the size of the query, key, and value vectors.;;\r\n    has_bias: whether the attention mechanism includes bias terms in its computations.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "keys", "type": "AnyTensorOrNone" },
      { "name": "values", "type": "AnyTensorOrNone" },
      { "name": "queries_weight", "type": "AnyTensor" },
      { "name": "queries_bias", "type": "AnyTensorOrNone" },
      { "name": "keys_weight", "type": "AnyTensorOrNone" },
      { "name": "keys_bias", "type": "AnyTensorOrNone" },
      { "name": "values_weight", "type": "AnyTensorOrNone" },
      { "name": "values_bias", "type": "AnyTensorOrNone" },
      { "name": "out_weight", "type": "AnyTensor" },
      { "name": "out_bias", "type": "AnyTensorOrNone" },
      { "name": "mask", "type": "AnyTensorOrNone" },
      { "name": "table", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "quant_param", "type": "DefaultValuedAttr" },
      { "name": "scale", "type": "F64Attr" },
      { "name": "head", "type": "I64Attr" },
      { "name": "dim", "type": "I64Attr" },
      { "name": "has_bias", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.AutoIncrease",
    "summary": "Auto increase",
    "description": "1.Op Introduction\r\n    increase by 1 in-place\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = AutoIncrease(input, const_val)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    const_val: specifies the constant value to be added to each element of the input tensor(positive, negative, or zero).;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "const_val", "type": "F64Attr" }
    ]
  },
  {
    "name": "tpu.Batch2Space",
    "summary": "Batch2Space operator",
    "description": "1.Op Introduction\r\n    Refer to `https://www.tensorflow.org/api_docs/python/tf/batch_to_space`\r\n\r\n    2.Math formula\r\n    ```math\r\n        h_croping = h * block_h - crop_top - crop_bottom,\r\n        w_croping = w * block_w - crop_left - crop_right,\r\n        [n, c, h, w] => [n / (block_h * block_w), c, h * block_h, w * block_w]\r\n        => [n / (block_h * block_w), c, h_croping, w_croping];\r\n        The format of input or output is NCHW.\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;\r\n\r\n    4.attributes\r\n    block_h: The height of the blocks used to rearrange the depth into spatial dimensions.;\r\n    block_w: The width of the blocks used to rearrange the depth into spatial dimensions.;\r\n    crops: It contains four ints with top, left, bottom, right.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "block_h", "type": "I64Attr" },
      { "name": "block_w", "type": "I64Attr" },
      { "name": "crops", "type": "I64ArrayAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.BatchNormBwd",
    "summary": "BatchNormalization operation",
    "description": "1.Op Introduction\r\n    Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs\r\n    with additional channel dimension) as described in the paper\r\n    Batch Normalization: Accelerating Deep Network Training by Reducing\r\n    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = \\frac{input - \\mathrm{E}[input]}{ \\variance + \\epsilon} * \\gamma + \\beta\r\n    ```\r\n\r\n    3.activation and weight\r\n    grad_out(act.): the gradients of the output of the Batch Normalization layer with respect to the loss. ;\r\n    input(act.): input tensor;\r\n    weight_opt(w.): the optimized weight (or scale) parameter.;\r\n    saved_mean(w.): the mean of the input tensor calculated during the forward pass.;\r\n    saved_invstd(w.): the saved inverse standard deviation (or the reciprocal of the standard deviation) of the input tensor,\r\n                      also computed during the forward pass.;\r\n    buffer(w.): serve as a temporary storage or workspace that may be used during the computation of gradients.;\r\n    The mean and standard-deviation are calculated per-dimension over\r\n    the mini-batches and $$\\gamma$$ and $$\\beta$$ are learnable parameter vectors\r\n    of size C (where C is the input channel size).\r\n\r\n    4.attribute\r\n    epsilon;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "grad_out", "type": "AnyTensor" },
      { "name": "input", "type": "AnyTensor" },
      { "name": "weight_opt", "type": "AnyTensorOrNone" },
      { "name": "saved_mean", "type": "AnyTensorOrNone" },
      { "name": "saved_invstd", "type": "AnyTensorOrNone" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "grad_in", "type": "AnyTensor" },
      { "name": "weight_grad", "type": "AnyTensorOrNone" },
      { "name": "bias_grad", "type": "AnyTensorOrNone" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.BatchNormTrain",
    "summary": "BatchNormalization operation",
    "description": "1.Op Introduction\r\n    Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs\r\n    with additional channel dimension) as described in the paper\r\n    Batch Normalization: Accelerating Deep Network Training by Reducing\r\n    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = \\frac{input - \\mathrm{E}[input]}{ \\variance + \\epsilon} * \\gamma + \\beta\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    mean(w.): mean of input tensor in dim C;\r\n    variance(w.): the spread of the input tensor values along the channel dimension for each mini-batch.;\r\n    gamma(w.): scalar;\r\n    beta(w.): scalar;\r\n    The mean and standard-deviation are calculated per-dimension over\r\n    the mini-batches and $$\\gamma$$ and $$\\beta$$ are learnable parameter vectors\r\n    of size C (where C is the input channel size).\r\n\r\n    4.attribute\r\n    epsilon;\r\n    momentum: Momentum is a hyperparameter that controls the moving average of the mean and variance of the input tensor.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "mean", "type": "AnyTensor" },
      { "name": "var", "type": "AnyTensor" },
      { "name": "gamma", "type": "AnyTensorOrNone" },
      { "name": "beta", "type": "AnyTensorOrNone" },
      { "name": "running_status_buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" },
      { "name": "mean_out", "type": "AnyTensor" },
      { "name": "saved_invstd", "type": "AnyTensor" },
      { "name": "running_mean", "type": "AnyTensor" },
      { "name": "running_var", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "momentum", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.BinaryConstShift",
    "summary": "Binary Const with shift operator",
    "description": "1.Op Introduction\r\n    The BinaryConstShift operator is a specialized tensor operation that combines binary arithmetic with constant scaling and shifting.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = saturation(input +/-/* scale >> -shift)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    scale: a scaling factor multiplies the input tensor.;\r\n    shift: a shift value applied to the quantized data before scaling.;\r\n    is_reverse: whether the subtraction operation is performed in reverse order.;\r\n    saturation: whether the output should be saturated.\r\n                When set true, the output will be clamped to a predefined range to prevent overflow or underflow during the operation.;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "scale", "type": "SI32Attr" },
      { "name": "mode", "type": "BinaryShiftAttr" },
      { "name": "shift", "type": "SI32Attr" },
      { "name": "is_reverse", "type": "DefaultValuedAttr" },
      { "name": "saturation", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.BinaryShift",
    "summary": "Binary with shift operator",
    "description": "1.Op Introduction\r\n    The BinaryShift operator is designed to perform binary operations on two input tensors with an additional shift operation.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = saturation(input1 +/-/* input2 >> -shift)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input1(act.): input tensor;\r\n    input2(act.): input tensor;\r\n\r\n    4.attribute\r\n    shift: a shift value applied to the quantized data before scaling.;\r\n    mode: the type of binary operation to be performed, addition, subtraction, or other types of binary operations.;\r\n    is_reverse: whether the subtraction operation is performed in reverse order.;\r\n    saturation: whether the output should be saturated.\r\n                When set to true, the output will be clamped to a predefined range to prevent overflow or underflow during the operation.;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input1", "type": "AnyRankedTensor" },
      { "name": "input2", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "BinaryShiftAttr" },
      { "name": "shift", "type": "SI32Attr" },
      { "name": "is_reverse", "type": "DefaultValuedAttr" },
      { "name": "saturation", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Buffer",
    "summary": "buffer operator",
    "description": "1.Op Introduction\r\n    A global buffer for operation, and free after op\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = \\text{buffer}(\\text{data})\r\n    ```\r\n\r\n    3.activation and weight\r\n    none\r\n\r\n    4.attribute\r\n    buffer_type: the type of buffer to be used for the operation.\r\n                 include Global memory buffer(GMEM)y, Local memory buffer(LMEM), Tensor memory buffer(TMEM);",
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "buffer_type", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tpu.Cast",
    "summary": "Cast operation",
    "description": "1.Op Introduction\r\n    The Tpu_CastOp is a tensor operation that performs type casting on the input tensor.\r\n\r\n    2.Math formula\r\n    output = Cast(input);\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    extra_input: whether additional input is required for the casting operation.;\r\n    ginfo: contains layer grouping information.;\r\n    with_scale: whether the casting operation should include a scaling factor.;\r\n    round_mode: the rounding mode to be used during the casting operation.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "extra_input", "type": "OptionalAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "with_scale", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.CastAdd",
    "summary": "add operator",
    "description": "Cast + Add; One of original Add is casted. Elementwise addition of input1 and input2. Axis of size 1 will be broadcast,\r\n    as necessary.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Clip",
    "summary": "Clip operator",
    "description": "1.Op Introduction\r\n    The operator limits the given input to a certain range.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = min      if input[i] < min;\r\n                        input[i] if input[i] >= min && input[i] <= max;\r\n                        max      if input[i] > max;\r\n\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    min: the minimum value that the elements of the input tensor can take.;\r\n    max: the maximum value that the elements of the input tensor can take.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "min", "type": "F64Attr" },
      { "name": "max", "type": "F64Attr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Compare",
    "summary": "Compare operator",
    "description": "1.Op Introduction\r\n    Returns the tensor resulted from performing the compare\r\n    operation elementwise on the input tensors A and B.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = 1 if lhs[i] mode rhs[i] is true\r\n                        0 otherwise\r\n    ```\r\n\r\n    3.activation and weight\r\n    lhs(act.): the first input tensor used as the left operand in the element-wise comparison.;\r\n    rhs(act.): the second input tensor used as the right operand in the element-wise comparison.;\r\n\r\n    4.attributes\r\n    mode: the type of comparison to be performed between the two input tensors.\r\n          mdoe include Equal, Not Equal, Less Than, Less Than or Equal, Greater Than and Greater Than or Equal;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "lhs", "type": "AnyRankedTensor" },
      { "name": "rhs", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "CompareModeAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.CompareConst",
    "summary": "CompareConst operator",
    "description": "1.Op Introduction\r\n    Returns the tensor resulted from performing the compare\r\n    operation elementwise on the input tensors A and Const.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = 1 if input[i] mode const_val is true\r\n                        0 otherwise\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    mode: the type of comparison to be performed between the two input tensors.\r\n          mdoe include Equal, Not Equal, Less Than, Less Than or Equal, Greater Than and Greater Than or Equal;\r\n    const_val: specifies the constant value to be added to each element of the input tensor(positive, negative, or zero).;\r\n    inversed: whether the mask should be inverted.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "CompareModeAttr" },
      { "name": "const_val", "type": "F64Attr" },
      { "name": "inversed", "type": "BoolAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Concat",
    "summary": "Concatate operation",
    "description": "1.Op Introduction\r\n    Concatenates the given sequence of seq tensors in the given dimension.\r\n    All tensors must either have the same shape (except in the concatenating dimension) or be empty.\r\n\r\n    2.Math formula\r\n    output = Concat(input1, input2, axis)\r\n           = input1[axis] + input2[axis];\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    axis: the dimension along which the input tensors will be concatenated.;\r\n    only_merge: whether the operation should only perform a merge of tensors without additional processing.;\r\n    // param for cv18xx\r\n    multipliers: applied during the concatenation process to adjust the values of the input tensors.;\r\n    rshifts: an array of right shift values corresponding to each input tensor.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    // for group\r\n    ginfo: contains layer group information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "SI32Attr" },
      { "name": "only_merge", "type": "DefaultValuedAttr" },
      { "name": "multipliers", "type": "OptionalAttr" },
      { "name": "rshifts", "type": "OptionalAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.ConstantFill",
    "summary": "constant fill operator",
    "description": "1.Op Introduction\r\n    fill the constant value\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = value * ones(shape(input))\r\n    ```\r\n    where, ones(shape(input)) generates a tensor of the same shape as the input tensor, filled with ones.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    value: the constant value that will fill the output tensor.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "value", "type": "F64Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Conv2D",
    "summary": "convolution 2d operator",
    "description": "1.Op Introduction\r\n    The Tpu_Conv2DOp operation implements a 2D convolution, which is a fundamental operation in many neural networks,particularly in convolutional neural networks (CNNs).\r\n    This operation takes an input tensor (often representing an image or feature map) and applies a set of learnable filters (kernels) to produce an output tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output(N, C_{out}, H, W) = \\sum_{C_{in}} input(N, C_{in}, H + sH * kH, W + sW * kW) * filter(C_{in}, C_{out}, kH, kW) + bias(C_{out})\r\n    ```\r\n    where, kH and kW are the height and width of the filter (kernel), sH and sW are the vertical and horizontal strides.\r\n            N is a batch size, C denotes a number of channels, H is a height of input, and W is width.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    filter(w.): the learnable weights of the convolution 2d operation.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attribute\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    stride: the stride for the cross-correlation, a single number or a tuple.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    groups: (optional)Number of blocked connections from input channels to output channels. Default: 1.;\r\n    dilation: controls the spacing between the kernel points;\r\n    inserts: additional parameters that may be used for specific optimizations or configurations.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    //new param\r\n    with_bias: whether to include a bias term in the convolution operation.;\r\n    weight_is_coeff: whether the weights (filters) are coefficients.;\r\n    coeff_merged: whether the coefficients have been merged.;\r\n    use_3ic_optimize: whether to use 3-input channel optimization.;\r\n      This Options call `use_3ic_optimize` is useful for speed up convolution computation:\r\n      use_3ic_optimize & 0x3 == 1  means merge kh to ic\r\n      use_3ic_optimize & 0x3 == 2  means merge kw to ic\r\n      use_3ic_optimize & 0x3 == 3  means merge kh and kw to ic\r\n      use_3ic_optimize & 0x10 != 0 means using tiu to do channel broadcast instead of gdma\r\n      use_3ic_optimize & 0x20 != 0 means input bcast addr use buffer (e.g. current op is used by more than one ops)\r\n      use_3ic_optimize & 0x20 != 0 means using tiu to do channel broadcast instead of gdma and input bcast addr use buffer\r\n                                         (e.g. current op is used by more than one ops)\r\n    kernel_zp: the zero-point for the kernel.It is used in quantized models to adjust the range of the weights.;\r\n    use_winograd: This attribute indicates whether to use the Winograd algorithm for convolution.;\r\n    multiplier: This parameter is used for scaling the output values.;\r\n    rshift: the number of bits to right-shift the quantized values.;\r\n    quant_mode: It determines how the output tensor should be quantized.;\r\n    round_mode: This parameter specifies the rounding mode to be used during quantization.;\r\n    ginfo: This attribute contains information about layer grouping, which can be useful for organizing layers in a neural network.;\r\n    // fuse leakyRelu\r\n    do_leaky_relu: whether to apply the Leaky ReLU activation function after the convolution operation.;\r\n    neg_slope: sets the slope for the negative part of the Leaky ReLU function, determining how much the output can be negative.;\r\n    multiplier_pos: This parameter specifies the multiplier for the positive part of the output, used in the context of quantization.;\r\n    multiplier_neg: This parameter specifies the multiplier for the negative part of the output, used in the context of quantization.;\r\n    rshift_pos: This attribute defines the right shift for the positive output values during quantization.;\r\n    rshift_neg: This attribute defines the right shift for the negative output values during quantization.;\r\n    out_f8_scales: This parameter contains the scaling factors for the output in FP8 (8-bit floating point) format.;\r\n    //nnvlc\r\n    support_compress: whether the operation supports compression.;\r\n    compress_info: contains information about the compression method used.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "filter", "type": "AnyRankedTensor" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "group", "type": "DefaultValuedAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "inserts", "type": "OptionalAttr" },
      { "name": "do_kernel_rotate", "type": "OptionalAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "with_bias", "type": "BoolAttr" },
      { "name": "weight_is_coeff", "type": "DefaultValuedAttr" },
      { "name": "coeff_merged", "type": "DefaultValuedAttr" },
      { "name": "use_3ic_optimize", "type": "DefaultValuedAttr" },
      { "name": "kernel_zp", "type": "DefaultValuedAttr" },
      { "name": "use_winograd", "type": "OptionalAttr" },
      { "name": "multiplier", "type": "OptionalAttr" },
      { "name": "rshift", "type": "OptionalAttr" },
      { "name": "weight_bits", "type": "OptionalAttr" },
      { "name": "quant_mode", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "do_leaky_relu", "type": "OptionalAttr" },
      { "name": "neg_slope", "type": "OptionalAttr" },
      { "name": "multiplier_pos", "type": "OptionalAttr" },
      { "name": "multiplier_neg", "type": "OptionalAttr" },
      { "name": "rshift_pos", "type": "OptionalAttr" },
      { "name": "rshift_neg", "type": "OptionalAttr" },
      { "name": "out_f8_scales", "type": "OptionalAttr" },
      { "name": "support_compress", "type": "DefaultValuedAttr" },
      { "name": "compress_info", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Conv3D",
    "summary": "convolution 2d operator",
    "description": "1.Op Introduction\r\n    The Tpu_Conv2DOp operation implements a 2D convolution, which is a fundamental operation in many neural networks,particularly in convolutional neural networks (CNNs).\r\n    This operation takes an input tensor (often representing an image or feature map) and applies a set of learnable filters (kernels) to produce an output tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output(N, C_{out}, D, H, H) = \\sum_{C_{in}} input(N, C_{in}, D + sD * kD, H + sH * kH, W + sW * kW) * filter(C_{in}, C_{out}, kD, kH, kW) + bias(C_{out})\r\n    ```\r\n    where, kD, kH and kW are the depth, height and width of the filter (kernel), sH and sW are the vertical and horizontal strides.\r\n            N is a batch size, C denotes a number of channels, H is a height of input, and W is width.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    filter(w.): the learnable weights of the convolution 2d operation.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attribute\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    stride: the stride for the cross-correlation, a single number or a tuple.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    groups: (optional)Number of blocked connections from input channels to output channels. Default: 1.;\r\n    dilation: controls the spacing between the kernel points;\r\n    inserts: additional parameters that may be used for specific optimizations or configurations.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    //new param\r\n    with_bias: whether to include a bias term in the convolution operation.;\r\n    kernel_zp: the zero-point for the kernel.It is used in quantized models to adjust the range of the weights.;\r\n    multiplier: This parameter is used for scaling the output values.;\r\n    rshift: the number of bits to right-shift the quantized values.;\r\n    quant_mode: It determines how the output tensor should be quantized.;\r\n    round_mode: This parameter specifies the rounding mode to be used during quantization.;\r\n    ginfo: This attribute contains information about layer grouping, which can be useful for organizing layers in a neural network.;\r\n    out_f8_scale: This parameter contains the scaling factors for the output in FP8 (8-bit floating point) format.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "filter", "type": "AnyRankedTensor" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "group", "type": "DefaultValuedAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "inserts", "type": "OptionalAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "with_bias", "type": "BoolAttr" },
      { "name": "kernel_zp", "type": "DefaultValuedAttr" },
      { "name": "multiplier", "type": "OptionalAttr" },
      { "name": "rshift", "type": "OptionalAttr" },
      { "name": "weight_bits", "type": "OptionalAttr" },
      { "name": "quant_mode", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "out_f8_scale", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Convbwd",
    "summary": "convolution backward",
    "description": "1.Op Introduction\r\n    calculate grad_input,grad_weight,grad_bias of convolution operation;\r\n\r\n    2.Math formula\r\n    ```math\r\n        input(N, C_{in}, D, H, W) = \\sum_{C_{out}} output(N, C_{output}, D + sD, H + sH * kH, W + sW * kW) * kernel(C_{in}, C_{out}, kD, kH, kW)\r\n    ```\r\n    3.activation and weight\r\n    grad_out(act.): how the loss changes with respect to the output of the convolution operation.;\r\n    input(act.): input tensor;\r\n    kernel(w.): weights (filters) ;\r\n    buffer(act.): intermediate computations or storage during the backward pass;\r\n\r\n    4.attribute\r\n    groups: the number of groups for grouped convolutions.;\r\n    input_shape: the shape of the input tensor.;\r\n    grad_out_shape: the shape of the tensor that is being backpropagated.;\r\n    kernel_shape: an array of integers.;\r\n    stride: an array of integers for each dimension.;\r\n    dilations: dilation rate for the convolution, controls the spacing between kernel elements.;\r\n    padding: an array of integers.;\r\n    inserts: additional parameters;\r\n    grad_input_enable: whether to compute the gradient with respect to the input tensor.;\r\n    grad_weight_enable: whether to compute the gradient with respect to the kernel tensor.;\r\n    grad_bias_enable: whether to compute the gradient with respect to the bias tensor.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "grad_out", "type": "AnyTensor" },
      { "name": "input", "type": "AnyTensor" },
      { "name": "kernel", "type": "AnyTensor" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "grad_input", "type": "AnyTensorOrNone" },
      { "name": "grad_weight", "type": "AnyTensorOrNone" },
      { "name": "grad_bias", "type": "AnyTensorOrNone" }
    ],
    "attributes": [
      { "name": "groups", "type": "I64Attr" },
      { "name": "input_shape", "type": "I64ArrayAttr" },
      { "name": "grad_out_shape", "type": "I64ArrayAttr" },
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "stride", "type": "I64ArrayAttr" },
      { "name": "dilations", "type": "I64ArrayAttr" },
      { "name": "padding", "type": "I64ArrayAttr" },
      { "name": "inserts", "type": "I64ArrayAttr" },
      { "name": "grad_input_enable", "type": "BoolAttr" },
      { "name": "grad_weight_enable", "type": "BoolAttr" },
      { "name": "grad_bias_enable", "type": "BoolAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.ConvBwd_Weight",
    "summary": "Convolution Backward operator",
    "description": "1.Op Introduction\r\n    Gradient of Weight in Convolution Backward.\r\n\r\n    2.Math formula\r\n    ```math\r\n            \\frac{\\partial L}{\\partial W} = \\sum_{n=1}^{N} \\sum_{c=1}^{C_{in}} \\sum_{h=1}^{H} \\sum_{w=1}^{W} \\text{input}[n, c, h, w] \\cdot \\text{grad\\_out}[n, :, h', w']\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    grad_output(act.): the gradient of the loss with respect to the output.;\r\n    gradout_transpose(w.): The transposed gradient of the output tensor.;\r\n\r\n    4.attributes\r\n    groups: Number of blocked connections from input channels to output channels. Default: 1.;\r\n    input_shape: The shape of the input tensor.;\r\n    grad_out_shape: The shape of the gradient output tensor.;\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    stride: the stride for the cross-correlation, a single number or a tuple.;\r\n    dilations: controls the spacing between the kernel points;\r\n    padding: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    grad_bias_enable: whether to compute the gradient for the bias term as well.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "gradout", "type": "AnyTensor" },
      { "name": "gradout_transpose", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "groups", "type": "I64Attr" },
      { "name": "input_shape", "type": "I64ArrayAttr" },
      { "name": "grad_out_shape", "type": "I64ArrayAttr" },
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "stride", "type": "I64ArrayAttr" },
      { "name": "dilations", "type": "I64ArrayAttr" },
      { "name": "padding", "type": "I64ArrayAttr" },
      { "name": "grad_bias_enable", "type": "BoolAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Copy",
    "summary": "TG copy operator.",
    "description": "1.Op Introduction\r\n    duplicating tensor data from an input to an output buffer.\r\n\r\n    2.Math formula\r\n    output = Copy(input, shape, input_stride, output_stride)\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    shape: 0: keep dim from input; -1: left dim from input.;\r\n    input_stride: input data stride(saved as I64ArrayAttr).\r\n    output_stride: output data stride(saved as I64ArrayAttr).\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "shape", "type": "I64ArrayAttr" },
      { "name": "input_stride", "type": "I64ArrayAttr" },
      { "name": "output_stride", "type": "I64ArrayAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.CoreBegin",
    "summary": "Begin op parallel to multi cores",
    "description": "1.Op Introduction\r\n    Begin of pattern to multi cores\r\n\r\n    2.Math formula\r\n        output = CoreBegin(input, pattern)\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;\r\n\r\n    4.attributes\r\n    pattern: the core operation pattern used for initializing parallel execution across multiple cores.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "pattern", "type": "Tpu_CorePatternAttr" }
    ]
  },
  {
    "name": "tpu.CoreEnd",
    "summary": "End op parallel to multi cores",
    "description": "1.Op Introduction\r\n    End of pattern to multi cores\r\n\r\n    2.Math formula\r\n        output = CoreEnd(input, pattern)\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;\r\n\r\n    4.attributes\r\n    pattern: the core operation pattern used for initializing parallel execution across multiple cores.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "pattern", "type": "Tpu_CorePatternAttr" }
    ]
  },
  {
    "name": "tpu.CoreParallel",
    "summary": "Parallel execution region in multi cores",
    "description": "1.Op Introduction\r\n    The ops in one parallel should run in parallel.\r\n\r\n    2.Math formula\r\n        output = CoreParallel(input, offset, size)\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;\r\n\r\n    4.attributes\r\n    offset: scalar;\r\n    size: the number of elements.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "offset", "type": "I64Attr" },
      { "name": "size", "type": "I64Attr" }
    ]
  },
  {
    "name": "tpu.Correlation",
    "summary": "Custom operator correlation",
    "description": "Multiply the sliced left_feature and right_feature based on max_disp;\r\n  then perform a reduce operation;\r\n  and finally concatenate the results.\r\n\r\n  2.Math formula\r\n  for i in range(max_disp):\r\n    if i > 0:\r\n        output[:, i, :, i:] = (left_feature[:, :, :, i:] * right_feature[:, :, :, :-i]).mean(dim=1)\r\n    else:\r\n        output[:, i, :, :] = (left_feature * right_feature).mean(dim=1)\r\n\r\n  3.activation and weight\r\n  input(act.): input tensor;\r\n\r\n  4.attribute\r\n  max_disp: The number of slicing iterations, which is also the size of the output dimension C.\r\n  num_groups: The number of batch groups.",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "max_disp", "type": "DefaultValuedAttr" },
      { "name": "num_groups", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tpu.Csc",
    "summary": "Color space convert for model's inputs",
    "description": "1.Op Introduction\r\n    Performs csc operation on inputs.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Csc(input, pixel_format, y_align, w_align, channel_align)\r\n    ```\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    channel_order: the order of the color channels in the output tensor.(e.g., RGB, BGR).;\r\n    pixel_format: required, pixel format type.;\r\n    aligned:  whether the pixel data should be aligned.;\r\n    pixel_type: the type of pixel data.;\r\n    y_align: width alignment of channel y.;\r\n    w_align: width alignment of channel uv.;\r\n    channel_align: alignment of channel.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "channel_order", "type": "OptionalAttr" },
      { "name": "pixel_format", "type": "StrAttr" },
      { "name": "aligned", "type": "DefaultValuedAttr" },
      { "name": "pixel_type", "type": "DefaultValuedAttr" },
      { "name": "y_align", "type": "DefaultValuedAttr" },
      { "name": "w_align", "type": "DefaultValuedAttr" },
      { "name": "channel_align", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.CumSum",
    "summary": "CumSum operator",
    "description": "1.Op Introduction\r\n    Returns the cumulative sum of elements of input in the dimension dim.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = \\sum{j=0, i}input[j]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    dim(w.): If set to 0, computed across rows, If set to 1, computed across columns.;\r\n\r\n    4.attributes\r\n    axis: the dimension of the input tensor.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "dim", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "I64Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Custom",
    "summary": "Custom operator",
    "description": "1.Op Introduction\r\n    Custom operator\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = CustomFunction(inputs, name, params)\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;\r\n    buffer(w.): temporary storage of intermediate results or states during computation.;\r\n\r\n    4.attributes\r\n    name: the name of the custom operation to be executed.;\r\n    params: A dictionary of parameters.;\r\n    ginfo: associated with layer grouping information.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" },
      { "name": "params", "type": "DictArrayAttr" },
      { "name": "ginfo", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.D2D",
    "summary": "Copy WeightOp to Device Mem",
    "description": "1.Op Introduction\r\n    for to alloc address, and can modify the data(with WeightOp to init it)\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = D2D(input, const_val)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    const_val: specifies the constant value to be added to each element of the input tensor(positive, negative, or zero).;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "const_val", "type": "F64Attr" }
    ]
  },
  {
    "name": "tpu.Deconv",
    "summary": "deconvolution operator",
    "description": "1.Op Introduction\r\n    Perform Deconvolution operation.\r\n\r\n    2.Math formula\r\n    The height and width of the output tensor can be calculated using the following formulas:\r\n    ```math\r\n            H_{out} = H_{in - 1} x stride[0] - 2 x pads[0] + H_k + output_padding[0]\r\n            W_{out} = W_{in - 1} x stride[1] - 2 x pads[1] + W_k + output_padding[1]\r\n    ```\r\n    The output tensor is computed as:\r\n    ```math\r\n            output(N, C_out, H_out, W_out) = \\sum(c_in) {\\sum(h_k) {\\sum(w_k){input(n, c_in, h_in, w_in) x filter(c_out, c_in, h_k, w_k)}}}\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    filter(w.): the learnable weights of the convolution 2d operation.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attributes\r\n    kernel_shape: the size of the convolution kernel (filter) as an array. ;\r\n    strides: the stride for the cross-correlation, a single number or a tuple.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    group: (optional)Number of blocked connections from input channels to output channels. Default: 1.;\r\n    dilations: controls the spacing between the kernel points;\r\n    output_padding: The value can be provided as a single integer or a tuple, allowing for different padding values for height and width.;\r\n    dynweight_reorderd: whether the weights (filters) should be reordered dynamically.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    with_bias: whether to include a bias term in the convolution operation.;\r\n    multiplier: Floating-point multiplication operations are usually converted to fixed-point multiplication operations.;\r\n    rshift: the number of bits to right-shift the quantized values.;\r\n    quant_mode: It determines how the output tensor should be quantized.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "filter", "type": "AnyRankedTensor" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "group", "type": "DefaultValuedAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "output_padding", "type": "OptionalAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "with_bias", "type": "BoolAttr" },
      { "name": "multiplier", "type": "OptionalAttr" },
      { "name": "rshift", "type": "OptionalAttr" },
      { "name": "quant_mode", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Deconv3D",
    "summary": "3D deconvolution operator",
    "description": "1.Op Introduction\r\n    \"Perform 3D deconvolution operation.\"\r\n\r\n    2.Math formula\r\n    ```math\r\n        O(n, d, h, w) = \\sum_{c=0}^{C_{\\text{in}}-1} \\sum_{k_d=0}^{K_d-1} \\sum_{k_h=0}^{K_h-1} \\sum_{k_w=0}^{K_w-1}I\\big(n, c, d' \\, , h' \\, , w'\\big) \\times F\\big(c, :, k_d, k_h, k_w\\big)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    filter(w.): the learnable weights of the convolution 2d operation.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attributes\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    strides: the stride for the cross-correlation, a single number or a tuple.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.;\r\n    group: (optional)Number of blocked connections from input channels to output channels. Default: 1.;\r\n    dilations: controls the spacing between the kernel points;\r\n    output_padding: The value can be provided as a single integer or a tuple, allowing for different padding values for height and width.;\r\n    dynweight_reorderd: whether the weights (filters) should be reordered dynamically.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    with_bias: whether to include a bias term in the convolution operation.;\r\n    multiplier: Floating-point multiplication operations are usually converted to fixed-point multiplication operations.;\r\n    rshift: the number of bits to right-shift the quantized values.;\r\n    quant_mode: It determines how the output tensor should be quantized.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "filter", "type": "AnyRankedTensor" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "group", "type": "DefaultValuedAttr" },
      { "name": "dilations", "type": "OptionalAttr" },
      { "name": "output_padding", "type": "OptionalAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "with_bias", "type": "BoolAttr" },
      { "name": "multiplier", "type": "OptionalAttr" },
      { "name": "rshift", "type": "OptionalAttr" },
      { "name": "quant_mode", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.DeformGather",
    "summary": "Deform gather operator",
    "description": "1.Op Introduction\r\n     The deform gather operator for deform_conv2d.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = input(offset) * use_mask * mask\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    offset(w.): the learnable offsets of the module of shape.;\r\n    mask(act.): place the pooled values in the output tensor.;\r\n    buffer(w.): temporary storage of intermediate results or states during computation.;\r\n\r\n    4.attributes\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    strides: the stride for the cross-correlation, a single number or a tuple.;\r\n    dilations: controls the spacing between the kernel points;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.;\r\n    deform_group: (optional)Number of blocked connections from input channels to output channels. Default: 1.;\r\n    use_mask: whether use mask for input tensor.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "offset", "type": "AnyRankedTensor" },
      { "name": "mask", "type": "AnyTensorOrNone" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "dilations", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "use_mask", "type": "DefaultValuedAttr" },
      { "name": "deform_group", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.DepackRaw",
    "summary": "Postprocess for raw image.",
    "description": "1.Op Introduction\r\n    (1) depack channel (b, bh * bw, ih + ph, iw + pw) -> (b, 1, oh * bh, ow * bw)\r\n    (2) cast to INT16 then postprocess img to raw pattern ( 3 byte 2 pixel )\r\n    (3) (b, bh * bw, ih + ph, iw + pw) 16bit -> (b, 1, oh * bh, ow * bw * 3 / 2) 8bit\r\n\r\n    2.Math formula\r\n    (1)Depacking:\r\n    The depacking operation can be represented as:\r\n    ```math\r\n        depacked_tensor = reshape(input, (b, 1, oh · bh,ow · bw));\r\n    ```\r\n    Here, input is the packed tensor, and depacked_tensor is the resulting tensor after reshaping.\r\n    (2)Casting to INT16:\r\n    After reshaping, the values are cast to 16-bit integers:\r\n    ```math\r\n        int16_tensor = cast(depacked_tensor, int16);\r\n    ```\r\n    (3)Postprocessing to Raw Pattern:\r\n    The conversion from INT16 back to the raw image format can be described as:\r\n    ```math\r\n        raw_image = int16_tensor x (255 / (white_level - black_level));\r\n    ```\r\n    This scaling ensures that the pixel values are appropriately adjusted based on the specified white and black levels.\r\n    (4)Final Output Tensor:\r\n    Finally, the output tensor is formed by packing the processed values into the required format:\r\n    ```math\r\n        output=reshape(raw_image, (b, 1, oh · bh, ow · bw · 2/3));\r\n    ```\r\n    This ensures that the output tensor has the correct dimensions and format for further processing.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    padding_h: the height of the padding applied to the input image.;\r\n    padding_w: the width of the padding applied to the input image.;\r\n    white_level: the maximum intensity level for normalization.;\r\n    black_level: the minimum intensity level for normalization.;\r\n    channel_order: the order of the color channels in the output tensor.(e.g., RGB, BGR).;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "padding_h", "type": "I64Attr" },
      { "name": "padding_w", "type": "I64Attr" },
      { "name": "white_level", "type": "F64Attr" },
      { "name": "black_level", "type": "F64Attr" },
      { "name": "channel_order", "type": "I64ArrayAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Depth2Space",
    "summary": "Depth2Space operator",
    "description": "1.Op Introduction\r\n    Refer to `https://github.com/onnx/onnx/blob/main/docs/Operators.md#depthtospace`\r\n    [n, c, h, w] => [n, c / (block_h * block_w), h * block_h, w * block_w];\r\n    if inversed, [n, c, h, w] => [n, c * block_h * block_w, h / block_h, w / block_w];\r\n    if DCR(depth-column-row), channel ordered by block_h * block_w * c;\r\n    else CRD(column-row-depth), channel ordered by c * block_h * block_w;\r\n    The format of input or output is NCHW or NHWC.\r\n\r\n    2.Math formula\r\n\r\n    (1)Standard Transformation:\r\n    Given an input tensor of shape ( (N, C, H, W) ):\r\n    The output tensor after applying the Depth2Space operation can be calculated as:\r\n    ```math\r\n        {output}(N_i, C_j', H_k, W_l) = input(N_i, C_j, k / block_h, l / block_w)\r\n    ```\r\n    where k / block_h and l / block_w are rounded down.\r\n\r\n    (2)Inverse Transformation:\r\n    Given an input tensor of shape ( (N, C, H, W) ):\r\n    The output tensor after applying the Depth2Space operation can be calculated as:\r\n    ```math\r\n        {output}(N_i, C_j, H_k, W_l) = input(N_i, C_j', (k x block_h + j / (C / (block_h x block_w))), (l x block_w + j % (C / (block_h x block_w))))\r\n    ```\r\n    where C / (block_h x block_w) is rounded down.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    block_h: The height of the blocks used to rearrange the depth into spatial dimensions.;\r\n    block_w: The width of the blocks used to rearrange the depth into spatial dimensions.;\r\n    is_CRD: whether the channel ordering is in Column-Row-Depth format.;\r\n    is_inversed: whether the channel ordering is in Column-Row-Depth format.;\r\n    in_is_NCHW: whether the input tensor is in NCHW format.;\r\n    out_is_NCHW: whether the output tensor should be in NCHW format.;\r\n    swap_cr: swaps the height and width dimensions in the output tensor.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "block_h", "type": "I64Attr" },
      { "name": "block_w", "type": "I64Attr" },
      { "name": "is_CRD", "type": "BoolAttr" },
      { "name": "is_inversed", "type": "BoolAttr" },
      { "name": "in_is_NCHW", "type": "DefaultValuedAttr" },
      { "name": "out_is_NCHW", "type": "DefaultValuedAttr" },
      { "name": "swap_cr", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.DequantInt",
    "summary": "dequant operation",
    "description": "1.Op Introduction\r\n    Dequant 8 bit data to 32/16 bit data.\r\n\r\n    2.Math formula\r\n    32/16bit(output) = DequantIntOp((8bit(input), shift) x multiplier) ≪ lshift;\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    multiplier: Floating-point multiplication operations are usually converted to fixed-point multiplication operations.;\r\n    shift: a shift value applied to the quantized data before scaling.;\r\n    lshift: a left shift operation applied to the dequantized data after scaling.;\r\n    quant_mode: It determines how the output tensor should be quantized.;\r\n    round_mode: This parameter specifies the rounding mode to be used during quantization.;\r\n    ginfo: contains layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "multiplier", "type": "SI32Attr" },
      { "name": "shift", "type": "I64Attr" },
      { "name": "lshift", "type": "DefaultValuedAttr" },
      { "name": "quant_mode", "type": "Tpu_DequantModeAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.DequantIntAxis",
    "summary": "dequant operation",
    "description": "1.Op Introduction\r\n    Dequant 8 bit data to 32/16 bit data, PerAxis(or PerChannel)\r\n\r\n    2.Math formula\r\n    32/16bit(output) = DequantIntAxisOp((8bit(input), quant, shift) x multiplier) ≪ lshift;\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    quant(act.): This attribute represents the quantization parameters tensor.\r\n                 It contains the values used for requantization, such as multipliers and shifts,\r\n                 which are specific to each axis or channel.;\r\n\r\n    4.attribute\r\n    lshift: a left shift operation applied to the dequantized data after scaling.;\r\n    quant_mode: It determines how the output tensor should be quantized.;\r\n    round_mode: This parameter specifies the rounding mode to be used during quantization.;\r\n    ginfo: contains layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "quant", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "lshift", "type": "DefaultValuedAttr" },
      { "name": "quant_mode", "type": "Tpu_DequantModeAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.DetectionOutput",
    "summary": "DetectionOutput operation",
    "description": "1.Op Introduction\r\n    Intended for use with MultiBox detection method to generate prior.\r\n\r\n    2.Math formula\r\n    ```math\r\n        1.Raw Detection Output\r\n            raw_output = {(b_i, c_i) | i = 1, 2,...,N}\r\n            b_i is the bounding box coordinates, c_i is the i-th confidence score.\r\n        2.Apply Confidence threshold\r\n            filtered_output = {(b_i, c_i) | c_i >= confidence_threshold}\r\n        3.Non-Maximum Suppression(NMS)\r\n            nms_output = NMS(filtered_output, nms_threshold)\r\n        4.Top K detections\r\n            output = top_k(nms_output, top_k)\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor;\r\n    buffer(w.): temporary storage of intermediate results or states during the LSTM computation.;\r\n\r\n    4.attributes\r\n    num_classes: total number of classes, including the background class.;\r\n    background_label_id: background class, differentiate between detected objects and the background.;\r\n    nms_threshold: The threshold used for Non-Maximum Suppression (NMS).;\r\n    top_k: The maximum number of predictions to be considered for each image.;\r\n    code_type: the encoding type for the bounding box coordinates.;\r\n    keep_top_k: The number of top scoring detections to keep after applying NMS.;\r\n    confidence_threshold: The minimum confidence score required for a detection to be considered valid.;\r\n    share_location: whether the bounding box locations are shared across different classes.;\r\n    variance_encoded_in_target: whether the variance for bounding box predictions is encoded in the target.;\r\n    eta: adjusts the confidence scores during NMS.;\r\n    onnx_nms: configuration for ONNX compatibility (default is 1).;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "num_classes", "type": "I64Attr" },
      { "name": "background_label_id", "type": "DefaultValuedAttr" },
      { "name": "nms_threshold", "type": "F64Attr" },
      { "name": "top_k", "type": "I64Attr" },
      { "name": "code_type", "type": "DetectionOutputCodeTypeAttr" },
      { "name": "keep_top_k", "type": "I64Attr" },
      { "name": "confidence_threshold", "type": "F64Attr" },
      { "name": "share_location", "type": "DefaultValuedAttr" },
      { "name": "variance_encoded_in_target", "type": "DefaultValuedAttr" },
      { "name": "eta", "type": "DefaultValuedAttr" },
      { "name": "onnx_nms", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.DevBegin",
    "summary": "Begin distribution tensors to multi device",
    "description": "1.Op Introduction\r\n    Tensors split to distributed device\r\n\r\n    2.Math formula\r\n        output = DevBegin(inputs, pattern, begin_methods, num_head, done)\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;\r\n\r\n    4.attributes\r\n    pattern: the core operation pattern used for initializing parallel execution across multiple cores.;\r\n    begin_methods: an array of strategies used to begin the distribution process across devices.;\r\n    num_head: the number of primary channels that are processed first or in parallel.;\r\n    done: a flag whether the distribution initialization process is completed.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "pattern", "type": "Tpu_DevPatternAttr" },
      { "name": "begin_methods", "type": "I64ArrayAttr" },
      { "name": "num_head", "type": "DefaultValuedAttr" },
      { "name": "done", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tpu.DevEnd",
    "summary": "End distribution tensors from outputs",
    "description": "1.Op Introduction\r\n    Tensors from distributed device connect together.\r\n\r\n    2.Math formula\r\n        output = DevBegin(inputs, pattern, begin_methods, num_head, done)\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;\r\n\r\n    4.attributes\r\n    pattern: the core operation pattern used for initializing parallel execution across multiple cores.;\r\n    end_methods: an array of strategies used to end the distribution process across devices.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "pattern", "type": "Tpu_DevPatternAttr" },
      { "name": "end_methods", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "tpu.Device2Host",
    "summary": "Device2Host Operation",
    "description": "1.Op Introduction\r\n    takes data from device mem to host mem\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = Device2Host(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "tpu.Div",
    "summary": "div operator",
    "description": "1.Op Introduction\r\n    Performs element-wise binary division.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = input/const_val or const_val/input\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    is_reverse: whether the subtraction operation is performed in reverse order.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    // quant param\r\n    multipliers: applied during the concatenation process to adjust the values of the input tensors.;\r\n    rshifts: an array of right shift values corresponding to each input tensor.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "is_reverse", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "multiplier", "type": "DefaultValuedAttr" },
      { "name": "rshift", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.DtypeCast",
    "summary": "Cast F32 to F16",
    "description": "1.Op Introduction\r\n    Cast F32 to F16\r\n    The Tpu_DtypeCastOp is a specialized operation designed to cast floating-point tensors from 32-bit precision (F32) to 16-bit precision (F16).\r\n\r\n    2.Math formula\r\n    FLOAT16(output) = DtypeCastOp (FLOAT32(input));\r\n\r\n    3.activation\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    extra_input: whether additional input is required for the casting operation.;\r\n    ginfo: contains layer grouping information.;\r\n    with_scale: whether the casting operation should include a scaling factor.;\r\n    round_mode: the rounding mode to be used during the casting operation.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "extra_input", "type": "OptionalAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "with_scale", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.EmbDenseBwd",
    "summary": "EmbDenseBwd operation for train",
    "description": "1.Op Introduction\r\n    layer normalization\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = grad_output[indices]\r\n    ```\r\n\r\n    3.activation and weight\r\n    grad_output(act.): the gradient of the loss with respect to the output.;\r\n    indices(w.): the indices of the input tokens or items.;\r\n\r\n    4.attributes\r\n    num_weights: the total number of embedding weights.;\r\n    padding_idx: a index for padding.;\r\n    scale_grad_by_freq: whether the gradient should be scaled by the inverse frequency of that token.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "grad_output", "type": "AnyTensor" },
      { "name": "indices", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "num_weights", "type": "SI32Attr" },
      { "name": "padding_idx", "type": "SI32Attr" },
      { "name": "scale_grad_by_freq", "type": "BoolAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.FAttention",
    "summary": "Flash Attention operator",
    "description": "1.Op Introduction\r\n    Performs a two dimensional matrix multiplication. This allows both inputs to\r\n    be activations, rather than reserving weights as an attribute in the\r\n    FULLY_CONNECTED operator.\r\n\r\n    2.Math formula\r\n    ```math\r\n        Attention(Q, K, V) = softmax(((Q x K^T) / \\sqrt{d_k}) + musk) x V;\r\n        head_i = Attention(Q x queries_weight, K x keys_weight, V x values_weight);\r\n        MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h) x out_weight + out_bias;\r\n        output = MultiHead(input x queries_weight + queries_bias, input x keys_weight + keys_bias, input x values_weight + values_bias).\r\n    ```\r\n\r\n    3.activation and weight\r\n    queries(act.): input tensor.;\r\n    keys(act.): The keys are derived from the input data and help the model determine which parts of the input are relevant for each query.;\r\n    values(act.): The values are the actual information that will be aggregated based on the attention scores computed from the queries and keys.;\r\n    mask(w.): apply masking during the attention computation, Masks can prevent the model from attending to certain positions in the input.;;\r\n    buffer(w.): temporary storage of intermediate results or states during computation.;\r\n\r\n    4.attributes\r\n    scale: scalar.;\r\n    batch: batch size.;\r\n    q_head: the number of query heads in the multi-head attention mechanism.;\r\n    kv_head: the number of key/value heads.;\r\n    dim: the size of the input features or the size of the query, key, and value vectors.;\r\n    mq: a dimension or a modifier related to the query matrix.;\r\n    mk: the key matrix.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "queries", "type": "AnyTensor" },
      { "name": "keys", "type": "AnyTensor" },
      { "name": "values", "type": "AnyTensor" },
      { "name": "mask", "type": "AnyTensorOrNone" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "scale", "type": "F64Attr" },
      { "name": "batch", "type": "I64Attr" },
      { "name": "q_head", "type": "I64Attr" },
      { "name": "kv_head", "type": "I64Attr" },
      { "name": "dim", "type": "I64Attr" },
      { "name": "mq", "type": "I64Attr" },
      { "name": "mk", "type": "I64Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.FusedActiveCast",
    "summary": "Active + Cast",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "Tpu_ActiveModeAttr" },
      { "name": "coeffs", "type": "OptionalAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Gather",
    "summary": "Gather operator",
    "description": "1.Op Introduction\r\n    Perform Gather operation on the given axis.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = input[indices]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    indices(w.): the indices of the elements to be gathered from the input tensor.;\r\n    buffer(w.): temporary storage of intermediate results or states during the computation.;\r\n\r\n    4.attributes\r\n    axis: the dimension of the input tensor.;\r\n    if_neg_index: how negative indices should be handled.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "indices", "type": "AnyRankedTensor" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "if_neg_index", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.GatherElements",
    "summary": "GatherElements operator",
    "description": "1.Op Introduction\r\n    Perform GatherElements operation on the given axis.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i_1, i_2, i_3,...i_k] = input[i_1, i_2, i_3,..., indices[i_k]]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    indices(w.): the indices of the elements to be gathered from the input tensor.;\r\n    indices_coeff(w.): a scaling or modifying factor for the indices.;\r\n    buffer(w.): temporary storage of intermediate results or states during the computation.;\r\n\r\n    4.attributes\r\n    axis: the dimension of the input tensor.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "indices", "type": "AnyTensor" },
      { "name": "indices_coeff", "type": "AnyTensorOrNone" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.GatherND",
    "summary": "GatherND operator",
    "description": "1.Op Introduction\r\n    This operator is the inverse of ScatterND.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output_i = input[indices_i]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input_data(act.): input tensor.;\r\n    indices(w.): which elements to gather from the input tensor.;\r\n\r\n    4.attributes\r\n    indice_dims: the number of dimensions in the indices tensor.;\r\n    batch_dims: the number of batch dimensions in the input tensor.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input_data", "type": "AnyRankedTensor" },
      { "name": "indices", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "indice_dims", "type": "OptionalAttr" },
      { "name": "batch_dims", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.GenericCpu",
    "summary": "generic cpu operator",
    "description": "1.Op Introduction\r\n    Generic Cpu Op.\r\n\r\n    2.Math formula\r\n    output = GenericCpu(inputs, param)\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;\r\n\r\n    4.attributes\r\n    cpu_op_name: the name of the CPU-specific operator to be executed.;\r\n    param: a set of parameters.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "cpu_op_name", "type": "StrAttr" },
      { "name": "param", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.GridSampleInDeformableAttn",
    "summary": "GridSampleInDeformableAttn operation",
    "description": "1.Op Introduction\r\n    Given an input and a flow-field grid, computes the output\r\n    using input values and pixel locations from grid.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[N, C, H', W'] = input[C, grid[N, H', W', 1], grid[N, H', W', 0]]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    grid(w.): The flow-field grid tensor that defines the pixel locations for sampling.;\r\n    buffer(w.): temporary storage of intermediate results or states during computation.;\r\n\r\n    4.attributes\r\n    mode: the type of binary operation to be performed, addition, subtraction, or other types of binary operations.;\r\n    padding_mode: padding mode for outside grid values, Int attribute [0, 1, 2],\r\n                                representing 'zero' | 'boundary' | 'reflection't.;\r\n    align_corners: whether to align the corners of the input and output tensors.;\r\n    scale: scalar.;\r\n    mean: mean values to subtract from each channel for normalization.;\r\n    need_permute: whether permutation of the output tensor is required;",
    "inputs": [
      { "name": "input_global_addr", "type": "Variadic" },
      { "name": "grid_global_addr", "type": "Variadic" },
      { "name": "attn_global_addr", "type": "Variadic" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output_global_addr", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "num_grid_samples", "type": "I64Attr" },
      { "name": "input_dims", "type": "I64Attr" },
      { "name": "input_n", "type": "I64ArrayAttr" },
      { "name": "input_c", "type": "I64ArrayAttr" },
      { "name": "input_d", "type": "I64ArrayAttr" },
      { "name": "input_h", "type": "I64ArrayAttr" },
      { "name": "input_w", "type": "I64ArrayAttr" },
      { "name": "grid_dout", "type": "I64Attr" },
      { "name": "grid_hout", "type": "I64Attr" },
      { "name": "grid_wout", "type": "I64Attr" },
      { "name": "interp_mode", "type": "I64Attr" },
      { "name": "padding_mode", "type": "I64Attr" },
      { "name": "align_corners", "type": "BoolAttr" }
    ]
  },
  {
    "name": "tpu.GridSampler",
    "summary": "GridSampler operation",
    "description": "1.Op Introduction\r\n    Given an input and a flow-field grid, computes the output\r\n    using input values and pixel locations from grid.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[N, C, H', W'] = input[C, grid[N, H', W', 1], grid[N, H', W', 0]]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    grid(w.): The flow-field grid tensor that defines the pixel locations for sampling.;\r\n    buffer(w.): temporary storage of intermediate results or states during computation.;\r\n\r\n    4.attributes\r\n    mode: the type of binary operation to be performed, addition, subtraction, or other types of binary operations.;\r\n    padding_mode: padding mode for outside grid values, Int attribute [0, 1, 2],\r\n                                representing 'zero' | 'boundary' | 'reflection't.;\r\n    align_corners: whether to align the corners of the input and output tensors.;\r\n    scale: scalar.;\r\n    mean: mean values to subtract from each channel for normalization.;\r\n    need_permute: whether permutation of the output tensor is required;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "grid", "type": "AnyTensor" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "I64Attr" },
      { "name": "padding_mode", "type": "I64Attr" },
      { "name": "align_corners", "type": "BoolAttr" },
      { "name": "mean", "type": "DefaultValuedAttr" },
      { "name": "scale", "type": "DefaultValuedAttr" },
      { "name": "need_permute", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Group",
    "summary": "Group operation",
    "description": "1.Op Introduction\r\n    Make ops in one group to inferece by local mem\r\n\r\n    2.Math formula\r\n        output = Group(nsecs, hsecs, dsecs, wsecs, csecs; swpipl_stage_num, group_type, flow, self_up_overlap_op, self_down_overlap_op, other_up_overlap_op, other_down_overlap_op, support_compress, run_core_id, core_slice_ncdhw)\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;\r\n\r\n    4.attributes\r\n    nsecs: the number of sections in the \"n\" (batch or number) dimension.;\r\n    hsecs: the number of sections in the \"h\" (height) dimension.;\r\n    dsecs: the number of sections in the \"d\" (depth) dimension.;\r\n    wsecs: the number of sections in the \"w\" (width) dimension.;\r\n    csecs: the number of sections in the \"c\" (channel) dimension.;\r\n    swpipl_stage_num: the number of stages in the pipeline, which related to swappable operations or pipelined processing.;\r\n    group_type: the type of the grouping strategy.;\r\n    flow: store both negative timestep indices and positive operation identifiers.;\r\n    self_up_overlap_op: overlapped in the upward (or previous) direction within the same group.;\r\n    self_down_overlap_op: overlapped in the downward (or subsequent) direction within the same group.;\r\n    other_up_overlap_op: Holds the operation identifiers in other groups overlapping in the upward direction.;\r\n    other_down_overlap_op: Holds the operation identifiers in other groups overlapping in the downward direction.;\r\n    support_compress: whether compression is supported for the load operation.;\r\n    run_core_id: an array of core IDs for running.;\r\n    core_slice_ncdhw: the tensor's dimensions (n, c, d, h, w) are partitioned.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "nsecs", "type": "I64Attr" },
      { "name": "hsecs", "type": "I64Attr" },
      { "name": "dsecs", "type": "I64Attr" },
      { "name": "wsecs", "type": "I64Attr" },
      { "name": "csecs", "type": "I64Attr" },
      { "name": "swpipl_stage_num", "type": "I64Attr" },
      { "name": "group_type", "type": "I64Attr" },
      { "name": "flow", "type": "DefaultValuedAttr" },
      { "name": "self_up_overlap_op", "type": "DefaultValuedAttr" },
      { "name": "self_down_overlap_op", "type": "DefaultValuedAttr" },
      { "name": "other_up_overlap_op", "type": "DefaultValuedAttr" },
      { "name": "other_down_overlap_op", "type": "DefaultValuedAttr" },
      { "name": "support_compress", "type": "DefaultValuedAttr" },
      { "name": "run_core_id", "type": "DefaultValuedAttr" },
      { "name": "core_slice_ncdhw", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tpu.GroupNorm",
    "summary": "GroupNorm operation",
    "description": "1.Op Introduction\r\n    group normalization\r\n\r\n    2.Math formula\r\n    ```math\r\n            mean_g = 1 / ((C / num_groups) * H * W) \\sum{i=1, C/num_groups} \\sum{j=1, H} \\sum{k=1, W} input_{n,c,j,k}\r\n            var_g = 1 / ((C / num_groups) * H * W) \\sum{i=1, C/num_groups} \\sum{j=1, H} \\sum{k=1, W} (input_{n,c,j,k} - mean_g) ^ 2\r\n            output_{n,c,j,k} = weight * (input_{n,c,j,k} - mean_g) / sqrt(var_g + eps) + bias\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    weight(w.): weight tensor;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n    // cv18xx\r\n    table(w.): store a lookup table that may assist in optimizing the matching process.;\r\n    mantissa_table(w.): stores a table of mantissa values used in calculations to improve precision or to handle specific numerical representations.;\r\n\r\n    4.attributes\r\n    eps: a small constant added to the denominator during normalization to prevent division by zero.;\r\n    num_groups: the number of groups to divide the input channels into for normalization.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "weight", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" },
      { "name": "table", "type": "AnyTensorOrNone" },
      { "name": "mantissa_table", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "num_groups", "type": "I64Attr" },
      { "name": "eps", "type": "F64Attr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.GroupNormTrain",
    "summary": "GroupNorm operation",
    "description": "1.Op Introduction\r\n    group normalization\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = \\frac{input - mean}{\\sqrt{\\sigma^2 + eps}} \\cdot weight + bias\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    weight(w.): weight tensor.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attributes\r\n    eps: a small constant added to the denominator during normalization to prevent division by zero.;\r\n    num_groups: number of groups to divide the channels into for normalization.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "weight", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" },
      { "name": "mean", "type": "AnyTensor" },
      { "name": "rstd", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "num_groups", "type": "I64Attr" },
      { "name": "eps", "type": "F64Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.GroupParallel",
    "summary": "Mutiple regions run in parallel.",
    "description": "1.Op Introduction\r\n    This operation is composed of numerous regions, with each region corresponding\r\n    to a subgraph. These subgraphs share identical computational patterns and are\r\n    distributed across various cores of a multi-core TPU for processing.\r\n\r\n    2.Math formula\r\n        output = GroupParallel(input)\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ]
  },
  {
    "name": "tpu.GRU",
    "summary": "GRU operator",
    "description": "1.Op Introduction\r\n    Perform RNN GRU operation.\r\n\r\n    2.Math formula\r\n    ```math\r\n        update gate(z_t):\r\n            z_t = Sigma(W_z · x_t + U_z ·h_(t-1) + b_z)\r\n        reset gate(r_t):\r\n            r_t = Sigma(W_r · x_t + U_r ·h_(t-1) + b_r)\r\n        Candidate Activation(h_t):\r\n            h_t = tanh(W_h · x_t + r_t \\odot (U_h · h_(t-1)) + b_h)\r\n        final output:\r\n            output = (1 - z_t) \\odot h_(t-1) + z_t \\odot h_t\r\n    ```\r\n    where, x_t is the input at time step (t), h_(t-1) is the hidden state from the previous time step.\r\n           W_z, W_r, W_h are the weight matrices for the input.\r\n           U_z, U_r, U_h are the weight matrices for the hidden state.\r\n           b_z, b_r, b_h are the bias vectors.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    filter(w.): the learnable weights of the convolution 2d operation.;\r\n    recurrence(w.): the previous hidden state influences the current hidden state.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n    initial_h(w.): the initial hidden state, which can be provided to start the GRU computation.;\r\n    buffer(w.): temporary storage of intermediate results or states during the GRU computation.;\r\n    sigmoid_table: a lookup table that contains pre-computed values for the sigmoid activation function.;\r\n    sigmoid_slope_table: This table contains pre-computed slopes (derivatives) of the sigmoid function, enhancing the training efficiency of the LSTM.;\r\n    tanh_table: contains pre-computed values for the hyperbolic tangent (tanh) activation function.;\r\n    tanh_slope_table: This table contains pre-computed slopes (derivatives) of the tanh function;\r\n\r\n    4.attributes\r\n    hidden_size: the number of units in the GRU cell,;\r\n    bidirectional: whether the GRU should be bidirectional;\r\n    linear_before_reset: whether to apply a linear transformation to the input before applying the reset gate.;\r\n    batch_first: the input and output tensors are provided in the shape (batch_size, seq_length, input_size).;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "filter", "type": "AnyTensorOrNone" },
      { "name": "recurrence", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" },
      { "name": "initial_h", "type": "AnyTensorOrNone" },
      { "name": "buffer", "type": "AnyTensorOrNone" },
      { "name": "sigmoid_table", "type": "AnyTensorOrNone" },
      { "name": "sigmoid_slope_table", "type": "AnyTensorOrNone" },
      { "name": "tanh_table", "type": "AnyTensorOrNone" },
      { "name": "tanh_slope_table", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTensorOrNone" },
      { "name": "Y_h", "type": "AnyTensorOrNone" }
    ],
    "attributes": [
      { "name": "hidden_size", "type": "I64Attr" },
      { "name": "bidirectional", "type": "BoolAttr" },
      { "name": "linear_before_reset", "type": "DefaultValuedAttr" },
      { "name": "batch_first", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Host2Device",
    "summary": "Host2Device Operation",
    "description": "1.Op Introduction\r\n    takes data from host mem to device mem\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = Host2Device(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "tpu.Identity",
    "summary": "identity operator",
    "description": "1.Op Introduction\r\n     identity operator.\r\n\r\n     2.Math formula\r\n    ```math\r\n        output = Identity(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ]
  },
  {
    "name": "tpu.If",
    "summary": "if operation",
    "description": "1.Op Introduction\r\n    If conditional\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = then_branch if cond is true\r\n                     else_branch if cond is false\r\n    ```\r\n\r\n    3.activation and weight\r\n    cond(act.): which branch of execution to follow.;",
    "inputs": [
      { "name": "cond", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "Variadic" }
    ]
  },
  {
    "name": "tpu.IndexPut",
    "summary": "IndexPut operation",
    "description": "1.Op Introduction\r\n    aten::index_put_\r\n    update specific elements of an input tensor at given indices with new values.\r\n\r\n    2.Math formula\r\n    ```math\r\n        if accumulate\r\n            input[indices] += values\r\n        else\r\n            input[indices] = values\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    indices(w.): the indices of the elements in the input tensor that should be updated.;\r\n    values(w.): the new values that will replace the existing values in the input tensor at the specified indices.;\r\n    //for accumulate is True\r\n    buffer(w.): temporary storage of intermediate results or states during the LSTM computation.;\r\n\r\n    4.attributes\r\n    accumulate: whether the operation should accumulate values at the specified indices or replace them.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "indices", "type": "AnyRankedTensor" },
      { "name": "values", "type": "AnyRankedTensor" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "accumulate", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.InstanceNorm",
    "summary": "InstanceNorm operation",
    "description": "1.Op Introduction\r\n    instance normalization.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output[i] = weight * (input[i] - mean) / sprt(var + eps) + bias\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    weight(w.): weight tensor;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n    // cv18xx\r\n    table(w.): store a lookup table that may assist in optimizing the matching process.;\r\n    mantissa_table(w.): stores a table of mantissa values used in calculations to improve precision or to handle specific numerical representations.;\r\n\r\n    4.attributes\r\n    eps: a small constant added to the denominator during normalization to prevent division by zero.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "weight", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" },
      { "name": "table", "type": "AnyTensorOrNone" },
      { "name": "mantissa_table", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "eps", "type": "F64Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Interp",
    "summary": "Interp operation",
    "description": "1.Op Introduction\r\n    Perform Interp on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n            H' = H x scale_h\r\n            W' = W x scale_w\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    shapeT(act.): an optional input tensor that specifies the desired shape for the output tensor.;\r\n    buffer(w.): temporary storage of intermediate results or states during the computation.;\r\n\r\n    4.attributes\r\n    scale_h: the scaling factor for the height (number of rows) of the input tensor.;\r\n    scale_w: the scaling factor for the width (number of columns) of the input tensor.;\r\n    mode: the type of binary operation to be performed, addition, subtraction, or other types of binary operations.;\r\n    coord_mode: whether the coordinates are normalized (ranging from 0 to 1) or absolute (based on pixel indices).;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "shapeT", "type": "AnyTensorOrNone" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "scale_h", "type": "F64Attr" },
      { "name": "scale_w", "type": "F64Attr" },
      { "name": "mode", "type": "Tpu_ResizeModeAttr" },
      { "name": "coord_mode", "type": "Tpu_ResizeCoordModeAttr" },
      { "name": "ppl_flag", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Join",
    "summary": "Join tensor to continues pieces",
    "description": "1.Op Introduction\r\n    The ops in one parallel should run in parallel.\r\n\r\n    2.Math formula\r\n        output = concat(input)\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ]
  },
  {
    "name": "tpu.LayerNorm",
    "summary": "LayerNorm operation",
    "description": "1.Op Introduction\r\n    layer normalization\r\n\r\n    2.Math formula\r\n    ```math\r\n        1.Normalization\r\n            mean = 1 / H \\sum{j=1, H} input[j]\r\n            var = 1 / H \\sum{j=1, H} (input[j] - mean) ^ 2\r\n        2.Layer Normalized Output\r\n            output[i] = weight * (input[i] - mean) / sprt(var + eps) + bias\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    weight(w.): weight tensor;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n    // cv18xx\r\n    table(w.): store a lookup table that may assist in optimizing the matching process.;\r\n    mantissa_table(w.): stores a table of mantissa values used in calculations to improve precision or to handle specific numerical representations.;\r\n\r\n    4.attributes\r\n    normalized_shape: the shape of the input tensor that will be normalized.;\r\n    axis: the dimension of the input tensor.;\r\n    eps: a small constant added to the denominator during normalization to prevent division by zero.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "weight", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" },
      { "name": "table", "type": "AnyTensorOrNone" },
      { "name": "mantissa_table", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "normalized_shape", "type": "I64ArrayAttr" },
      { "name": "axis", "type": "SI32Attr" },
      { "name": "eps", "type": "F64Attr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.LayerNormBwd",
    "summary": "LayerNorm operation for train",
    "description": "1.Op Introduction\r\n    layer normalization\r\n\r\n    2.Math formula\r\n    ```math\r\n        1.gradient input\r\n            grad_input = 1 / N(weight / sqrt(variance + eps) 1 / N \\sum{i=1, N}grad_out)\r\n                            - (input - mean) / N * \\sum(i=1, N)weight * grad_out / sqrt(variance + eps)\r\n        2.gradient weight\r\n            grad_weight = \\sum(i=1, N)grad_out * (input - mean) / sqrt(variance + eps)\r\n        3.gradient bias\r\n            grad_bias = \\sum(i=1, N)grad_out\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    grad_out(w.): the gradient of the loss with respect to the output of the layer normalization.;\r\n    mean(w.): mean values to subtract from each channel for normalization.;\r\n    variance(w.): adjust the predicted boxes during the training process.;\r\n    weight(w.): weight tensor.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attributes\r\n    normalized_shape:  the shape of the input tensor dimensions.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "grad_out", "type": "AnyTensor" },
      { "name": "input", "type": "AnyTensor" },
      { "name": "mean", "type": "AnyTensor" },
      { "name": "variance", "type": "AnyTensor" },
      { "name": "weight", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "grad_input", "type": "AnyTensorOrNone" },
      { "name": "grad_weight", "type": "AnyTensorOrNone" },
      { "name": "grad_bias", "type": "AnyTensorOrNone" }
    ],
    "attributes": [
      { "name": "normalized_shape", "type": "I64ArrayAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.LayerNormCast",
    "summary": "LayerNorm + Requant",
    "description": "LayerNorm + Requant",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "weight", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "SI32Attr" },
      { "name": "eps", "type": "F64Attr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "isCastAtEnd", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.LayerNormTrain",
    "summary": "LayerNorm operation for train",
    "description": "1.Op Introduction\r\n    layer normalization in train.\r\n\r\n    2.Math formula\r\n    ```math\r\n        1.Normalization\r\n            mean = 1 / H \\sum{j=1, H} input[j]\r\n            var = 1 / H \\sum{j=1, H} (input[j] - mean) ^ 2\r\n        2.Layer Normalized Output\r\n            output[i] = weight * (input[i] - mean) / sprt(var + eps) + bias\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    weight(w.): weight tensor.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attributes\r\n    normalized_shape: the shape of the input tensor dimensions.;\r\n    axis: the dimension of the input tensor.;\r\n    eps: a small constant added to the denominator during normalization to prevent division by zero.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "weight", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" },
      { "name": "mean", "type": "AnyRankedTensor" },
      { "name": "variance", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "normalized_shape", "type": "I64ArrayAttr" },
      { "name": "axis", "type": "SI32Attr" },
      { "name": "eps", "type": "F64Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.LeakyRelu",
    "summary": "leakyrelu operation",
    "description": "1.Op Introduction\r\n    The LeakyRelu operation multiples alpha with negative values, and the others keep changeless.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = alpha * input, if input < 0\r\n            output = input, if input >= 0\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    alpha: a scalar factor.;\r\n    multiplier: Floating-point multiplication operations are usually converted to fixed-point multiplication operations.;\r\n    multiplier_neg: specify the multiplier for negative input values.;\r\n    rshift: the number of bits to right-shift the quantized values.;\r\n    rshift_neg: specifies the number of bits to right-shift the quantized negative values.;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "alpha", "type": "OptionalAttr" },
      { "name": "multiplier", "type": "OptionalAttr" },
      { "name": "multiplier_neg", "type": "OptionalAttr" },
      { "name": "rshift", "type": "OptionalAttr" },
      { "name": "rshift_neg", "type": "OptionalAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Load",
    "summary": "Load operation",
    "description": "1.Op Introduction\r\n    load input or weight from gmem to lmem;\r\n    if do_bcast, [1,1,1,w] will load to [1,npu,1,w]\r\n\r\n    2.Math formula\r\n    output = Load(input, do_bcast, use_3ic_optimize, lmem_type, support_compress);\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    do_bcast: whether broadcasting.;\r\n    use_3ic_optimize: whether the 3-IC (Three Input Channels) optimization;\r\n    lmem_type: the type of local memory (lmem) to be used when loading data from global memory (gmem).;\r\n    ginfo: associated with layer grouping information.;\r\n    support_compress: whether compression is supported for the load operation.;\r\n    compress_info: used in conjunction with support_compress to specify details like compression schemes, ratios, or any relevant metadata.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "do_bcast", "type": "DefaultValuedAttr" },
      { "name": "use_3ic_optimize", "type": "DefaultValuedAttr" },
      { "name": "lmem_type", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "support_compress", "type": "DefaultValuedAttr" },
      { "name": "is_idx_weight", "type": "DefaultValuedAttr" },
      { "name": "compress_info", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.LoadToL2M",
    "summary": "Load operation",
    "description": "1.Op Introduction\r\n    load weight from gmem to l2mem;\r\n\r\n    2.Math formula\r\n    output = Load(input, id, l2m_addr);\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    id: differentiate or index various instances of the load operation, ensuring correctly matched.;\r\n    l2m_addr: the address or offset within the local L2 memory (l2mem).;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "buffer", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "id", "type": "I64Attr" }
    ]
  },
  {
    "name": "tpu.LogicalAnd",
    "summary": "logical and operation",
    "description": "1.Op Introduction\r\n    logical and operation between two variables\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = input1 and input2\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "tpu.Loop",
    "summary": "Loop operation",
    "description": "1.Op Introduction\r\n    Generic Looping construct, support while/do_while/for/forerver etc:\r\n\r\n    2.Math formula\r\n    none\r\n\r\n    3.activation and weight\r\n    AnyTensor;",
    "inputs": [
      { "name": "M", "type": "AnyTypeOf" },
      { "name": "cond", "type": "AnyTypeOf" },
      { "name": "v_initial", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "v_final_and_scan_outputs", "type": "Variadic" }
    ]
  },
  {
    "name": "tpu.LRN",
    "summary": "Local Response Normalization",
    "description": "1.Op Introduction\r\n    It normalizes over local input regions. The local region is defined across the channels.\r\n    2.Math formula\r\n    ```math\r\n            output[i, j, k] = \\frac{input[i, j, k]}{(bias + \\alpha \\sum_{c=\\max(0,k-\\text{size})}^{\\min(N-1,k+\\text{size})} input[i, j, c]^2)^{\\beta}}\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    table(w.): store a lookup table that may assist in optimizing the matching process.;\r\n    mantissa(w.): store the mantissa values, which can be part of the normalization process.;\r\n\r\n    4.attributes\r\n    size: how many neighboring channels are considered during the normalization process.;\r\n    alpha: a scaling factor;\r\n    beta: a scaling factor;\r\n    bias: A floating-point value added to the normalization denominator to prevent division by zero.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "table", "type": "AnyTensorOrNone" },
      { "name": "mantissa", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "size", "type": "I64Attr" },
      { "name": "alpha", "type": "DefaultValuedAttr" },
      { "name": "beta", "type": "DefaultValuedAttr" },
      { "name": "bias", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.LSTM",
    "summary": "LSTM operator",
    "description": "1.Op Introduction\r\n    Perform RNN LSTM operation.\r\n\r\n    2.Math formula\r\n    ```math\r\n        forget gate(f_t):\r\n            f_t = Sigma(W_f · x_t + U_f · h_(t-1) + b_f)\r\n        input gate(i_t):\r\n            i_t = Sigma(W_i · x_t + U_i · h_(t-1) + b_i)\r\n        Candidate cell state(C_t):\r\n            C_t = tanh(W_C · x_t + U_C · h_(t-1) + b_C)\r\n        cell state update(c_t):\r\n            c_t = f_t \\odot c_(t-1) + i_t \\odot C_t\r\n        output gate(o_t):\r\n            o_t = Sigma(W_o · x_t + U_o · h_(t-1) + b_o)\r\n        hidden state output(h_t):\r\n            h_t = o_t \\odot tanh(c_t)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    filter(w.): the learnable weights of the convolution 2d operation.;\r\n    recurrence(w.): the previous hidden state influences the current hidden state.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n    initial_h(w.): the initial hidden state, which can be provided to start the LSTM computation.;\r\n    initial_c(w.): the initial cell state, which can be provided to start the LSTM computation.;\r\n    cont(w.): control weights or additional context that may be provided to influence the LSTM's behavior.;\r\n    buffer(w.): temporary storage of intermediate results or states during the LSTM computation.;\r\n    sigmoid_table: a lookup table that contains pre-computed values for the sigmoid activation function.;\r\n    sigmoid_slope_table: This table contains pre-computed slopes (derivatives) of the sigmoid function, enhancing the training efficiency of the LSTM.;\r\n    tanh_table: contains pre-computed values for the hyperbolic tangent (tanh) activation function.;\r\n    tanh_slope_table: This table contains pre-computed slopes (derivatives) of the tanh function;\r\n\r\n    4.attributes\r\n    hidden_size: the number of units in the LSTM cell, ;\r\n    bidirectional: A boolean indicating whether the LSTM should be bidirectional;\r\n    batch_first: the input and output tensors are provided in the shape (batch_size, seq_length, input_size).;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "filter", "type": "AnyTensorOrNone" },
      { "name": "recurrence", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" },
      { "name": "initial_h", "type": "AnyTensorOrNone" },
      { "name": "initial_c", "type": "AnyTensorOrNone" },
      { "name": "cont", "type": "AnyTensorOrNone" },
      { "name": "buffer", "type": "AnyTensorOrNone" },
      { "name": "sigmoid_table", "type": "AnyTensorOrNone" },
      { "name": "sigmoid_slope_table", "type": "AnyTensorOrNone" },
      { "name": "tanh_table", "type": "AnyTensorOrNone" },
      { "name": "tanh_slope_table", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "Y", "type": "AnyTensorOrNone" },
      { "name": "Y_h", "type": "AnyTensorOrNone" },
      { "name": "Y_c", "type": "AnyTensorOrNone" }
    ],
    "attributes": [
      { "name": "hidden_size", "type": "I64Attr" },
      { "name": "bidirectional", "type": "BoolAttr" },
      { "name": "batch_first", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Lut",
    "summary": "Lut operator",
    "description": "1.Op Introduction\r\n    lookup table in index [0-255], y[i] = table(x[i])\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = table(input[i])\r\n    ```\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    table(w.): map input values to corresponding output values.;\r\n\r\n    4.attributes\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "table", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.LutBF16",
    "summary": "LutBF16 operator",
    "description": "1.Op Introduction\r\n    input and output is BF16, input BF16 split as exponent and mantissa,\r\n    get output by exponent table and mantissa table\r\n    BF16 mode, lookup table in index [0-255], y[i] = table(x[i]).\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = table(input[i])\r\n    ```\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    table(w.): map input values to corresponding output values.;\r\n    mantissa(w.): the mantissa component of the lookup table weights.;\r\n\r\n    4.attributes\r\n    max_range: the maximum value in the range of valid input values.;\r\n    min_range: the minimum value in the range of valid input values.;\r\n    lut_mode: the mode of operation for the lookup table.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "table", "type": "AnyRankedTensor" },
      { "name": "mantissa", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "max_range", "type": "DefaultValuedAttr" },
      { "name": "min_range", "type": "DefaultValuedAttr" },
      { "name": "lut_mode", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.MaskedFill",
    "summary": "MaskedFill operator",
    "description": "1.Op Introduction\r\n    Return elements, either from X or Const, depending on condition.\r\n\r\n    2.Math formula\r\n    ```math\r\n                     brn                if inversed and cond=0\r\n            output = brn + const_val    if inversed and cond!=0\r\n                     brn + const_val    if !inversed and cond!=0\r\n                     brn                if !inversed and cond=0\r\n    ```\r\n        If inversed is true, the operation fills the elements of brn where cond is zero with const_val, while leaving other elements unchanged.\r\n        If inversed is false, the operation fills the elements of brn where cond is non-zero with const_val, while leaving other elements unchanged.\r\n\r\n    3.activation and weight\r\n    cond(act.): a tensor that serves as the condition for selecting elements from the true branch (tbrn) or the false branch (fbrn).;\r\n    brn(w.): the input tensor from which elements will be selected based on the condition provided by the cond tensor.;\r\n\r\n    4.attributes\r\n    inversed: whether the mask should be inverted.;\r\n    const_val: specifies the constant value to be added to each element of the input tensor(positive, negative, or zero).;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "cond", "type": "AnyRankedTensor" },
      { "name": "brn", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "inversed", "type": "BoolAttr" },
      { "name": "const_val", "type": "F64Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.MaskRCNN_BboxPooler",
    "summary": "Bbox_Pooler gen by PPL",
    "description": "1.Op Introduction\r\n    MaskRCNN_BBox_Pooler, the 1st ROIAlign in MaskRCNN.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = ROIAlign(feature map, rois_multi_batch)\r\n    ```\r\n\r\n    3.activation and weight\r\n    ptr_feat0(act.): Pointer to the feature map at level 0.;\r\n    ptr_feat1(act.): Pointer to the feature map at level 1.;\r\n    ptr_feat2(act.): Pointer to the feature map at level 2.;\r\n    ptr_feat3(act.): Pointer to the feature map at level 3.;\r\n    rois_multi_batch(w.): ROIs (Regions of Interest) for multiple batches.;\r\n\r\n    4.attributes\r\n    ROI_NUM_LEVELS: The number of levels in the ROI feature pyramid.;\r\n    ROI_H: The height of the pooled ROI features.;\r\n    ROI_W: The width of the pooled ROI features.;\r\n    CHANNEL_ROI: The number of channels in the pooled ROI features.;\r\n    ROI_SLICE: The number of slices or segments;\r\n    ROI_PH: The height of the ROI in the feature map.;\r\n    ROI_PW: The width of the ROI in the feature map.;\r\n    ROI_LEN: The length of the ROIs being processed.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "ptr_feat0", "type": "AnyTensor" },
      { "name": "ptr_feat1", "type": "AnyTensor" },
      { "name": "ptr_feat2", "type": "AnyTensor" },
      { "name": "ptr_feat3", "type": "AnyTensor" },
      { "name": "rois_multi_batch", "type": "AnyTensor" },
      { "name": "ptr_tmp_res", "type": "AnyTensorOrNone" },
      { "name": "ptr_rois_tmp", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "result_res", "type": "AnyTensor" },
      { "name": "result_rois", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "ROI_NUM_LEVELS", "type": "I64Attr" },
      { "name": "ROI_H", "type": "I64Attr" },
      { "name": "ROI_W", "type": "I64Attr" },
      { "name": "CHANNEL_ROI", "type": "I64Attr" },
      { "name": "ROI_SLICE", "type": "I64Attr" },
      { "name": "ROI_PH", "type": "I64Attr" },
      { "name": "ROI_PW", "type": "I64Attr" },
      { "name": "ROI_LEN", "type": "I64Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.MaskRCNN_GetBboxB",
    "summary": "GetBboxB operator",
    "description": "1.Op Introduction\r\n     MaskRCNN_GetBboxB, the 2nd onnx_nms in MaskRCNN\r\n\r\n    2.Math formula\r\n    ```math\r\n            decode_bbox_i = bbox_means + scale_factor x (ptr_bbox[i] x bbox_stds + [res_bbox0[i] res_bbox1[i] res_bbox[i]])\r\n            score_i = ptr_score[i] (or a combination of res_score0,res_score1,res_score2,res_score3)\r\n            {bboxs, lables} = NMS({decode_bbox_i}, {score_i}, nms_iou_thr)\r\n    ```\r\n\r\n    3.activation and weight\r\n    ptr_rois(act.): candidate regions of interest for possible objects.;\r\n    ptr_bbox(act.): the bounding box predictions.;\r\n    ptr_score(act.): confidence scores associated with each proposal.;\r\n    max_val(w.): max scores the bbox predictions.;\r\n    scale_factor(w.): scale the decoded bounding box coordinates.;\r\n    stds(w.): standard deviation values for each channel.;\r\n    means(w.): mean values to subtract from each channel for normalization.;\r\n    res_bbox(w.): bounding box prediction values.;\r\n    res_bbox1(w.): one branch of the processed bounding box predictions.;\r\n    res_bbox0(w.): another branch or variant of the bounding box decoding results.;\r\n    res_score0(w.): one set of confidence scores associated with the proposals.;\r\n    res_score1(w.): an additional set of confidence scores.;\r\n    res_score2(w.): another set of scoring values.;\r\n    res_score3(w.):  final selection of bounding boxes.;\r\n    res_label2(w.): the predicted class labels.;\r\n    result_list(w.): the final list of processed results or proposal indices after filtering and ranking.;\r\n    keep_3nch(w.): whether retains proposals having three-channel inputs.;\r\n    keep_u32_1h(w.): whether using a single 32-bit unsigned integer per element.;\r\n    glb_buffer_boxes(w.): A global temporary buffer used to store bounding box.;\r\n    glb_buffer_scores(w.): A global temporary buffer for storing scores associated with the candidate bounding boxes.;\r\n    glb_buffer_nms(w.): This global buffer holds intermediate NMS-related information.;\r\n    glb_buffer_nonzero(w.): stores the non-zero elements (or valid indices) from processed tensors.;\r\n    result_valid_ind(w.): the indices of proposals.;\r\n    glb_lables(w.): A global buffer dedicated to storing the class labels.;\r\n    glb_lables_expand(w.): an expanded version of the global labels.;\r\n\r\n    4.attributes\r\n    threshold_score_eq: A threshold value used to filter out proposals with a low confidence score before applying NMS.;\r\n    wh_ratio_log: A logarithmic scaling factor, adjust the width-to-height ratio during decoding of bounding boxes.;\r\n    nms_iou_thr: IoU (Intersection over Union) threshold.;\r\n    delta2bbox_means: Mean values used to decode the bounding box regression.;\r\n    delta2bbox_stds_0: Standard deviation (first component) for scaling the decoded bbox values.;\r\n    delta2bbox_stds_1: Standard deviation (second component) for scaling the decoded bbox values.;\r\n    NUM_INDEXES: the number of indexes (or anchors).;\r\n    NUM_CLASSES: The total number of object classes.;\r\n    TOPK_ONNX_NMS: to select a fixed number of candidates.;\r\n    NUM_CLASSES_getBboxB: Number of classes used in this bounding box decoding step.;\r\n    MAX_NMS_LENGTH_GetBboxB: Maximum number of bounding box candidates.;\r\n    MAX_PER_IMG: The maximum number of detections allowed per image.;\r\n    MAX_PER_IMG_GetBboxB: maximum number of bounding boxes after the final processing.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "ptr_rois", "type": "AnyTensor" },
      { "name": "ptr_bbox", "type": "AnyTensor" },
      { "name": "ptr_score", "type": "AnyTensor" },
      { "name": "max_val", "type": "AnyTensor" },
      { "name": "scale_factor", "type": "AnyTensor" },
      { "name": "means", "type": "AnyTensorOrNone" },
      { "name": "stds", "type": "AnyTensorOrNone" },
      { "name": "res_bbox", "type": "AnyTensorOrNone" },
      { "name": "res_bbox1", "type": "AnyTensorOrNone" },
      { "name": "res_bbox0", "type": "AnyTensorOrNone" },
      { "name": "res_score0", "type": "AnyTensorOrNone" },
      { "name": "res_score1", "type": "AnyTensorOrNone" },
      { "name": "res_score2", "type": "AnyTensorOrNone" },
      { "name": "res_score3", "type": "AnyTensorOrNone" },
      { "name": "res_label2", "type": "AnyTensorOrNone" },
      { "name": "result_list", "type": "AnyTensorOrNone" },
      { "name": "keep_3nch", "type": "AnyTensorOrNone" },
      { "name": "keep_u32_1h", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_boxes", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_scores", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_nms", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_nonzero", "type": "AnyTensorOrNone" },
      { "name": "result_valid_ind", "type": "AnyTensorOrNone" },
      { "name": "glb_lables", "type": "AnyTensorOrNone" },
      { "name": "glb_lables_expand", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "result_det_bboxes", "type": "AnyTensor" },
      { "name": "result_det_labels", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "threshold_score_eq", "type": "F64Attr" },
      { "name": "wh_ratio_log", "type": "F64Attr" },
      { "name": "nms_iou_thr", "type": "F64Attr" },
      { "name": "delta2bbox_means", "type": "F64Attr" },
      { "name": "delta2bbox_stds_0", "type": "F64Attr" },
      { "name": "delta2bbox_stds_1", "type": "F64Attr" },
      { "name": "NUM_INDEXES", "type": "I64Attr" },
      { "name": "NUM_CLASSES", "type": "I64Attr" },
      { "name": "TOPK_ONNX_NMS", "type": "I64Attr" },
      { "name": "NUM_CLASSES_getBboxB", "type": "I64Attr" },
      { "name": "MAX_NMS_LENGTH_GetBboxB", "type": "I64Attr" },
      { "name": "MAX_PER_IMG", "type": "I64Attr" },
      { "name": "MAX_PER_IMG_GetBboxB", "type": "I64Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.MaskRCNN_MaskPooler",
    "summary": "Mask_Pooler gen by PPL",
    "description": "1.Op Introduction\r\n    MaskRCNN_Mask_Pooler, the 2st ROIAlign in MaskRCNN\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = \\text{ROIAlign}\\Bigl( \\{x_i\\}_{i=0}^3, \\, \\text{det\\_bboxes\\_multi\\_batch}, \\, \\text{det\\_labels\\_multi\\_batch}, \\, \\text{scale\\_factor}, \\, ROI\\_NUM\\_LEVELS, \\, ROI\\_H, \\, ROI\\_W, \\, CHANNEL\\_ROI, \\, ROI\\_SLICE, \\, ROI\\_PH, \\, ROI\\_PW, \\, ROI\\_LEN \\Bigr)\r\n    ```\r\n\r\n    3.activation and weight\r\n    x_0(act.): first level of the feature pyramid.;\r\n    x_1(act.): second level of the feature pyramid.;\r\n    x_2(act.): third level of the feature pyramid.;\r\n    x_3(act.): fourth level of the feature pyramid.;\r\n    det_bboxes_multi_batch(act.): detected bounding boxes over multiple batches.;\r\n    det_labels_multi_batch(act.): class labels associated with the detected bounding boxes across multiple batches.;\r\n    scale_factor(w.): scale the decoded bounding box coordinates.;\r\n    ptr_rois_buff(w.): a temporary buffer that holds precomputed or intermediate Regions of Interest (ROIs).;\r\n    result_filled_det_bboxes(w.): stores the processed or “filled” detection bounding boxes.;\r\n    result_filled_det_labels(w.): corresponding class labels that have been associated with the detected bounding boxes.;\r\n    ptr_tmp_res(w.):  temporary result buffer used internally during the pooling process.;\r\n    ptr_rois_tmp(w.): a temporary workspace that stores intermediate ROI values.;\r\n\r\n    4.attributes\r\n    ROI_NUM_LEVELS: the number of feature levels (or pyramid levels) available for ROI pooling.;\r\n    ROI_H: target height of the pooled region for each ROI.;\r\n    ROI_W: target width of the pooled region for each ROI.;\r\n    CHANNEL_ROI: number of channels to be kept or considered when performing ROIAlign.;\r\n    ROI_SLICE: slicing strategy for ROIs, if the ROI needs to be segmented into sub-regions for finer pooling.;\r\n    ROI_PH: Padding height for the ROI.;\r\n    ROI_PW: Padding width for the ROI.;\r\n    ROI_LEN: total length (or area) of the ROI.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "x_0", "type": "AnyTensor" },
      { "name": "x_1", "type": "AnyTensor" },
      { "name": "x_2", "type": "AnyTensor" },
      { "name": "x_3", "type": "AnyTensor" },
      { "name": "det_bboxes_multi_batch", "type": "AnyTensor" },
      { "name": "det_labels_multi_batch", "type": "AnyTensor" },
      { "name": "scale_factor", "type": "AnyTensor" },
      { "name": "ptr_rois_buff", "type": "AnyTensorOrNone" },
      { "name": "result_filled_det_bboxes", "type": "AnyTensorOrNone" },
      { "name": "result_filled_det_labels", "type": "AnyTensorOrNone" },
      { "name": "ptr_tmp_res", "type": "AnyTensorOrNone" },
      { "name": "ptr_rois_tmp", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "result_res", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "ROI_NUM_LEVELS", "type": "I64Attr" },
      { "name": "ROI_H", "type": "I64Attr" },
      { "name": "ROI_W", "type": "I64Attr" },
      { "name": "CHANNEL_ROI", "type": "I64Attr" },
      { "name": "ROI_SLICE", "type": "I64Attr" },
      { "name": "ROI_PH", "type": "I64Attr" },
      { "name": "ROI_PW", "type": "I64Attr" },
      { "name": "ROI_LEN", "type": "I64Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.MaskRCNN_RPNGetBboxes",
    "summary": "RPN_get_bboxes gen by PPL",
    "description": "1.Op Introduction\r\n    MaskRCNN_RPN_get_bboxes, the sub-block with 1st NMS between RPN_head and 1st ROIAlign.\r\n\r\n    2.Math formula\r\n    ```math\r\n        1.Score Filtering\r\n            valid_indices_i = cls_scores_i > conf_threshold (for each level i)\r\n        2.Bounding Box Adjustment\r\n            adjusted_bboxes_i = anchors_i + bbox_preds_i * delta2bbox_std_i + delta2bbox_mean_i\r\n        3.IoU Calculation\r\n            iou = calculate_iou(adjusted_bboxes_i, ground_truth_boxes)\r\n        4.NMS Application\r\n            final_bboxes = nms(adjusted_bboxes_i[valid_indices_i], iou_threshold)\r\n        5.final output\r\n            output = concatenate(final_bboxes)\r\n    ```\r\n    3.activation and weight\r\n    cls_scores_0: the class scores0 for each anchor;\r\n    cls_scores_1(act.): the class scores1 for each anchor;\r\n    cls_scores_2: the class scores2 for each anchor;\r\n    cls_scores_3: the class scores3 for each anchor;\r\n    cls_scores_4: the class scores4 for each anchor;\r\n    bbox_preds_0: the bounding box0 predictions;\r\n    bbox_preds_1: the bounding box1 predictions;\r\n    bbox_preds_2: the bounding box2 predictions;\r\n    bbox_preds_3: the bounding box3 predictions;\r\n    bbox_preds_4: the bounding box4 predictions;\r\n    max_shape: the maximum dimensions of the output bounding boxes.;\r\n    mlvl_anchors_0: the multi-level anchors0 used for generating bounding box proposals.;\r\n    mlvl_anchors_1: the multi-level anchors1 used for generating bounding box proposals.;\r\n    mlvl_anchors_2: the multi-level anchors2 used for generating bounding box proposals.;\r\n    mlvl_anchors_3: the multi-level anchors3 used for generating bounding box proposals.;\r\n    mlvl_anchors_4: the multi-level anchors4 used for generating bounding box proposals.;\r\n    batch_mlvl_scores: the class scores for all anchors across multiple feature levels in a batch.;\r\n    batch_mlvl_anchors: the multi-level anchors generated for the entire batch.;\r\n    batch_mlvl_rpn_bbox_pred: the bounding box predictions for each anchor across multiple levels in the batch.;\r\n    batch_mlvl_proposals: the proposed bounding boxes generated for the entire batch after processing;\r\n    batch_mlvl_ids: the ids associated with the proposals for each anchor across the batch.;\r\n    glb_buffer_tmp_scores_stretched: the stretched class scores for processing before NMS.;\r\n    glb_buffer_ranked_scores: The buffer stores the ranked class scores after sorting the top proposals.;\r\n    glb_buffer_rank_inds_int32: the indices of the ranked scores in 32-bit integer format;\r\n    glb_buffer_rank_inds_u32: stores the indices in 32-bit unsigned integer format.;\r\n    glb_topk_inds: This buffer holds the indices of the top K proposals after the ranking process.;\r\n    glb_buffer_gather_1: gathering specific data from the ranked scores or proposals.;\r\n    glb_buffer_gather_2: Similar to glb_buffer_gather_1;\r\n    glb_buffer_rpn_bbox_permuted: the permuted bounding box predictions.;\r\n    glb_buffer_nonzero: non-zero entries from the scores or proposals.;\r\n    result_valid_ind: the valid indices of the resulting proposals after processing.;\r\n    glb_buffer_gather_boxes: gather the final bounding boxes from the proposals after NMS.;\r\n    glb_buffer_gather_scores: holds the final scores associated with the gathered bounding boxes.;\r\n    keep_3nch: maintain a specific format or structure for the gathered results(three-channel format).;\r\n    keep_u32_1h: manage the format or structure of the output in a specific way.;\r\n    glb_buffer_boxes: the final bounding boxes ready for output after all processing steps.;\r\n    glb_buffer_scores: the final class scores corresponding to the output bounding boxes.;\r\n    glb_buffer_nms: This buffer is used during the Non-Maximum Suppression process to manage overlapping proposals.;\r\n    gather_mlvl_proposals: gather proposals from multiple levels for final processing.;\r\n    gather_mlvl_scores: gather scores from multiple levels for the final proposal selection.;\r\n    gather_mlvl_ids: gather ids from multiple levels to track proposals across the batch.;\r\n    glb_buffer_result_list: the final list of results.;\r\n\r\n    4.attribute\r\n    delta2bbox_mean_0: the means used to normalize the bounding box0 deltas for the corresponding feature levels.;\r\n    delta2bbox_mean_1: the means used to normalize the bounding box1 deltas for the corresponding feature levels.;\r\n    delta2bbox_mean_2: the means used to normalize the bounding box2 deltas for the corresponding feature levels.;\r\n    delta2bbox_mean_3: the means used to normalize the bounding box3 deltas for the corresponding feature levels.;\r\n    delta2bbox_std_0: the standard deviations used to normalize the bounding box0 deltas for the corresponding feature levels.;\r\n    delta2bbox_std_1: the standard deviations used to normalize the bounding box1 deltas for the corresponding feature levels.;\r\n    delta2bbox_std_2: the standard deviations used to normalize the bounding box2 deltas for the corresponding feature levels.;\r\n    delta2bbox_std_3: the standard deviations used to normalize the bounding box3 deltas for the corresponding feature levels.;\r\n    delta2bbox_max_scalar_c: a scalar value;\r\n    iou_threshold: filtering out low-quality proposals during NMS.;\r\n    conf_threshold: a confidence score threshold.;\r\n    MAX_LENGTH_STATIC_STRECHED: the maximum length for the output list of bounding boxes after processing.;\r\n    NUM_INDEXES: the number of indexes.;\r\n    NUM_CLASSES: the number of classes.;\r\n    CHANNEL_RPN_BBOXES: the number of channels for the bounding box predictions.;\r\n    CHANNEL_RPN_SCORES: the number of channels used for the class score predictions.;\r\n    NMS_PRE: the number of proposals to be considered before NMS.;\r\n    HARDWARE_FACTOR_TOPK: how many top proposals to keep.;\r\n    NMS_MAX_LENGTH: the maximum number of boxes after NMS.;\r\n    TOPK_ONNX_NMS: the number of top proposals when using ONNX format for NMS.;\r\n    H_RPN_DYN_MAX: the maximum height for the dynamic RPN output.;\r\n    W_RPN_DYN_MAX: the maximum width for the dynamic RPN output.;\r\n    MAX_PER_IMG: the maximum number of proposals to be generated per image.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "cls_scores_0", "type": "AnyTensor" },
      { "name": "cls_scores_1", "type": "AnyTensor" },
      { "name": "cls_scores_2", "type": "AnyTensor" },
      { "name": "cls_scores_3", "type": "AnyTensor" },
      { "name": "cls_scores_4", "type": "AnyTensor" },
      { "name": "bbox_preds_0", "type": "AnyTensor" },
      { "name": "bbox_preds_1", "type": "AnyTensor" },
      { "name": "bbox_preds_2", "type": "AnyTensor" },
      { "name": "bbox_preds_3", "type": "AnyTensor" },
      { "name": "bbox_preds_4", "type": "AnyTensor" },
      { "name": "max_shape", "type": "AnyTensor" },
      { "name": "mlvl_anchors_0", "type": "AnyTensor" },
      { "name": "mlvl_anchors_1", "type": "AnyTensor" },
      { "name": "mlvl_anchors_2", "type": "AnyTensor" },
      { "name": "mlvl_anchors_3", "type": "AnyTensor" },
      { "name": "mlvl_anchors_4", "type": "AnyTensor" },
      { "name": "batch_mlvl_scores", "type": "AnyTensorOrNone" },
      { "name": "batch_mlvl_anchors", "type": "AnyTensorOrNone" },
      { "name": "batch_mlvl_rpn_bbox_pred", "type": "AnyTensorOrNone" },
      { "name": "batch_mlvl_proposals", "type": "AnyTensorOrNone" },
      { "name": "batch_mlvl_ids", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_tmp_scores_stretched", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_ranked_scores", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_rank_inds_int32", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_rank_inds_u32", "type": "AnyTensorOrNone" },
      { "name": "glb_topk_inds", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_gather_1", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_gather_2", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_rpn_bbox_permuted", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_nonzero", "type": "AnyTensorOrNone" },
      { "name": "result_valid_ind", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_gather_boxes", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_gather_scores", "type": "AnyTensorOrNone" },
      { "name": "keep_3nch", "type": "AnyTensorOrNone" },
      { "name": "keep_u32_1h", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_boxes", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_scores", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_nms", "type": "AnyTensorOrNone" },
      { "name": "gather_mlvl_proposals", "type": "AnyTensorOrNone" },
      { "name": "gather_mlvl_scores", "type": "AnyTensorOrNone" },
      { "name": "gather_mlvl_ids", "type": "AnyTensorOrNone" },
      { "name": "glb_buffer_result_list", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "result_list", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "delta2bbox_mean_0", "type": "F64Attr" },
      { "name": "delta2bbox_mean_1", "type": "F64Attr" },
      { "name": "delta2bbox_mean_2", "type": "F64Attr" },
      { "name": "delta2bbox_mean_3", "type": "F64Attr" },
      { "name": "delta2bbox_std_0", "type": "F64Attr" },
      { "name": "delta2bbox_std_1", "type": "F64Attr" },
      { "name": "delta2bbox_std_2", "type": "F64Attr" },
      { "name": "delta2bbox_std_3", "type": "F64Attr" },
      { "name": "delta2bbox_max_scalar_c", "type": "F64Attr" },
      { "name": "iou_threshold", "type": "F64Attr" },
      { "name": "conf_threshold", "type": "F64Attr" },
      { "name": "MAX_LENGTH_STATIC_STRECHED", "type": "I64Attr" },
      { "name": "NUM_INDEXES", "type": "I64Attr" },
      { "name": "NUM_CLASSES", "type": "I64Attr" },
      { "name": "CHANNEL_RPN_BBOXES", "type": "I64Attr" },
      { "name": "CHANNEL_RPN_SCORES", "type": "I64Attr" },
      { "name": "NMS_PRE", "type": "I64Attr" },
      { "name": "HARDWARE_FACTOR_TOPK", "type": "I64Attr" },
      { "name": "NMS_MAX_LENGTH", "type": "I64Attr" },
      { "name": "TOPK_ONNX_NMS", "type": "I64Attr" },
      { "name": "H_RPN_DYN_MAX", "type": "I64Attr" },
      { "name": "W_RPN_DYN_MAX", "type": "I64Attr" },
      { "name": "MAX_PER_IMG", "type": "I64Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.MatchTemplate",
    "summary": "Opencv MatchTemplate operator",
    "description": "1.Op Introduction\r\n    Perform opencv MatchTemplate operation.\r\n\r\n    2.Math formula\r\n    ```math\r\n            R(x, y) = \\sum_{i=0}^{T_w-1} \\sum_{j=0}^{T_h-1} I(x+i, y+j) \\cdot T(i, j)\r\n    ```\r\n    where:\r\n    R(x, y) is the result of the match at position(x, y).\r\n    I is the input image.\r\n    T is the template image.\r\n    T_w and T_h are the width and height of the template.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor(source image).;\r\n    match(w.): the template image that will be matched against the input image.;\r\n    table(w.): store a lookup table that may assist in optimizing the matching process.;\r\n    mantissa_table(w.): stores a table of mantissa values used in calculations to improve precision or to handle specific numerical representations.;\r\n\r\n    4.attributes\r\n    mode: the method of template matching to be used (e.g., correlation, squared difference, etc.).;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "match", "type": "AnyTensorOrNone" },
      { "name": "table", "type": "AnyTensorOrNone" },
      { "name": "mantissa_table", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "MatchTemplateModeAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.MatMul",
    "summary": "matmul operator",
    "description": "1.Op Introduction\r\n    Performs a two dimensional matrix multiplication. This allows both inputs to\r\n    be activations, rather than reserving weights as an attribute in the\r\n    FULLY_CONNECTED operator.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = input x right + bias\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): the first input tensor;\r\n    right(act.): the second input tensor;\r\n    bias(w.): an optional tensor can be added to the result of the matrix multiplication. ;\r\n    multi(w.): used during the matrix multiplication operation.;\r\n    buffer(w.): temporary storage of intermediate results or states during computation.;\r\n\r\n    4.attribute\r\n    right_transpose: whether to transpose the right input tensor before performing the multiplication.;\r\n    left_transpose: whether to transpose the input tensor before performing the multiplication.;\r\n    output_transpose: whether to transpose the output tensor after the multiplication.;\r\n    hdim_is_batch: whether the first dimension of the input tensor represents the batch size.;\r\n    keep_dims: whether to keep the dimensions of the output tensor the same as the input tensors.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    multipliers: an integer array that specifies scaling factors to be applied to the results of the matrix multiplication.;\r\n    rshifts: an integer array that indicates the right shift values.;\r\n    right_zp: the zero point for the right input tensor.;\r\n    input_zp: the zero point for the left input tensor.;\r\n    quant_mode: It determines how the output tensor should be quantized.;\r\n    ginfo: associated with layer grouping information.;\r\n    multi_core: executed using multiple cores.;\r\n    fuse_rq: fuse the requantization step with the matrix multiplication operation.;\r\n    round_mode: the rounding mode to be used during the casting operation.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "right", "type": "AnyRankedTensor" },
      { "name": "bias", "type": "AnyTensorOrNone" },
      { "name": "multi", "type": "AnyTensorOrNone" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "left_transpose", "type": "DefaultValuedAttr" },
      { "name": "right_transpose", "type": "DefaultValuedAttr" },
      { "name": "output_transpose", "type": "DefaultValuedAttr" },
      { "name": "hdim_is_batch", "type": "DefaultValuedAttr" },
      { "name": "keep_dims", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "multipliers", "type": "DefaultValuedAttr" },
      { "name": "rshifts", "type": "DefaultValuedAttr" },
      { "name": "right_zp", "type": "DefaultValuedAttr" },
      { "name": "input_zp", "type": "DefaultValuedAttr" },
      { "name": "quant_mode", "type": "DefaultValuedAttr" },
      { "name": "left_reuse", "type": "DefaultValuedAttr" },
      { "name": "out_f8_scales", "type": "OptionalAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multi_core", "type": "OptionalAttr" },
      { "name": "fuse_rq", "type": "DefaultValuedAttr" },
      { "name": "weight_bits", "type": "OptionalAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.MatMulLut",
    "summary": "MatMul + Lut",
    "description": "MatMul + Lut",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "table", "type": "AnyRankedTensor" },
      { "name": "right", "type": "AnyRankedTensor" },
      { "name": "bias", "type": "AnyTensorOrNone" },
      { "name": "multi", "type": "AnyTensorOrNone" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "left_transpose", "type": "DefaultValuedAttr" },
      { "name": "right_transpose", "type": "DefaultValuedAttr" },
      { "name": "output_transpose", "type": "DefaultValuedAttr" },
      { "name": "hdim_is_batch", "type": "DefaultValuedAttr" },
      { "name": "keep_dims", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "multipliers", "type": "DefaultValuedAttr" },
      { "name": "rshifts", "type": "DefaultValuedAttr" },
      { "name": "right_zp", "type": "DefaultValuedAttr" },
      { "name": "input_zp", "type": "DefaultValuedAttr" },
      { "name": "quant_mode", "type": "DefaultValuedAttr" },
      { "name": "left_reuse", "type": "DefaultValuedAttr" },
      { "name": "out_f8_scales", "type": "OptionalAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multi_core", "type": "OptionalAttr" },
      { "name": "fuse_rq", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Max",
    "summary": "max operator",
    "description": "1.Op Introduction\r\n    Elementwise max of input1 and input2. All inputs and outputs must have the same data type.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = max(input1,input2, ..., inputN)\r\n    ```\r\n    Where input1, input2, ..., inputN are the input tensors.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    coeff: It is an array and allows for scaling the output of the addition operation.;\r\n    // quant param\r\n    multipliers: applied during the concatenation process to adjust the values of the input tensors.;\r\n    rshifts: an array of right shift values corresponding to each input tensor.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "coeff", "type": "OptionalAttr" },
      { "name": "multipliers", "type": "OptionalAttr" },
      { "name": "rshifts", "type": "OptionalAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.MaxConst",
    "summary": "max_const operator",
    "description": "1.Op Introduction\r\n    max of one input and one const.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = Max(input, const_val)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    const_val: the constant value to be added to each element of the input tensor(positive, negative, or zero).;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    // quant param\r\n    multipliers: applied during the concatenation process to adjust the values of the input tensors.;\r\n    rshifts: an array of right shift values corresponding to each input tensor.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "const_val", "type": "F64Attr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "multiplier", "type": "DefaultValuedAttr" },
      { "name": "rshift", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.MaxPoolingIndicesBwd",
    "summary": "MaxPoolingIndicesBwd operator",
    "description": "1.Op Introduction\r\n    Integrates some operations related to MaxPoolingIndicesBwd.\r\n\r\n    2.Math formula\r\n    ```math\r\n          grad\\_input(p) = \\sum_{q} \\delta\\big(p = indices[q]\\big) \\times grad\\_output(q)\r\n    ```\r\n\r\n    3.activation and weight\r\n    grad_output(act.): the gradient of the loss with respect to the output.;\r\n    indices(w.): the indices of the input tokens or items.;\r\n\r\n    4.attributes\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    strides: the stride for the cross-correlation, a single number or a tuple.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    dilations: controls the spacing between the kernel points;\r\n    input_shape: The shape of the input tensor.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "grad_output", "type": "AnyTensor" },
      { "name": "indices", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "grad_input", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "dilations", "type": "I64ArrayAttr" },
      { "name": "input_shape", "type": "I64ArrayAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.MaxPoolWithMask",
    "summary": "max pool with operator",
    "description": "1.Op Introduction\r\n    This performs an  max pooling over the given input tensor. A sliding\r\n    window of size given by <kernel size> is passed over the input tensor.\r\n    get output tensor and mask tensor\r\n\r\n    2.Math formula\r\n    ```math\r\n        output(N, C, H', W') = MaxPool(max(input(N, C, H, W)))\r\n        maskOutput(N, C, H', W') = MaxPoolWithMask(argmax(input(N, C, H, W)))\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    strides: the stride for the cross-correlation, a single number or a tuple.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    layer_group: multiple layers to be processed together or treated as a single unit during computation.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" },
      { "name": "mask", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "layer_group", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.MaxUnpool",
    "summary": "MaxUnpool operation",
    "description": "1.Op Introduction\r\n    Perform MaxUnpool on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output1(N, C, H', W') = input(N, C, H, W) #if (h,w) is the index of the max value in the mask, otherwise is 0;\r\n        output = scale * output1\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    mask(act.): place the pooled values in the output tensor.;\r\n\r\n    4.attribute\r\n    scale_h: scaling factor for the height dimension of the output tensor.;\r\n    scale_w: scaling factor for the width dimension of the output tensor.;\r\n    ginfo: group information for layer grouping;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "mask", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "scale_h", "type": "I64Attr" },
      { "name": "scale_w", "type": "I64Attr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.MeanRstd",
    "summary": "Compute Mean, Rstd, Running_mean, Running_var in batchnorm train op",
    "description": "1.Op Introduction\r\n    computes the mean, reverse standard deviation (Rstd), running mean, and running variance.\r\n\r\n    2.Math formula\r\n    ```math\r\n        mean(x),1/sqrt(var+eps),(1-momentum)*running_mean + momentum*mean,(1-momentum)*running_var + momentum*var\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    running_mean(w.): mean during running.;\r\n    running_var(w.): variances during running.;\r\n    weight(w.): weight tensor.;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n\r\n    4.attributes\r\n    eps: a small constant added to the denominator during normalization to prevent division by zero.;\r\n    momentum: hyperparameter;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "running_mean", "type": "AnyTensor" },
      { "name": "running_var", "type": "AnyTensor" },
      { "name": "weight", "type": "AnyTensor" },
      { "name": "bias", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "mean", "type": "AnyTensor" },
      { "name": "rstd", "type": "AnyTensor" },
      { "name": "running_mean_update", "type": "AnyTensor" },
      { "name": "running_var_update", "type": "AnyTensor" },
      { "name": "scale", "type": "AnyTensor" },
      { "name": "bias_new", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "eps", "type": "F64Attr" },
      { "name": "momentum", "type": "F64Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.MeanStdScale",
    "summary": "MeanStdScale, it's for preprocess.",
    "description": "1.Op Introduction\r\n    for preprocess, do multiplier&rshift quantize.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = (input - mean) / std * scale + zero_points\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    f32_param(w.): a weight tensor parameter of type float32.;\r\n\r\n    4.attributes\r\n    quant_mode: the mode or method used for quantization during the requantization operation.;\r\n    customization_format: custom format for the input data.;\r\n    channel_order: The order of color channels in the input tensor.;\r\n    sign: if output is signed.;\r\n    scale: scalar.;\r\n    std: standard deviation values for each channel.;\r\n    mean: mean values to subtract from each channel for normalization.;\r\n    zero_points: zero point values for each channel.;\r\n    resize_dims: resize the input tensor' dimensions.;\r\n    multi: used during the matrix multiplication operation.;\r\n    rshift: right shift values corresponding to each input tensor.;\r\n    offset: Scalar;\r\n    rounding_mode: The rounding method to use during quantization.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "f32_param", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "quant_mode", "type": "StrAttr" },
      { "name": "customization_format", "type": "StrAttr" },
      { "name": "channel_order", "type": "StrAttr" },
      { "name": "sign", "type": "DefaultValuedAttr" },
      { "name": "scale", "type": "F64ArrayAttr" },
      { "name": "std", "type": "F64ArrayAttr" },
      { "name": "mean", "type": "F64ArrayAttr" },
      { "name": "zero_points", "type": "F64ArrayAttr" },
      { "name": "resize_dims", "type": "I64ArrayAttr" },
      { "name": "multi", "type": "I32ArrayAttr" },
      { "name": "rshift", "type": "I32ArrayAttr" },
      { "name": "offset", "type": "I32ArrayAttr" },
      { "name": "rounding_mode", "type": "StrAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Min",
    "summary": "min operator",
    "description": "1.Op Introduction\r\n    Elementwise min of input1 and input2. All inputs and outputs must have the same data type.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = min(input1,input2, ..., inputN)\r\n    ```\r\n    Where input1, input2, ..., inputN are the input tensors.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    coeff: It is an array and allows for scaling the output of the addition operation.;\r\n    // quant param\r\n    multipliers: applied during the concatenation process to adjust the values of the input tensors.;\r\n    rshifts: an array of right shift values corresponding to each input tensor.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "coeff", "type": "OptionalAttr" },
      { "name": "multipliers", "type": "OptionalAttr" },
      { "name": "rshifts", "type": "OptionalAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.MinConst",
    "summary": "min_const operator",
    "description": "1.Op Introduction\r\n    min of one input and one const.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = Min(input, const_val)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    const_val: the constant value to be added to each element of the input tensor(positive, negative, or zero).;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    // quant param\r\n    multipliers: applied during the concatenation process to adjust the values of the input tensors.;\r\n    rshifts: an array of right shift values corresponding to each input tensor.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "const_val", "type": "F64Attr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "multiplier", "type": "DefaultValuedAttr" },
      { "name": "rshift", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Mmap2Rgbmap",
    "summary": "Postprocess for isp mmap2rgbmap.",
    "description": "1.Op Introduction\r\n    Postprocess for isp mmap2rgbmap.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = Postprocess(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Move",
    "summary": "local mem move op for layer group lmem alloc",
    "description": "1.Op Introduction\r\n    local mem move op for layer group lmem alloc\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = Move(input, move_src_add, move_dest_add, move_size, ts_id)\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;\r\n\r\n    4.attributes\r\n    name: a human-readable identifier for the move operation.;\r\n    move_src_add: source addresses (offsets) in the local memory.;\r\n    move_dest_add: destination addresses (offsets) in the local memory.;\r\n    move_size: A list of sizes corresponding to each move operation.;\r\n    ts_id: A unique identifier (timestamp id or task id);",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" },
      { "name": "move_src_add", "type": "I64ArrayAttr" },
      { "name": "move_dest_add", "type": "I64ArrayAttr" },
      { "name": "move_size", "type": "I64ArrayAttr" },
      { "name": "ts_id", "type": "I64Attr" }
    ]
  },
  {
    "name": "tpu.Mul",
    "summary": "mul operator",
    "description": "1.Op Introduction\r\n    Elementwise multiplication of input1 and input2. input1 and input2 are tensors.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = ReLU((input1 * input2))\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    // quant param\r\n    multipliers: applied during the concatenation process to adjust the values of the input tensors.;\r\n    rshifts: an array of right shift values corresponding to each input tensor.;\r\n    quant_mode: It determines how the output tensor should be quantized.;\r\n    out_f8_scales: This parameter contains the scaling factors for the output in FP8 (8-bit floating point) format.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "multiplier", "type": "DefaultValuedAttr" },
      { "name": "rshift", "type": "DefaultValuedAttr" },
      { "name": "quant_mode", "type": "DefaultValuedAttr" },
      { "name": "out_f8_scales", "type": "OptionalAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.MulConst",
    "summary": "mul const operator",
    "description": "1.Op Introduction\r\n    Elementwise mul of input1 and input2. Input2 is constant.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = input * const_val\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    const_val: the constant value to be added to each element of the input tensor(positive, negative, or zero).;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    // quant param\r\n    multipliers: applied during the concatenation process to adjust the values of the input tensors.;\r\n    rshifts: an array of right shift values corresponding to each input tensor.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "const_val", "type": "F64Attr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "multiplier", "type": "DefaultValuedAttr" },
      { "name": "rshift", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.MulShift",
    "summary": "MulShift operator",
    "description": "1.Op Introduction\r\n    performs an element-wise multiplication of the input tensor (after adjusting for a zero-point offset)\r\n\r\n    2.Math formula\r\n        output = int8(input-zx) * multiplier >> rshift + zy\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    multiplier: Floating-point multiplication operations are usually converted to fixed-point multiplication operations.;\r\n    rshift: the number of bits to right-shift the quantized values.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "multiplier", "type": "SI32Attr" },
      { "name": "rshift", "type": "SI32Attr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Nms",
    "summary": "NMS operator",
    "description": "1.Op Introduction\r\n    tpu nms\r\n\r\n    2.Math formula\r\n    ```math\r\n        IOU(a, b) = Area(a \\cap b) / Area(a \\cup b)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): Variadic input tensor.;\r\n    buffer(w.): a temporary storage area for intermediate calculations or results during the NMS process.;\r\n\r\n    4.attributes\r\n    center_point_box: whether the bounding boxes are defined by their center points.;\r\n    max_output_size: the maximum number of boxes to be output after NMS.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "center_point_box", "type": "I64Attr" },
      { "name": "max_output_size", "type": "I64Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.NonZero",
    "summary": "NonZero operation",
    "description": "1.Op Introduction\r\n    Returns the indices of the elements that are non-zero\r\n    (in row-major order - by dimension).\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = input[i1, i2, i3,...in] != 0\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    buffer(w.): temporary storage of intermediate results or states during computation.;\r\n\r\n    4.attributes\r\n    order: the order in which the non-zero indices should be returned.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "order", "type": "NonZeroOrderAttr" }
    ]
  },
  {
    "name": "tpu.OutBuffer",
    "summary": "OutBuffer for store op",
    "description": "1.Op Introduction\r\n     This operation stores storeOp output data on DDR when some tentor in layer-group\r\n     is not output to returnOp but storeAndLoad is needed.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = storeOp_output\r\n        \"storeOp_output\" represents the data produced by the store operation that is saved on DDR.\r\n    ```\r\n\r\n    3.activation and weight\r\n    none\r\n\r\n    4.attributes\r\n    need_dump: whether the stored output data should be additionally dumped for debugging or analysis.;",
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "need_dump", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tpu.PackRaw",
    "summary": "Preprocess for raw image.",
    "description": "1.Op Introduction\r\n    (1) preprocess raw image from 3 byte 2 pixel to 2 byte 1 pixel,\r\n        for each row pixel fill 0 in top 4 bits;\r\n        for each img block pack to 4 channels pattern;\r\n    (2) cast to float then normalize and clip by black & white level;\r\n    (3) cast to odtype then pad img to align padding param and move to ddr;\r\n\r\n    2.Math formula\r\n    (1)Pixel Conversion:\r\n    For each pixel in the input tensor, the conversion can be represented as:\r\n    ```math\r\n        packed_pixel = (input & 0x3FF) (keeping lower 10 bits) (with top 4 bits set to 0);\r\n    ```\r\n    Here, input_pixel is the original pixel value, and packed_pixel is the newly formatted pixel value.\r\n    (2)Normalization:\r\n    The normalization step can be described as:\r\n    ```math\r\n        normalized_value = (packed_pixel - black_level) / (white_level - black_level);\r\n    ```\r\n    This formula ensures that the pixel values are scaled between 0 and 1 based on the specified black and white levels.\r\n    (3)Clipping:\r\n    After normalization, the values are clipped to ensure they remain within the valid range:\r\n    ```math\r\n        clipped_value = clip(normalized_value, 0, 1);\r\n    ```\r\n    (4)Output Casting and Padding:\r\n    Finally, the output tensor is formed by casting the clipped values to the desired output data type and applying padding:\r\n    ```math\r\n        output = pad(cast(clipped_value), padding_params);\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    high_table(w.): defines the upper bounds for pixel values during normalization.;\r\n    low_table(w.): defines the lower bounds for pixel values.;\r\n\r\n    4.attribute\r\n    white_level: the maximum intensity level for normalization.;\r\n    black_level: the minimum intensity level for normalization.;\r\n    threshold: a threshold value that can be used during the normalization or clipping stages.;\r\n    channel_order: the order of the color channels in the output tensor.(e.g., RGB, BGR).;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "high_table", "type": "AnyTensorOrNone" },
      { "name": "low_table", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "white_level", "type": "F64Attr" },
      { "name": "black_level", "type": "F64Attr" },
      { "name": "threshold", "type": "F64Attr" },
      { "name": "channel_order", "type": "I64ArrayAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Pad",
    "summary": "Pad operation",
    "description": "1.Op Introduction\r\n    This operation pads a tensor according to the paddings you specify.\r\n    paddings is an integer tensor with shape [2, n], where n is the rank of tensor.\r\n    For each dimension D of input, paddings[0, D] indicates how many values to add\r\n    before the contents of tensor in that dimension, and paddings[1, D] indicates\r\n    how many values to add after the contents of tensor in that dimension.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = input(padding, val, mode)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    paddingsT(act.):  the padding values for each dimension.;\r\n    // for cv18xx reflect mode\r\n    left_select(w.):reflect the tensor values when adding padding on the left side.;\r\n    right_select(w.):reflect the tensor values when adding padding on the right side.;\r\n    buffer(w.): temporary storage of intermediate results or states during computation.;\r\n\r\n    4.attribute\r\n    paddings: defines how much padding to add before and after the contents of the input tensor for each dimension. ;\r\n    val: the value to be used for padding the input tensor. ;\r\n    mode: the padding mode include constant(Pads with a constant value); reflect(Pads with a reflection of the tensor values); replicate(Pads by replicating the edge values of the tensor).;\r\n    with_insert_zero : When with_insert_zero is true, it means that the PadOp first performs the insert zero operation, and then conducts the Padding expansion. In this case, the mode can only be PaddingMode::constant. When with_insert_zero is false, the insert zero operation is not performed.\r\n    inserts : When with_insert_zero is true, the number of zeros inserted between points in the h and w dimensions.\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "paddingsT", "type": "Optional" },
      { "name": "left_select", "type": "AnyTensorOrNone" },
      { "name": "right_select", "type": "AnyTensorOrNone" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "paddings", "type": "I64ArrayAttr" },
      { "name": "val", "type": "DefaultValuedAttr" },
      { "name": "mode", "type": "DefaultValuedAttr" },
      { "name": "with_insert_zero", "type": "DefaultValuedAttr" },
      { "name": "insert_zeros", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Permute",
    "summary": "Permute operator",
    "description": "1.Op Introduction\r\n    Perform permute on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output(...dim2, dim1, dim0) = PermuteOp(input(dim0, dim1, dim2...order))\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    buffer(w.): temporary storage of intermediate results or states during computation.;\r\n\r\n    4.attribute\r\n    order: An array of integers specifying the permutation order of the input tensor's dimensions.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "order", "type": "I64ArrayAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.PixelNorm",
    "summary": "PixelNorm operation",
    "description": "1.Op Introduction\r\n    pixel normalization (normalize along c-axis)\r\n\r\n    2.Math formula\r\n    ```math\r\n            norm_{n, i, j} = sqrt(1 /C \\sum{c=1, C}input_{n, c, i, j} ^ 2) + eps\r\n            output_{n, c, i, j} = weight * input_{n, c, i, j} / norm_{n, i, j} + bias\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    weight(w.): weight tensor;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n    // cv18xx\r\n    table(w.): store a lookup table that may assist in optimizing the matching process.;\r\n    mantissa_table(w.): stores a table of mantissa values used in calculations to improve precision or to handle specific numerical representations.;\r\n\r\n    4.attributes\r\n    eps: a small constant added to the denominator during normalization to prevent division by zero.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "weight", "type": "AnyTensorOrNone" },
      { "name": "bias", "type": "AnyTensorOrNone" },
      { "name": "table", "type": "AnyTensorOrNone" },
      { "name": "mantissa_table", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "eps", "type": "F64Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Pool1D",
    "summary": "pool operator",
    "description": "1.Op Introduction\r\n    This performs an  pooling over the given input tensor. A sliding\r\n    window of size given by <kernel size> is passed over the input tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output(N, C, H', W') = Pool(max(input(N, C, H, W)))\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    stride: the stride for the cross-correlation, a single number or a tuple.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    pool_mode: ;\r\n    pad_value: whether to retain the dimensions of the input tensor in the output.\r\n                If true, will have the same number of dimensions as the input tensor.;\r\n    is_adaptive: whether the pooling operation is adaptive.\r\n                 If true, adjusts the kernel size based on the input size to produce a specified output size.\r\n    count_include_pad: whether to include the padded values in the pooling count.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    ceil_mode: whether to use ceiling or floor when calculating the output size.;\r\n    /// symmetric quantize param\r\n    multipliers: an array of multipliers used for quantization, It allows for scaling the input values before the addition operation.;\r\n    rshift: right shift values corresponding to each input tensor.;\r\n    /// asymmetric quantize param\r\n    scale: Scalar;\r\n    offset: Scalar;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;\r\n    first_round_mode: the rounding behavior applied to the scaled value before the offset is added.;\r\n    layer_group: multiple layers to be processed together or treated as a single unit during computation.;\r\n    /// fp8 quantize param\r\n    fp8_out_scale: scaling factor used for the output tensor when utilizing 8-bit floating-point (FP8) representation.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "pool_mode", "type": "Tpu_PoolModeAttr" },
      { "name": "pad_value", "type": "DefaultValuedAttr" },
      { "name": "is_adaptive", "type": "DefaultValuedAttr" },
      { "name": "count_include_pad", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "ceil_mode", "type": "OptionalAttr" },
      { "name": "multiplier", "type": "OptionalAttr" },
      { "name": "rshift", "type": "OptionalAttr" },
      { "name": "scale", "type": "OptionalAttr" },
      { "name": "offset", "type": "OptionalAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "first_round_mode", "type": "DefaultValuedAttr" },
      { "name": "layer_group", "type": "OptionalAttr" },
      { "name": "fp8_out_scale", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Pool2D",
    "summary": "pool operator",
    "description": "1.Op Introduction\r\n    This performs an  pooling over the given input tensor. A sliding\r\n    window of size given by <kernel size> is passed over the input tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output(N, C, H', W') = Pool(max(input(N, C, H, W)))\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    stride: the stride for the cross-correlation, a single number or a tuple.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    pool_mode: ;\r\n    pad_value: whether to retain the dimensions of the input tensor in the output.\r\n                If true, will have the same number of dimensions as the input tensor.;\r\n    is_adaptive: whether the pooling operation is adaptive.\r\n                 If true, adjusts the kernel size based on the input size to produce a specified output size.\r\n    count_include_pad: whether to include the padded values in the pooling count.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    ceil_mode: whether to use ceiling or floor when calculating the output size.;\r\n    /// symmetric quantize param\r\n    multipliers: an array of multipliers used for quantization, It allows for scaling the input values before the addition operation.;\r\n    rshift: right shift values corresponding to each input tensor.;\r\n    /// asymmetric quantize param\r\n    scale: Scalar;\r\n    offset: Scalar;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;\r\n    first_round_mode: the rounding behavior applied to the scaled value before the offset is added.;\r\n    layer_group: multiple layers to be processed together or treated as a single unit during computation.;\r\n    /// fp8 quantize param\r\n    fp8_out_scale: scaling factor used for the output tensor when utilizing 8-bit floating-point (FP8) representation.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "pool_mode", "type": "Tpu_PoolModeAttr" },
      { "name": "pad_value", "type": "DefaultValuedAttr" },
      { "name": "is_adaptive", "type": "DefaultValuedAttr" },
      { "name": "count_include_pad", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "ceil_mode", "type": "OptionalAttr" },
      { "name": "multiplier", "type": "OptionalAttr" },
      { "name": "rshift", "type": "OptionalAttr" },
      { "name": "scale", "type": "OptionalAttr" },
      { "name": "offset", "type": "OptionalAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "first_round_mode", "type": "DefaultValuedAttr" },
      { "name": "layer_group", "type": "OptionalAttr" },
      { "name": "fp8_out_scale", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Pool3D",
    "summary": "pool operator",
    "description": "1.Op Introduction\r\n    This performs an  pooling over the given input tensor. A sliding\r\n    window of size given by <kernel size> is passed over the input tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output(N, C, H', W') = Pool(max(input(N, C, H, W)))\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    kernel_shape: the size of the convolution kernel (filter) as an array.;\r\n    stride: the stride for the cross-correlation, a single number or a tuple.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.\r\n    pool_mode: ;\r\n    pad_value: whether to retain the dimensions of the input tensor in the output.\r\n                If true, will have the same number of dimensions as the input tensor.;\r\n    is_adaptive: whether the pooling operation is adaptive.\r\n                 If true, adjusts the kernel size based on the input size to produce a specified output size.\r\n    count_include_pad: whether to include the padded values in the pooling count.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    ceil_mode: whether to use ceiling or floor when calculating the output size.;\r\n    /// symmetric quantize param\r\n    multipliers: an array of multipliers used for quantization, It allows for scaling the input values before the addition operation.;\r\n    rshift: right shift values corresponding to each input tensor.;\r\n    /// asymmetric quantize param\r\n    scale: Scalar;\r\n    offset: Scalar;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;\r\n    first_round_mode: the rounding behavior applied to the scaled value before the offset is added.;\r\n    layer_group: multiple layers to be processed together or treated as a single unit during computation.;\r\n    /// fp8 quantize param\r\n    fp8_out_scale: scaling factor used for the output tensor when utilizing 8-bit floating-point (FP8) representation.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "kernel_shape", "type": "I64ArrayAttr" },
      { "name": "strides", "type": "I64ArrayAttr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "pool_mode", "type": "Tpu_PoolModeAttr" },
      { "name": "pad_value", "type": "DefaultValuedAttr" },
      { "name": "count_include_pad", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "multiplier", "type": "OptionalAttr" },
      { "name": "rshift", "type": "OptionalAttr" },
      { "name": "scale", "type": "OptionalAttr" },
      { "name": "offset", "type": "OptionalAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "first_round_mode", "type": "DefaultValuedAttr" },
      { "name": "layer_group", "type": "OptionalAttr" },
      { "name": "fp8_out_scale", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.PoolMask",
    "summary": "pool mask operator",
    "description": "1.Op Introduction\r\n    pooling mask on input\r\n\r\n    2.Math formula\r\n    ```math\r\n        output1(N, C, H', W') = Pool(max(input(N, C, H, W)))\r\n        maskOutput(N, C, H', W') = PoolMask(argmax(input(N, C, H, W)))\r\n        output = scale * output1\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    scale: a scaling factor is applied to the output of the pooling operation, can adjust the intensity or magnitude of the output mask.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "scale", "type": "I64Attr" }
    ]
  },
  {
    "name": "tpu.PReluOp",
    "summary": "PReluOp operator",
    "description": "1.Op Introduction\r\n    Parametric Rectified Linear Unit is an activation function.\r\n\r\n    2.Math formula\r\n    ```math\r\n            f(x) = slope * x   for x < 0\r\n            f(x) = x           for x >= 0\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    slope(w.): the activation function for negative input values.;\r\n\r\n    4.attributes\r\n    rshift: the number of bits to right-shift the quantized values.;\r\n    rshift_pos: This attribute defines the right shift for the positive output values during quantization.;\r\n    multiplier_pos: This parameter specifies the multiplier for the positive part of the output, used in the context of quantization.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "slope", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "rshift", "type": "DefaultValuedAttr" },
      { "name": "rshift_pos", "type": "OptionalAttr" },
      { "name": "multiplier_pos", "type": "OptionalAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Preprocess",
    "summary": "FusePreprocess, it's just a placeholder op.",
    "description": "1.Op Introduction\r\n    It may be divided to permute + slice + scale/scale_lut ops.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = \\text{Preprocess}(input, \\text{resize_dims}, \\text{mean}, \\text{scale}, \\text{channel_order})\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    quant_mode: It determines how the output tensor should be quantized.;\r\n    customization_format: define how input data should be structured or formatted before being processed by the model.;\r\n    channel_order: the order of the color channels in the output tensor.(e.g., RGB, BGR).;\r\n    resize_dims: the target dimensions to which the input tensor should be resized.;\r\n    scale: a scaling factor applied to the attention scores before they are passed through the softmax function.;\r\n    mean: the mean values that should be subtracted from the input tensor during normalization.;\r\n    sign: whether the preprocessing operation should include a sign adjustment.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "quant_mode", "type": "StrAttr" },
      { "name": "customization_format", "type": "StrAttr" },
      { "name": "channel_order", "type": "StrAttr" },
      { "name": "resize_dims", "type": "I64ArrayAttr" },
      { "name": "scale", "type": "F64ArrayAttr" },
      { "name": "mean", "type": "F64ArrayAttr" },
      { "name": "sign", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.RandnLike",
    "summary": "RandnLike operator",
    "description": "1.Op Introduction\r\n    create a tensor with the same shape as input, and fill with value from normal distribution\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = randn(shape(input))\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    randn_data(w.): the characteristics of the normal distribution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "randn_data", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "tpu.Range",
    "summary": "Range operator",
    "description": "1.Op Introduction\r\n    range op.\r\n    generates a sequence of evenly spaced values within a specified range.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = [x | x = start + n * delta, n is an integer, and start ≤ x < limit]\r\n    ```\r\n\r\n    3.activation and weight\r\n    start(act.): The starting value of the sequence. This can be a tensor or None. If None, it defaults to 0.;\r\n    limit(w.): The exclusive upper limit of the sequence.;\r\n    delta(w.): The increment between each value;",
    "inputs": [
      { "name": "start", "type": "AnyTensor" },
      { "name": "limit", "type": "AnyTensor" },
      { "name": "delta", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "tpu.Reciprocal",
    "summary": "ConstantBinary (Div) operator",
    "description": "1.Op Introduction\r\n    The Reciprocal operator is a tensor operation that performs division of a constant scalar value by an input tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = const_val / input\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    const_val: specifies the constant value to be added to each element of the input tensor(positive, negative, or zero).;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "const_val", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Reduce",
    "summary": "Reduce operator",
    "description": "1.Op Introduction\r\n    Computes the mean/max/prod/sum of the input tensor's element along the provided axes.\r\n\r\n    2.Math formula\r\n    ```math\r\n            1.Sum\r\n                output[i_1, i_2, i_3,..., i_k] = \\sum{j in A}input[i_1, i_2,..., j, ..., i_n]\r\n                where ( A ) is the set of axes to reduce.\r\n            2.Mean\r\n                output[i_1, i_2, i_3,..., i_k] = 1 / count (\\sum{j in A}input[i_1, i_2,..., j, ..., i_n])\r\n                where count is the number of elements being summed along the axes ( A ).\r\n            3.Max\r\n                output[i_1, i_2, i_3,..., i_k] = max{j in A}input[i_1, i_2,..., j, ..., i_n]\r\n            4.Product\r\n                output[i_1, i_2, i_3,..., i_k] = \\prod_{j=1}^m(input[i_1, i_2,..., i_k, j])\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    reciprocal_mantissa_table(w.): store the mantissa values of the reciprocal.;\r\n    buffer(w.): temporary storage of intermediate results or states during computation.;\r\n\r\n    4.attributes\r\n    axes: the dimensions (axes) of the input tensor that should be squeezed (removed).;\r\n    keepdims: whether to retain the dimensions of the input tensor in the output.\r\n               If true, will have the same number of dimensions as the input tensor.;\r\n    mode: the type of binary operation to be performed, addition, subtraction, or other types of binary operations.;\r\n    multiplier: Floating-point multiplication operations are usually converted to fixed-point multiplication operations.;\r\n    rshift: the number of bits to right-shift the quantized values.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "buffer", "type": "AnyTensorOrNone" },
      { "name": "reciprocal_mantissa_table", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "axes", "type": "I64ArrayAttr" },
      { "name": "keepdims", "type": "BoolAttr" },
      { "name": "mode", "type": "ReduceModeAttr" },
      { "name": "multiplier", "type": "OptionalAttr" },
      { "name": "rshift", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Relu",
    "summary": "Relu operator",
    "description": "1.Op Introduction\r\n    ReLU with a scalar maximum value.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = ReluOp(input) -> (0, 1)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    ginfo: associated with layer grouping information.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.RequantFp",
    "summary": "requant float operation",
    "description": "1.Op Introduction\r\n    Requant 32/16/8 bit data to int8 or uint8 or fp8 data, by float scale and float offset, when to fp8 data, offset is not used;\r\n\r\n    2.Math formula\r\n    int8/uint8/fp8(output) = round(float32/float16/float8(input) x scale + offset);\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    scale: Scalar;\r\n    offset: Scalar;\r\n    quant_mode: It determines how the output tensor should be quantized.;\r\n    round_mode: This parameter specifies the rounding mode to be used during quantization.;\r\n    first_round_mode: the rounding behavior applied to the scaled value before the offset is added.;\r\n    ginfo: contains layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "scale", "type": "F64Attr" },
      { "name": "offset", "type": "DefaultValuedAttr" },
      { "name": "quant_mode", "type": "Tpu_RequantModeAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "first_round_mode", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.RequantFpAxis",
    "summary": "requant float operation",
    "description": "1.Op Introduction\r\n    Requant 32/16/8 bit data to int8 or uint8 data, PerAxis(or PerChannel);\r\n\r\n    2.Math formula\r\n    int8/uint8(output) = RequantFpAxisOp(float32/float16/float8(input),quant);\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    quant(act.): This attribute represents the quantization parameters tensor.\r\n                 It contains the values used for requantization, such as multipliers and shifts,\r\n                 which are specific to each axis or channel.;\r\n\r\n    4.attribute\r\n    quant_mode: It determines how the output tensor should be quantized.;\r\n    round_mode: This parameter specifies the rounding mode to be used during quantization.;\r\n    first_round_mode: the rounding behavior applied to the scaled value before the offset is added.;\r\n    ginfo: contains layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "quant", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "quant_mode", "type": "Tpu_RequantModeAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "first_round_mode", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.RequantInt",
    "summary": "requant operation",
    "description": "1.Op Introduction\r\n    Requant 32/16/8 bit data to int8 or uint8 data, by int multiplier and int shift;\r\n\r\n    2.Math formula\r\n    int8/uint8(output) = RequantIntOp (int32/16/8(input));\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    multiplier: Floating-point multiplication operations are usually converted to fixed-point multiplication operations.;\r\n    rshift: the number of bits to right-shift the quantized values.;\r\n    quant_mode: It determines how the output tensor should be quantized.;\r\n    round_mode: This parameter specifies the rounding mode to be used during quantization.;\r\n    ginfo: contains layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "multiplier", "type": "SI32Attr" },
      { "name": "rshift", "type": "SI32Attr" },
      { "name": "quant_mode", "type": "Tpu_RequantModeAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.RequantIntAxis",
    "summary": "requant operation",
    "description": "1.Op Introduction\r\n    Requant 32/16/8 bit data to int8 or uint8 data, PerAxis(or PerChannel);\r\n\r\n    2.Math formula\r\n    int8/uint8(output) = RequantIntAxisOp(int32/16/8(input), quant);\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    quant(act.): the quantization parameters tensor.;\r\n\r\n    4.attribute\r\n    quant_mode: It determines how the output tensor should be quantized.;\r\n    round_mode: This parameter specifies the rounding mode to be used during quantization.;\r\n    ginfo: contains layer grouping information.;\r\n    rq_axis: the axis along which the requantization is performed.;\r\n    fuse_rq_axis: whether to fuse the requantization operation with subsequent operations along the specified axis.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "quant", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "quant_mode", "type": "Tpu_RequantModeAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "rq_axis", "type": "DefaultValuedAttr" },
      { "name": "fuse_rq_axis", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Reshape",
    "summary": "Reshape operation",
    "description": "1.Op Introduction\r\n    Returns a tensor with the same type/values as the input, with a new shape\r\n    specified by the shape argument. Reshape may operate on tensors of any rank.\r\n    No data conversion happens during a reshape operation.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = ReshapeOp(input, shape)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    buffer(w.): temporary storage of intermediate results or states during computation.;\r\n\r\n    4.attribute\r\n    shape: 0: keep dim from input; -1: left dim from input.;\r\n    flatten_start_dim: the starting dimension from which to begin flattening the input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "buffer", "type": "Optional" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "shape", "type": "DefaultValuedAttr" },
      { "name": "flatten_start_dim", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tpu.Reverse",
    "summary": "Reverse operation",
    "description": "1.Op Introduction\r\n    Reverse on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = ReverseOp(input, axis)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    axis: the dimension of reverse;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "I64Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.RMSNorm",
    "summary": "RMSNorm operation",
    "description": "1.Op Introduction\r\n    A simplification of the original layer normalization (LayerNorm).\r\n    Only normalize the last dimension of tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = gamma * input / sqrt(mean(input ^ 2) + epsilon)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    gamma(w.): scalar.;\r\n\r\n    4.attributes\r\n    eps: a small constant added to the denominator during normalization to prevent division by zero.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "gamma", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "eps", "type": "F64Attr" },
      { "name": "weight_keep_f32", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.RoiAlign",
    "summary": "RoiAlign operator",
    "description": "1.Op Introduction\r\n    RoiAlign consumes an input tensor X and region of interests\r\n    (rois) to apply pooling across each RoI.\r\n\r\n    2.Math formula\r\n    ```math\r\n        1.ROI coordinate scaling[x1, y1, x2, y2]\r\n            x_scaled = x x spatial_scale\r\n            y_scaled = y x spatial_scale\r\n        2.Delineation of grid sub-areas\r\n            bin_height = (y2_scaled - y1_scaled) / output_height\r\n            bin_width  = (x2_scaled - x1_scaled) / output_width\r\n        3.align_corners -> true\r\n            x_grid = x1_scaled + (i + 0.5) x bin_width\r\n            y_grid = y1_scaled + (j + 0.5) x bin_height\r\n        output = RoiAlign(input, rois, output_height, output_width, sampling_ratio, spatial_scale, align_corners)\r\n    ```\r\n\r\n    3.activated and weight\r\n    input(act.): input tensor(4D);\r\n    rois: RoIs (Regions of Interest) to pool over;\r\n          rois is 2-D input of shape (num_rois, 4) given as [[x1, y1, x2, y2], ...].\r\n\r\n    4.attributes\r\n    mode: the pooling mode to be used when extracting features from the RoIs on the input feature maps.;\r\n    output_height: the height of the output feature maps.;\r\n    output_width: the width of the output feature maps.;\r\n    sampling_ratio: the number of sampling points in each direction (height and width).;\r\n    spatial_scale: a scaling factor that maps the input coordinates (RoIs) to the input feature map's scale.;\r\n    align_corners: whether to align the corners of the input and output tensors.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "rois", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "RoiAlignModeAttr" },
      { "name": "output_height", "type": "I64Attr" },
      { "name": "output_width", "type": "I64Attr" },
      { "name": "sampling_ratio", "type": "I64Attr" },
      { "name": "spatial_scale", "type": "F64Attr" },
      { "name": "align_corners", "type": "BoolAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.RoiExtractor",
    "summary": "RoiExtractor operator",
    "description": "1.Op Introduction\r\n    RoiExtractor consumes an input tensor X and region of interests\r\n    (rois) to apply pooling across each RoI.\r\n\r\n    2.Math formula\r\n      ```math\r\n        1.ROI coordinate scaling[x1, y1, x2, y2]\r\n            x_scaled = x x spatial_scale\r\n            y_scaled = y x spatial_scale\r\n        2.Delineation of grid sub-areas\r\n            bin_height = (y2_scaled - y1_scaled) / output_height\r\n            bin_width  = (x2_scaled - x1_scaled) / output_width\r\n        3.align_corners -> true\r\n            x_grid = x1_scaled + (i + 0.5) x bin_width\r\n            y_grid = y1_scaled + (j + 0.5) x bin_height\r\n        output = RoiAlign(input, rois, output_height, output_width, sampling_ratio, spatial_scale, align_corners)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    rois(act.): RoIs (Regions of Interest) to pool over;\r\n                rois is 2-D input of shape (num_rois, 4) given as [[x1, y1, x2, y2], ...]\r\n    target_lvls(act.): 1-D tensor with each element denoting the index of the corresponding image in the batch.\r\n\r\n    4.attributes\r\n    mode: the pooling mode to be used when extracting features from the RoIs on the input feature maps.;\r\n    num_levels: the number of feature levels.;\r\n    output_height: the height of the output feature maps.;\r\n    output_width: the width of the output feature maps.;\r\n    sampling_ratio: the number of sampling points in each direction (height and width).;\r\n    spatial_scale: a scaling factor that maps the input coordinates (RoIs) to the input feature map's scale.;\r\n    align_corners: whether to align the corners of the input and output tensors.;\r\n    is_static: whether the RoI extraction process has a fixed (static) configuration.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "rois", "type": "AnyRankedTensor" },
      { "name": "target_lvls", "type": "AnyRankedTensor" },
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "mode", "type": "RoiAlignModeAttr" },
      { "name": "num_levels", "type": "I64Attr" },
      { "name": "output_height", "type": "I64Attr" },
      { "name": "output_width", "type": "I64Attr" },
      { "name": "sampling_ratio", "type": "I64Attr" },
      { "name": "spatial_scales", "type": "F64ArrayAttr" },
      { "name": "align_corners", "type": "BoolAttr" },
      { "name": "is_static", "type": "BoolAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Rope",
    "summary": "Rope Operator",
    "description": "1.Op Introduction\r\n    rope operator.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output=saturation((input1 x shift(input2, mul1_shift))⊕(input3 x shift(input2, mul2_shift))) + shift(input3, dd_shift)\r\n    ```\r\n    The operator ⊕ represents the addition of the two multiplicative results.\r\n    The function shift(input,shift_value) applies a shift to the input tensor based on the provided shift value.\r\n    The saturation function ensures that the output remains within a defined range, preventing overflow or underflow.\r\n\r\n    3.activation and weight\r\n    input1(act.): input tensor;\r\n    input2(act.): input tensor;\r\n    input3(act.): input tensor;\r\n\r\n    4.attribute\r\n    is_permute_optimize:whether to apply optimization for permuting the input tensors.;\r\n    mul1_round_mode: the rounding mode to be used for the first multiplication operation.;\r\n    mul2_round_mode: Similar to mul1_round_mode, this attribute defines the rounding mode for the second multiplication operation.;\r\n    add_round_mode: the rounding mode for the addition operation.;\r\n    mul1_shift: the number of bits to shift the result of the first multiplication.;\r\n    mul2_shift: Similar to mul1_shift, this attribute defines the number of bits to shift for the second multiplication operation.;\r\n    add_shift: the number of bits to shift the result of the addition operation.;\r\n    mul1_saturation: whether the output of the first multiplication should be saturated.\r\n                     When set to true, the result will be clamped to prevent overflow or underflow.;\r\n    mul2_saturation: Similar to mul1_saturation, this attribute specifies whether saturation should be applied to the second multiplication's output.;\r\n    add_saturation: whether to apply saturation to the output of the addition operation.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input1", "type": "AnyRankedTensor" },
      { "name": "input2", "type": "AnyRankedTensor" },
      { "name": "input3", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "is_permute_optimize", "type": "DefaultValuedAttr" },
      { "name": "mul1_round_mode", "type": "DefaultValuedAttr" },
      { "name": "mul2_round_mode", "type": "DefaultValuedAttr" },
      { "name": "add_round_mode", "type": "DefaultValuedAttr" },
      { "name": "mul1_shift", "type": "DefaultValuedAttr" },
      { "name": "mul2_shift", "type": "DefaultValuedAttr" },
      { "name": "add_shift", "type": "DefaultValuedAttr" },
      { "name": "mul1_saturation", "type": "DefaultValuedAttr" },
      { "name": "mul2_saturation", "type": "DefaultValuedAttr" },
      { "name": "add_saturation", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Scale",
    "summary": "Scale operator",
    "description": "1.Op Introduction\r\n    Y = X * S + B,\r\n    where the shape of X/Y is [n, c, h, w] and the shape of S/B is [1, c, 1, 1].\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = input x scale + bias\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    scale(w.): scalar;\r\n    bias(w.): the learnable bias of the module of shape (out_channels).;\r\n    lshift: a left shift operation applied to the dequantized data after scaling.;\r\n\r\n    4.attributes\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "scale", "type": "AnyRankedTensor" },
      { "name": "bias", "type": "AnyRankedTensor" },
      { "name": "lshift", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.ScaleLut",
    "summary": "scale lut operator.",
    "description": "1.Op Introduction\r\n    Performs scale on input, y = input * scale + bias.\r\n\r\n    Interfaces or Traits:\r\n      `NoSideEffect`\r\n      `TpuOpCommonInterface`    : support common TPU TG Op interface.\r\n      `TpuTGOpCodegenInterface` : support generate TPU instuctions.\r\n\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = input * scale + bias\r\n    ```\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    table(w.): store a lookup table that may assist in optimizing the matching process.;\r\n\r\n    4.attributes\r\n    scale: each channel scale.;\r\n    bias: each channel bias.;\r\n    sign: if output is signed.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "table", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "scale", "type": "F64ArrayAttr" },
      { "name": "bias", "type": "F64ArrayAttr" },
      { "name": "sign", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.ScatterElements",
    "summary": "ScatterElements op",
    "description": "1.Op Introduction\r\n    ScatterElements takes three inputs data, updates, and indices of the same rank r >= 1 and an optional attribute axis that\r\n    identifies an axis of data (by default, the outer-most axis, that is axis 0). The output of the operation is produced by\r\n    creating a copy of the input data, and then updating its value to values specified by updates at specific index\r\n    positions specified by indices. Its output shape is the same as the shape of data.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = ScatterElements(input[axis], updates, indices)\r\n    ```\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    indices(w.): Tensor of int32/int64 indices, of r >= 1 (same rank as input).\r\n             All index values are expected to be within bounds [-s, s-1] along axis of size s.\r\n    updates(w.): Tensor of rank r >=1 (same rank and shape as indices).\r\n    indices_coeff(w.): a scaling or modifying factor for the indices.;\r\n    buffer(w.): temporary storage of intermediate results or states during the LSTM computation.;\r\n\r\n    4.attributes\r\n    axis: the dimension of the input tensor.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "indices", "type": "AnyRankedTensor" },
      { "name": "updates", "type": "AnyRankedTensor" },
      { "name": "indices_coeff", "type": "AnyTensorOrNone" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "I64Attr" },
      { "name": "reduction", "type": "DefaultValuedAttr" },
      { "name": "nc_can_split", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.ScatterND",
    "summary": "ScatterND operator",
    "description": "1.Op Introduction\r\n    The output of the operation is produced by creating a copy of the input data,\r\n    and then updating its value to values specified by updates at\r\n    specific index positions specified by indices.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = ScatterND(input_data[indices], updates, reduction)\r\n    ```\r\n\r\n    3.activated and weight\r\n    input_data(act.): input tensor;\r\n    indices(w.): Tensor of rank q >= 1.;\r\n    updates(w.): Tensor of rank q + r - indices_shape[-1] - 1.;\r\n    buffer(w.): temporary storage of intermediate results or states during the LSTM computation.;\r\n\r\n    4.attributes\r\n    reduction: Type of reduction to apply: none (0 default), add(1), sub(2), max(3), min(4), mul(5).;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input_data", "type": "AnyRankedTensor" },
      { "name": "indices", "type": "AnyRankedTensor" },
      { "name": "updates", "type": "AnyRankedTensor" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "reduction", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.SelectiveScan",
    "summary": "2D Selective Scan Operator (specialized for VMamba)",
    "description": "1. Op Introduction\r\n    Performs structured state space modeling (SSM) on 2D image data using a bidirectional scanning mechanism.\r\n    Core component of the VMamba architecture that enables efficient long-range dependency modeling in visual data.\r\n\r\n    2. Math formula\r\n    For each scanning direction:\r\n    ```\r\n    h_t = δA_t ⊙ h_{t-1} + δB_t ⊙ u_t\r\n    y_t = c_t ⊙ h_t\r\n    ```\r\n    Final output:\r\n    ```\r\n    output = concat(y_forward, y_backward) + u ⊙ D\r\n    ```\r\n    code:\r\n\r\n    for i in range(L):\r\n        x_up = deltaA_up[:, :, i, :] * x_up + deltaB_u_up[:, :, i, :]\r\n        x_down = deltaA_down[:, :, L - 1 - i, :] * x_down + deltaB_u_down[:, :, L - 1 - i, :]\r\n        y_up[i, :, :] = x_up[0, :, :] * c_up[i, :, 0, :]\r\n        y_down[L - 1 - i, :, :] = x_down[0, :, :] * c_down[L - 1 - i, :, 0, :]\r\n\r\n    y = concat((y_up, y_down), dim=1)\r\n    out = y if D is None else y + u * D\r\n\r\n    3. Input parameters:\r\n    u: Input feature map tensor (after linear projection) [N, C, L, Batch]\r\n    c: State-to-output projection weights [N, C, L, Batch]\r\n    D: Residual connection weights (optional) [N, C]\r\n    δA: Discretized state transition matrix [N, C, L, Batch]\r\n    δB_u: Combined input projection and discretized control matrix [N, C, L, Batch]\r\n\r\n    4. Attributes:\r\n    direction: Bidirectional scanning scheme (forward & reverse)\r\n    time_dim: Sequence length dimension (L = H × W)\r\n    channel_split: Channel dimension partitioning factor (C → C//2)\r\n    residual: Whether to apply residual connection (D term)\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "Cs", "type": "AnyTensor" },
      { "name": "deltaA", "type": "AnyTensor" },
      { "name": "deltaB_u", "type": "AnyTensor" },
      { "name": "us", "type": "AnyTensor" },
      { "name": "Ds", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Shape",
    "summary": "Shape operation",
    "description": "1.Op Introduction\r\n    Takes a tensor as input and outputs an 1D int tensor containing the shape of the input tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = shape(input[d1, d2,...,dn])\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "tpu.ShapeArith",
    "summary": "Cpu data operation",
    "description": "1.Op Introduction\r\n    Arithmetic implementation for simple data in host memory such as 'Shape, Index, Stride' etc.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = ShapeArith(input, type)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    type: the kind of arithmetic operation.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "type", "type": "StrAttr" }
    ]
  },
  {
    "name": "tpu.ShapeAssign",
    "summary": "ShapeAssign operator",
    "description": "1.Op Introduction\r\n    reshape for dynamic shape\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = ShapeAssign(input, shape)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    shape(w.): 0: keep dim from input; -1: left dim from input.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "shape", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "tpu.ShapeCast",
    "summary": "Shape Cast operation",
    "description": "1.Op Introduction\r\n    designed to perform shape casting, which allows the transformation of the shape of an input tensor while preserving its data type.\r\n\r\n    2.Math formula\r\n    output = reshape(input, new_shape);\r\n\r\n    3.activation\r\n    input(act.): input tensor;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ]
  },
  {
    "name": "tpu.ShapeClip",
    "summary": "ShapeClip operation running on CPU",
    "description": "1.Op Introduction\r\n    The operator limits the given input to a certain range.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = min      if input[i] < min;\r\n                        input[i] if input[i] >= min && input[i] <= max;\r\n                        max      if input[i] > max;\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    min: the minimum value that the elements of the input tensor can take.;\r\n    max: the maximum value that the elements of the input tensor can take.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "min", "type": "F32Attr" },
      { "name": "max", "type": "F32Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.ShapePack",
    "summary": "Shape Concatate operation",
    "description": "1.Op Introduction\r\n    Concatenates the given sequence of seq tensors in the given dimension.\r\n    All tensors must either have the same shape (except in the concatenating dimension) or be empty.\r\n\r\n    2.Math formula\r\n    output = ShapePack(input, axis)\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor;\r\n\r\n    4.attribute\r\n    axis: the dimension of the input tensor.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "SI32Attr" }
    ]
  },
  {
    "name": "tpu.ShapePow",
    "summary": "ShapePow operation running on CPU",
    "description": "1.Op Introduction\r\n    perform an element-wise power calculation on the input tensor.\r\n\r\n    2.Math formula\r\n        output = input ^ n\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    exponent:;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "exponent", "type": "F32Attr" }
    ]
  },
  {
    "name": "tpu.ShapeReduce",
    "summary": "ShapeReduce operator",
    "description": "1.Op Introduction\r\n    ShapeReduce Operation on input.\r\n\r\n    2.Math formula\r\n    \\text{output} = \\text{scale} \\times \\mathrm{reduce}\\left(\\text{input}, \\text{axes}; \\, \\text{mode}\\right)\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    axes: the dimensions (axes) of the input tensor that should be squeezed (removed).;\r\n    keepdims: whether to retain the dimensions of the input tensor in the output.\r\n               If true, will have the same number of dimensions as the input tensor.;\r\n    mode: the type of binary operation to be performed, addition, subtraction, or other types of binary operations.;\r\n    scale: scalar.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "axes", "type": "I64ArrayAttr" },
      { "name": "keepdims", "type": "BoolAttr" },
      { "name": "mode", "type": "ReduceModeAttr" },
      { "name": "scale", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.ShapeReshape",
    "summary": "ShapeReshape operation",
    "description": "1.Op Introduction\r\n    Returns a tensor with the same type/values as the input, with a new shape\r\n    specified by the shape argument. Reshape may operate on tensors of any rank.\r\n    No data conversion happens during a reshape operation.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = ShapeReshapeOp(input, shape)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    shape: 0: keep dim from input; -1: left dim from input.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "shape", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tpu.ShapeReverse",
    "summary": "ShapeReverse operation",
    "description": "1.Op Introduction\r\n    Reverse on input\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = ShapeReverse(input, axis)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    axis: the dimension of reverse;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "I64Attr" }
    ]
  },
  {
    "name": "tpu.ShapeScatterElements",
    "summary": "ScatterElements op",
    "description": "1.Op Introduction\r\n    ScatterElements takes three inputs data, updates, and indices of the same rank r >= 1 and an optional attribute axis that\r\n    identifies an axis of data (by default, the outer-most axis, that is axis 0). The output of the operation is produced by\r\n    creating a copy of the input data, and then updating its value to values specified by updates at specific index\r\n    positions specified by indices. Its output shape is the same as the shape of data.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = ScatterElements(input[axis], updates, indices)\r\n    ```\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    indices(w.): Tensor of int32/int64 indices, of r >= 1 (same rank as input).\r\n             All index values are expected to be within bounds [-s, s-1] along axis of size s.\r\n    updates(w.): Tensor of rank r >=1 (same rank and shape as indices).\r\n    indices_coeff(w.): a scaling or modifying factor for the indices.;\r\n    buffer(w.): temporary storage of intermediate results or states during the LSTM computation.;\r\n\r\n    4.attributes\r\n    axis: the dimension of the input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "indices", "type": "AnyTensor" },
      { "name": "updates", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "I64Attr" },
      { "name": "reduction", "type": "DefaultValuedAttr" },
      { "name": "nc_can_split", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tpu.ShapeSlice",
    "summary": "ShapeSlice operator",
    "description": "1.Op Introduction\r\n    Slice Operation on shape-type tensor.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = input[offset[j] : ends[j] : steps[j]]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    offsetT(w.): the starting indices for each slice along the specified axes.;\r\n    endsT(w.): the ending indices for each slice along the specified axes.;\r\n    stepsT(w.): the step sizes for each slice along the specified axes.;\r\n\r\n    4.attribute\r\n    offset: An array of the starting indices for slicing along each axis.;\r\n    steps: An array of the step sizes for slicing along each axis.;\r\n    ends: An array of the ending indices for slicing along each axis.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "offsetT", "type": "AnyTensorOrNone" },
      { "name": "endsT", "type": "AnyTensorOrNone" },
      { "name": "stepsT", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "offset", "type": "I64ArrayAttr" },
      { "name": "steps", "type": "I64ArrayAttr" },
      { "name": "ends", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "tpu.ShapeSqueeze",
    "summary": "Onnx-Style ShapeSqueeze operator",
    "description": "1.Op Introduction\r\n    ShapeSqueeze Operation on input.\r\n\r\n    2.Math formula\r\n        output = ShapeSqueeze(input, axes)\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    axes: An array of the axes along which to perform the slicing operation.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "axes", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "tpu.ShapeTile",
    "summary": "Shape Tile operation",
    "description": "1.Op Introduction\r\n    Returns a tensor with the same type as the input, with a new shape\r\n    specified by the shape argument.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i_1, i_2, i_3,...i_k] = input[i_1 mod d_1, i_2 mod d_2, i_3 mod d_3,..., i_k mod d_k]\r\n    ```\r\n    where d_j represents the corresponding dimension size of the input tensor after tiling.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    tileT(w.): how many times to replicate the input tensor along each dimension.;\r\n    buffer(w.): temporary storage of intermediate results or states during the computation.;\r\n\r\n    4.attributes\r\n    tile: the number of times to replicate the input tensor along each dimension.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "tileT", "type": "Optional" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "tile", "type": "DefaultValuedAttr" }
    ]
  },
  {
    "name": "tpu.ShapeTranspose",
    "summary": "ShapeTransposeOp operator",
    "description": "1.Op Introduction\r\n    Perform permute on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output(dim1, dim0) = TransposeOp(input(dim0, dim1))\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    order: the order in which the non-zero indices should be returned.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "order", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "tpu.ShapeUnsqueeze",
    "summary": "Onnx-Style ShapeUnsqueeze operator",
    "description": "1.Op Introduction\r\n    ShapeUnsqueeze Operation on input.\r\n\r\n    2.Math formula\r\n        output = ShapeUnsqueeze(input, axes)\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    axes: An array of the axes along which to perform the slicing operation.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "axes", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "tpu.ShuffleChannel",
    "summary": "ShuffleChannel operator",
    "description": "1.Op Introduction\r\n    Perform ShuffleChannel on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output(N, C, H, W) = input(N, Shuffle(C), H, W)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attribute\r\n    group: An integer specifying the number of groups to divide the channels into.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "group", "type": "I64Attr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Slice",
    "summary": "Slice operator",
    "description": "1.Op Introduction\r\n    Slice Operation on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = input[offset[j] : ends[j] : steps[j]]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    offsetT(w.): the starting indices for each slice along the specified axes.;\r\n    endsT(w.): the ending indices for each slice along the specified axes.;\r\n    stepsT(w.): the step sizes for each slice along the specified axes.;\r\n    buffer(w.): temporary storage of intermediate results or states during computation.;\r\n\r\n    4.attribute\r\n    offset: An array of the starting indices for slicing along each axis.;\r\n    steps: An array of the step sizes for slicing along each axis.;\r\n    ends: An array of the ending indices for slicing along each axis.;\r\n    axes: An array of the axes along which to perform the slicing operation.;\r\n    hasparamConvert_axes: whether parameter conversion is needed for the specified axes.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "offsetT", "type": "AnyTensorOrNone" },
      { "name": "endsT", "type": "AnyTensorOrNone" },
      { "name": "stepsT", "type": "AnyTensorOrNone" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "offset", "type": "I64ArrayAttr" },
      { "name": "steps", "type": "I64ArrayAttr" },
      { "name": "ends", "type": "I64ArrayAttr" },
      { "name": "axes", "type": "DefaultValuedAttr" },
      { "name": "hasparamConvert_axes", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.SliceMerge",
    "summary": "SliceMerge",
    "description": "1.Op Introduction\r\n    When there are multiple slices in the layer-group,\r\n    this operation combines the store op output of each slice to output to the yield op.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output =  SliceMerge(input)\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ]
  },
  {
    "name": "tpu.Softmax",
    "summary": "softmax operator",
    "description": "1.Op Introduction\r\n    Integrates some operations related to softmax.\r\n\r\n    2.Math formula\r\n    ```math\r\n            \\text{output}[i] = \\frac{e^{\\text{input}[i]}}{\\sum_{j} e^{\\text{input}[j]}}\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    table(w.): additional computations or transformations during the softmax operation.;\r\n    slope_table(w.): contain scaling factors or slopes that can adjust the output.;\r\n    reciprocal_table(w.): holds precomputed reciprocal values that can be used to optimize the division operations;\r\n    reciprocal_mantissa_table(w.): store the mantissa values of the reciprocal.;\r\n    buffer(w.): temporary storage of intermediate results or states during computation.;\r\n\r\n    4.attributes\r\n    axis: the dimension of the input tensor.;\r\n    log: when set to true, indicates that the output should be computed in log space.;\r\n    beta: scaling factor.;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "table", "type": "AnyTensorOrNone" },
      { "name": "slope_table", "type": "AnyTensorOrNone" },
      { "name": "reciprocal_table", "type": "AnyTensorOrNone" },
      { "name": "reciprocal_mantissa_table", "type": "AnyTensorOrNone" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "SI32Attr" },
      { "name": "log", "type": "DefaultValuedAttr" },
      { "name": "beta", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.SoftmaxBwd",
    "summary": "softmax backward operator",
    "description": "1.Op Introduction\r\n    Integrates some operations related to softmax backward.\r\n\r\n    2.Math formula\r\n    ```math\r\n            grad_input[i] = softmax(output)[i] * (grad_output[i] - \\sum{j}grad_output[j] * softmax(output)[j])\r\n    ```\r\n\r\n    3.activation and weight\r\n    grad_output(act.): the gradient of the loss with respect to the output.;\r\n    output(act.): output tensor.;\r\n\r\n    4.attributes\r\n    dim: If set to 0, computed across rows, If set to 1, computed across columns.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "grad_output", "type": "AnyTensor" },
      { "name": "output", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "grad_input", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "dim", "type": "SI32Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.SoftmaxCast",
    "summary": "softmax operator",
    "description": "Softmax + Requant",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "table", "type": "AnyTensorOrNone" },
      { "name": "slope_table", "type": "AnyTensorOrNone" },
      { "name": "reciprocal_table", "type": "AnyTensorOrNone" },
      { "name": "reciprocal_mantissa_table", "type": "AnyTensorOrNone" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "SI32Attr" },
      { "name": "log", "type": "DefaultValuedAttr" },
      { "name": "beta", "type": "DefaultValuedAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Sort",
    "summary": "Sort operation",
    "description": "1.Op Introduction\r\n    Integrates some operations related to Sort.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Sort(input, axis, descending)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    buffer(w.): a temporary storage area for intermediate calculations or results during the NMS process.;\r\n\r\n    4.attributes\r\n    axis: the dimension of the input tensor.;\r\n    descending: the order of sorting.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "values", "type": "AnyTensorOrNone" },
      { "name": "indices", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "axis", "type": "I64Attr" },
      { "name": "descending", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Space2Batch",
    "summary": "Space2Batch operator",
    "description": "1.Op Introduction\r\n    Refer to `https://www.tensorflow.org/api_docs/python/tf/space_to_batch`\r\n\r\n    2.Math formula\r\n    ```math\r\n        h_padding = h + pad_top + pad_bottom,\r\n        w_padding = w + pad_left + pad_right,\r\n        [n, c, h, w] => [n, c, h_padding, w_padding]\r\n        =>[n * block_h * block_w, c, h_padding / block_h, w / block_w];\r\n        h_padding and w_padding should satisfy:\r\n        h_padding % block_h = 0, w_padding % block_w = 0\r\n        The format of input or output is NCHW.\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor.;\r\n    buffer(w.): temporary storage of intermediate results or states during computation.;\r\n\r\n    4.attributes\r\n    block_h: The height of the blocks used to rearrange the depth into spatial dimensions.;\r\n    block_w: The width of the blocks used to rearrange the depth into spatial dimensions.;\r\n    pads: the amount of padding applied to the input. It contains four ints with top, left, bottom, right.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "block_h", "type": "I64Attr" },
      { "name": "block_w", "type": "I64Attr" },
      { "name": "pads", "type": "I64ArrayAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Split",
    "summary": "split tensor to continues pieces",
    "description": "1.Op Introduction\r\n    The ops in one parallel should run in parallel.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = input[i * split_size: (i + 1) * split_size] for i = 0, 1, ... num - 1\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "outputs", "type": "Variadic" }
    ]
  },
  {
    "name": "tpu.Squeeze",
    "summary": "Onnx-Style Squeeze operator",
    "description": "1.Op Introduction\r\n    Squeeze Operation on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = squeeze(input, axes)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    axes: the dimensions (axes) of the input tensor that should be squeezed (removed).;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "axes", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "tpu.Store",
    "summary": "Store operation",
    "description": "1.Op Introduction\r\n    store weight from gmem to l2mem;\r\n\r\n    2.Math formula\r\n    output = store(input, input_gmem, ginfo, support_compress, compress_info, l2m_addr)\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n    input_gmem(w.): the input tensor containing the weights stored in global memory (gmem).;\r\n\r\n    4.attributes\r\n    ginfo: associated with layer grouping information.;\r\n    support_compress: whether compression is supported for the load operation.;\r\n    compress_info: used in conjunction with support_compress to specify details like compression schemes, ratios, or any relevant metadata.;\r\n    l2m_addr: the address or offset within the local L2 memory (l2mem).;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "support_compress", "type": "DefaultValuedAttr" },
      { "name": "compress_info", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.StridedSlice",
    "summary": "Strided Slice operator",
    "description": "1.Op Introduction\r\n    Strided Slice Operation on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i] = input[starts[j] + i * strides[j]]\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    starts(w.): the starting indices for each dimension of the input tensor.;\r\n    ends(w.): the ending indices for each dimension of the input tensor.;\r\n    strides(w.):  the stride values for each dimension, determining the step size between indices in the slicing operation.;\r\n\r\n    4.attribute\r\n    begin_mask: If set, the start index for that dimension is considered as 0.;\r\n    end_mask: If set, the end index for that dimension is considered as the size of the dimension.;\r\n    ellipsis_mask: whether allowing for the selection of all dimensions in between specified slices.;\r\n    new_axis_mask: which dimensions should be added as new axes in the output tensor.;\r\n    shrink_axis_mask: which dimensions should be removed from the output tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "starts", "type": "AnyRankedTensor" },
      { "name": "ends", "type": "AnyRankedTensor" },
      { "name": "strides", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "begin_mask", "type": "I64Attr" },
      { "name": "end_mask", "type": "I64Attr" },
      { "name": "ellipsis_mask", "type": "I64Attr" },
      { "name": "new_axis_mask", "type": "I64Attr" },
      { "name": "shrink_axis_mask", "type": "I64Attr" }
    ]
  },
  {
    "name": "tpu.Sub",
    "summary": "sub operator",
    "description": "1.Op Introduction\r\n    Elementwise subtraction of input1 and input2. Axis of size 1 will be broadcast,\r\n    as necessary.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = ReLU((input1 - input2; dim))\r\n    ```\r\n    Axis of size 1 will be broadcast if necessary.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    is_reverse: whether the subtraction operation is performed in reverse order.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    coeff: It is an array and allows for scaling the output of the addition operation.;\r\n    // quant param\r\n    multipliers: applied during the concatenation process to adjust the values of the input tensors.;\r\n    rshifts: an array of right shift values corresponding to each input tensor.;\r\n    f8_scales: scaling factors for FP8 (8-bit floating point) quantization.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "is_reverse", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "coeff", "type": "OptionalAttr" },
      { "name": "multipliers", "type": "OptionalAttr" },
      { "name": "rshifts", "type": "OptionalAttr" },
      { "name": "f8_scales", "type": "OptionalAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.SubConst",
    "summary": "sub const operator",
    "description": "1.Op Introduction\r\n    Elementwise subtraction of input1 and input2. Input1 or Input2 is constant.\r\n    as necessary.\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = input - const_val or const_val - input\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attribute\r\n    const_val: the constant value to be added to each element of the input tensor(positive, negative, or zero).;\r\n    is_reverse: This boolean attribute indicates whether the subtraction operation is performed in reverse order.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    // quant param\r\n    multiplier: applied during the concatenation process to adjust the values of the input tensors.;\r\n    rshift: right shift values corresponding to each input tensor.;\r\n    f8_scale: scaling factors for FP8 (8-bit floating point) quantization.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "const_val", "type": "F64Attr" },
      { "name": "is_reverse", "type": "DefaultValuedAttr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "multiplier", "type": "DefaultValuedAttr" },
      { "name": "rshift", "type": "DefaultValuedAttr" },
      { "name": "f8_scale", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.SwapChannel",
    "summary": "SwapChannel operator.",
    "description": "1.Op Introduction\r\n    Swap Channel on input.\r\n\r\n    Interfaces or Traits:\r\n      `NoSideEffect`\r\n      `TpuOpCommonInterface`    : support common TPU TG Op interface.\r\n      `TpuTGOpCodegenInterface` : support generate TPU instuctions.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output(h, w, c) = input(h, w, channel_order)\r\n    ```\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    channel_order: channel swap order.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "channel_order", "type": "I64ArrayAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.SwapDimInner",
    "summary": "SwapDimInner operator.",
    "description": "1.Op Introduction\r\n    a dimension-swapping operation based on a specified offset.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = SwapDimInner(input, offset)\r\n    ```\r\n    3.activation and weight\r\n    input(act.): input tensor;\r\n\r\n    4.attributes\r\n    offset: the position at which the input tensor is split.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "offset", "type": "I64ArrayAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Tile",
    "summary": "Tile operation",
    "description": "1.Op Introduction\r\n    Returns a tensor with the same type as the input, with a new shape\r\n    specified by the shape argument.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i_1, i_2, i_3,...i_k] = input[i_1 mod d_1, i_2 mod d_2, i_3 mod d_3,..., i_k mod d_k]\r\n    ```\r\n    where d_j represents the corresponding dimension size of the input tensor after tiling.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    tileT(w.): how many times to replicate the input tensor along each dimension.;\r\n    buffer(w.): temporary storage of intermediate results or states during the computation.;\r\n\r\n    4.attributes\r\n    tile: the number of times to replicate the input tensor along each dimension.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "tileT", "type": "Optional" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "tile", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.TopK",
    "summary": "TopK operation",
    "description": "1.Op Introduction\r\n    Integrates some operations related to topk.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output_values, output_indices = TopK(input, K, axis, largest, sorted)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n    kT(w.): provide a specific tensor for K values. This allows for dynamic specification of K.;\r\n\r\n    4.attributes\r\n    axis: the dimension of the input tensor.;\r\n    K: how many of the largest (or smallest, depending on the largest attribute) values will be returned. defaults is -1;\r\n    largest: whether to retrieve the largest or smallest values.;\r\n    sorted: whether the output values should be sorted in descending order (if largest is true) or ascending order (if largest is false).;\r\n    values_used_only: whether to return only the values of the top K elements without their corresponding indices.;\r\n    buffer_val: the provision of a buffer tensor where the output values can be stored.;\r\n    buffer_idx: the provision of a buffer tensor for storing the output indices of the top K elements.;\r\n    replace_topk_indices: whether to replace the indices of the top K elements in the output with new indices.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "kT", "type": "Optional" },
      { "name": "buffer_val", "type": "AnyTensorOrNone" },
      { "name": "buffer_idx", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "values", "type": "AnyTensorOrNone" },
      { "name": "indices", "type": "AnyTensorOrNone" }
    ],
    "attributes": [
      { "name": "axis", "type": "I64Attr" },
      { "name": "K", "type": "I64Attr" },
      { "name": "largest", "type": "DefaultValuedAttr" },
      { "name": "sorted", "type": "DefaultValuedAttr" },
      { "name": "values_used_only", "type": "DefaultValuedAttr" },
      { "name": "replace_topk_indices", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Trilu",
    "summary": "Trilu operation",
    "description": "1.Op Introduction\r\n    Returns the upper or lower triangular part of input.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Triu(input, diagonal) if upper = 1\r\n            output = Tril(input, diagonal) if upper = 0\r\n    ```\r\n    where, Triu() return the upper triangular part of the input tensor.\r\n           Tril() return the lower triangular part of the input tensor.\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    upper: whether to extract the upper or lower triangular part of the input tensor.;\r\n    diagonal: 0 refers to the main diagonal, positive values indicate diagonals above the main diagonal,\r\n              and negative values indicate diagonals below the main diagonal.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "upper", "type": "SI32Attr" },
      { "name": "diagonal", "type": "SI32Attr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Unsqueeze",
    "summary": "Onnx-Style Unsqueeze operator",
    "description": "1.Op Introduction\r\n    Unsqueeze Operation on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = unsqueeze(input, axes)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    axes: the dimensions (axes) of the input tensor that should be squeezed (removed).;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "axes", "type": "I64ArrayAttr" }
    ]
  },
  {
    "name": "tpu.Upsample",
    "summary": "Upsample operation",
    "description": "1.Op Introduction\r\n    Perform nearest upsample on input.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output[i, j] = Upsample(input[i / scale_h, j / scale_w])\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    scale_h: the scaling factor for the height (number of rows) of the input tensor.;\r\n    scale_w: the scaling factor for the width (number of columns) of the input tensor.;\r\n    do_relu: If set true, the output will be activated via the ReLU function after the calculation is complete.;\r\n    relu_limit: If set -1.0, it means that there is no upper limit and the output will only be affected by the ReLU function.;\r\n    ginfo: associated with layer grouping information.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" },
      { "name": "indices", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "scale_h", "type": "I64Attr" },
      { "name": "scale_w", "type": "I64Attr" },
      { "name": "do_relu", "type": "DefaultValuedAttr" },
      { "name": "relu_limit", "type": "DefaultValuedAttr" },
      { "name": "ginfo", "type": "OptionalAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Weight2Activation",
    "summary": "Weight to activation operator",
    "description": "1.Op Introduction\r\n    Convert weight tensor to activation tensor\r\n\r\n    2.Math formula\r\n    ```math\r\n            \\text{output}[i] = \\text{input}[i] \\quad \\forall i\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;",
    "inputs": [
      { "name": "input", "type": "AnyRankedTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ]
  },
  {
    "name": "tpu.WeightReorder",
    "summary": "WeightReorder operator",
    "description": "1.Op Introduction\r\n    reorder Weight.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = Reorder(input, reorder_mode)\r\n    ```\r\n\r\n    3.activation and weight\r\n    input(act.): input tensor.;\r\n\r\n    4.attributes\r\n    reorder_mode: rearranging the weight tensor, such as sorting, shuffling, or applying a specific permutation.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "input", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyTensor" }
    ],
    "attributes": [
      { "name": "reorder_mode", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Where",
    "summary": "Where operator",
    "description": "1.Op Introduction\r\n    Return elements, either from X or Y, depending on condition.\r\n\r\n    2.Math formula\r\n    ```math\r\n            output = tbrn if condition else fbrn\r\n    ```\r\n\r\n    3.activation and weight\r\n    cond(act.): a tensor that serves as the condition for selecting elements from the true branch (tbrn) or the false branch (fbrn).;\r\n    tbrn(w.): the tensor that will be selected when the condition is true.;\r\n    fbrn(w.): the tensor that will be selected when the condition is false.;\r\n    buffer(w.): temporary storage of intermediate results or states during the LSTM computation.;\r\n\r\n    4.attributes\r\n    x_is_const: the tensor for the true branch (tbrn) is a constant.;\r\n    y_is_const: the tensor for the false branch (fbrn) is a constant.;\r\n    x_const_val: the constant value to be used for the true branch if tbrn is not provided or is constant.;\r\n    y_const_val: the constant value to be used for the false branch if fbrn is not provided or is constant.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "cond", "type": "AnyRankedTensor" },
      { "name": "tbrn", "type": "AnyTensorOrNone" },
      { "name": "fbrn", "type": "AnyTensorOrNone" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "x_is_const", "type": "DefaultValuedAttr" },
      { "name": "y_is_const", "type": "DefaultValuedAttr" },
      { "name": "x_const_val", "type": "DefaultValuedAttr" },
      { "name": "y_const_val", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" },
      { "name": "do_core_parallel", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.WhereBnbwdOp",
    "summary": "WhereBnbwd operation",
    "description": "1.Op Introduction\r\n    Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs\r\n    with additional channel dimension) as described in the paper\r\n    Batch Normalization: Accelerating Deep Network Training by Reducing\r\n    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\r\n\r\n    2.Math formula\r\n    ```math\r\n        output = \\frac{input - \\mathrm{E}[input]}{ \\sqrt{\\mathrm{Var}[input] + \\epsilon}} * \\gamma + \\beta\r\n    ```\r\n    The mean and standard-deviation are calculated per-dimension over\r\n    the mini-batches and $$\\gamma$$ and $$\\beta$$ are learnable parameter vectors\r\n    of size C (where C is the input channel size).\r\n\r\n    3.activation and weight\r\n    where_output(act.): an optional intermediate output from a \"where\" operation applied during the forward pass.;\r\n    where_grad_out(act.): the gradients computed with respect to the \"where\" output.;\r\n    bnbwd_input(act.): the primary input to the batch normalization backward operation.;\r\n    bnbwd_weight(w.): re-scale the normalized input.;\r\n    bnbwd_bias(w.): re-centering the normalized values.;\r\n    bnbwd_saved_mean(w.): the per-dimension mean values computed during the forward pass.;\r\n    bnbwd_saved_invstd(w.): stores the inverse standard deviation computed during the forward pass.;\r\n    buffer(w.): serve as a temporary storage or workspace that may be used during the computation of gradients.;\r\n\r\n    4.attributes\r\n    epsilon:;\r\n    do_recompute: whether recomputed during the backward pass instead of being stored from the forward pass.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "where_output", "type": "AnyTensorOrNone" },
      { "name": "where_grad_out", "type": "AnyTensor" },
      { "name": "bnbwd_input", "type": "AnyTensor" },
      { "name": "bnbwd_weight", "type": "AnyTensorOrNone" },
      { "name": "bnbwd_bias", "type": "AnyTensorOrNone" },
      { "name": "bnbwd_saved_mean", "type": "AnyTensorOrNone" },
      { "name": "bnbwd_saved_invstd", "type": "AnyTensorOrNone" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "grad_in", "type": "AnyTensor" },
      { "name": "weight_grad", "type": "AnyTensorOrNone" },
      { "name": "bias_grad", "type": "AnyTensorOrNone" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "DefaultValuedAttr" },
      { "name": "do_recompute", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Yield",
    "summary": "Yield values to parent operation",
    "description": "1.Op Introduction\r\n    yields values to its parent operation.\r\n\r\n    2.Math formula\r\n        output_i = input_i i = 1, 2, ..., N\r\n\r\n    3.activation and weight\r\n    operands(act.): a variadic number of input tensors.;",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ]
  },
  {
    "name": "tpu.YoloDetection",
    "summary": "YoloDetection operator",
    "description": "1.Op Introduction\r\n    Perform yolo detection on feature map.\r\n\r\n    2.Math formula\r\n    ```math\r\n        1.Feature Map output\r\n            raw_predictions = {(b_i, c_i, p_i) | i = 1, 2,...,N}\r\n            b_i is the bounding box coordinates, c_i is the i-th class score, p_i is the i-th obj score.\r\n        2.Apply Objectness Threshold\r\n            filtered_predictions = {(b_i, c_i, p_i) | p_i >= obj_threshold}\r\n        3.Non-Maximum Suppression(NMS)\r\n            nms_output = NMS(filtered_predictions, nms_threshold)\r\n        4.Top K Detections\r\n            output = top_k(nms_output, keep_topk)\r\n    ```\r\n\r\n    3.activation and weight\r\n    inputs(act.): input tensor;\r\n    buffer(w.): temporary storage of intermediate results or states during the LSTM computation.;\r\n\r\n    4.attributes\r\n    net_input_h: The height of the input image.;\r\n    net_input_w: The width of the input image;\r\n    nms_threshold: The threshold used for Non-Maximum Suppression (NMS).;\r\n    obj_threshold: The minimum confidence score required for an object detection to be considered valid.;\r\n    keep_topk: The maximum number of detections to keep after applying NMS.;\r\n    anchors: A list of anchor box dimensions.;\r\n    version: The version of the YOLO model being used.;\r\n    class_num: The number of classes that the YOLO model can predict.;\r\n    num_boxes: The number of bounding boxes that the model predicts for each grid cell in the feature map.;\r\n    agnostic_nms: whether to use class-agnostic NMS.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "buffer", "type": "AnyTensorOrNone" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "net_input_h", "type": "I64Attr" },
      { "name": "net_input_w", "type": "I64Attr" },
      { "name": "nms_threshold", "type": "F64Attr" },
      { "name": "obj_threshold", "type": "F64Attr" },
      { "name": "keep_topk", "type": "I64Attr" },
      { "name": "anchors", "type": "F64ArrayAttr" },
      { "name": "version", "type": "YoloVersionAttr" },
      { "name": "class_num", "type": "DefaultValuedAttr" },
      { "name": "num_boxes", "type": "DefaultValuedAttr" },
      { "name": "agnostic_nms", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "tpu.Yuv2rgbFormula",
    "summary": "Yuv2rgbFormulaOp operator",
    "description": "1.Op Introduction\r\n    Yuv2rgbFormulaOp operator.\r\n\r\n    2.Math formula\r\n    ```math\r\n            (R) = (Y + 1.402 x (V - 128))\r\n            (G) = (Y - 0.344136 x (U - 128) - 0.714136 x (V - 128))\r\n            (B) = (Y + 1.772 x (U - 128))\r\n\r\n    ```\r\n\r\n    3.activation and weight\r\n    YUV(act.): input tensor.;\r\n\r\n    4.attributes\r\n    src_format: the source format of the input YUV data.;\r\n    dst_format: the desired destination format for the output RGB data.;\r\n    image_format: how the YUV data should be processed and how the output RGB data should be structured.;\r\n    formula_mode: the mode of the conversion formula used for the YUV to RGB transformation. ;\r\n    round_mode: how values are rounded during the conversion from higher precision to lower precision.;\r\n    multicore: whether op supports multicore execution.;",
    "inputs": [
      { "name": "YUV", "type": "AnyTensor" }
    ],
    "outputs": [
      { "name": "output", "type": "AnyRankedTensor" }
    ],
    "attributes": [
      { "name": "src_format", "type": "UI32Attr" },
      { "name": "dst_format", "type": "UI32Attr" },
      { "name": "image_format", "type": "Tpu_ImageOutFormatAttr" },
      { "name": "formula_mode", "type": "Tpu_Yuv2rgbFormulaAttr" },
      { "name": "round_mode", "type": "DefaultValuedAttr" },
      { "name": "multicore", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "transform.alternatives",
    "summary": "Attempts sequences of transforms until one succeeds",
    "description": "This op may have an arbitrary number of regions, each of which represents a\n    sequence of transform operations to be applied to the same payload IR. The\n    regions are visited in order of appearance, and transforms in them are\n    applied in their respective order of appearance. If one of these transforms\n    fails to apply, the remaining ops in the same region are skipped an the next\n    region is attempted. If all transformations in a region succeed, the\n    remaining regions are skipped and the entire \"alternatives\" transformation\n    succeeds. If all regions contained a failing transformation, the entire\n    \"alternatives\" transformation fails.\n\n    It is up to the nested operations to define which errors are \"recoverable\"\n    (or \"silenceable\") and allow another alternatives to be attempted, and which\n    errors should be propagated without attempting the other alternatives.\n\n    The single operand of this operation is the scope in which the alternative\n    transformation sequences are attempted, that is, an operation in the payload\n    IR that contains all the other operations that may be modified by the\n    transformations. The scope operation must be isolated from above. There is\n    no check that the transforms are indeed scoped as their \"apply\" methods can\n    be arbitrarily complex. Therefore it is the responsibility of the user to\n    ensure that the transforms are scoped correctly, or to produce an\n    irrecoverable error and thus abort the execution without attempting the\n    remaining alternatives. Note that the payload IR outside of the given scope\n    is not necessarily in the valid state, or even accessible to the\n    transformation.\n\n    The changes to the IR within the scope performed by transforms in the failed\n    alternative region are reverted before attempting the next region.\n    Practically, this is achieved by cloning the scope. Therefore it is advised\n    to limit the scope as much as possible and place the most likely\n    alternatives early in the region list. The operation is also isolated from\n    above and requires rediscovering the operations within the given scope to\n    avoid additional handle invalidation. The latter restriction may be lifted\n    in the future.\n\n    Each of the regions may yield transform IR handles. The handles of the first\n    successful alternative region are returned as the results of the\n    \"alternatives\" op. Therefore, each alternative region must yield the same\n    number of results, which should also match the number and the types of the\n    \"alternatives\" op results.\n\n    Remark: this op allows one to implement a simple \"try\" construct as follows:\n\n    ```mlir\n    %result = transform.alternatives %scope {\n    ^bb0(%arg0: !transform.any_op):\n      // Try a fallible transformation.\n      %0 = transform.fallible %arg0 // ...\n      // If succeeded, yield the the result of the transformation.\n      transform.yield %0 : !transform.any_op\n    }, {\n    ^bb0(%arg0: !transform.any_op):\n      // Otherwise, the second alternative is tried and it always succeeds by\n      // returning the original handle.\n      transform.yield %arg0 : !transform.any_op\n    }\n    ```",
    "inputs": [
      { "name": "scope", "type": "Optional" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "($scope^ `:` type($scope))? (`->` type($results)^)? attr-dict-with-keyword regions"
  },
  {
    "name": "transform.annotate",
    "summary": "Annotates the target operation with an attribute by name",
    "description": "Adds an attribute with the given `name` to the `target` operation. An\n    optional `param` handle can be provided to give the attribute a specific\n    value, else a UnitAttr is added. A single attribute will be broadcasted to\n    all target operations, otherwise the attributes will be mapped 1:1 based on\n    the order within the handles.\n\n    Produces a silenceable failure if the length of the parameter payload does\n    not match the length of the target payload. Does not consume the provided\n    handles.",
    "inputs": [
      { "name": "target", "type": "TransformHandleTypeInterface" },
      { "name": "param", "type": "Optional" }
    ],
    "attributes": [
      { "name": "name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$target $name attr-dict (`=` $param^)?`:` type($target) (`,` type($param)^)?"
  },
  {
    "name": "transform.apply_conversion_patterns",
    "summary": "Applies conversion patterns to the body of the targeted op",
    "description": "This transform applies the specified conversion patterns to the targeted op\n    and all nested ops. By default, this transform applies a \"full\" dialect\n    conversion. If the `partial_conversion` unit attribute is present, this\n    transform applies a partial dialect conversion.\n\n    The patterns that should be applied are specified in the first graph region\n    of this op. They must implement the\n    `ConversionPatternDescriptorOpInterface`. The order in which patterns are\n    applied is unspecified; i.e., the ordering of ops in the region of this op\n    is irrelevant.\n\n    The second, optional graph region contains exactly one op that specifies\n    default type converter that should be used with this dialect conversion. If\n    provided, this op must implement the `TypeConverterBuilderOpInterface`.\n    Type converters are a property of conversion patterns: each conversion\n    pattern stores the type converter that should be used in its C++ class. Each\n    conversion pattern descriptor can optionally specify a type converter in its\n    `getTypeConverter` interface method. If no type converter is specified in\n    this method, the default type converter of the dialect conversion is used.\n    Default type converters are useful if the same type converter should be used\n    for multiple sets of conversion patterns. (Patterns that should not use this\n    default type converter specify their own type converter.)\n\n    The `legal_ops`, `illegal_ops`, `legal_dialects`, `illegal_dialects`\n    attributes specify the conversion target.\n\n    This transform modifies the payload. By default, it consumes the `target`\n    handle. It does not produce any handles.\n\n    If the `preserve_handles` attribute is set, this transform does not consume\n    the `target` handle and instead updates handles based on notifications from\n    a tracking listener that is attached to the dialect conversion, similar to\n    `transform.apply_patterns`. Only replacements via `RewriterBase::replaceOp`\n    or `replaceOpWithNewOp` are considered \"payload op replacements\". In\n    contrast to `transform.apply_patterns`, we allow replacement ops even if the\n    op name has changed. This is because conversion patterns are expected to\n    lower ops to different ops (from a different dialect). More details can be\n    found at the documentation site of `TrackingListener`.\n\n    This transform produces a silenceable failure if the dialect conversion was\n    unsuccessful or the tracking listener failed to find a replacement op.",
    "inputs": [
      { "name": "target", "type": "TransformHandleTypeInterface" }
    ],
    "attributes": [
      { "name": "legal_ops", "type": "OptionalAttr" },
      { "name": "illegal_ops", "type": "OptionalAttr" },
      { "name": "legal_dialects", "type": "OptionalAttr" },
      { "name": "illegal_dialects", "type": "OptionalAttr" },
      { "name": "partial_conversion", "type": "UnitAttr" },
      { "name": "preserve_handles", "type": "UnitAttr" }
    ],
    "assemblyFormat": "`to` $target $patterns\n    (`with` `type_converter` $default_type_converter_region^)?\n    attr-dict `:` type($target)"
  },
  {
    "name": "transform.apply_conversion_patterns.dialect_to_llvm",
    "description": "Collects patterns that convert ops from the specified dialect to LLVM\n    dialect ops. These patterns require an \"LLVMTypeConverter\".\n\n    Note: Only dialects that implement the `ConvertToLLVMPatternInterface` are\n    supported. Any conversion target modifications by interface implementations\n    are currently ignored. The conversion target is fully specified by the\n    enclosing \"apply_conversion_patterns\" op.",
    "attributes": [
      { "name": "dialect_name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$dialect_name attr-dict"
  },
  {
    "name": "transform.apply_cse",
    "summary": "Eliminate common subexpressions in the body of the target op",
    "description": "This transform applies common subexpression elimination (CSE) to the body\n    of the targeted op.\n\n    This transform reads the target handle and modifies the payload. Existing\n    handles to operations inside of the targeted op are retained and updated if\n    necessary. Note that this can lead to situations where a handle, that was\n    previously mapped to multiple distinct (but equivalent) operations, is now\n    mapped to the same operation multiple times.",
    "inputs": [
      { "name": "target", "type": "TransformHandleTypeInterface" }
    ],
    "assemblyFormat": "`to` $target attr-dict `:` type($target)"
  },
  {
    "name": "transform.apply_dce",
    "summary": "Eliminate dead operations in the body of the target op",
    "description": "This transform applies dead code elimination (DCE) to the body of the\n    targeted op.\n\n    Note: \"transform.apply_patterns\" with an empty region can also be used to\n    remove dead ops. However, that op applies additional simplifications such as\n    op folding and region simplification.\n\n    This transform reads the target handle and modifies the payload. Note that\n    this transform may silently remove payload ops from handles.",
    "inputs": [
      { "name": "target", "type": "TransformHandleTypeInterface" }
    ],
    "assemblyFormat": "`to` $target attr-dict `:` type($target)"
  },
  {
    "name": "transform.apply_licm",
    "summary": "Move loop-invariant code out of a loop-like op",
    "description": "This transform moves side-effect free, loop invariant code out of the\n    targeted loop-like op. The targeted op must implement the\n    `LoopLikeOpInterface`.\n\n    Note: To move invariant ops from a loop nest, this transform must be applied\n    to each loop of the loop nest, starting with the inner-most loop.\n\n    This transform reads the target handle and modifies the payload.",
    "inputs": [
      { "name": "target", "type": "TransformHandleTypeInterface" }
    ],
    "assemblyFormat": "`to` $target attr-dict `:` type($target)"
  },
  {
    "name": "transform.apply_patterns",
    "summary": "Greedily applies patterns to the body of the targeted op",
    "description": "This transform greedily applies the specified patterns to the body of the\n    targeted op until a fixpoint was reached. Patterns are not applied to the\n    targeted op itself.\n\n    The patterns that should be applied are specified in the graph region of\n    this op. They must implement the `PatternDescriptorOpInterface`. The order\n    in which patterns are applied is unspecified; i.e., the ordering of ops in\n    the region of this op is irrelevant.\n\n    If `apple_cse` is set, the greedy pattern rewrite is interleaved with\n    common subexpression elimination (CSE): both are repeated until a fixpoint\n    is reached.\n\n    This transform only reads the target handle and modifies the payload. If a\n    pattern erases or replaces a tracked op, the mapping is updated accordingly.\n\n    Only replacements via `RewriterBase::replaceOp` or `replaceOpWithNewOp` are\n    considered \"payload op replacements\". Furthermore, only if the replacement\n    values are defined by the same op and that op has the same type as the\n    original op, the mapping is updated. Otherwise, this transform produces a\n    silenceable failure. More details can be found at the documentation site of\n    `TrackingListener`.\n\n    This transform also produces a silenceable failure if the pattern\n    application did not converge within the default number of\n    iterations/rewrites of the greedy pattern rewrite driver.",
    "inputs": [
      { "name": "target", "type": "TransformHandleTypeInterface" }
    ],
    "attributes": [
      { "name": "apply_cse", "type": "UnitAttr" },
      { "name": "max_iterations", "type": "DefaultValuedAttr" },
      { "name": "max_num_rewrites", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "`to` $target $patterns attr-dict `:` type($target)"
  },
  {
    "name": "transform.apply_patterns.canonicalization",
    "summary": "Populates canonicalization patterns",
    "description": "This op populates all canonicalization patterns of all loaded dialects in\n    an `apply_patterns` transform.",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "transform.apply_registered_pass",
    "summary": "Applies the specified registered pass or pass pipeline",
    "description": "This transform applies the specified pass or pass pipeline to the targeted\n    ops. The name of the pass/pipeline is specified as a string attribute, as\n    set during pass/pipeline registration.\n\n    Optionally, pass options may be specified via a DictionaryAttr. This\n    dictionary is converted to a string -- formatted `key=value ...` -- which\n    is expected to be in the exact format used by the pass on the commandline.\n    Values are either attributes or (SSA-values of) Transform Dialect params.\n    For example:\n\n    ```mlir\n    transform.apply_registered_pass \"canonicalize\"\n        with options = { \"top-down\" = false,\n                         \"max-iterations\" = %max_iter,\n                         \"test-convergence\" = true,\n                         \"max-num-rewrites\" = %max_rewrites }\n        to %module\n    : (!transform.any_param, !transform.any_param, !transform.any_op) -> !transform.any_op\n    ```\n\n    Options' values which are `ArrayAttr`s are converted to comma-separated\n    lists of options. Likewise for params which associate multiple values.\n\n    This op first looks for a pass pipeline with the specified name. If no such\n    pipeline exists, it looks for a pass with the specified name. If no such\n    pass exists either, this op fails definitely.\n\n    This transform consumes the target handle and produces a new handle that is\n    mapped to the same op. Passes are not allowed to remove/modify the operation\n    that they operate on, so the target op is guaranteed to still exist. The\n    target handle is invalidated because a pass may arbitrarily modify the body\n    of targeted ops.",
    "inputs": [
      { "name": "target", "type": "TransformHandleTypeInterface" },
      { "name": "dynamic_options", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "TransformHandleTypeInterface" }
    ],
    "attributes": [
      { "name": "pass_name", "type": "StrAttr" },
      { "name": "options", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$pass_name (`with` `options` `=`\n      custom<ApplyRegisteredPassOptions>($options, $dynamic_options)^)?\n      `to` $target attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "transform.collect_matching",
    "summary": "Collects all payload ops that match the given named matcher",
    "description": "Collects operations or other payload IR objects nested under `root`\n    (inclusive) that match the given matcher expressed as a named sequence. The\n    matcher sequence must accept exactly one argument that it is not allowed to\n    modify. It must yield as many values as this op has results. Each of the\n    yielded values must be associated with exactly one payload object. If any\n    operation in the matcher sequence produces a silenceable failure, the\n    matcher advances to the next payload operation in the walk order without\n    finishing the sequence.\n\n    The i-th result of this operation is constructed by concatenating the i-th\n    yielded payload IR objects of all successful matcher sequence applications.\n    All results are guaranteed to be mapped to the same number of payload IR\n    objects.\n\n    The operation succeeds unless the matcher sequence produced a definite\n    failure for any invocation.",
    "inputs": [
      { "name": "root", "type": "TransformHandleTypeInterface" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "matcher", "type": "SymbolRefAttr" }
    ],
    "assemblyFormat": "$matcher `in` $root attr-dict `:` functional-type($root, $results)"
  },
  {
    "name": "transform.foreach",
    "summary": "Executes the body for each element of the payload",
    "description": "Execute the op's body - its single region block - exactly once per\n    element of the payload associated to a target handle. The body's\n    transformations are applied in order of appearance until reaching the\n    (implicit) YieldOp terminator.\n\n    Each iteration gets executed by co-indexing the payloads of the arguments\n    and mapping the body's arguments to these tuples, as though iterating over\n    the zipped together `targets`. As such, in each iteration, the size of the\n    payload of each of the body's block arguments is exactly one. The attribute\n    `zip_shortest` can be used if the targets vary in their number of payloads;\n    this will limit the iterations to only the number of payloads found in the\n    shortest target.\n\n    This op always reads the target handles. Furthermore, it consumes a handle\n    if there is a transform op in the body that consumes the corresponding\n    block argument. Handles can point to ops, values, or parameters.\n\n    #### Return Modes\n\n    This op produces as many result handles as the body's terminating YieldOp\n    has operands. For each result, the payloads of the corresponding YieldOp\n    operand are merged and mapped to the same resulting handle.\n\n    If the target handles do not associate payloads of the same size, a\n    silencable failure will be generated.\n\n    During application, if any transformation in the sequence fails, the entire\n    sequence fails immediately with the same failure, leaving the payload IR in\n    a potentially invalid state, i.e., this operation offers no transformation\n    rollback capabilities.",
    "inputs": [
      { "name": "targets", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "with_zip_shortest", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$targets oilist(`with_zip_shortest` $with_zip_shortest) `:` type($targets) (`->` type($results)^)? $body attr-dict"
  },
  {
    "name": "transform.foreach_match",
    "summary": "Applies named sequences when a named matcher succeeds",
    "description": "Given a pair of co-indexed lists of transform dialect symbols (such as\n    `transform.named_sequence`), walks the payload IR associated with the root\n    handle and interprets the symbols as matcher/action pairs by applying the\n    body of the corresponding symbol definition. The symbol from the first list\n    is the matcher part: if it results in a silenceable error, the error is\n    silenced and the next matcher is attempted. Definite failures from any\n    matcher stop the application immediately and are propagated unconditionally.\n    If none of the matchers succeeds, the next payload operation in walk order\n    (post-order at the moment of writing, double check `Operation::walk`) is\n    matched. If a matcher succeeds, the co-indexed action symbol is applied and\n    the following matchers are not applied to the same payload operation. If the\n    action succeeds, the next payload operation in walk order is matched. If it\n    fails, both silenceable and definite errors are propagated as the result of\n    this op; propagation of silenceable errors is postponed until the end of the\n    walk.\n\n    The matcher symbol must take at least one operand of a type that implements\n    the same transform dialect interface as the `root` operand (a check is\n    performed at application time to see if the associated payload satisfies the\n    constraints of the actual type), and may take additional operands with a\n    similar type requirement. It must not consume operands as multiple matchers\n    may be applied. The matcher may produce any number of results. The action\n    symbol paired with the matcher must take the same number of arguments as the\n    matcher has results, and these arguments must implement the same transform\n    dialect interfaces, but not necessarily have the exact same type (again, a\n    check is performed at application time to see if the associated payload\n    satisfies the constraints of actual types on both sides).\n\n    The action symbol may have results that are accumulated from all actions and\n    returned from the `foreach_match` operation on success. Unless the\n    `flatten_results` attribute is present, each action result must be\n    associated with exactly one payload entity. The actions are expected to only\n    modify payload operations nested in the `root` payload operations associated\n    with the operand of this transform operation. Furthermore, the actions may\n    not modify operations outside of the currently matched payload operation,\n    e.g., they may not modify sibling or parent operations. If such behavior is\n    desired, the parent must be matched first and the nested operations obtained\n    by traversing the IR from the parent. This is due to the matching being\n    performed as a post-order IR walk.\n\n    This operation consumes the operand and produces a new handle associated\n    with the same payload. This is necessary to trigger invalidation of handles\n    to any of the payload operations nested in the payload operations associated\n    with the operand, as those are likely to be modified by actions.\n\n    By default, the root payload operation associated with the operand is not\n    matched. This is to support the conservative case where applied actions may\n    invalidate the root payload operation. If the optional `restrict_root`\n    attribute is set, the root operand is guaranteed to not be invalidated by any\n    of the applied actions. In such cases, the root payload operation is also\n    matched. This is useful because matching the root payload operation is a\n    common idiom, when e.g. matching a func.func directly and operations nested\n    under it.\n\n    The operation succeeds if none of the matchers produced a definite failure\n    during application and if all of the applied actions produced success. Note\n    that it also succeeds if all the matchers failed on all payload operations,\n    i.e. failure to apply is not an error. The operation produces a silenceable\n    failure if any applied action produced a silenceable failure. In this case,\n    the resulting handle is associated with an empty payload. The operation\n    produces a definite failure if any of the applied matchers or actions\n    produced a definite failure.",
    "inputs": [
      { "name": "root", "type": "TransformHandleTypeInterface" },
      { "name": "forwarded_inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "updated", "type": "TransformHandleTypeInterface" },
      { "name": "forwarded_outputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "restrict_root", "type": "UnitAttr" },
      { "name": "flatten_results", "type": "UnitAttr" },
      { "name": "matchers", "type": "SymbolRefArrayAttr" },
      { "name": "actions", "type": "SymbolRefArrayAttr" }
    ],
    "assemblyFormat": "oilist( `restrict_root` $restrict_root\n          | `flatten_results` $flatten_results\n          )\n    `in`\n    $root (`,` $forwarded_inputs^)?\n    custom<ForeachMatchSymbols>($matchers, $actions)\n    attr-dict\n    `:` functional-type(operands, results)"
  },
  {
    "name": "transform.get_consumers_of_result",
    "summary": "Get handle to the consumers of this operation's result number",
    "description": "The handle defined by this Transform op corresponds to all operations that\n    consume the SSA value defined by the `target` and `result_number`\n    arguments.\n    This operation applies to a single payload operation, otherwise it produces\n    a definite failure.\n    The return handle points to the consuming operations operations, which can\n    be empty.",
    "inputs": [
      { "name": "target", "type": "TransformHandleTypeInterface" }
    ],
    "outputs": [
      { "name": "consumers", "type": "TransformHandleTypeInterface" }
    ],
    "attributes": [
      { "name": "result_number", "type": "I64Attr" }
    ],
    "assemblyFormat": "$target `[` $result_number `]` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "transform.get_defining_op",
    "summary": "Get handle to the defining op of a value",
    "description": "The handle defined by this Transform op corresponds to the defining op of\n    the targeted value.\n\n    This transform produces a silenceable failure if the targeted value is a\n    block argument.",
    "inputs": [
      { "name": "target", "type": "TransformValueHandleTypeInterface" }
    ],
    "outputs": [
      { "name": "result", "type": "TransformHandleTypeInterface" }
    ],
    "assemblyFormat": "$target attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "transform.get_operand",
    "summary": "Get a handle to the operand(s) of the targeted op",
    "description": "The handle defined by this Transform op corresponds to the operands of the\n    given `target` operation specified by the given set of positions. There are\n    three possible modes:\n\n     - Position list directly, i.e. `%target[0, 1, 2]`. This will return the\n       operands at the specified positions.\n     - Inverted position list, i.e. `%target[except(0, 1, 2)]`. This will return\n       all operands except those at the given positions.\n     - All, i.e. `%target[all]`. This will return all operands of the operation.\n    \n    This transform produces a silenceable failure if any of the operand indices\n    exceeds the number of operands in the target. It reads the target handle and\n    produces the result handle.",
    "inputs": [
      { "name": "target", "type": "TransformHandleTypeInterface" }
    ],
    "outputs": [
      { "name": "result", "type": "TransformValueHandleTypeInterface" }
    ],
    "attributes": [
      { "name": "raw_position_list", "type": "DenseI64ArrayAttr" },
      { "name": "is_inverted", "type": "UnitAttr" },
      { "name": "is_all", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$target `[`custom<TransformMatchDims>($raw_position_list, $is_inverted, $is_all)`]` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "transform.get_parent_op",
    "summary": "Gets handles to the closest parent ops",
    "description": "The handle defined by this Transform op corresponds to the parents of the\n    targeted payload ops (in the same order).\n\n    Requirements that parent ops must fulfill can be optionally specified. In\n    that case for each target op, the closest parent op that fulfills all\n    requirements, is returned.\n    - `isolated_from_above`: the parent op must be isolated from above\n    - `allow_empty_results`: get_parent_op is allowed to return an empty list\n      and still succeeds. In such a case, if `get_parent_op` fails for any\n      operation in the list, the entire transform returns an empty handle.\n    - `op_name`: the parent op must have the specified name\n    - `nth_parent`: get the n-th parent of that satisfies the above requirements\n\n    If `deduplicate` is set, the result handle does not contain any duplicate\n    ops. For example, given the list\n    \"(childof(A), childof(B), childof(B), childof(A), childof(B))\", the\n    resulting list will be just \"(A, B)\". Note that no other semantic ordering\n    is applied, e.g., \"B\" may itself be a parent of \"A\". This may have an impact\n    on the further transformation applied to the handle produced here.\n\n    If any of the given Payload IR ops has no such suitable parent, then:\n      - if `allow_empty_results` is set, the result handle is empty\n      - otherwise, the transformation produces a silenceable failure.",
    "inputs": [
      { "name": "target", "type": "TransformHandleTypeInterface" }
    ],
    "outputs": [
      { "name": "parent", "type": "TransformHandleTypeInterface" }
    ],
    "attributes": [
      { "name": "isolated_from_above", "type": "UnitAttr" },
      { "name": "allow_empty_results", "type": "UnitAttr" },
      { "name": "op_name", "type": "OptionalAttr" },
      { "name": "deduplicate", "type": "UnitAttr" },
      { "name": "nth_parent", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "$target attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "transform.get_producer_of_operand",
    "summary": "Get handle to the producer of this operation's operand number",
    "description": "The handle defined by this Transform op corresponds to operation that\n    produces the SSA value defined by the `target` and `operand_number`\n    arguments. If the origin of the SSA value is not an operations (i.e. it is\n    a block argument), the transform produces a silenceable failure.\n    The return handle points to only the subset of successfully produced\n    computational operations, which can be empty.",
    "inputs": [
      { "name": "target", "type": "TransformHandleTypeInterface" }
    ],
    "outputs": [
      { "name": "producer", "type": "TransformHandleTypeInterface" }
    ],
    "attributes": [
      { "name": "operand_number", "type": "I64Attr" }
    ],
    "assemblyFormat": "$target `[` $operand_number `]` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "transform.get_result",
    "summary": "Get a handle to the result(s) of the targeted op",
    "description": "The handle defined by this Transform op correspond to the OpResults of the\n    given `target` operation. Optionally `result_number` can be specified to\n    select a specific result.\n    \n    This transform fails silently if the targeted operation does not have enough\n    results. It reads the target handle and produces the result handle.\n\n    The handle defined by this Transform op corresponds to the results of the\n    given `target` operation specified by the given set of positions. There are\n    three possible modes:\n\n     - Position list directly, i.e. `%target[0, 1, 2]`. This will return the\n       results at the specified positions.\n     - Inverted position list, i.e. `%target[except(0, 1, 2)]`. This will return\n       all results except those at the given positions.\n     - All, i.e. `%target[all]`. This will return all results of the operation.\n    \n    This transform produces a silenceable failure if any of the result indices\n    exceeds the number of results returned by the target. It reads the target\n    handle and produces the result handle.",
    "inputs": [
      { "name": "target", "type": "TransformHandleTypeInterface" }
    ],
    "outputs": [
      { "name": "result", "type": "TransformValueHandleTypeInterface" }
    ],
    "attributes": [
      { "name": "raw_position_list", "type": "DenseI64ArrayAttr" },
      { "name": "is_inverted", "type": "UnitAttr" },
      { "name": "is_all", "type": "UnitAttr" }
    ],
    "assemblyFormat": "$target `[`custom<TransformMatchDims>($raw_position_list, $is_inverted, $is_all)`]` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "transform.get_type",
    "summary": "Get a parameter containing the type of the given value",
    "description": "This operation creates a new Transform parameter containing the\n    type(s) of the value(s) associated with the operand handle.\n\n    This transform never fails.",
    "inputs": [
      { "name": "value", "type": "TransformValueHandleTypeInterface" }
    ],
    "outputs": [
      { "name": "type_param", "type": "TransformParamTypeInterface" }
    ],
    "attributes": [
      { "name": "elemental", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`elemental` $elemental^)? $value attr-dict `:`functional-type(operands, results)"
  },
  {
    "name": "transform.include",
    "summary": "Includes a named transform sequence",
    "description": "The application of this transform operation is equivalent to applying the\n    operations contained in the named transform sequence with operands being\n    remapped to block arguments. The behavior of the operation when a\n    transformation in the included named sequence produces a silenceable error\n    is controlled by the `failure_propagation_mode` attribute. When set to\n    `propagate`, the failure of any nested transformation in the sequence\n    implies immediate failure of the entire sequence with a silenceable error,\n    and no further transformation is attempted. When set to `suppress`,\n    silenceable errors in nested operations are ignored and further\n    transformations are applied. Beware that even silenceable errors may leave\n    the payload IR in a state unsuitable for further transformations. It is the\n    responsibility of the user to ensure the following transformations are\n    robust enough when errors are suppressed. Definite errors are propagated\n    immediately regardless of the mode. The objects associated with the results\n    of this operation are the same as those associated with the operands of the\n    `transform.yield` in the referenced named sequence.",
    "inputs": [
      { "name": "failure_propagation_mode", "type": "FailurePropagationMode" },
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "target", "type": "SymbolRefAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$target `failures` `(` $failure_propagation_mode `)``(` $operands `)` attr-dict `:` functional-type($operands, $results)"
  },
  {
    "name": "transform.match.operation_empty",
    "summary": "Matches if the handle is not associated to any op",
    "description": "Succeeds if the handle is not associated to any op.",
    "inputs": [
      { "name": "operand_handle", "type": "TransformHandleTypeInterface" }
    ],
    "assemblyFormat": "$operand_handle attr-dict `:` type($operand_handle)"
  },
  {
    "name": "transform.match.operation_name",
    "summary": "Matches a single operation of one of the given kinds",
    "description": "Succeeds if the operation associated with the operand handle has one of the\n    given operation names. Produces a silenceable failure otherwise.\n\n    If more than one payload operation is associated with the operand handle,\n    produces a definite failure.",
    "inputs": [
      { "name": "operand_handle", "type": "TransformHandleTypeInterface" }
    ],
    "attributes": [
      { "name": "op_names", "type": "StrArrayAttr" }
    ],
    "assemblyFormat": "$operand_handle $op_names attr-dict `:` type($operand_handle)"
  },
  {
    "name": "transform.match.param.cmpi",
    "summary": "Matches if two parameter lists are associated with the same value",
    "description": "Succeeds if all of the co-indexed values associated with the given\n    parameters relate as specified by the predicate (greater than, less than,\n    equal to, or their combinations). Comparison treats all values as signed.\n    Produces a silenceable failure otherwise.",
    "inputs": [
      { "name": "param", "type": "TransformParamTypeInterface" },
      { "name": "reference", "type": "TransformParamTypeInterface" }
    ],
    "attributes": [
      { "name": "predicate", "type": "MatchCmpIPredicateAttr" }
    ],
    "assemblyFormat": "$predicate $param `,` $reference attr-dict `:` type($param)"
  },
  {
    "name": "transform.merge_handles",
    "summary": "Merges handles into one pointing to the union of payload ops",
    "description": "Creates a new Transform IR handle value that points to the same Payload IR\n    operations/values/parameters as the operand handles. The Payload IR elements\n    are listed in the same order as they are in the operand handles, grouped by\n    operand handle, e.g., all Payload IR associated with the first handle comes\n    first, then all Payload IR associated with the second handle and so on. If\n    `deduplicate` is set, do not add the given Payload IR operation, value, or\n    parameter more than once to the final list regardless of it coming from the\n    same or different handles. Consumes the operands and produces a new handle.",
    "inputs": [
      { "name": "handles", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Transform_AnyHandleOrParamType" }
    ],
    "attributes": [
      { "name": "deduplicate", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`deduplicate` $deduplicate^)? $handles attr-dict `:` type($result)"
  },
  {
    "name": "transform.named_sequence",
    "summary": "Named transform sequence that can be included elsewhere",
    "description": "Defines a named (callable, function-like) sequence of other Transform\n    dialect operations that can be included using `transform.include` as part of\n    another Transform dialect construct. This sequence is not processed\n    immediately but rather dispatched to when the inclusion is processed. The\n    arguments and results can be used to communicate a subset of mapping into\n    the named sequence. The sequence must consist of a single block and end with\n    a `transform.yield` terminator. The operands of the terminator become the\n    results of the `transform.include`.\n\n    When dispatched to, the operations in the named sequence are executed one by\n    one, similarly to the regular unnamed sequence. The failure propagation mode\n    is specified on the `transform.include`. Different inclusions may use\n    different failure propagation modes. This transform operation always\n    succeeds by itself, but the inclusion may fail if any of the operations\n    fail.\n\n    Named sequences can only appear at the top-level of the Transform dialect\n    nesting structure. That is, they cannot be nested in other Transform dialect\n    operations. Furthermore, one of the ancestors must have the `SymbolTable`\n    trait and have the `transform.with_named_sequence` attribute attached.\n\n    Named sequences may include other named sequences via `transform.include`,\n    but recursion is *not* allowed.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_type", "type": "TypeAttrBase" },
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "transform.num_associations",
    "summary": "Returns the number of payload objects associated with the argument",
    "description": "Given an argument, handle or parameter, returns a new parameter associated\n    with a single 64-bit number that corresponds to the number of payload\n    objects (operations or values for a handle, attributes for a parameter)\n    associated with the argument.\n\n    Always succeeds.",
    "inputs": [
      { "name": "handle", "type": "Transform_AnyHandleOrParamType" }
    ],
    "outputs": [
      { "name": "num", "type": "TransformParamTypeInterface" }
    ],
    "assemblyFormat": "$handle attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "transform.param.constant",
    "summary": "Produces a new transform dialect parameter value associated with the given attribute",
    "description": "Produces a new transform dialect parameter associated with the singleton\n    list containing the given attribute. The operation itself always succeeds,\n    but the general association check may fail if the parameter type does not\n    accept the given kind of attribute as valid.",
    "outputs": [
      { "name": "param", "type": "TransformParamTypeInterface" }
    ],
    "attributes": [
      { "name": "value", "type": "AnyAttr" }
    ],
    "assemblyFormat": "$value attr-dict `->` type($param)"
  },
  {
    "name": "transform.replicate",
    "summary": "Lists payload ops multiple times in the new handle",
    "description": "Produces a new handle associated with a list of payload IR ops that is\n    computed by repeating the list of payload IR ops associated with the\n    operand handle as many times as the \"pattern\" handle has associated\n    operations. For example, if pattern is associated with [op1, op2] and the\n    operand handle is associated with [op3, op4, op5], the resulting handle\n    will be associated with [op3, op4, op5, op3, op4, op5].\n\n    This transformation is useful to \"align\" the sizes of payload IR lists\n    before a transformation that expects, e.g., identically-sized lists. For\n    example, a transformation may be parameterized by same notional per-target\n    size computed at runtime and supplied as another handle, the replication\n    allows this size to be computed only once and used for every target instead\n    of replicating the computation itself.\n\n    Note that it is undesirable to pass a handle with duplicate operations to\n    an operation that consumes the handle. Handle consumption often indicates\n    that the associated payload IR ops are destroyed, so having the same op\n    listed more than once will lead to double-free. Single-operand\n    MergeHandlesOp may be used to deduplicate the associated list of payload IR\n    ops when necessary. Furthermore, a combination of ReplicateOp and\n    MergeHandlesOp can be used to construct arbitrary lists with repetitions.",
    "inputs": [
      { "name": "pattern", "type": "TransformHandleTypeInterface" },
      { "name": "handles", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "replicated", "type": "Variadic" }
    ],
    "assemblyFormat": "`num` `(` $pattern `)` $handles attr-dict `:` type($pattern) `,` type($handles)"
  },
  {
    "name": "transform.select",
    "summary": "Select payload ops by name",
    "description": "The handle defined by this Transform op corresponds to all operations among\n    `target` that have the specified properties. Currently the following\n    properties are supported:\n\n    - `op_name`: The op must have the specified name.\n\n    The result payload ops are in the same relative order as the targeted ops.\n    This transform op reads the `target` handle and produces the `result`\n    handle. It reads the payload, but does not modify it.",
    "inputs": [
      { "name": "target", "type": "TransformHandleTypeInterface" }
    ],
    "outputs": [
      { "name": "result", "type": "TransformHandleTypeInterface" }
    ],
    "attributes": [
      { "name": "op_name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$op_name `in` $target attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "transform.sequence",
    "summary": "Contains a sequence of other transform ops to apply",
    "description": "The transformations indicated by the sequence are applied in order of their\n    appearance. Each value produced by a transformation within the sequence\n    corresponds to a group of operations or values in the payload IR, or to a\n    group of parameters, depending on the type of the value. The behavior of the\n    operation when a nested transformation produces a silenceable error is\n    controlled by the `failure_propagation_mode` attribute. When set to\n    `propagate`, the failure of any nested transformation in the sequence\n    implies immediate failure of the entire sequence with a silenceable error,\n    and no further transformation is attempted. When set to `suppress`,\n    silenceable errors in nested operations are ignored and further\n    transformations are applied. Beware that even silenceable errors may leave\n    the payload IR in a state unsuitable for further transformations. It is the\n    responsibility of the caller to ensure the following transformations are\n    robust enough when errors are suppressed. Definite errors reported by nested\n    transformations abort the sequence regardless of the propagation mode. The\n    set of modes may be extended in the future, e.g., to collect silenceable\n    errors and report them after attempting all transformations in the sequence.\n\n    The entry block of this operation has a single argument that maps to either\n    the operand if provided or the top-level container operation of the payload\n    IR, typically the root operation of the pass interpreting the transform\n    dialect. Operand omission is only allowed for sequences not contained in\n    another sequence.\n\n    The type of the block argument must match the type of the operand. If the\n    sequence is a top-level transform (without an operand), it can be used for\n    matching operations if the specified type within the top-level container\n    payload IR (including the container op itself). E.g.:\n\n    ```mlir\n    transform.sequence failures(propagate) {\n    ^bb1(%arg1: !transform.any_op):\n      // %arg1 is mapped to the top-level container of the payload IR, which is\n      // typically a module\n    }\n\n    transform.sequence failures(propagate) {\n    ^bb1(%arg1: !transform.op<\"func.func>\"):\n      // %arg1 is mapped to all \"func.func\" ops within and including the\n      // top-level container of the payload IR. Nested operations that have the\n      // specified op type are not included.\n    }\n    ```\n\n    The body of the sequence terminates with an implicit or explicit\n    `transform.yield` op. The operands of the terminator are returned as the\n    results of the sequence op.",
    "inputs": [
      { "name": "failure_propagation_mode", "type": "FailurePropagationMode" },
      { "name": "root", "type": "Optional" },
      { "name": "extra_bindings", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "custom<SequenceOpOperands>($root, type($root), $extra_bindings, type($extra_bindings)) (`->` type($results)^)? `failures` `(` $failure_propagation_mode `)` attr-dict-with-keyword regions"
  },
  {
    "name": "transform.split_handle",
    "summary": "Splits a handle or parameter into multiple values",
    "description": "Splits `handle` into one or multiple handles, as specified by the number\n    of results of this operation. `handle` should be mapped to as many payload\n    ops, values or parameteres as there are results. Otherwise, this transform\n    will fail producing a silenceable failure by default. Each result handle\n    is mapped to exactly one payload unless specified otherwise by attributes\n    described below. The order of the payloads is preserved,  i.e., the i-th\n    payload is mapped to the i-th result handle.\n\n    This operation is useful for ensuring a statically known number of\n    payloads are tracked by the source `handle` and to extract them into\n    individual handles that can be further manipulated in isolation.\n\n    If there are more payloads than results, the remaining payloads are mapped to\n    the result with index `overflow_result`. If no `overflow_result` is\n    specified, the transform produces a silenceable failure.\n\n    If there are fewer payload ops than results, the transform produces a\n    silenceable failure if `fail_on_payload_too_small` is set to \"true\".\n    Otherwise, it succeeds and the remaining result handles are not mapped to\n    anything. It also succeeds if `handle` is empty and\n    `pass_through_empty_handle` is set to \"true\", regardless of\n    `fail_on_payload_too_small`.",
    "inputs": [
      { "name": "handle", "type": "Transform_AnyHandleOrParamType" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "pass_through_empty_handle", "type": "DefaultValuedAttr" },
      { "name": "fail_on_payload_too_small", "type": "DefaultValuedAttr" },
      { "name": "overflow_result", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$handle attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "transform.verify",
    "summary": "Verifies the targeted ops",
    "description": "This transform verifies the targeted ops. If at least one op fails to\n    verify, the transform produces a definite failure.\n\n    Note: This op was designed for debugging purposes and should be used like an\n    assertion. It is intentional that this op produces a definite failure and\n    not a silenceable one. Correctness of the program should not depend on this\n    op.\n\n    This transform reads the target handle.",
    "inputs": [
      { "name": "target", "type": "TransformHandleTypeInterface" }
    ],
    "assemblyFormat": "$target attr-dict `:` type($target)"
  },
  {
    "name": "transform.yield",
    "summary": "Yields operation handles from a transform IR region",
    "description": "This terminator operation yields operation handles from regions of the\n    transform IR ops back to the containing op. It is not itself associated with\n    any transformation on the payload IR and is used for flow purposes only.",
    "inputs": [
      { "name": "operands", "type": "Arg" }
    ],
    "assemblyFormat": "operands attr-dict (`:` type($operands)^)?"
  },
  {
    "name": "ub.poison",
    "summary": "Poisoned constant operation.",
    "description": "The `poison` operation materializes a compile-time poisoned constant value\n    to indicate deferred undefined behavior.\n    `value` attribute is needed to indicate an optional additional poison\n    semantics (e.g. partially poisoned vectors), default value indicates results\n    is fully poisoned.\n\n    Examples:\n\n    ```\n    // Short form\n    %0 = ub.poison : i32\n    // Long form\n    %1 = ub.poison <#custom_poison_elements_attr> : vector<4xi64>\n    ```",
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "value", "type": "DefaultValuedAttr" }
    ],
    "assemblyFormat": "attr-dict (`<` $value^ `>`)? `:` type($result)"
  },
  {
    "name": "util.align",
    "summary": "Aligns up to a power-of-two alignment if required.",
    "description": "Aligns |value| up to the given power-of-two |alignment| if required.",
    "inputs": [
      { "name": "value", "type": "SignlessIntegerOrIndexLike" },
      { "name": "alignment", "type": "SignlessIntegerOrIndexLike" }
    ],
    "outputs": [
      { "name": "result", "type": "SignlessIntegerOrIndexLike" }
    ],
    "assemblyFormat": "$value `,` $alignment attr-dict `:` type($result)"
  },
  {
    "name": "util.assume.int",
    "summary": "Memorializes assumptions about index/integer values.",
    "description": "This op is used to memorialize the result of some integer analysis or\n    outside knowledge across a boundary beyond which such information can\n    not be easily recovered. Assumptions are made per op/result pair.\n\n    Assumptions are tied to operands as rows of permutations of an\n    `#util.assume.int` per operand. The number of permutations is the rank.\n    Typically multiple permutations record a specific subset of assumptions\n    broken down per call-site in some way that is meaningful to the receiver.\n    Implementations can use this information to specialize on each\n    permutation if it is meaninful to do so (i.e. vs unioning across them).\n    In such cases, there will typically be one such op at the top of a\n    function or scope which passes all covered operands through it.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "assumptions", "type": "Util_MultiValueIntAssumptionAttrList" }
    ]
  },
  {
    "name": "util.buffer.alloc",
    "summary": "Allocates a buffer with undefined contents.",
    "description": "Allocates a buffer with undefined contents. Consumers of the allocated\n    result must assume nothing of the contents.",
    "inputs": [
      { "name": "storage_size", "type": "Util_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Util_BufferType" }
    ],
    "attributes": [
      { "name": "alignment", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "`uninitialized`\n    attr-dict\n    `:`\n    type($result) `` `{` $storage_size `}`"
  },
  {
    "name": "util.buffer.compare",
    "summary": "Compares a range of two buffers.",
    "description": "Returns true if the two ranges are bitwise equivalent, somewhat like memcmp.",
    "inputs": [
      { "name": "lhs", "type": "Util_BufferType" },
      { "name": "lhs_size", "type": "Util_Size" },
      { "name": "lhs_offset", "type": "Util_Offset" },
      { "name": "rhs", "type": "Util_BufferType" },
      { "name": "rhs_size", "type": "Util_Size" },
      { "name": "rhs_offset", "type": "Util_Offset" },
      { "name": "length", "type": "Util_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "I1" }
    ],
    "assemblyFormat": "$lhs `[` $lhs_offset `]` `,`\n    $rhs `[` $rhs_offset `]` `,`\n    $length `:`\n    type($lhs) `` `{` $lhs_size `}` `,`\n    type($rhs) `` `{` $rhs_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "util.buffer.constant",
    "summary": "Constant host-side byte buffer.",
    "description": "Defines a compile-time byte buffer based on the given attribute value.\n    The attribute will be serialized into the canonical IREE format for the\n    chosen host target.",
    "outputs": [
      { "name": "result", "type": "Util_BufferType" }
    ],
    "attributes": [
      { "name": "name", "type": "OptionalAttr" },
      { "name": "value", "type": "Util_AnySerializableAttr" },
      { "name": "alignment", "type": "OptionalAttr" },
      { "name": "mime_type", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($name^)? attr-dict `:` type($result) `=` $value"
  },
  {
    "name": "util.buffer.copy",
    "summary": "Copies a range of bytes between buffers.",
    "description": "Copies a range of bytes as with memcpy (no overlapping).",
    "inputs": [
      { "name": "source", "type": "Util_BufferType" },
      { "name": "source_size", "type": "Util_Size" },
      { "name": "source_offset", "type": "Util_Offset" },
      { "name": "target", "type": "Util_BufferType" },
      { "name": "target_size", "type": "Util_Size" },
      { "name": "target_offset", "type": "Util_Offset" },
      { "name": "length", "type": "Util_Size" }
    ],
    "assemblyFormat": "$source `[` $source_offset `]` `,`\n    $target `[` $target_offset `]` `,`\n    $length `:`\n    type($source) `` `{` $source_size `}` `->`\n    type($target) `` `{` $target_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "util.buffer.dealloc",
    "summary": "Deallocates a buffer.",
    "description": "Hints that the buffer contents can be discarded. Buffers are reference\n    counted and other owners may keep it live beyond the dealloc.",
    "inputs": [
      { "name": "operand", "type": "Util_BufferType" },
      { "name": "operand_size", "type": "Util_Size" }
    ],
    "assemblyFormat": "$operand `:` type($operand) `{` $operand_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "util.buffer.fill",
    "summary": "Fills a range of bytes with a value.",
    "description": "Fills the contents of the buffer in the given byte range with a pattern.\n    The offset and length must match the natural alignment of the pattern type.",
    "inputs": [
      { "name": "pattern", "type": "Util_FillPattern" },
      { "name": "target", "type": "Util_BufferType" },
      { "name": "target_size", "type": "Util_Size" },
      { "name": "target_offset", "type": "Util_Offset" },
      { "name": "length", "type": "Util_Size" }
    ],
    "assemblyFormat": "$pattern `,`\n    $target `[` $target_offset `for` $length `]` `:`\n    type($pattern) `->`\n    type($target) `` `{` $target_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "util.buffer.hash",
    "summary": "Computes the hash of a byte range of a buffer.",
    "description": "Computes the SipHash-2-4 of a value at a byte offset with the given length.\n    This always uses a seed of `0x0001020304...0e0f` and produces a single 64\n    bit value.",
    "inputs": [
      { "name": "source", "type": "Util_BufferType" },
      { "name": "source_size", "type": "Util_Size" },
      { "name": "source_offset", "type": "Util_Offset" },
      { "name": "length", "type": "Util_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "I64" }
    ],
    "assemblyFormat": "$source `[` $source_offset `for` $length `]`\n    `:` type($source) `` `{` $source_size `}` `->` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "util.buffer.load",
    "summary": "Loads a value from a buffer.",
    "description": "Loads a value at a byte offset. Must be aligned to the natural size of the\n    result type.",
    "inputs": [
      { "name": "source", "type": "Util_BufferType" },
      { "name": "source_size", "type": "Util_Size" },
      { "name": "source_offset", "type": "Util_Offset" },
      { "name": "length", "type": "Util_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Util_Primitive" }
    ],
    "assemblyFormat": "$source `[` $source_offset `for` $length `]`\n    `:` type($source) `` `{` $source_size `}` `->` type($result)\n    attr-dict-with-keyword"
  },
  {
    "name": "util.buffer.size",
    "summary": "Returns the total buffer storage size in bytes.",
    "description": "Returns the total length of the buffer in bytes from its base offset.",
    "inputs": [
      { "name": "operand", "type": "Util_BufferType" }
    ],
    "outputs": [
      { "name": "result", "type": "Util_Size" }
    ],
    "assemblyFormat": "$operand\n    `:` type($operand)\n    attr-dict-with-keyword"
  },
  {
    "name": "util.buffer.slice",
    "summary": "Clones a subregion of a buffer.",
    "description": "Returns a copy of the contents from the source buffer.",
    "inputs": [
      { "name": "source", "type": "Util_BufferType" },
      { "name": "source_size", "type": "Util_Size" },
      { "name": "source_offset", "type": "Util_Offset" },
      { "name": "result_size", "type": "Util_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Util_BufferType" }
    ],
    "attributes": [
      { "name": "alignment", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$source `[` $source_offset `]` attr-dict `:`\n    type($source) `` `{` $source_size `}` `->`\n    type($result) `` `{` $result_size `}`"
  },
  {
    "name": "util.buffer.storage",
    "summary": "Returns the underlying buffer storage range.",
    "description": "Returns the buffer storage as a memref that must be offset and restricted to\n    the returned range. The memref may be of any type and the user is\n    responsible for ensuring that the reinterpret_cast-like behavior makes sense\n    for the data they are accessing.",
    "inputs": [
      { "name": "operand", "type": "Util_BufferType" },
      { "name": "operand_size", "type": "Util_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyMemRef" },
      { "name": "offset", "type": "Util_Offset" }
    ],
    "assemblyFormat": "$operand\n    `:` type($operand) `` `{` $operand_size `}` `->` `(` type($result) `,` type($offset) `)`\n    attr-dict-with-keyword"
  },
  {
    "name": "util.buffer.store",
    "summary": "Stores a value into a buffer.",
    "description": "Stores a value at a byte offset. Must be aligned to the natural size of the\n    source type.",
    "inputs": [
      { "name": "source", "type": "Util_Primitive" },
      { "name": "target", "type": "Util_BufferType" },
      { "name": "target_size", "type": "Util_Size" },
      { "name": "target_offset", "type": "Util_Offset" },
      { "name": "length", "type": "Util_Size" }
    ],
    "assemblyFormat": "$source `,`\n    $target `[` $target_offset `for` $length `]`\n    `:` type($source) `->` type($target) `` `{` $target_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "util.buffer.subspan",
    "summary": "Returns a reference to a subrange of a buffer.",
    "description": "Returns a logical view into an underlying source buffer. This induces\n    aliasing and multiple SSA values may allow access to the same underlying\n    buffer storage.\n\n    Subspans are a compiler-only concept and are propagated by an analysis pass\n    to result in absolute offsets on accesses any place the subrange would have\n    been used.",
    "inputs": [
      { "name": "source", "type": "Util_BufferType" },
      { "name": "source_size", "type": "Util_Size" },
      { "name": "source_offset", "type": "Util_Offset" },
      { "name": "result_size", "type": "Util_Size" }
    ],
    "outputs": [
      { "name": "result", "type": "Util_BufferType" }
    ],
    "assemblyFormat": "$source `[` $source_offset `]` `:`\n    type($source) `` `{` $source_size `}` `->`\n    type($result) `` `{` $result_size `}`\n    attr-dict-with-keyword"
  },
  {
    "name": "util.call",
    "summary": "Function call operation.",
    "description": "Represents a direct call to a function that is within the same symbol scope\n    as the call. The operands and result types of the call must match the\n    specified function type.\n\n    Calls support tied operands which indicate that specific results alias\n    a specific operand. The operand and result types are allowed to differ if\n    a cast is performed within the callee.\n\n    Example:\n    ```mlir\n    util.func @fn(%arg0: i32, %arg1: tensor<f32>) -> (f32, %arg1 as tensor<i32>)\n    ...\n    %0 = util.call @fn(%0, %1) : (i32, tensor<f32>) -> (f32, %1 as tensor<i32>)\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "callee", "type": "FlatSymbolRefAttr" },
      { "name": "tied_operands", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$callee `(` $operands `)`\n    attr-dict `:`\n    custom<OperandTypeList>(type($operands))\n    `->`\n    custom<TiedFunctionResultList>(ref($operands),\n                                   ref(type($operands)),\n                                   type($results),\n                                   $tied_operands)"
  },
  {
    "name": "util.cast",
    "summary": "Casts one util type to another ala static_cast/dynamic_cast.",
    "description": "Performs a type cast between object types known to the util dialect.",
    "inputs": [
      { "name": "operand", "type": "AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `to` type($result)"
  },
  {
    "name": "util.cmp.eq",
    "summary": "Compares two values for equality.",
    "description": "Compares two operands for equality. This is intended for comparing IREE\n    reference types (like !util.buffer) that cannot be used with std.cmpi.",
    "inputs": [
      { "name": "lhs", "type": "AnyType" },
      { "name": "rhs", "type": "AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "I1" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "util.cmp.ne",
    "summary": "Compares two values for inequality.",
    "description": "Compares two operands for inequality. This is intended for comparing IREE\n    reference types (like !util.buffer) that cannot be used with std.cmpi.",
    "inputs": [
      { "name": "lhs", "type": "AnyType" },
      { "name": "rhs", "type": "AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "I1" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "util.func",
    "summary": "Function operation containing a CFG region.",
    "description": "An operation declaring a callable function.\n\n    An external function declaration (used when referring to a function declared\n    in some other module) has no body.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "tied_operands", "type": "OptionalAttr" },
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "inlining_policy", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "util.global",
    "summary": "Stateful global variable declaration.",
    "description": "Declares a global variable that maintains its value across invocations.\n    The value is tied to the execution context of the module and different\n    contexts will have different variable storage.\n\n    Globals can be initialized with an `initial_value` attribute that specifies\n    their value at module initialization time. This initial value is evaluated\n    in module definition order: a global with an initial value is considered\n    initialized at its definition point in the module, not when first accessed.\n\n    During module initialization:\n    * Globals with `initial_value` attributes conceptually have their value\n      materialized and stored before any subsequent initialization points\n    * Initialization order follows module definition order exactly\n    * Globals are visible to initializers but only after their definition point\n\n    Example:\n    ```mlir\n    // A is initialized to 1 at this point in module initialization.\n    util.global @A = 1 : i32\n\n    // This initializer can read A (it's defined above).\n    util.initializer {\n      %a = util.global.load @A : i32  // Loads 1\n    }\n\n    // B is initialized to 2 here, after the above initializer runs.\n    util.global @B = 2 : i32\n    ```\n\n    Globals marked as mutable can be modified by stores anywhere in the program.\n    Immutable globals have strict initialization rules:\n    * Can only be initialized once - either by `initial_value` or by stores\n      in initializers, never both.\n    * Stores to immutable globals are only allowed in:\n      - `util.initializer` ops directly\n      - Functions that are only called from initializers (initializer-only)\n    * Stores from externally-reachable functions are forbidden.\n    * Initialization must respect module order - initializers can only\n      access globals defined before them.\n\n    Stores to immutable globals inside control flow regions (scf.if, scf.for,\n    scf.while, etc) in initializer-only functions will generate warnings as\n    they may indicate complex initialization patterns that could be fragile:\n    ```mlir\n    util.func private @init_func() {\n      scf.if %cond {\n        // Warning: conditional store in initializer-only function\n        util.global.store %val, @immutable_global : i32\n      }\n    }\n    ```\n    While such patterns are allowed they should be used with caution as the\n    initialization behavior depends on runtime control flow and the compiler\n    may disable optimizations on those globals.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "type", "type": "TypeAttr" },
      { "name": "is_mutable", "type": "UnitAttr" },
      { "name": "initial_value", "type": "OptionalAttr" },
      { "name": "inlining_policy", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    (`mutable` $is_mutable^)?\n    $sym_name\n    attr-dict\n    custom<TypeOrAttr>($type, $initial_value)"
  },
  {
    "name": "util.global.address",
    "summary": "Returns an address reference to a global.",
    "description": "Returns the address of a global as a typed reference. Can be used with the\n    global load and store indirect ops.",
    "outputs": [
      { "name": "result", "type": "Util_AnyGlobalPtr" }
    ],
    "attributes": [
      { "name": "global", "type": "Util_GlobalRefAttr" },
      { "name": "is_immutable", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`immutable` $is_immutable^)?\n    $global attr-dict `:` qualified(type($result))"
  },
  {
    "name": "util.global.load",
    "summary": "Loads a value from a global variable.",
    "description": "Returns a global variable value. |is_immutable| is a reflection of the\n    mutability of the loaded global to minimize the need to traverse symbol\n    tables.",
    "inputs": [
      { "name": "global", "type": "Arg" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "is_immutable", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`immutable` $is_immutable^)?\n    $global attr-dict `:` type($result)"
  },
  {
    "name": "util.global.load.indirect",
    "summary": "Loads a value from a global variable.",
    "description": "Returns a copy of the global variable value.",
    "inputs": [
      { "name": "global", "type": "Arg" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "is_immutable", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`immutable` $is_immutable^)?\n    $global attr-dict `:` qualified(type($global)) `->` type($result)"
  },
  {
    "name": "util.global.store",
    "summary": "Stores a value into a global variable.",
    "description": "Stores a copy of the value into a global variable.",
    "inputs": [
      { "name": "value", "type": "AnyType" },
      { "name": "global", "type": "Arg" }
    ],
    "assemblyFormat": "$value `,` $global attr-dict `:` type($value)"
  },
  {
    "name": "util.global.store.indirect",
    "summary": "Stores a value into a global variable.",
    "description": "Stores a copy of the value into a global variable.",
    "inputs": [
      { "name": "value", "type": "AnyType" },
      { "name": "global", "type": "Arg" }
    ],
    "assemblyFormat": "$value `,` $global attr-dict `:` type($value) `->` qualified(type($global))"
  },
  {
    "name": "util.initializer",
    "summary": "Global initialization function.",
    "description": "A function that is called in definition order upon module initialization.\n    Must not load any globals that are defined or initialized after it in the\n    module.\n\n    Module initialization follows a strict execution order to ensure\n    correctness:\n    * Initialization points: two types of operations define initialization:\n      - `util.initializer` ops: explicit initialization functions\n      - `util.global` ops with initial values: implicit initialization via\n        attributes\n    * Execution order: initialization proceeds in exact module definition order:\n      - Operations execute sequentially from top to bottom\n      - Each `util.global` with an initial value conceptually materializes its\n        value and stores it before the next initialization point\n      - Each `util.initializer` executes its entire body before proceeding\n    * Dependency rules:\n      - Initializers may only access globals defined before them in module order\n      - Globals with initial values are considered \"initialized\" at their\n        definition point, not when first accessed\n      - Function calls within initializers observe the current initialization\n        state when the call is made\n    * Transformation guarantees:\n      - Combining passes preserve exact initialization order\n      - Globals with initial values may be converted to explicit initialization\n        while maintaining their position in the initialization sequence\n      - The final initialization state must be identical to sequential execution\n\n    Example execution:\n    ```mlir\n    util.global @A = 1 : i32           // A = 1\n    util.initializer { ... }           // Executes with A = 1\n    util.global @B = 2 : i32           // B = 2\n    util.initializer {\n      %a = util.global.load @A : i32   // Loads 1\n      %b = util.global.load @B : i32   // Loads 2\n    }\n    ```",
    "attributes": [
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "util.list.construct",
    "summary": "Constructs a list with the given initial values.",
    "description": "Creates a new list with the given values added in order. The list will be\n    allocated with an initial capacity equal to the number of values. This is a\n    pseudo-operation that expands to a list create, resize, and a series of\n    sets.",
    "inputs": [
      { "name": "values", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Util_AnyListType" }
    ],
    "assemblyFormat": "custom<ValueTypeList>($values, type($values))\n    attr-dict `:` qualified(type($result))"
  },
  {
    "name": "util.list.create",
    "summary": "Creates a new empty list.",
    "description": "Creates a new empty list with an optional initial capacity.",
    "inputs": [
      { "name": "initial_capacity", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "Util_AnyListType" }
    ],
    "assemblyFormat": "($initial_capacity^)? attr-dict `:` qualified(type($result))"
  },
  {
    "name": "util.list.get",
    "summary": "Element accessor.",
    "description": "Returns the value of the element at the given index. Note that the value\n    may be null if the element is null or the type does not match.",
    "inputs": [
      { "name": "list", "type": "Util_AnyListType" },
      { "name": "index", "type": "Index" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "$list `[` $index `]` attr-dict `:` custom<ListTypeGet>(type($list), type($result))"
  },
  {
    "name": "util.list.resize",
    "summary": "Resizes the list to a new count in elements.",
    "description": "Resizes the list to contain `new_size` elements. This will either truncate\n    the list if the existing size is greater than `new_size` or extend the list\n    with the default list value of the element type.",
    "inputs": [
      { "name": "list", "type": "Util_AnyListType" },
      { "name": "new_size", "type": "Index" }
    ],
    "assemblyFormat": "operands attr-dict `:` qualified(type($list))"
  },
  {
    "name": "util.list.set",
    "summary": "Element mutator.",
    "description": "Sets the element at the given index to the new value.",
    "inputs": [
      { "name": "list", "type": "Util_AnyListType" },
      { "name": "index", "type": "Index" },
      { "name": "value", "type": "AnyType" }
    ],
    "assemblyFormat": "$list `[` $index `]` `,` $value attr-dict `:` custom<ListTypeSet>(type($list), type($value))"
  },
  {
    "name": "util.list.size",
    "summary": "The size of the list in elements.",
    "description": "Returns the current size of the list in elements.",
    "inputs": [
      { "name": "list", "type": "Util_AnyListType" }
    ],
    "outputs": [
      { "name": "result", "type": "Index" }
    ],
    "assemblyFormat": "operands attr-dict `:` qualified(type($list))"
  },
  {
    "name": "util.null",
    "summary": "Returns a null type value.",
    "description": "Defines an SSA value that is lowered into dialects supporting\n    null/undefined/optional/etc values.",
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "assemblyFormat": "attr-dict `:` type($result)"
  },
  {
    "name": "util.numeric.optional_narrow",
    "summary": "memorializes an optional numeric narrowing that is valid.",
    "description": "Serves as a placeholder for points in the computation where an optional\n    numeric narrowing can be performed without loss of information. Such ops\n    can guide optimization passes wishing to perform precision reduction.\n\n    In addition to the operand and result type, this op takes an additional\n    `semantic_type` attribute representing the semantic target type which can\n    be:\n      * FloatType\n      * Signed IntegerType\n      * Unsigned IntegerType\n\n    Note that this `semantic_type` must be a sign-carrying integer if using an\n    integer type and cannot be IndexType (i.e. it can be used to indicate a\n    possible narrowing of an IndexType to a specific integer).\n\n    If the operand is a TensorType, then the result must be a TensorType. The\n    `semantic_type` constrains the element type.\n\n    Optionally, the minimum and maximum integer values (for integer semantic\n    types) are tracked if known.",
    "inputs": [
      { "name": "operand", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "semantic_type", "type": "TypeAttr" },
      { "name": "min_value", "type": "OptionalAttr" },
      { "name": "max_value", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$operand `:` type($operand) `as` $semantic_type attr-dict"
  },
  {
    "name": "util.optimization_barrier",
    "summary": "Prevents compiler optimizations across a value.",
    "description": "Wraps any operands in an unoptimizable identity to prevent its results from\n    being folded. It will be dropped during the final step in compilation and\n    has no effect at runtime.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict\n    ($operands^ `:` type($operands))?"
  },
  {
    "name": "util.range.extents",
    "summary": "Returns the min/max of a union of a set of ranges.",
    "description": "Computes min(offsets) and max(offsets + lengths). Though it's possible to\n    express this with standard arithmetic this op enables more semantically\n    meaningful folding/optimizations.",
    "inputs": [
      { "name": "offsets", "type": "Variadic" },
      { "name": "lengths", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "min", "type": "Util_Range" },
      { "name": "max", "type": "Util_Range" }
    ],
    "assemblyFormat": "custom<RangeList>($offsets, $lengths) attr-dict `:` type($min)"
  },
  {
    "name": "util.range.max",
    "summary": "Returns the max of all values.",
    "description": "Computes the max of a variadic list of operands. Though it's possible to\n    express this with standard arithmetic this op enables more semantically\n    meaningful folding/optimizations.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Util_Range" }
    ],
    "assemblyFormat": "$operands attr-dict `:` type($result)"
  },
  {
    "name": "util.range.min",
    "summary": "Returns the min of all values.",
    "description": "Computes the min of a variadic list of operands. Though it's possible to\n    express this with standard arithmetic this op enables more semantically\n    meaningful folding/optimizations.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Util_Range" }
    ],
    "assemblyFormat": "$operands attr-dict `:` type($result)"
  },
  {
    "name": "util.return",
    "summary": "Return from a util.initializer.",
    "description": "Returns control from an initializer function.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict\n    ($operands^ `:` type($operands))?"
  },
  {
    "name": "util.scf.unreachable",
    "summary": "Non-terminator unreachable for use in SCF regions.",
    "description": "Marks a point as unreachable within SCF regions. Unlike `util.unreachable`\n    this is not a terminator and must be followed by appropriate region\n    terminators (e.g. `scf.yield`).\n\n    When not in an SCF region (e.g. `util.func`) this op canonicalizes to an\n    `util.unreachable` terminator that preserves the intended behavior.\n\n    Note that this op has side effects to prevent code motion: reaching this op\n    at runtime is observable indirectly (by way of what ops _don't_ execute). At\n    minimum presence of the op indicates all following ops in the same region\n    are unreachable and depending on the parent op may indicate the entire\n    parent op is unreachable. The behavior is roughly equivalent to having\n    lowered SCF to CFG and used `util.unreachable`.\n\n    Example:\n    ```mlir\n    scf.while (...) {\n      ...\n      scf.condition(%true) ...\n    } do {\n      ...\n      util.scf.unreachable \"infinite loop body\"\n      scf.yield  // Required terminator with poison values\n    }\n    ```",
    "attributes": [
      { "name": "message", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($message^)? attr-dict"
  },
  {
    "name": "util.sizeof",
    "summary": "Returns the size in bytes of a datatype.",
    "description": "Most datatypes have a static size at all layers of the compilation stack.\n    However, those that only have a size for certain lowering flows can be\n    challenging. This op represents such sizes in a way that can be specialized\n    later.\n\n    Returns the size in bytes, rounded up to the next whole byte of the\n    specified type. This op will fold to a constant index value for IntegerType\n    and FloatType. All others are not folded.",
    "outputs": [
      { "name": "size", "type": "Index" }
    ],
    "attributes": [
      { "name": "sizedType", "type": "TypeAttr" }
    ],
    "assemblyFormat": "$sizedType attr-dict-with-keyword"
  },
  {
    "name": "util.status.check_ok",
    "summary": "Raises a global failure if a status is not 'ok'.",
    "description": "When the status is not 'ok' this signals a runtime failure that causes the\n    entire active invocation - and possibly *all* in-flight and pending\n    invocations - to fail with the given status. The status will be propagated\n    back via the available runtime error handling mechanisms such as semaphores\n    or synchronous invocation results.\n\n    As the IREE execution model is deeply pipelined it's possible that failures\n    have a latency between when they are emitted and when the application can\n    observe the failure. It's also possible that other work that is in-flight\n    or pending when the failure occurs will complete.",
    "inputs": [
      { "name": "status", "type": "Util_Status" }
    ],
    "attributes": [
      { "name": "message", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$status (`,` $message^)? attr-dict"
  },
  {
    "name": "util.switch",
    "summary": "Primitive switch operation.",
    "description": "Returns the value with the given `index` in `values` or `default_value` if\n    the index is out of bounds.\n\n    ```mlir\n    // Switch %index to cases of %c100/%c200/%c300 if index==0, ==1, ==2.\n    // If %index is out of range (<0 or >2) then default to %c5.\n    %0 = util.switch %index[%c100, %c200, %c300] else %c5 : i32\n    ```",
    "inputs": [
      { "name": "index", "type": "Index" },
      { "name": "default_value", "type": "Util_Primitive" },
      { "name": "values", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Util_Primitive" }
    ],
    "assemblyFormat": "type($default_value) `from`\n    custom<TypedValueList>(ref(type($default_value)), $values, type($values))\n    `at` $index\n    `else` $default_value\n    attr-dict\n    `:` type($result)"
  },
  {
    "name": "util.unfoldable_constant",
    "summary": "A constant that cannot be folded by the compiler.",
    "description": "Similar to a std.constant, but is declared as having a side effect and has\n    no folder. This is really just syntactic sugar as it is canonicalized to a\n    std.constant wrapped in an util.optimization_barrier.",
    "attributes": [
      { "name": "value", "type": "AnyAttr" }
    ]
  },
  {
    "name": "util.unreachable",
    "summary": "Unreachable assertion op.",
    "description": "Signals to the compiler that the parent block should not be reachable.\n    This may be converted into a runtime assertion, though ideally they are\n    stripped during translation.\n\n    ```mlir\n    ^bb0:\n      %true = arith.constant true\n      cond_br %true, ^bb2, ^bb1\n    ^bb1:\n      // Indicates that this branch should never be taken.\n      util.unreachable \"shouldn't be here\"\n    ^bb2:\n      ...\n\n    ```",
    "attributes": [
      { "name": "message", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($message^)? attr-dict"
  },
  {
    "name": "vector.bitcast",
    "summary": "bitcast casts between vectors",
    "description": "The bitcast operation casts between vectors of the same rank, the minor 1-D\n    vector size is casted to a vector with a different element type but same\n    bitwidth. In case of 0-D vectors, the bitwidth of element types must be\n    equal.\n\n    Example:\n\n    ```mlir\n    // Example casting to a smaller element type.\n    %1 = vector.bitcast %0 : vector<5x1x4x3xf32> to vector<5x1x4x6xi16>\n\n    // Example casting to a bigger element type.\n    %3 = vector.bitcast %2 : vector<10x12x8xi8> to vector<10x12x2xi32>\n\n    // Example casting to an element type of the same size.\n    %5 = vector.bitcast %4 : vector<5x1x4x3xf32> to vector<5x1x4x3xi32>\n\n    // Example casting of 0-D vectors.\n    %7 = vector.bitcast %6 : vector<f32> to vector<i32>\n    ```",
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($result)"
  },
  {
    "name": "vector.broadcast",
    "summary": "broadcast operation",
    "description": "Broadcasts the scalar or k-D vector value in the source operand\n    to a n-D result vector such that the broadcast makes sense, i.e.,\n    the source operand is duplicated to match the given rank and sizes\n    in the result vector. The legality rules are:\n    * the source operand must have the same element type as the result type\n    * a k-D vector <s_1 x .. x s_k x type> can be broadcast to\n      a n-D vector <t_1 x .. x t_n x type> if\n       * k <= n, and\n       * the sizes in the trailing dimensions n-k < i <= n with j=i+k-n\n          match exactly as s_j = t_i or s_j = 1:\n       ```\n           t_1 x   ..  t_n-k x t_n-k+1 x .. x t_i x .. x t_n\n                               s_1     x .. x s_j x .. x s_k\n               <duplication>         <potential stretch>\n       ```\n       * in addition, any scalable unit dimension, `[1]`, must match exactly.\n\n    The source operand is duplicated over all the missing leading dimensions\n    and stretched over the trailing dimensions where the source has a non-equal\n    dimension of 1 (stretching a trailing dimension is also referred to as\n    \"dim-1\" broadcasting). These rules imply that any scalar broadcast (k=0) to\n    any shaped vector with the same element type is always legal.\n\n    Example:\n\n    ```mlir\n    %0 = arith.constant 0.0 : f32\n    %1 = vector.broadcast %0 : f32 to vector<16xf32>\n    %2 = vector.broadcast %1 : vector<16xf32> to vector<4x16xf32>\n    ```",
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($vector)"
  },
  {
    "name": "vector.compressstore",
    "summary": "writes elements selectively from a vector as defined by a mask",
    "description": "The compress store operation writes elements from a vector into memory as\n    defined by a base with indices and a mask vector. Compression only applies\n    to the innermost dimension. When the mask is set, the corresponding element\n    from the vector is written next to memory.  Otherwise, no action is taken\n    for the element. Informally the semantics are:\n\n    ```\n    index = i\n    if (mask[0]) base[index++] = value[0]\n    if (mask[1]) base[index++] = value[1]\n    etc.\n    ```\n\n    Note that the index increment is done conditionally.\n\n    If a mask bit is set and the corresponding index is out-of-bounds for the\n    given base, the behavior is undefined. If a mask bit is not set, no value\n    is stored regardless of the index, and the index is allowed to be\n    out-of-bounds.\n\n    The compress store can be used directly where applicable, or can be used\n    during progressively lowering to bring other memory operations closer to\n    hardware ISA support for a compress. The semantics of the operation closely\n    correspond to those of the `llvm.masked.compressstore`\n    [intrinsic](https://llvm.org/docs/LangRef.html#llvm-masked-compressstore-intrinsics).\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    store operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violating this requirement\n    triggers immediate undefined behavior.\n\n    Note, at the moment this Op is only available for fixed-width vectors.\n\n    Examples:\n\n    ```mlir\n    vector.compressstore %base[%i], %mask, %value\n      : memref<?xf32>, vector<8xi1>, vector<8xf32>\n\n    vector.compressstore %base[%i, %j], %mask, %value\n      : memref<?x?xf32>, vector<16xi1>, vector<16xf32>\n    ```",
    "assemblyFormat": "$base `[` $indices `]` `,` $mask `,` $valueToStore attr-dict `:` type($base) `,` type($mask) `,` type($valueToStore)"
  },
  {
    "name": "vector.constant_mask",
    "summary": "creates a constant vector mask",
    "description": "Creates and returns a vector mask where elements of the result vector\n    are set to '0' or '1', based on whether the element indices are contained\n    within a hyper-rectangular region specified by the 'mask_dim_sizes'\n    array attribute argument. Each element of the 'mask_dim_sizes' array,\n    specifies an exclusive upper bound [0, mask-dim-size-element-value)\n    for a unique dimension in the vector result. The conjunction of the ranges\n    define a hyper-rectangular region within which elements values are set to 1\n    (otherwise element values are set to 0). Each value of 'mask_dim_sizes' must\n    be non-negative and not greater than the size of the corresponding vector\n    dimension (as opposed to vector.create_mask which allows this). Sizes that\n    correspond to scalable dimensions are implicitly multiplied by vscale,\n    though currently only zero (none set) or the size of the dim/vscale\n    (all set) are supported.\n\n    Example:\n\n    ```mlir\n    // create a constant vector mask of size 4x3xi1 with elements in range\n    // 0 <= row <= 2 and 0 <= col <= 1 are set to 1 (others to 0).\n    %1 = vector.constant_mask [3, 2] : vector<4x3xi1>\n\n    print %1\n                  columns\n                0    1    2\n              |------------\n            0 | 1    1    0\n      rows  1 | 1    1    0\n            2 | 1    1    0\n            3 | 0    0    0\n    ```",
    "assemblyFormat": "$mask_dim_sizes attr-dict `:` type(results)"
  },
  {
    "name": "vector.contract",
    "summary": "vector contraction operation",
    "description": "Computes the sum of products of vector elements along contracting\n    dimension pairs from 2 vectors of rank M and N respectively, adds this\n    intermediate result to the accumulator argument of rank K, and returns a\n    vector result of rank K (where K = num_lhs_free_dims + num_rhs_free_dims +\n    num_batch_dims (see dimension type descriptions below)). For K = 0 (no\n    free or batch dimensions), the accumulator and output are a scalar.\n\n    If operands and the result have types of different bitwidths, operands are\n    promoted to have the same bitwidth as the result before performing the\n    contraction. For integer types, only signless integer types are supported,\n    and the promotion happens via sign extension.\n\n    An iterator type attribute list must be specified, where each element of\n    the list represents an iterator with one of the following types:\n\n    *   \"reduction\": reduction dimensions are present in the lhs and rhs\n        arguments but not in the output (and accumulator\n        argument). These are the dimensions along which the vector\n        contraction op computes the sum of products, and\n        contracting dimension pair dimension sizes must match\n        between lhs/rhs.\n\n    *   \"parallel\": Batch dimensions are iterator type \"parallel\", and\n        are non-contracting dimensions present in the lhs, rhs and\n        output. The lhs/rhs co-iterate along the batch dimensions,\n        which should be expressed in their indexing maps.\n\n        Free dimensions are iterator type \"parallel\", and are\n        non-contraction, non-batch dimensions accessed by either the\n        lhs or rhs (but not both). The lhs and rhs free dimensions\n        are unrelated to each other and do not co-iterate, which\n        should be expressed in their indexing maps.\n\n    An indexing map attribute list must be specified with an entry for lhs, rhs\n    and acc arguments. An indexing map attribute specifies a mapping from each\n    iterator in the iterator type list, to each dimension of an N-D vector.\n\n    An optional kind attribute may be used to specify the combining function\n    between the intermediate result and accumulator argument of rank K. This\n    attribute can take the values `add`/`mul`/`minsi`/`minui`/`maxsi`/`maxui`\n    /`and`/`or`/`xor` for integers, and `add`/`mul`/`minnumf`/`maxnumf`\n    /`minimumf`/`maximumf` for floats. The default is `add`.\n\n    Example:\n\n    ```mlir\n    // Simple DOT product (K = 0).\n    #contraction_accesses = [\n     affine_map<(i) -> (i)>,\n     affine_map<(i) -> (i)>,\n     affine_map<(i) -> ()>\n    ]\n    #contraction_trait = {\n      indexing_maps = #contraction_accesses,\n      iterator_types = [\"reduction\"]\n    }\n    %3 = vector.contract #contraction_trait %0, %1, %2\n      : vector<10xf32>, vector<10xf32> into f32\n\n    // 2D vector contraction with one contracting dimension (matmul, K = 2).\n    #contraction_accesses = [\n      affine_map<(i, j, k) -> (i, k)>,\n      affine_map<(i, j, k) -> (k, j)>,\n      affine_map<(i, j, k) -> (i, j)>\n    ]\n    #contraction_trait = {\n      indexing_maps = #contraction_accesses,\n      iterator_types = [\"parallel\", \"parallel\", \"reduction\"]\n    }\n\n    %3 = vector.contract #contraction_trait %0, %1, %2\n      : vector<4x3xf32>, vector<3x7xf32> into vector<4x7xf32>\n\n    // 4D to 3D vector contraction with two contracting dimensions and\n    // one batch dimension (K = 3).\n    #contraction_accesses = [\n      affine_map<(b0, f0, f1, c0, c1) -> (c0, b0, c1, f0)>,\n      affine_map<(b0, f0, f1, c0, c1) -> (b0, c1, c0, f1)>,\n      affine_map<(b0, f0, f1, c0, c1) -> (b0, f0, f1)>\n    ]\n    #contraction_trait = {\n      indexing_maps = #contraction_accesses,\n      iterator_types = [\"parallel\", \"parallel\", \"parallel\",\n                        \"reduction\", \"reduction\"]\n    }\n\n    %4 = vector.contract #contraction_trait %0, %1, %2\n        : vector<7x8x16x15xf32>, vector<8x16x7x5xf32> into vector<8x15x5xf32>\n\n    // Vector contraction with mixed typed. lhs/rhs have different element\n    // types than accumulator/result.\n    %5 = vector.contract #contraction_trait %0, %1, %2\n      : vector<10xf16>, vector<10xf16> into f32\n\n    // Contract with max (K = 0).\n    #contraction_accesses = [\n     affine_map<(i) -> (i)>,\n     affine_map<(i) -> (i)>,\n     affine_map<(i) -> ()>\n    ]\n    #contraction_trait = {\n      indexing_maps = #contraction_accesses,\n      iterator_types = [\"reduction\"],\n      kind = #vector.kind<maxnumf>\n    }\n    %6 = vector.contract #contraction_trait %0, %1, %2\n      : vector<10xf32>, vector<10xf32> into f32\n    ```"
  },
  {
    "name": "vector.create_mask",
    "summary": "creates a vector mask",
    "description": "Creates and returns a vector mask where elements of the result vector\n    are set to '0' or '1', based on whether the element indices are contained\n    within a hyper-rectangular region specified by the operands. Specifically,\n    each operand specifies a range [0, operand-value) for a unique dimension in\n    the vector result. The conjunction of the operand ranges define a\n    hyper-rectangular region within which elements values are set to 1\n    (otherwise element values are set to 0). If operand-value is negative, it is\n    treated as if it were zero, and if it is greater than the corresponding\n    dimension size, it is treated as if it were equal to the dimension size.\n\n    Example:\n\n    ```mlir\n    // create a vector mask of size 4x3xi1 where elements in range\n    // 0 <= row <= 2 and 0 <= col <= 1 are set to 1 (others to 0).\n    %1 = vector.create_mask %c3, %c2 : vector<4x3xi1>\n\n    print %1\n                  columns\n                0    1    2\n              |------------\n            0 | 1    1    0\n      rows  1 | 1    1    0\n            2 | 1    1    0\n            3 | 0    0    0\n    ```",
    "assemblyFormat": "$operands attr-dict `:` type(results)"
  },
  {
    "name": "vector.deinterleave",
    "summary": "constructs two vectors by deinterleaving an input vector",
    "description": "The deinterleave operation constructs two vectors from a single input\n        vector. The first result vector contains the elements from even indexes\n        of the input, and the second contains elements from odd indexes. This is\n        the inverse of a `vector.interleave` operation.\n\n        Each output's trailing dimension is half of the size of the input\n        vector's trailing dimension. This operation requires the input vector\n        to have a rank > 0 and an even number of elements in its trailing\n        dimension.\n\n        The operation supports scalable vectors.\n\n        Example:\n        ```mlir\n        %0, %1 = vector.deinterleave %a\n                   : vector<8xi8> -> vector<4xi8>\n        %2, %3 = vector.deinterleave %b\n                   : vector<2x8xi8> -> vector<2x4xi8>\n        %4, %5 = vector.deinterleave %c\n                   : vector<2x8x4xi8> -> vector<2x8x2xi8>\n        %6, %7 = vector.deinterleave %d\n                   : vector<[8]xf32> -> vector<[4]xf32>\n        %8, %9 = vector.deinterleave %e\n                   : vector<2x[6]xf64> -> vector<2x[3]xf64>\n        %10, %11 = vector.deinterleave %f\n                   : vector<2x4x[6]xf64> -> vector<2x4x[3]xf64>\n        ```",
    "inputs": [
      { "name": "source", "type": "AnyVectorOfNonZeroRank" }
    ],
    "outputs": [
      { "name": "res1", "type": "AnyVectorOfNonZeroRank" },
      { "name": "res2", "type": "AnyVectorOfNonZeroRank" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source) `->` type($res1)"
  },
  {
    "name": "vector.expandload",
    "summary": "reads elements from memory and spreads them into a vector as defined by a mask",
    "description": "The expand load reads elements from memory into a vector as defined by a\n    base with indices and a mask vector. Expansion only applies to the innermost\n    dimension. When the mask is set, the next element is read from memory.\n    Otherwise, the corresponding element is taken from a pass-through vector.\n    Informally the semantics are:\n\n    ```\n    index = i\n    result[0] := if mask[0] then base[index++] else pass_thru[0]\n    result[1] := if mask[1] then base[index++] else pass_thru[1]\n    etc.\n    ```\n\n    Note that the index increment is done conditionally.\n\n    If a mask bit is set and the corresponding index is out-of-bounds for the\n    given base, the behavior is undefined. If a mask bit is not set, the value\n    comes from the pass-through vector regardless of the index, and the index is\n    allowed to be out-of-bounds.\n\n    The expand load can be used directly where applicable, or can be used\n    during progressively lowering to bring other memory operations closer to\n    hardware ISA support for an expand. The semantics of the operation closely\n    correspond to those of the `llvm.masked.expandload`\n    [intrinsic](https://llvm.org/docs/LangRef.html#llvm-masked-expandload-intrinsics).\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    load operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violating this requirement\n    triggers immediate undefined behavior.\n\n    Note, at the moment this Op is only available for fixed-width vectors.\n\n    Examples:\n\n    ```mlir\n    %0 = vector.expandload %base[%i], %mask, %pass_thru\n       : memref<?xf32>, vector<8xi1>, vector<8xf32> into vector<8xf32>\n\n    %1 = vector.expandload %base[%i, %j], %mask, %pass_thru\n       : memref<?x?xf32>, vector<16xi1>, vector<16xf32> into vector<16xf32>\n    ```",
    "assemblyFormat": "$base `[` $indices `]` `,` $mask `,` $pass_thru attr-dict `:` type($base) `,` type($mask) `,` type($pass_thru) `into` type($result)"
  },
  {
    "name": "vector.extract",
    "summary": "extract operation",
    "description": "Extracts an (n − k)-D result sub-vector from an n-D source vector at a\n    specified k-D position. When n = k, the result degenerates to a scalar\n    element.\n\n    Static and dynamic indices must be greater or equal to zero and less than\n    the size of the corresponding dimension. The result is undefined if any\n    index is out-of-bounds. The value `-1` represents a poison index, which\n    specifies that the extracted element is poison.\n\n    Example:\n\n    ```mlir\n    %1 = vector.extract %0[3]: vector<8x16xf32> from vector<4x8x16xf32>\n    %2 = vector.extract %0[2, 1, 3]: f32 from vector<4x8x16xf32>\n    %4 = vector.extract %0[%a, %b, %c]: f32 from vector<4x8x16xf32>\n    %5 = vector.extract %0[2, %b]: vector<16xf32> from vector<4x8x16xf32>\n    %6 = vector.extract %10[-1, %c]: f32 from vector<4x16xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyVectorOfAnyRank" },
      { "name": "dynamic_position", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyType" }
    ],
    "attributes": [
      { "name": "static_position", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$source ``\n    custom<DynamicIndexList>($dynamic_position, $static_position)\n    attr-dict `:` type($result) `from` type($source)"
  },
  {
    "name": "vector.extract_strided_slice",
    "summary": "extract_strided_slice operation",
    "description": "Takes an n-D vector, k-D `offsets` integer array attribute, a k-sized\n    `sizes` integer array attribute, a k-sized `strides` integer array\n    attribute and extracts the n-D subvector at the proper offset.\n\n    At the moment strides must contain only 1s.\n\n    Returns an n-D vector where the first k-D dimensions match the `sizes`\n    attribute. The returned subvector contains the elements starting at offset\n    `offsets` and ending at `offsets + sizes`.\n\n    Example:\n\n    ```mlir\n    %1 = vector.extract_strided_slice %0\n        {offsets = [0, 2], sizes = [2, 4], strides = [1, 1]}:\n      vector<4x8x16xf32> to vector<2x4x16xf32>\n\n    // TODO: Evolve to a range form syntax similar to:\n    %1 = vector.extract_strided_slice %0[0:2:1][2:4:1]\n      vector<4x8x16xf32> to vector<2x4x16xf32>\n    ```\n\n    TODO: Implement support for poison indices.",
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type(results)"
  },
  {
    "name": "vector.fma",
    "summary": "vector fused multiply-add",
    "description": "Multiply-add expressions operate on n-D vectors and compute a fused\n    pointwise multiply-and-accumulate: `$result = $lhs * $rhs + $acc`.\n    All operands and result have the same vector type. The semantics\n    of the operation correspond to those of the `llvm.fma`\n    [intrinsic](https://llvm.org/docs/LangRef.html#int-fma). In the\n    particular case of lowering to LLVM, this is guaranteed to lower\n    to the `llvm.fma.*` intrinsic.\n\n    Example:\n\n    ```mlir\n    %3 = vector.fma %0, %1, %2: vector<8x16xf32>\n    ```",
    "assemblyFormat": "$lhs `,` $rhs `,` $acc attr-dict `:` type($lhs)"
  },
  {
    "name": "vector.from_elements",
    "summary": "operation that defines a vector from scalar elements",
    "description": "This operation defines a vector from one or multiple scalar elements. The\n    scalar elements are arranged in row-major within the vector. The number of\n    elements must match the number of elements in the result type. All elements\n    must have the same type, which must match the element type of the result\n    vector type. Scalable vectors are not supported.\n\n    Examples:\n\n    ```mlir\n    // Define a 0-D vector.\n    %0 = vector.from_elements %f1 : vector<f32>\n    // [%f1]\n\n    // Define a 1-D vector.\n    %1 = vector.from_elements %f1, %f2 : vector<2xf32>\n    // [%f1, %f2]\n\n    // Define a 2-D vector.\n    %2 = vector.from_elements %f1, %f2, %f3, %f4, %f5, %f6 : vector<2x3xf32>\n    // [[%f1, %f2, %f3], [%f4, %f5, %f6]]\n\n    // Define a 3-D vector.\n    %3 = vector.from_elements %f1, %f2, %f3, %f4, %f5, %f6 : vector<3x1x2xf32>\n    // [[[%f1, %f2]], [[%f3, %f4]], [[%f5, %f6]]]\n    ```",
    "inputs": [
      { "name": "elements", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "dest", "type": "AnyFixedVectorOfAnyRank" }
    ],
    "assemblyFormat": "$elements attr-dict `:` type($dest)"
  },
  {
    "name": "vector.gather",
    "summary": "Gathers elements from memory or ranked tensor into a vector as defined by an\n    index vector and a mask vector.",
    "description": "The gather operation returns an n-D vector whose elements are either loaded\n    from a k-D memref or tensor, or taken from an n-D pass-through vector, depending\n    on the values of an n-D mask vector.\n\n    If a mask bit is set, the corresponding result element is taken from `base`\n    at an index defined by k indices and n-D `index_vec`. Otherwise, the element\n    is taken from the pass-through vector. As an example, suppose that `base` is\n    3-D and the result is 2-D:\n\n    ```mlir\n    func.func @gather_3D_to_2D(\n        %base: memref<?x10x?xf32>, %ofs_0: index, %ofs_1: index, %ofs_2: index,\n        %indices: vector<2x3xi32>, %mask: vector<2x3xi1>,\n        %fall_thru: vector<2x3xf32>) -> vector<2x3xf32> {\n            %result = vector.gather %base[%ofs_0, %ofs_1, %ofs_2]\n                                   [%indices], %mask, %fall_thru : [...]\n            return %result : vector<2x3xf32>\n    }\n    ```\n\n    The indexing semantics are then,\n\n    ```\n    result[i,j] := if mask[i,j] then base[i0, i1, i2 + indices[i,j]]\n                   else pass_thru[i,j]\n    ```\n    The index into `base` only varies in the innermost ((k-1)-th) dimension.\n\n    If a mask bit is set and the corresponding index is out-of-bounds for the\n    given base, the behavior is undefined. If a mask bit is not set, the value\n    comes from the pass-through vector regardless of the index, and the index is\n    allowed to be out-of-bounds.\n\n    The gather operation can be used directly where applicable, or can be used\n    during progressively lowering to bring other memory operations closer to\n    hardware ISA support for a gather.\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    gather operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violating this requirement\n    triggers immediate undefined behavior.\n\n    Examples:\n\n    ```mlir\n    // 1-D memref gathered to 2-D vector.\n    %0 = vector.gather %base[%c0][%v], %mask, %pass_thru\n       : memref<?xf32>, vector<2x16xi32>, vector<2x16xi1>, vector<2x16xf32> into vector<2x16xf32>\n\n    // 2-D memref gathered to 1-D vector.\n    %1 = vector.gather %base[%i, %j][%v], %mask, %pass_thru\n       : memref<16x16xf32>, vector<16xi32>, vector<16xi1>, vector<16xf32> into vector<16xf32>\n    ```",
    "assemblyFormat": "$base `[` $offsets `]` `[` $indices `]` `,` $mask `,` $pass_thru attr-dict `:` type($base) `,` type($indices)  `,` type($mask) `,` type($pass_thru) `into` type($result)",
    "category": "Tensor"
  },
  {
    "name": "vector.insert",
    "summary": "insert operation",
    "description": "Inserts an (n - k)-D sub-vector (value-to-store) into an n-D destination\n    vector at a specified k-D position. When n = 0, value-to-store degenerates\n    to a scalar element inserted into the n-D destination vector.\n\n    Static and dynamic indices must be greater or equal to zero and less than\n    the size of the corresponding dimension. The result is undefined if any\n    index is out-of-bounds. The value `-1` represents a poison index, which\n    specifies that the resulting vector is poison.\n\n    Example:\n\n    ```mlir\n    %2 = vector.insert %0, %1[3] : vector<8x16xf32> into vector<4x8x16xf32>\n    %5 = vector.insert %3, %4[2, 1, 3] : f32 into vector<4x8x16xf32>\n    %11 = vector.insert %9, %10[%a, %b, %c] : f32 into vector<4x8x16xf32>\n    %12 = vector.insert %4, %10[2, %b] : vector<16xf32> into vector<4x8x16xf32>\n    %13 = vector.insert %20, %1[-1, %c] : f32 into vector<4x16xf32>\n    ```",
    "inputs": [
      { "name": "valueToStore", "type": "AnyType" },
      { "name": "dest", "type": "AnyVectorOfAnyRank" },
      { "name": "dynamic_position", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyVectorOfAnyRank" }
    ],
    "attributes": [
      { "name": "static_position", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$valueToStore `,` $dest custom<DynamicIndexList>($dynamic_position, $static_position)\n    attr-dict `:` type($valueToStore) `into` type($dest)"
  },
  {
    "name": "vector.insert_strided_slice",
    "summary": "strided_slice operation",
    "description": "Takes a k-D valueToStore vector, an n-D destination vector (n >= k), n-sized\n    `offsets` integer array attribute, a k-sized `strides` integer array attribute\n    and inserts the k-D valueToStore vector as a strided subvector at the proper offset\n    into the n-D destination vector.\n\n    At the moment strides must contain only 1s.\n\n    Returns an n-D vector that is a copy of the n-D destination vector in which\n    the last k-D dimensions contain the k-D valueToStore vector elements strided at\n    the proper location as specified by the offsets.\n\n    Example:\n\n    ```mlir\n    %2 = vector.insert_strided_slice %0, %1\n        {offsets = [0, 0, 2], strides = [1, 1]}:\n      vector<2x4xf32> into vector<16x4x8xf32>\n    ```",
    "assemblyFormat": "$valueToStore `,` $dest attr-dict `:` type($valueToStore) `into` type($dest)"
  },
  {
    "name": "vector.interleave",
    "summary": "constructs a vector by interleaving two input vectors",
    "description": "The interleave operation constructs a new vector by interleaving the\n    elements from the trailing (or final) dimension of two input vectors,\n    returning a new vector where the trailing dimension is twice the size.\n\n    Note that for the n-D case this differs from the interleaving possible with\n    `vector.shuffle`, which would only operate on the leading dimension.\n\n    Another key difference is this operation supports scalable vectors, though\n    currently a general LLVM lowering is limited to the case where only the\n    trailing dimension is scalable.\n\n    Example:\n    ```mlir\n    %a = arith.constant dense<[0, 1]> : vector<2xi32>\n    %b = arith.constant dense<[2, 3]> : vector<2xi32>\n    // The value of `%0` is `[0, 2, 1, 3]`.\n    %0 = vector.interleave %a, %b : vector<2xi32> -> vector<4xi32>\n\n    // Examples showing allowed input and result types.\n    %1 = vector.interleave %c, %d : vector<f16> -> vector<2xf16>\n    %2 = vector.interleave %e, %f : vector<6x3xf32> -> vector<6x6xf32>\n    %3 = vector.interleave %g, %h : vector<[4]xi32> -> vector<[8]xi32>\n    %4 = vector.interleave %i, %j : vector<2x4x[2]xf64> -> vector<2x4x[4]xf64>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "AnyVectorOfAnyRank" },
      { "name": "rhs", "type": "AnyVectorOfAnyRank" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyVectorOfNonZeroRank" }
    ],
    "assemblyFormat": "$lhs `,` $rhs  attr-dict `:` type($lhs) `->` type($result)"
  },
  {
    "name": "vector.load",
    "summary": "reads an n-D slice of memory into an n-D vector",
    "description": "The 'vector.load' operation reads an n-D slice of memory into an n-D\n    vector. It takes a 'base' memref, an index for each memref dimension and a\n    result vector type as arguments. It returns a value of the result vector\n    type. The 'base' memref and indices determine the start memory address from\n    which to read. Each index provides an offset for each memref dimension\n    based on the element type of the memref. The shape of the result vector\n    type determines the shape of the slice read from the start memory address.\n    The elements along each dimension of the slice are strided by the memref\n    strides. When loading more than 1 element, only unit strides are allowed\n    along the most minor memref dimension. These constraints guarantee that\n    elements read along the first dimension of the slice are contiguous in\n    memory.\n\n    The memref element type can be a scalar or a vector type. If the memref\n    element type is a scalar, it should match the element type of the result\n    vector. If the memref element type is vector, it should match the result\n    vector type.\n\n    Example: 0-D vector load on a scalar memref.\n    ```mlir\n    %result = vector.load %base[%i, %j] : memref<100x100xf32>, vector<f32>\n    ```\n\n    Example: 1-D vector load on a scalar memref.\n    ```mlir\n    %result = vector.load %base[%i, %j] : memref<100x100xf32>, vector<8xf32>\n    ```\n\n    Example: 1-D vector load on a vector memref.\n    ```mlir\n    %result = vector.load %memref[%i, %j] : memref<200x100xvector<8xf32>>, vector<8xf32>\n    ```\n\n    Example:  2-D vector load on a scalar memref.\n    ```mlir\n    %result = vector.load %memref[%i, %j] : memref<200x100xf32>, vector<4x8xf32>\n    ```\n\n    Example:  2-D vector load on a vector memref.\n    ```mlir\n    %result = vector.load %memref[%i, %j] : memref<200x100xvector<4x8xf32>>, vector<4x8xf32>\n    ```\n\n    Representation-wise, the 'vector.load' operation permits out-of-bounds\n    reads. Support and implementation of out-of-bounds vector loads is\n    target-specific. No assumptions should be made on the value of elements\n    loaded out of bounds. Not all targets may support out-of-bounds vector\n    loads.\n\n    Example:  Potential out-of-bound vector load.\n    ```mlir\n    %result = vector.load %memref[%index] : memref<?xf32>, vector<8xf32>\n    ```\n\n    Example:  Explicit out-of-bound vector load.\n    ```mlir\n    %result = vector.load %memref[%c0] : memref<7xf32>, vector<8xf32>\n    ```\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    load operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violating this requirement\n    triggers immediate undefined behavior.",
    "inputs": [
      { "name": "base", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyVectorOfAnyRank" }
    ],
    "attributes": [
      { "name": "nontemporal", "type": "DefaultValuedOptionalAttr" },
      { "name": "alignment", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$base `[` $indices `]` attr-dict `:` type($base) `,` type($result)"
  },
  {
    "name": "vector.mask",
    "summary": "Predicates a maskable vector operation",
    "description": "The `vector.mask` is a `MaskingOpInterface` operation that predicates the\n    execution of another operation. It takes an `i1` vector mask and an\n    optional passthru vector as arguments.\n\n    A implicitly `vector.yield`-terminated region encloses the operation to be\n    masked. Values used within the region are captured from above. Only one\n    *maskable* operation can be masked with a `vector.mask` operation at a time.\n    An operation is *maskable* if it implements the `MaskableOpInterface`. The\n    terminator yields all results from the maskable operation to the result of\n    this operation. No other values are allowed to be yielded.\n\n    An empty `vector.mask` operation is currently legal to enable optimizations\n    across the `vector.mask` region. However, this might change in the future\n    once vector transformations gain better support for `vector.mask`.\n    TODO: Consider making empty `vector.mask` illegal.\n\n    The vector mask argument holds a bit for each vector lane and determines\n    which vector lanes should execute the maskable operation and which ones\n    should not. The `vector.mask` operation returns the value produced by the\n    masked execution of the nested operation, if any. The masked-off lanes in\n    the result vector are taken from the corresponding lanes of the pass-thru\n    argument, if provided, or left unmodified, otherwise. At this point, 0-D\n    vectors are not supported by `vector.mask`. They may be supported in the\n    future.\n\n    The `vector.mask` operation does not prescribe how a maskable operation\n    should be masked or how a masked operation should be lowered. Masking\n    constraints and some semantic details are provided by each maskable\n    operation through the `MaskableOpInterface`. Lowering of masked operations\n    is implementation defined. For instance, scalarizing the masked operation\n    or executing the operation for the masked-off lanes are valid lowerings as\n    long as the execution of masked-off lanes does not change the observable\n    behavior of the program.\n\n    Examples:\n\n    ```\n      %0 = vector.mask %mask { vector.reduction <add>, %a : vector<8xi32> into i32 } : vector<8xi1> -> i32\n    ```\n\n    ```\n      %0 = vector.mask %mask, %passthru { arith.divsi %a, %b : vector<8xi32> } : vector<8xi1> -> vector<8xi32>\n    ```\n\n    ```\n      vector.mask %mask { vector.transfer_write %val, %t0[%idx] : vector<16xf32>, memref<?xf32> } : vector<16xi1>\n    ```\n\n    ```\n      vector.mask %mask { vector.transfer_write %val, %t0[%idx] : vector<16xf32>, tensor<?xf32> } : vector<16xi1> -> tensor<?xf32>\n    ```",
    "inputs": [
      { "name": "mask", "type": "VectorOfNonZeroRankOf" },
      { "name": "passthru", "type": "Optional" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "vector.maskedload",
    "summary": "loads elements from memory into a vector as defined by a mask vector",
    "description": "The masked load reads elements from memory into a vector as defined\n    by a base with indices and a mask vector. When the mask is set, the\n    element is read from memory. Otherwise, the corresponding element is taken\n    from a pass-through vector. Informally the semantics are:\n    ```\n    result[0] := if mask[0] then base[i + 0] else pass_thru[0]\n    result[1] := if mask[1] then base[i + 1] else pass_thru[1]\n    etc.\n    ```\n\n    If a mask bit is set and the corresponding index is out-of-bounds for the\n    given base, the behavior is undefined. If a mask bit is not set, the value\n    comes from the pass-through vector regardless of the index, and the index is\n    allowed to be out-of-bounds.\n\n    The masked load can be used directly where applicable, or can be used\n    during progressively lowering to bring other memory operations closer to\n    hardware ISA support for a masked load. The semantics of the operation\n    closely correspond to those of the `llvm.masked.load`\n    [intrinsic](https://llvm.org/docs/LangRef.html#llvm-masked-load-intrinsics).\n\n    Examples:\n\n    ```mlir\n    %0 = vector.maskedload %base[%i], %mask, %pass_thru\n       : memref<?xf32>, vector<8xi1>, vector<8xf32> into vector<8xf32>\n\n    %1 = vector.maskedload %base[%i, %j], %mask, %pass_thru\n       : memref<?x?xf32>, vector<16xi1>, vector<16xf32> into vector<16xf32>\n    ```\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    load operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violating this requirement\n    triggers immediate undefined behavior.",
    "assemblyFormat": "$base `[` $indices `]` `,` $mask `,` $pass_thru attr-dict `:` type($base) `,` type($mask) `,` type($pass_thru) `into` type($result)"
  },
  {
    "name": "vector.maskedstore",
    "summary": "stores elements from a vector into memory as defined by a mask vector",
    "description": "The masked store operation writes elements from a vector into memory\n    as defined by a base with indices and a mask vector. When the mask is\n    set, the corresponding element from the vector is written to memory. Otherwise,\n    no action is taken for the element. Informally the semantics are:\n    ```\n    if (mask[0]) base[i+0] = value[0]\n    if (mask[1]) base[i+1] = value[1]\n    etc.\n    ```\n\n    If a mask bit is set and the corresponding index is out-of-bounds for the\n    given base, the behavior is undefined. If a mask bit is not set, no value\n    is stored regardless of the index, and the index is allowed to be\n    out-of-bounds.\n\n    The masked store can be used directly where applicable, or can be used\n    during progressively lowering to bring other memory operations closer to\n    hardware ISA support for a masked store. The semantics of the operation\n    closely correspond to those of the `llvm.masked.store`\n    [intrinsic](https://llvm.org/docs/LangRef.html#llvm-masked-store-intrinsics).\n\n    Examples:\n\n    ```mlir\n    vector.maskedstore %base[%i], %mask, %value\n      : memref<?xf32>, vector<8xi1>, vector<8xf32>\n\n    vector.maskedstore %base[%i, %j], %mask, %value\n      : memref<?x?xf32>, vector<16xi1>, vector<16xf32>\n    ```\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    store operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violating this requirement\n    triggers immediate undefined behavior.",
    "assemblyFormat": "$base `[` $indices `]` `,` $mask `,` $valueToStore attr-dict `:` type($base) `,` type($mask) `,` type($valueToStore)"
  },
  {
    "name": "vector.multi_reduction",
    "summary": "Multi-dimensional reduction operation",
    "description": "Reduces an n-D vector into an (n-k)-D vector (or a scalar when k == n)\n    using the given operation: `add`/`mul`/`minsi`/`minui`/`maxsi`/`maxui`\n    /`and`/`or`/`xor` for integers, and `add`/`mul`/`minnumf`/`maxnumf`/`minimumf`\n    /`maximumf` for floats.\n    Takes an initial accumulator operand.\n\n    Example:\n\n    ```mlir\n    %1 = vector.multi_reduction <add>, %0, %acc0 [1, 3] :\n      vector<4x8x16x32xf32> to vector<4x16xf32>\n    %2 = vector.multi_reduction <add>, %1, %acc1 [0, 1] :\n      vector<4x16xf32> to f32\n    ```",
    "assemblyFormat": "$kind `,` $source `,` $acc attr-dict $reduction_dims `:` type($source) `to` type($dest)"
  },
  {
    "name": "vector.outerproduct",
    "summary": "vector outerproduct with optional fused add",
    "description": "Takes 2 1-D vectors and returns the 2-D vector containing the outer-product,\n    as illustrated below:\n    ```\n     outer |   [c, d]\n     ------+------------\n       [a, | [ [a*c, a*d],\n        b] |   [b*c, b*d] ]\n    ```\n    This operation also accepts a 1-D vector lhs and a scalar rhs. In this\n    case a simple AXPY operation is performed, which returns a 1-D vector.\n    ```\n        [a, b] * c = [a*c, b*c]\n    ```\n\n    An optional extra vector argument with the same shape as the output\n    vector may be specified in which case the operation returns the sum of\n    the outer-product and the extra vector. In this multiply-accumulate\n    scenario for floating-point arguments, the rounding mode is enforced\n    by guaranteeing that a fused-multiply add operation is emitted. When\n    lowered to the LLVMIR dialect, this form emits `llvm.intr.fma`, which\n    is guaranteed to lower to actual `fma` instructions on x86.\n\n    An optional kind attribute may be specified to be: `add`/`mul`/`minsi`\n    /`minui`/`maxsi`/`maxui`/`and`/`or`/`xor` for integers, and `add`/`mul`\n    /`minnumf`/`maxnumf`/`minimumf`/`maximumf` for floats. The default is\n    `add`.\n\n    Example:\n\n    ```\n    %2 = vector.outerproduct %0, %1: vector<4xf32>, vector<8xf32>\n    return %2: vector<4x8xf32>\n\n    %3 = vector.outerproduct %0, %1, %2:\n      vector<4xf32>, vector<8xf32>, vector<4x8xf32>\n    return %3: vector<4x8xf32>\n\n    %4 = vector.outerproduct %0, %1, %2 {kind = #vector.kind<maxnumf>}:\n      vector<4xf32>, vector<8xf32>, vector<4x8xf32>\n    return %3: vector<4x8xf32>\n\n    %6 = vector.outerproduct %4, %5: vector<10xf32>, f32\n    return %6: vector<10xf32>\n\n    ```"
  },
  {
    "name": "vector.print",
    "summary": "print operation (for testing and debugging)",
    "description": "Prints the source vector (or scalar) to stdout in a human-readable format\n    (for testing and debugging). No return value.\n\n    Example:\n\n    ```mlir\n    %v = arith.constant dense<0.0> : vector<4xf32>\n    vector.print %v : vector<4xf32>\n    ```\n\n    When lowered to LLVM, the vector print is decomposed into elementary\n    printing method calls that at runtime will yield:\n\n    ```\n    ( 0.0, 0.0, 0.0, 0.0 )\n    ```\n\n    This is printed to stdout via a small runtime support library, which only\n    needs to provide a few printing methods (single value for all data\n    types, opening/closing bracket, comma, newline).\n\n    By default `vector.print` adds a newline after the vector, but this can be\n    controlled by the `punctuation` attribute. For example, to print a comma\n    after instead do:\n\n    ```mlir\n    vector.print %v : vector<4xf32> punctuation <comma>\n    ```\n\n    Note that it is possible to use the punctuation attribute alone. The\n    following will print a single newline:\n\n    ```mlir\n    vector.print punctuation <newline>\n    ```\n\n    Additionally, to aid with debugging and testing `vector.print` can also\n    print constant strings:\n\n    ```mlir\n    vector.print str \"Hello, World!\"\n    ```",
    "assemblyFormat": "($source^ `:` type($source))?\n        oilist(\n            `str` $stringLiteral\n          | `punctuation` $punctuation)\n        attr-dict"
  },
  {
    "name": "vector.reduction",
    "summary": "reduction operation",
    "description": "Reduces an 1-D vector \"horizontally\" into a scalar using the given\n    operation: `add`/`mul`/`minsi`/`minui`/`maxsi`/`maxui`/`and`/`or`/`xor` for\n    integers, and `add`/`mul`/`minnumf`/`maxnumf`/`minimumf`/`maximumf` for\n    floats. Reductions also allow an optional fused accumulator.\n\n    Note that these operations are restricted to 1-D vectors to remain\n    close to the corresponding LLVM intrinsics:\n\n    http://llvm.org/docs/LangRef.html#vector-reduction-intrinsics\n\n    Example:\n\n    ```mlir\n    %1 = vector.reduction <add>, %0 : vector<16xf32> into f32\n\n    %3 = vector.reduction <xor>, %2 : vector<4xi32> into i32\n\n    %4 = vector.reduction <mul>, %0, %1 : vector<16xf32> into f32\n    ```",
    "assemblyFormat": "$kind `,` $vector (`,` $acc^)? (`fastmath` `` $fastmath^)? attr-dict `:` type($vector) `into` type($dest)"
  },
  {
    "name": "vector.scalable.extract",
    "summary": "extract subvector from scalable vector operation",
    "description": "Takes rank-1 source vector and a position `pos` within the source\n    vector, and extracts a subvector starting from that position.\n\n    The extraction position must be a multiple of the minimum size of the result\n    vector. For the operation to be well defined, the destination vector must\n    fit within the source vector from the specified position. Since the source\n    vector is scalable and its runtime length is unknown, the validity of the\n    operation can't be verified nor guaranteed at compile time.\n\n    Example:\n\n    ```mlir\n    %1 = vector.scalable.extract %0[8] : vector<4xf32> from vector<[8]xf32>\n    %3 = vector.scalable.extract %2[0] : vector<[4]xf32> from vector<[8]xf32>\n    ```\n\n    Invalid example:\n    ```mlir\n    %1 = vector.scalable.extract %0[5] : vector<4xf32> from vector<[16]xf32>\n    ```",
    "assemblyFormat": "$source `[` $pos `]` attr-dict `:` type($result) `from` type($source)"
  },
  {
    "name": "vector.scalable.insert",
    "summary": "insert subvector into scalable vector operation",
    "description": "This operations takes a rank-1 fixed-length or scalable subvector and\n    inserts it within the destination scalable vector starting from the\n    position specificed by `pos`. If the source vector is scalable, the\n    insertion position will be scaled by the runtime scaling factor of the\n    source subvector.\n\n    The insertion position must be a multiple of the minimum size of the source\n    vector. For the operation to be well defined, the source vector must fit in\n    the destination vector from the specified position. Since the destination\n    vector is scalable and its runtime length is unknown, the validity of the\n    operation can't be verified nor guaranteed at compile time.\n\n    Example:\n\n    ```mlir\n    %2 = vector.scalable.insert %0, %1[8] : vector<4xf32> into vector<[16]xf32>\n    %5 = vector.scalable.insert %3, %4[0] : vector<8xf32> into vector<[4]xf32>\n    %8 = vector.scalable.insert %6, %7[0] : vector<[4]xf32> into vector<[8]xf32>\n    ```\n\n    Invalid example:\n    ```mlir\n    %2 = vector.scalable.insert %0, %1[5] : vector<4xf32> into vector<[16]xf32>\n    ```",
    "assemblyFormat": "$valueToStore `,` $dest `[` $pos `]` attr-dict `:` type($valueToStore) `into` type($dest)"
  },
  {
    "name": "vector.scan",
    "summary": "Scan operation",
    "description": "Performs an inclusive/exclusive scan on an n-D vector along a single\n    dimension returning an n-D result vector using the given\n    operation (`add`/`mul`/`minsi`/`minui`/`maxsi`/`maxui`/`and`/`or`/`xor` for\n    integers, and `add`/`mul`/`minnumf`/`maxnumf`/`minimumf`/`maximumf` for\n    floats), and a specified value for the initial value. The operator returns\n    the result of scan as well as the result of the last reduction in the scan.\n\n    Example:\n\n    ```mlir\n    %1:2 = vector.scan <add>, %0, %acc {inclusive = false, reduction_dim = 1 : i64} :\n      vector<4x8x16x32xf32>, vector<4x16x32xf32>\n    ```",
    "assemblyFormat": "$kind `,` $source `,` $initial_value attr-dict `:` type($source) `,` type($initial_value)"
  },
  {
    "name": "vector.scatter",
    "summary": "scatters elements from a vector into memory as defined by an index vector\n    and a mask vector",
    "description": "The scatter operation stores elements from a n-D vector into memory as\n    defined by a base with indices and an additional n-D index vector, but\n    only if the corresponding bit in a n-D mask vector is set. Otherwise, no\n    action is taken for that element. Informally the semantics are:\n    ```\n    if (mask[0]) base[index[0]] = value[0]\n    if (mask[1]) base[index[1]] = value[1]\n    etc.\n    ```\n\n    If a mask bit is set and the corresponding index is out-of-bounds for the\n    given base, the behavior is undefined. If a mask bit is not set, no value\n    is stored regardless of the index, and the index is allowed to be\n    out-of-bounds.\n\n    If the index vector contains two or more duplicate indices, the behavior is\n    undefined. Underlying implementation may enforce strict sequential\n    semantics.\n    TODO: always enforce strict sequential semantics?\n\n    The scatter operation can be used directly where applicable, or can be used\n    during progressively lowering to bring other memory operations closer to\n    hardware ISA support for a scatter. The semantics of the operation closely\n    correspond to those of the `llvm.masked.scatter`\n    [intrinsic](https://llvm.org/docs/LangRef.html#llvm-masked-scatter-intrinsics).\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    scatter operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violating this requirement\n    triggers immediate undefined behavior.\n\n    Examples:\n\n    ```mlir\n    vector.scatter %base[%c0][%v], %mask, %value\n        : memref<?xf32>, vector<16xi32>, vector<16xi1>, vector<16xf32>\n\n    vector.scatter %base[%i, %j][%v], %mask, %value\n        : memref<16x16xf32>, vector<16xi32>, vector<16xi1>, vector<16xf32>\n    ```",
    "assemblyFormat": "$base `[` $offsets `]` `[` $indices `]` `,` $mask `,` $valueToStore attr-dict `:` type($base) `,` type($indices)  `,` type($mask) `,` type($valueToStore)",
    "category": "Tensor"
  },
  {
    "name": "vector.shape_cast",
    "summary": "shape_cast casts between vector shapes",
    "description": "Casts to a vector with the same number of elements, element type, and\n    number of scalable dimensions.\n\n    It is currently assumed that this operation does not require moving data,\n    and that it will be folded away before lowering vector operations.\n\n    There is an exception to the folding expectation when targeting\n    llvm.intr.matrix operations. We need a type conversion back and forth from a\n    2-D MLIR vector to a 1-D flattened LLVM vector.shape_cast lowering to LLVM\n    is supported in that particular case, for now.\n\n    Examples:\n\n    ```mlir\n    %1 = vector.shape_cast %0 : vector<4x3xf32> to vector<3x2x2xf32>\n\n    // with 2 scalable dimensions (number of which must be preserved).\n    %3 = vector.shape_cast %2 : vector<[2]x3x[4]xi8> to vector<3x[1]x[8]xi8>\n    ```",
    "assemblyFormat": "$source attr-dict `:` type($source) `to` type($result)"
  },
  {
    "name": "vector.shuffle",
    "summary": "shuffle operation",
    "description": "The shuffle operation constructs a permutation (or duplication) of elements\n    from two input vectors, returning a vector with the same element type as\n    the input and a length that is the same as the shuffle mask. The two input\n    vectors must have the same element type, same rank, and trailing dimension\n    sizes and shuffles their values in the leading dimension (which may differ\n    in size) according to the given mask. The legality rules are:\n    * the two operands must have the same element type as the result\n      - Either, the two operands and the result must have the same\n        rank and trailing dimension sizes, viz. given two k-D operands\n                v1 : <s_1 x s_2 x .. x s_k x type> and\n                v2 : <t_1 x t_2 x .. x t_k x type>\n        we have s_i = t_i for all 1 < i <= k\n      - Or, the two operands must be 0-D vectors and the result is a 1-D vector.\n    * the mask length equals the leading dimension size of the result\n    * numbering the input vector indices left to right across the operands, all\n      mask values must be within range, viz. given two k-D operands v1 and v2\n      above, all mask values are in the range [0,s_1+t_1). The value `-1`\n      represents a poison mask value, which specifies that the selected element\n      is poison.\n\n    Note, scalable vectors are not supported.\n\n    Example:\n\n    ```mlir\n    %0 = vector.shuffle %a, %b[0, 3]\n               : vector<2xf32>, vector<2xf32>       ; yields vector<2xf32>\n    %1 = vector.shuffle %c, %b[0, 1, 2]\n               : vector<2x16xf32>, vector<1x16xf32> ; yields vector<3x16xf32>\n    %2 = vector.shuffle %a, %b[3, 2, 1, 0]\n               : vector<2xf32>, vector<2xf32>       ; yields vector<4xf32>\n    %3 = vector.shuffle %a, %b[0, 1]\n               : vector<f32>, vector<f32>           ; yields vector<2xf32>\n    %4 = vector.shuffle %a, %b[0, 4, -1, -1, -1, -1]\n               : vector<4xf32>, vector<4xf32>       ; yields vector<6xf32>\n    ```",
    "assemblyFormat": "operands $mask attr-dict `:` type(operands)"
  },
  {
    "name": "vector.step",
    "summary": "A linear sequence of values from 0 to N",
    "description": "A `step` operation produces an index vector, i.e. a 1-D vector of values of\n    index type that represents a linear sequence from 0 to N-1, where N is the\n    number of elements in the `result` vector.\n\n    Supports fixed-width and scalable vectors.\n\n    Examples:\n\n    ```mlir\n    %0 = vector.step : vector<4xindex> ; [0, 1, 2, 3]\n    %1 = vector.step : vector<[4]xindex> ; [0, 1, .., <vscale * 4 - 1>]\n    ```",
    "outputs": [
      { "name": "result", "type": "VectorOfRankAndType" }
    ],
    "assemblyFormat": "attr-dict `:` type($result)"
  },
  {
    "name": "vector.store",
    "summary": "writes an n-D vector to an n-D slice of memory",
    "description": "The 'vector.store' operation writes an n-D vector to an n-D slice of memory.\n    It takes the vector value to be stored, a 'base' memref and an index for\n    each memref dimension. The 'base' memref and indices determine the start\n    memory address from which to write. Each index provides an offset for each\n    memref dimension based on the element type of the memref. The shape of the\n    vector value to store determines the shape of the slice written from the\n    start memory address. The elements along each dimension of the slice are\n    strided by the memref strides. When storing more than 1 element, only unit\n    strides are allowed along the most minor memref dimension. These constraints\n    guarantee that elements written along the first dimension of the slice are\n    contiguous in memory.\n\n    The memref element type can be a scalar or a vector type. If the memref\n    element type is a scalar, it should match the element type of the value\n    to store. If the memref element type is vector, it should match the type\n    of the value to store.\n\n    Example: 0-D vector store on a scalar memref.\n    ```mlir\n    vector.store %valueToStore, %memref[%i, %j] : memref<200x100xf32>, vector<f32>\n    ```\n\n    Example: 1-D vector store on a scalar memref.\n    ```mlir\n    vector.store %valueToStore, %memref[%i, %j] : memref<200x100xf32>, vector<8xf32>\n    ```\n\n    Example: 1-D vector store on a vector memref.\n    ```mlir\n    vector.store %valueToStore, %memref[%i, %j] : memref<200x100xvector<8xf32>>, vector<8xf32>\n    ```\n\n    Example:  2-D vector store on a scalar memref.\n    ```mlir\n    vector.store %valueToStore, %memref[%i, %j] : memref<200x100xf32>, vector<4x8xf32>\n    ```\n\n    Example:  2-D vector store on a vector memref.\n    ```mlir\n    vector.store %valueToStore, %memref[%i, %j] : memref<200x100xvector<4x8xf32>>, vector<4x8xf32>\n    ```\n\n    Representation-wise, the 'vector.store' operation permits out-of-bounds\n    writes. Support and implementation of out-of-bounds vector stores are\n    target-specific. No assumptions should be made on the memory written out of\n    bounds. Not all targets may support out-of-bounds vector stores.\n\n    Example:  Potential out-of-bounds vector store.\n    ```mlir\n    vector.store %valueToStore, %memref[%index] : memref<?xf32>, vector<8xf32>\n    ```\n\n    Example:  Explicit out-of-bounds vector store.\n    ```mlir\n    vector.store %valueToStore, %memref[%c0] : memref<7xf32>, vector<8xf32>\n    ```\n\n    An optional `alignment` attribute allows to specify the byte alignment of the\n    store operation. It must be a positive power of 2. The operation must access\n    memory at an address aligned to this boundary. Violating this requirement\n    triggers immediate undefined behavior.",
    "inputs": [
      { "name": "valueToStore", "type": "AnyVectorOfAnyRank" },
      { "name": "base", "type": "Arg" },
      { "name": "indices", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "nontemporal", "type": "DefaultValuedOptionalAttr" },
      { "name": "alignment", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$valueToStore `,` $base `[` $indices `]` attr-dict `:` type($base) `,` type($valueToStore)"
  },
  {
    "name": "vector.to_elements",
    "summary": "operation that decomposes a vector into all its scalar elements",
    "description": "This operation decomposes all the scalar elements from a vector. The\n    decomposed scalar elements are returned in row-major order. The number of\n    scalar results must match the number of elements in the input vector type.\n    All the result elements have the same result type, which must match the\n    element type of the input vector. Scalable vectors are not supported.\n\n    Examples:\n\n    ```mlir\n    // Decompose a 0-D vector.\n    %0 = vector.to_elements %v0 : vector<f32>\n    // %0 = %v0[0]\n\n    // Decompose a 1-D vector.\n    %0:2 = vector.to_elements %v1 : vector<2xf32>\n    // %0#0 = %v1[0]\n    // %0#1 = %v1[1]\n\n    // Decompose a 2-D.\n    %0:6 = vector.to_elements %v2 : vector<2x3xf32>\n    // %0#0 = %v2[0, 0]\n    // %0#1 = %v2[0, 1]\n    // %0#2 = %v2[0, 2]\n    // %0#3 = %v2[1, 0]\n    // %0#4 = %v2[1, 1]\n    // %0#5 = %v2[1, 2]\n\n    // Decompose a 3-D vector.\n    %0:6 = vector.to_elements %v3 : vector<3x1x2xf32>\n    // %0#0 = %v3[0, 0, 0]\n    // %0#1 = %v3[0, 0, 1]\n    // %0#2 = %v3[1, 0, 0]\n    // %0#3 = %v3[1, 0, 1]\n    // %0#4 = %v3[2, 0, 0]\n    // %0#5 = %v3[2, 0, 1]\n    ```",
    "inputs": [
      { "name": "source", "type": "AnyVectorOfAnyRank" }
    ],
    "outputs": [
      { "name": "elements", "type": "Variadic" }
    ],
    "assemblyFormat": "$source attr-dict `:` type($source)"
  },
  {
    "name": "vector.transfer_read",
    "summary": "Reads a supervector from memory into an SSA vector value.",
    "description": "The `vector.transfer_read` op performs a read from a slice within a\n    [MemRef](../LangRef.md#memref-type) or a Ranked\n    [Tensor](../LangRef.md#tensor-type) supplied as its first operand\n    into a [vector](../LangRef.md#vector-type) of the same base elemental type.\n\n    A memref/tensor operand with vector element type, must have its vector\n    element type match a suffix (shape and element type) of the vector (e.g.\n    memref<3x2x6x4x3xf32>, vector<1x1x4x3xf32>).\n\n    The slice is further defined by a full-rank index within the MemRef/Tensor,\n    supplied as the operands `[1 .. 1 + rank(memref/tensor))` that defines the\n    starting point of the transfer (e.g. `%A[%i0, %i1, %i2]`).\n\n    The permutation_map [attribute](../LangRef.md#attributes) is an\n    [affine-map](Affine.md#affine-maps) which specifies the transposition on the\n    slice to match the vector shape. The permutation map may be implicit and\n    omitted from parsing and printing if it is the canonical minor identity map\n    (i.e. if it does not permute or broadcast any dimension).\n\n    The size of the slice is specified by the size of the vector, given as the\n    return type.\n\n    An SSA value `padding` of the same elemental type as the MemRef/Tensor is\n    provided to specify a fallback value in the case of out-of-bounds accesses\n    and/or masking.\n\n    An optional SSA value `mask` may be specified to mask out elements read from\n    the MemRef/Tensor. The `mask` type is an `i1` vector with a shape that\n    matches how elements are read from the MemRef/Tensor, *before* any\n    permutation or broadcasting. Elements whose corresponding mask element is\n    `0` are masked out and replaced with `padding`.\n\n    For every vector dimension, the boolean array attribute `in_bounds`\n    specifies if the transfer is guaranteed to be within the source bounds. If\n    set to \"false\", accesses (including the starting point) may run\n    out-of-bounds along the respective vector dimension as the index increases.\n    Non-vector dimensions *must* always be in-bounds. The `in_bounds` array\n    length has to be equal to the vector rank. This attribute has a default\n    value: `false` (i.e. \"out-of-bounds\"). When skipped in the textual IR, the\n    default value is assumed. Similarly, the OP printer will omit this\n    attribute when all dimensions are out-of-bounds (i.e. the default value is\n    used).\n\n    A `vector.transfer_read` can be lowered to a simple load if all dimensions\n    are specified to be within bounds and no `mask` was specified.\n\n    This operation is called 'read' by opposition to 'load' because the\n    super-vector granularity is generally not representable with a single\n    hardware register. A `vector.transfer_read` is thus a mid-level abstraction\n    that supports super-vectorization with non-effecting padding for full-tile\n    only operations.\n\n    More precisely, let's dive deeper into the permutation_map for the following\n    MLIR:\n\n    ```mlir\n    vector.transfer_read %A[%expr1, %expr2, %expr3, %expr4]\n      { permutation_map : (d0,d1,d2,d3) -> (d2,0,d0) } :\n      memref<?x?x?x?xf32>, vector<3x4x5xf32>\n    ```\n\n    This operation always reads a slice starting at `%A[%expr1, %expr2, %expr3,\n    %expr4]`. The size of the slice can be inferred from the resulting vector\n    shape and walking back through the permutation map: 3 along d2 and 5 along\n    d0, so the slice is: `%A[%expr1 : %expr1 + 5, %expr2, %expr3:%expr3 + 3, %expr4]`\n\n    That slice needs to be read into a `vector<3x4x5xf32>`. Since the\n    permutation map is not full rank, there must be a broadcast along vector\n    dimension `1`.\n\n    A notional lowering of vector.transfer_read could generate code resembling:\n\n    ```mlir\n    // %expr1, %expr2, %expr3, %expr4 defined before this point\n    // alloc a temporary buffer for performing the \"gather\" of the slice.\n    %tmp = memref.alloc() : memref<vector<3x4x5xf32>>\n    for %i = 0 to 3 {\n      affine.for %j = 0 to 4 {\n        affine.for %k = 0 to 5 {\n          // Note that this load does not involve %j.\n          %a = load %A[%expr1 + %k, %expr2, %expr3 + %i, %expr4] : memref<?x?x?x?xf32>\n          // Update the temporary gathered slice with the individual element\n          %slice = memref.load %tmp : memref<vector<3x4x5xf32>> -> vector<3x4x5xf32>\n          %updated = vector.insert %a, %slice[%i, %j, %k] : f32 into vector<3x4x5xf32>\n          memref.store %updated, %tmp : memref<vector<3x4x5xf32>>\n    }}}\n    // At this point we gathered the elements from the original\n    // memref into the desired vector layout, stored in the `%tmp` allocation.\n    %vec = memref.load %tmp : memref<vector<3x4x5xf32>> -> vector<3x4x5xf32>\n    ```\n\n    On a GPU one could then map `i`, `j`, `k` to blocks and threads. Notice that\n    the temporary storage footprint could conceptually be only `3 * 5` values but\n    `3 * 4 * 5` values are actually transferred between `%A` and `%tmp`.\n\n    Alternatively, if a notional vector broadcast operation were available, we\n    could avoid the loop on `%j` and the lowered code would resemble:\n\n    ```mlir\n    // %expr1, %expr2, %expr3, %expr4 defined before this point\n    %tmp = memref.alloc() : memref<vector<3x4x5xf32>>\n    for %i = 0 to 3 {\n      affine.for %k = 0 to 5 {\n        %a = load %A[%expr1 + %k, %expr2, %expr3 + %i, %expr4] : memref<?x?x?x?xf32>\n        %slice = memref.load %tmp : memref<vector<3x4x5xf32>> -> vector<3x4x5xf32>\n        // Here we only store to the first element in dimension one\n        %updated = vector.insert %a, %slice[%i, 0, %k] : f32 into vector<3x4x5xf32>\n        memref.store %updated, %tmp : memref<vector<3x4x5xf32>>\n    }}\n    // At this point we gathered the elements from the original\n    // memref into the desired vector layout, stored in the `%tmp` allocation.\n    // However we haven't replicated them alongside the first dimension, we need\n    // to broadcast now.\n    %partialVec = load %tmp : memref<vector<3x4x5xf32>> -> vector<3x4x5xf32>\n    %vec = broadcast %tmpvec, 1 : vector<3x4x5xf32>\n    ```\n\n    where `broadcast` broadcasts from element 0 to all others along the\n    specified dimension. This time, the number of loaded element is `3 * 5`\n    values.\n    An additional `1` broadcast is required. On a GPU this broadcast could be\n    implemented using a warp-shuffle if loop `j` were mapped to `threadIdx.x`.\n\n    Syntax\n    ```\n    operation ::= ssa-id `=` `vector.transfer_read` ssa-use-list\n      `{` attribute-entry `} :` memref-type `,` vector-type\n    ```\n\n    Example:\n\n    ```mlir\n    // Read the slice `%A[%i0, %i1:%i1+256, %i2:%i2+32]` into vector<32x256xf32>\n    // and pad with %f0 to handle the boundary case:\n    %f0 = arith.constant 0.0f : f32\n    affine.for %i0 = 0 to %0 {\n      affine.for %i1 = 0 to %1 step 256 {\n        affine.for %i2 = 0 to %2 step 32 {\n          %v = vector.transfer_read %A[%i0, %i1, %i2], (%f0)\n               {permutation_map: (d0, d1, d2) -> (d2, d1)} :\n               memref<?x?x?xf32>, vector<32x256xf32>\n    }}}\n\n    // or equivalently (rewrite with vector.transpose)\n    %f0 = arith.constant 0.0f : f32\n    affine.for %i0 = 0 to %0 {\n      affine.for %i1 = 0 to %1 step 256 {\n        affine.for %i2 = 0 to %2 step 32 {\n          %v0 = vector.transfer_read %A[%i0, %i1, %i2], (%f0)\n               {permutation_map: (d0, d1, d2) -> (d1, d2)} :\n               memref<?x?x?xf32>, vector<256x32xf32>\n          %v = vector.transpose %v0, [1, 0] :\n              vector<256x32xf32> to vector<32x256f32>\n    }}}\n\n    // Read the slice `%A[%i0, %i1]` (i.e. the element `%A[%i0, %i1]`) into\n    // vector<128xf32>. The underlying implementation will require a 1-D vector\n    // broadcast:\n    affine.for %i0 = 0 to %0 {\n      affine.for %i1 = 0 to %1 {\n        %3 = vector.transfer_read %A[%i0, %i1]\n             {permutation_map: (d0, d1) -> (0)} :\n             memref<?x?xf32>, vector<128xf32>\n      }\n    }\n\n    // Read from a memref with vector element type.\n    %4 = vector.transfer_read %arg1[%c3, %c3], %vf0\n      {permutation_map = (d0, d1)->(d0, d1)}\n        : memref<?x?xvector<4x3xf32>>, vector<1x1x4x3xf32>\n\n    // Read from a tensor with vector element type.\n    %4 = vector.transfer_read %arg1[%c3, %c3], %vf0\n      {permutation_map = (d0, d1)->(d0, d1)}\n        : tensor<?x?xvector<4x3xf32>>, vector<1x1x4x3xf32>\n\n    // Special encoding for 0-d transfer with 0-d tensor/memref, vector shape\n    // {1} and permutation_map () -> (0).\n    %0 = vector.transfer_read %arg0[], %f0 {permutation_map = affine_map<()->(0)>} :\n      tensor<f32>, vector<1xf32>\n    ```"
  },
  {
    "name": "vector.transfer_write",
    "summary": "The vector.transfer_write op writes a supervector to memory.",
    "description": "The `vector.transfer_write` op performs a write from a\n    [vector](../LangRef.md#vector-type), supplied as its first operand, into a\n    slice within a [MemRef](../LangRef.md#memref-type) or a Ranked\n    [Tensor](../LangRef.md#tensor-type) of the same base elemental type,\n    supplied as its second operand.\n\n    A vector memref/tensor operand must have its vector element type match a\n    suffix (shape and element type) of the vector (e.g. memref<3x2x6x4x3xf32>,\n    vector<1x1x4x3xf32>). If the operand is a tensor, the operation returns a\n    new tensor of the same type.\n\n    The slice is further defined by a full-rank index within the MemRef/Tensor,\n    supplied as the operands `[2 .. 2 + rank(memref/tensor))` that defines the\n    starting point of the transfer (e.g. `%A[%i0, %i1, %i2, %i3]`).\n\n    The permutation_map [attribute](../LangRef.md#attributes) is an\n    [affine-map](Affine.md#affine-maps) which specifies the transposition on the\n    slice to match the vector shape. The permutation map may be implicit and\n    omitted from parsing and printing if it is the canonical minor identity map\n    (i.e. if it does not permute any dimension). In contrast to `transfer_read`,\n    write ops cannot have broadcast dimensions.\n\n    The size of the slice is specified by the size of the vector.\n\n    An optional SSA value `mask` may be specified to mask out elements written\n    to the MemRef/Tensor. The `mask` type is an `i1` vector with a shape that\n    matches how elements are written into the MemRef/Tensor, *after* applying\n    any permutation. Elements whose corresponding mask element is `0` are\n    masked out.\n\n    For every vector dimension, the boolean array attribute `in_bounds`\n    specifies if the transfer is guaranteed to be within the source bounds. If\n    set to \"false\", accesses (including the starting point) may run\n    out-of-bounds along the respective vector dimension as the index increases.\n    Non-vector dimensions *must* always be in-bounds. The `in_bounds` array\n    length has to be equal to the vector rank. This attribute has a default\n    value: `false` (i.e. \"out-of-bounds\"). When skipped in the textual IR, the\n    default value is assumed. Similarly, the OP printer will omit this\n    attribute when all dimensions are out-of-bounds (i.e. the default value is\n    used).\n\n     A `vector.transfer_write` can be lowered to a simple store if all\n     dimensions are specified to be within bounds and no `mask` was specified.\n\n    This operation is called 'write' by opposition to 'store' because the\n    super-vector granularity is generally not representable with a single\n    hardware register. A `vector.transfer_write` is thus a\n    mid-level abstraction that supports super-vectorization with non-effecting\n    padding for full-tile-only code. It is the responsibility of\n    `vector.transfer_write`'s implementation to ensure the memory writes are\n    valid. Different lowerings may be pertinent depending on the hardware\n    support.\n\n    Example:\n\n    ```mlir\n    // write vector<16x32x64xf32> into the slice\n    //   `%A[%i0, %i1:%i1+32, %i2:%i2+64, %i3:%i3+16]`:\n    for %i0 = 0 to %0 {\n      affine.for %i1 = 0 to %1 step 32 {\n        affine.for %i2 = 0 to %2 step 64 {\n          affine.for %i3 = 0 to %3 step 16 {\n            %val = `ssa-value` : vector<16x32x64xf32>\n            vector.transfer_write %val, %A[%i0, %i1, %i2, %i3]\n              {permutation_map: (d0, d1, d2, d3) -> (d3, d1, d2)} :\n              vector<16x32x64xf32>, memref<?x?x?x?xf32>\n    }}}}\n\n    // or equivalently (rewrite with vector.transpose)\n    for %i0 = 0 to %0 {\n      affine.for %i1 = 0 to %1 step 32 {\n        affine.for %i2 = 0 to %2 step 64 {\n          affine.for %i3 = 0 to %3 step 16 {\n            %val = `ssa-value` : vector<16x32x64xf32>\n            %valt = vector.transpose %val, [1, 2, 0] :\n                  vector<16x32x64xf32> -> vector<32x64x16xf32>\n            vector.transfer_write %valt, %A[%i0, %i1, %i2, %i3]\n              {permutation_map: (d0, d1, d2, d3) -> (d1, d2, d3)} :\n              vector<32x64x16xf32>, memref<?x?x?x?xf32>\n    }}}}\n\n    // write to a memref with vector element type.\n    vector.transfer_write %4, %arg1[%c3, %c3]\n      {permutation_map = (d0, d1)->(d0, d1)}\n        : vector<1x1x4x3xf32>, memref<?x?xvector<4x3xf32>>\n\n    // return a tensor where the vector is inserted into the source tensor.\n    %5 = vector.transfer_write %4, %arg1[%c3, %c3]\n      {permutation_map = (d0, d1)->(d0, d1)}\n        : vector<1x1x4x3xf32>, tensor<?x?xvector<4x3xf32>>\n\n    // Special encoding for 0-d transfer with 0-d tensor/memref, vector shape\n    // {1} and permutation_map () -> (0).\n    %1 = vector.transfer_write %0, %arg0[] {permutation_map = affine_map<()->(0)>} :\n      vector<1xf32>, tensor<f32>\n    ```"
  },
  {
    "name": "vector.transpose",
    "summary": "vector transpose operation",
    "description": "Takes a n-D vector and returns the transposed n-D vector defined by\n    the permutation of ranks in the n-sized integer array attribute (in case\n    of 0-D vectors the array attribute must be empty).\n\n    In the operation\n\n    ```mlir\n    %1 = vector.transpose %0, [i_1, .., i_n]\n      : vector<d_1 x .. x d_n x f32>\n      to vector<d_trans[0] x .. x d_trans[n-1] x f32>\n    ```\n\n    the `permutation` array [i_1, .., i_n] must be a permutation of [0, .., n-1].\n\n    Example:\n\n    ```mlir\n    %1 = vector.transpose %0, [1, 0] : vector<2x3xf32> to vector<3x2xf32>\n\n     [ [a, b, c],       [ [a, d],\n       [d, e, f] ]  ->    [b, e],\n                          [c, f] ]\n    ```",
    "inputs": [
      { "name": "vector", "type": "AnyVectorOfAnyRank" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyVectorOfAnyRank" }
    ],
    "attributes": [
      { "name": "permutation", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$vector `,` $permutation attr-dict `:` type($vector) `to` type($result)",
    "category": "Transform"
  },
  {
    "name": "vector.type_cast",
    "summary": "type_cast op converts a scalar memref to a vector memref",
    "description": "Performs a conversion from a memref with scalar element to a memref with a\n    *single* vector element, copying the shape of the memref to the vector. This\n    is the minimal viable operation that is required to makeke\n    super-vectorization operational. It can be seen as a special case of the\n    `view` operation but scoped in the super-vectorization context.\n\n    Example:\n\n    ```mlir\n    %A  = memref.alloc() : memref<5x4x3xf32>\n    %VA = vector.type_cast %A : memref<5x4x3xf32> to memref<vector<5x4x3xf32>>\n    ```",
    "assemblyFormat": "$memref attr-dict `:` type($memref) `to` type($result)"
  },
  {
    "name": "vector.vscale",
    "summary": "Load vector scale size",
    "description": "The `vscale` op returns the scale of the scalable vectors, a positive\n    integer value that is constant at runtime but unknown at compile-time.\n    The scale of the vector indicates the multiplicity of the vectors and\n    vector operations. For example, a `vector<[4]xi32>` is equivalent to\n    `vscale` consecutive `vector<4xi32>`; and an operation on a\n    `vector<[4]xi32>` is equivalent to performing that operation `vscale`\n    times, once on each `<4xi32>` segment of the scalable vector. The `vscale`\n    op can be used to calculate the step in vector-length agnostic (VLA) loops.\n    Right now we only support one contiguous set of scalable dimensions, all of\n    them grouped and scaled with the value returned by 'vscale'.",
    "outputs": [
      { "name": "res", "type": "Index" }
    ],
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "vector.yield",
    "summary": "Terminates and yields values from vector regions.",
    "description": "\"vector.yield\" yields an SSA value from the Vector dialect op region and\n    terminates the regions. The semantics of how the values are yielded is\n    defined by the parent operation.\n    If \"vector.yield\" has any operands, the operands must correspond to the\n    parent operation's results.\n    If the parent operation defines no value the vector.yield may be omitted\n    when printing the region.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "vhlo.abs_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.add_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.after_all_v1",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.all_gather_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "all_gather_dim", "type": "VHLO_AnyAttr" },
      { "name": "replica_groups", "type": "VHLO_AnyAttr" },
      { "name": "channel_id", "type": "VHLO_AnyAttr" },
      { "name": "use_global_device_ids", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.all_gather_v2",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "all_gather_dim", "type": "VHLO_AnyAttr" },
      { "name": "replica_groups", "type": "VHLO_AnyAttr" },
      { "name": "channel_id", "type": "VHLO_AnyAttr" },
      { "name": "use_global_device_ids", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.all_reduce_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "replica_groups", "type": "VHLO_AnyAttr" },
      { "name": "channel_id", "type": "VHLO_AnyAttr" },
      { "name": "use_global_device_ids", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.all_reduce_v2",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "replica_groups", "type": "VHLO_AnyAttr" },
      { "name": "channel_id", "type": "VHLO_AnyAttr" },
      { "name": "use_global_device_ids", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.all_to_all_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "split_dimension", "type": "VHLO_AnyAttr" },
      { "name": "concat_dimension", "type": "VHLO_AnyAttr" },
      { "name": "split_count", "type": "VHLO_AnyAttr" },
      { "name": "replica_groups", "type": "VHLO_AnyAttr" },
      { "name": "channel_id", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.all_to_all_v2",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "split_dimension", "type": "VHLO_AnyAttr" },
      { "name": "concat_dimension", "type": "VHLO_AnyAttr" },
      { "name": "split_count", "type": "VHLO_AnyAttr" },
      { "name": "replica_groups", "type": "VHLO_AnyAttr" },
      { "name": "channel_id", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.and_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.atan2_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.batch_norm_grad_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "scale", "type": "VHLO_AnyType" },
      { "name": "mean", "type": "VHLO_AnyType" },
      { "name": "variance", "type": "VHLO_AnyType" },
      { "name": "grad_output", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "grad_operand", "type": "VHLO_AnyType" },
      { "name": "grad_scale", "type": "VHLO_AnyType" },
      { "name": "grad_offset", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "VHLO_AnyAttr" },
      { "name": "feature_index", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.batch_norm_inference_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "scale", "type": "VHLO_AnyType" },
      { "name": "offset", "type": "VHLO_AnyType" },
      { "name": "mean", "type": "VHLO_AnyType" },
      { "name": "variance", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "VHLO_AnyAttr" },
      { "name": "feature_index", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.batch_norm_training_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "scale", "type": "VHLO_AnyType" },
      { "name": "offset", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "output", "type": "VHLO_AnyType" },
      { "name": "batch_mean", "type": "VHLO_AnyType" },
      { "name": "batch_var", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "epsilon", "type": "VHLO_AnyAttr" },
      { "name": "feature_index", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.bitcast_convert_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.broadcast_in_dim_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.broadcast_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "broadcast_sizes", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.call_v1",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "callee", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.case_v1",
    "inputs": [
      { "name": "index", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "vhlo.cbrt_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.cbrt_v2",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.ceil_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.cholesky_v1",
    "inputs": [
      { "name": "a", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "lower", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.clamp_v1",
    "inputs": [
      { "name": "min", "type": "VHLO_AnyType" },
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "max", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.collective_broadcast_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "replica_groups", "type": "VHLO_AnyAttr" },
      { "name": "channel_id", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.collective_permute_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "source_target_pairs", "type": "VHLO_AnyAttr" },
      { "name": "channel_id", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.compare_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "comparison_direction", "type": "VHLO_AnyAttr" },
      { "name": "compare_type", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.complex_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.composite_v1",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "name", "type": "VHLO_AnyAttr" },
      { "name": "composite_attributes", "type": "VHLO_AnyAttr" },
      { "name": "decomposition", "type": "VHLO_AnyAttr" },
      { "name": "version", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.concatenate_v1",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "dimension", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.constant_v1",
    "outputs": [
      { "name": "output", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "value", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.convert_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.convolution_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "window_strides", "type": "VHLO_AnyAttr" },
      { "name": "padding", "type": "VHLO_AnyAttr" },
      { "name": "lhs_dilation", "type": "VHLO_AnyAttr" },
      { "name": "rhs_dilation", "type": "VHLO_AnyAttr" },
      { "name": "window_reversal", "type": "VHLO_AnyAttr" },
      { "name": "input_batch_dimension", "type": "VHLO_AnyAttr" },
      { "name": "input_feature_dimension", "type": "VHLO_AnyAttr" },
      { "name": "input_spatial_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "kernel_input_feature_dimension", "type": "VHLO_AnyAttr" },
      { "name": "kernel_output_feature_dimension", "type": "VHLO_AnyAttr" },
      { "name": "kernel_spatial_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "output_batch_dimension", "type": "VHLO_AnyAttr" },
      { "name": "output_feature_dimension", "type": "VHLO_AnyAttr" },
      { "name": "output_spatial_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "feature_group_count", "type": "VHLO_AnyAttr" },
      { "name": "batch_group_count", "type": "VHLO_AnyAttr" },
      { "name": "precision_config", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.cosine_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.cosine_v2",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.count_leading_zeros_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.create_token_v1",
    "outputs": [
      { "name": "output", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.cross-replica-sum_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "replica_groups", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.custom_call_v1",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "call_target_name", "type": "VHLO_AnyAttr" },
      { "name": "has_side_effect", "type": "VHLO_AnyAttr" },
      { "name": "backend_config", "type": "VHLO_AnyAttr" },
      { "name": "api_version", "type": "VHLO_AnyAttr" },
      { "name": "called_computations", "type": "VHLO_AnyAttr" },
      { "name": "operand_layouts", "type": "VHLO_AnyAttr" },
      { "name": "result_layouts", "type": "VHLO_AnyAttr" },
      { "name": "output_operand_aliases", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.divide_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.dot_general_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "lhs_batching_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "rhs_batching_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "lhs_contracting_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "rhs_contracting_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "precision_config", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.dot_general_v2",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "lhs_batching_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "rhs_batching_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "lhs_contracting_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "rhs_contracting_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "precision_config", "type": "VHLO_AnyAttr" },
      { "name": "lhs_precision_type", "type": "VHLO_AnyAttr" },
      { "name": "rhs_precision_type", "type": "VHLO_AnyAttr" },
      { "name": "accumulation_type", "type": "VHLO_AnyAttr" },
      { "name": "lhs_component_count", "type": "VHLO_AnyAttr" },
      { "name": "rhs_component_count", "type": "VHLO_AnyAttr" },
      { "name": "num_primitive_operations", "type": "VHLO_AnyAttr" },
      { "name": "allow_imprecise_accumulation", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.dot_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "precision_config", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.dynamic_broadcast_in_dim_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "output_dimensions", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "broadcast_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "known_expanding_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "known_nonexpanding_dimensions", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.dynamic_conv_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" },
      { "name": "d_padding", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "window_strides", "type": "VHLO_AnyAttr" },
      { "name": "padding", "type": "VHLO_AnyAttr" },
      { "name": "lhs_dilation", "type": "VHLO_AnyAttr" },
      { "name": "rhs_dilation", "type": "VHLO_AnyAttr" },
      { "name": "window_reversal", "type": "VHLO_AnyAttr" },
      { "name": "input_batch_dimension", "type": "VHLO_AnyAttr" },
      { "name": "input_feature_dimension", "type": "VHLO_AnyAttr" },
      { "name": "input_spatial_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "kernel_input_feature_dimension", "type": "VHLO_AnyAttr" },
      { "name": "kernel_output_feature_dimension", "type": "VHLO_AnyAttr" },
      { "name": "kernel_spatial_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "output_batch_dimension", "type": "VHLO_AnyAttr" },
      { "name": "output_feature_dimension", "type": "VHLO_AnyAttr" },
      { "name": "output_spatial_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "feature_group_count", "type": "VHLO_AnyAttr" },
      { "name": "batch_group_count", "type": "VHLO_AnyAttr" },
      { "name": "precision_config", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.dynamic_conv_v2",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" },
      { "name": "padding", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "window_strides", "type": "VHLO_AnyAttr" },
      { "name": "lhs_dilation", "type": "VHLO_AnyAttr" },
      { "name": "rhs_dilation", "type": "VHLO_AnyAttr" },
      { "name": "window_reversal", "type": "VHLO_AnyAttr" },
      { "name": "input_batch_dimension", "type": "VHLO_AnyAttr" },
      { "name": "input_feature_dimension", "type": "VHLO_AnyAttr" },
      { "name": "input_spatial_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "kernel_input_feature_dimension", "type": "VHLO_AnyAttr" },
      { "name": "kernel_output_feature_dimension", "type": "VHLO_AnyAttr" },
      { "name": "kernel_spatial_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "output_batch_dimension", "type": "VHLO_AnyAttr" },
      { "name": "output_feature_dimension", "type": "VHLO_AnyAttr" },
      { "name": "output_spatial_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "feature_group_count", "type": "VHLO_AnyAttr" },
      { "name": "batch_group_count", "type": "VHLO_AnyAttr" },
      { "name": "precision_config", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.dynamic_gather_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "start_indices", "type": "VHLO_AnyType" },
      { "name": "slice_sizes", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "offset_dims", "type": "VHLO_AnyAttr" },
      { "name": "collapsed_slice_dims", "type": "VHLO_AnyAttr" },
      { "name": "start_index_map", "type": "VHLO_AnyAttr" },
      { "name": "index_vector_dim", "type": "VHLO_AnyAttr" },
      { "name": "indices_are_sorted", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.dynamic_gather_v2",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "start_indices", "type": "VHLO_AnyType" },
      { "name": "slice_sizes", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "offset_dims", "type": "VHLO_AnyAttr" },
      { "name": "collapsed_slice_dims", "type": "VHLO_AnyAttr" },
      { "name": "operand_batching_dims", "type": "VHLO_AnyAttr" },
      { "name": "start_indices_batching_dims", "type": "VHLO_AnyAttr" },
      { "name": "start_index_map", "type": "VHLO_AnyAttr" },
      { "name": "index_vector_dim", "type": "VHLO_AnyAttr" },
      { "name": "indices_are_sorted", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.dynamic_iota_v1",
    "inputs": [
      { "name": "output_shape", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "iota_dimension", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.dynamic_pad_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "padding_value", "type": "VHLO_AnyType" },
      { "name": "edge_padding_low", "type": "VHLO_AnyType" },
      { "name": "edge_padding_high", "type": "VHLO_AnyType" },
      { "name": "interior_padding", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.dynamic_reshape_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "output_shape", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.dynamic_slice_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "start_indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "slice_sizes", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.dynamic_update_slice_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "update", "type": "VHLO_AnyType" },
      { "name": "start_indices", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.einsum_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "einsum_config", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.exponential_minus_one_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.exponential_minus_one_v2",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.exponential_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.exponential_v2",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.fft_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "fft_type", "type": "VHLO_AnyAttr" },
      { "name": "fft_length", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.floor_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.func_v1",
    "attributes": [
      { "name": "sym_name", "type": "VHLO_AnyAttr" },
      { "name": "function_type", "type": "VHLO_AnyAttr" },
      { "name": "sym_visibility", "type": "VHLO_AnyAttr" },
      { "name": "arg_attrs", "type": "VHLO_AnyAttr" },
      { "name": "res_attrs", "type": "VHLO_AnyAttr" }
    ],
    "assemblyFormat": "custom<FunctionBody>($sym_name, $body, $function_type) attr-dict"
  },
  {
    "name": "vhlo.gather_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "start_indices", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "offset_dims", "type": "VHLO_AnyAttr" },
      { "name": "collapsed_slice_dims", "type": "VHLO_AnyAttr" },
      { "name": "start_index_map", "type": "VHLO_AnyAttr" },
      { "name": "index_vector_dim", "type": "VHLO_AnyAttr" },
      { "name": "slice_sizes", "type": "VHLO_AnyAttr" },
      { "name": "indices_are_sorted", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.gather_v2",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "start_indices", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "offset_dims", "type": "VHLO_AnyAttr" },
      { "name": "collapsed_slice_dims", "type": "VHLO_AnyAttr" },
      { "name": "operand_batching_dims", "type": "VHLO_AnyAttr" },
      { "name": "start_indices_batching_dims", "type": "VHLO_AnyAttr" },
      { "name": "start_index_map", "type": "VHLO_AnyAttr" },
      { "name": "index_vector_dim", "type": "VHLO_AnyAttr" },
      { "name": "slice_sizes", "type": "VHLO_AnyAttr" },
      { "name": "indices_are_sorted", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.get_dimension_size_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "dimension", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.get_tuple_element_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "index", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.if_v1",
    "inputs": [
      { "name": "pred", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "vhlo.imag_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.infeed_v1",
    "inputs": [
      { "name": "token", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "infeed_config", "type": "VHLO_AnyAttr" },
      { "name": "layout", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.iota_v1",
    "outputs": [
      { "name": "output", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "iota_dimension", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.is_finite_v1",
    "inputs": [
      { "name": "x", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "y", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.log_plus_one_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.log_plus_one_v2",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.log_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.log_v2",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.logistic_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.logistic_v2",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.map_v1",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "dimensions", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.maximum_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.minimum_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.multiply_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.negate_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.not_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.optimization_barrier_v1",
    "inputs": [
      { "name": "operand", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "Variadic" }
    ]
  },
  {
    "name": "vhlo.or_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.outfeed_v1",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "token", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "outfeed_config", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.pad_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "padding_value", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "edge_padding_low", "type": "VHLO_AnyAttr" },
      { "name": "edge_padding_high", "type": "VHLO_AnyAttr" },
      { "name": "interior_padding", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.partition_id_v1",
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.popcnt_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.power_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.real_dynamic_slice_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "start_indices", "type": "VHLO_AnyType" },
      { "name": "limit_indices", "type": "VHLO_AnyType" },
      { "name": "strides", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.real_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.recv_v1",
    "inputs": [
      { "name": "token", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "channel_id", "type": "VHLO_AnyAttr" },
      { "name": "channel_type", "type": "VHLO_AnyAttr" },
      { "name": "is_host_transfer", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.recv_v2",
    "inputs": [
      { "name": "token", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "channel_id", "type": "VHLO_AnyAttr" },
      { "name": "channel_type", "type": "VHLO_AnyAttr" },
      { "name": "is_host_transfer", "type": "VHLO_AnyAttr" },
      { "name": "source_target_pairs", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.reduce_precision_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "output", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "exponent_bits", "type": "VHLO_AnyAttr" },
      { "name": "mantissa_bits", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.reduce_scatter_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "scatter_dimension", "type": "VHLO_AnyAttr" },
      { "name": "replica_groups", "type": "VHLO_AnyAttr" },
      { "name": "channel_id", "type": "VHLO_AnyAttr" },
      { "name": "use_global_device_ids", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.reduce_v1",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "init_values", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimensions", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.reduce_window_v1",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "init_values", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "window_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "window_strides", "type": "VHLO_AnyAttr" },
      { "name": "base_dilations", "type": "VHLO_AnyAttr" },
      { "name": "window_dilations", "type": "VHLO_AnyAttr" },
      { "name": "padding", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.remainder_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.replica_id_v1",
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.reshape_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.return_v1",
    "inputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "vhlo.reverse_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "dimensions", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.rng_bit_generator_v1",
    "inputs": [
      { "name": "initial_state", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "output_state", "type": "VHLO_AnyType" },
      { "name": "output", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "rng_algorithm", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.rng_v1",
    "inputs": [
      { "name": "a", "type": "VHLO_AnyType" },
      { "name": "b", "type": "VHLO_AnyType" },
      { "name": "shape", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "rng_distribution", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.round_nearest_afz_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.round_nearest_even_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.rsqrt_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.rsqrt_v2",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.scatter_v1",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "scatter_indices", "type": "VHLO_AnyType" },
      { "name": "updates", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "update_window_dims", "type": "VHLO_AnyAttr" },
      { "name": "inserted_window_dims", "type": "VHLO_AnyAttr" },
      { "name": "scatter_dims_to_operand_dims", "type": "VHLO_AnyAttr" },
      { "name": "index_vector_dim", "type": "VHLO_AnyAttr" },
      { "name": "indices_are_sorted", "type": "VHLO_AnyAttr" },
      { "name": "unique_indices", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.scatter_v2",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "scatter_indices", "type": "VHLO_AnyType" },
      { "name": "updates", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "update_window_dims", "type": "VHLO_AnyAttr" },
      { "name": "inserted_window_dims", "type": "VHLO_AnyAttr" },
      { "name": "input_batching_dims", "type": "VHLO_AnyAttr" },
      { "name": "scatter_indices_batching_dims", "type": "VHLO_AnyAttr" },
      { "name": "scatter_dims_to_operand_dims", "type": "VHLO_AnyAttr" },
      { "name": "index_vector_dim", "type": "VHLO_AnyAttr" },
      { "name": "indices_are_sorted", "type": "VHLO_AnyAttr" },
      { "name": "unique_indices", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.select_and_scatter_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "source", "type": "VHLO_AnyType" },
      { "name": "init_value", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "window_dimensions", "type": "VHLO_AnyAttr" },
      { "name": "window_strides", "type": "VHLO_AnyAttr" },
      { "name": "padding", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.select_v1",
    "inputs": [
      { "name": "pred", "type": "VHLO_AnyType" },
      { "name": "on_true", "type": "VHLO_AnyType" },
      { "name": "on_false", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.send_v1",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "token", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "channel_id", "type": "VHLO_AnyAttr" },
      { "name": "channel_type", "type": "VHLO_AnyAttr" },
      { "name": "is_host_transfer", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.send_v2",
    "inputs": [
      { "name": "inputs", "type": "Variadic" },
      { "name": "token", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "channel_id", "type": "VHLO_AnyAttr" },
      { "name": "channel_type", "type": "VHLO_AnyAttr" },
      { "name": "is_host_transfer", "type": "VHLO_AnyAttr" },
      { "name": "source_target_pairs", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.set_dimension_size_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "size", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "dimension", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.shift_left_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.shift_right_arithmetic_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.shift_right_logical_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.sign_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.sine_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.sine_v2",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.slice_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "start_indices", "type": "VHLO_AnyAttr" },
      { "name": "limit_indices", "type": "VHLO_AnyAttr" },
      { "name": "strides", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.sort_v1",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "dimension", "type": "VHLO_AnyAttr" },
      { "name": "is_stable", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.sqrt_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.sqrt_v2",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.subtract_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.tan_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.tan_v2",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.tanh_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.tanh_v2",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "result_accuracy", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.torch_index_select_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" },
      { "name": "index", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "dim", "type": "VHLO_AnyAttr" },
      { "name": "batch_dims", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.transpose_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "permutation", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.triangular_solve_v1",
    "inputs": [
      { "name": "a", "type": "VHLO_AnyType" },
      { "name": "b", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "left_side", "type": "VHLO_AnyAttr" },
      { "name": "lower", "type": "VHLO_AnyAttr" },
      { "name": "unit_diagonal", "type": "VHLO_AnyAttr" },
      { "name": "transpose_a", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.tuple_v1",
    "inputs": [
      { "name": "val", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.unary_einsum_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ],
    "attributes": [
      { "name": "einsum_config", "type": "VHLO_AnyAttr" }
    ]
  },
  {
    "name": "vhlo.uniform_dequantize_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.uniform_quantize_v1",
    "inputs": [
      { "name": "operand", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vhlo.while_v1",
    "inputs": [
      { "name": "operand", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ]
  },
  {
    "name": "vhlo.xor_v1",
    "inputs": [
      { "name": "lhs", "type": "VHLO_AnyType" },
      { "name": "rhs", "type": "VHLO_AnyType" }
    ],
    "outputs": [
      { "name": "result", "type": "VHLO_AnyType" }
    ]
  },
  {
    "name": "vm.abs.f32",
    "summary": "Floating point absolute-value operation.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.abs.f64",
    "summary": "Floating point absolute-value operation.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.abs.i32",
    "summary": "Integer absolute-value operation.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.abs.i64",
    "summary": "Integer absolute-value operation.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.add.f32",
    "summary": "Floating-point add operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.add.f64",
    "summary": "Floating-point add operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.add.i32",
    "summary": "Integer add operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.add.i64",
    "summary": "Integer add operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.and.i32",
    "summary": "Integer binary and operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.and.i64",
    "summary": "Integer binary and operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.atan.f32",
    "summary": "Arcus tangent of the given value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.atan.f64",
    "summary": "Arcus tangent of the given value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.atan2.f32",
    "summary": "2-argument arcus tangent of the given values.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.atan2.f64",
    "summary": "2-argument arcus tangent of the given values.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.bitcast.f32.i32",
    "summary": "Bitcast from a 32-bit float-point value to a 32-bit integer.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.bitcast.f64.i64",
    "summary": "Bitcast from a 64-bit float-point value to a 64-bit integer.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.bitcast.i32.f32",
    "summary": "Bitcast from a 32-bit integer to a 32-bit float-point value.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.bitcast.i64.f64",
    "summary": "Bitcast from a 64-bit integer to a 64-bit float-point value.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.br",
    "summary": "Unconditional branch operation.",
    "description": "Represents an unconditional branch operation that branches to a target block\n    with the given set of arguments.\n\n    ```\n    ^bb0(...):\n      vm.br ^bb1(%a)\n    ^bb1(%blockArg1):\n      ...\n   ```",
    "inputs": [
      { "name": "destOperands", "type": "Variadic" }
    ],
    "successors": [
      {
        "name": "dest"
      }
    ],
    "assemblyFormat": "$dest (`(` $destOperands^ `:` type($destOperands) `)`)? attr-dict"
  },
  {
    "name": "vm.br_table",
    "summary": "Branch table operation.",
    "description": "Represents a branch table instructing execution to branch to the block with\n    the specified index. If the index is out of bounds then execution will\n    branch to the default block.\n\n    ```\n    vm.br_table %index {\n      default: ^bb1(%a : i64),\n      0: ^bb2,\n      1: ^bb3(%c : i64)\n    }\n   ```",
    "inputs": [
      { "name": "index", "type": "I32" },
      { "name": "defaultOperands", "type": "Variadic" },
      { "name": "caseOperands", "type": "VariadicOfVariadic" }
    ],
    "attributes": [
      { "name": "case_operand_segments", "type": "DenseI32ArrayAttr" }
    ],
    "successors": [
      {
        "name": "defaultDestination"
      },
      {
        "name": "caseDestinations"
      }
    ],
    "assemblyFormat": "$index ` ` `{` `\\n`\n    custom<BranchTableCases>(\n        $defaultDestination, $defaultOperands, type($defaultOperands),\n        $caseDestinations, $caseOperands, type($caseOperands))\n    `}`\n    attr-dict"
  },
  {
    "name": "vm.break",
    "summary": "Unconditional debug break operation.",
    "description": "Breaks into the attached debugger or asks for attaching a debugger. After\n    resuming (or if a debugger is not attached) execution will continue at the\n    target block.",
    "inputs": [
      { "name": "destOperands", "type": "Variadic" }
    ],
    "successors": [
      {
        "name": "dest"
      }
    ],
    "assemblyFormat": "$dest (`(` $destOperands^ `:` type($destOperands) `)`)? attr-dict"
  },
  {
    "name": "vm.buffer.alloc",
    "summary": "Allocates a new zero-initialized buffer.",
    "description": "Allocates a new zero-initialized buffer with the given size in bytes.",
    "inputs": [
      { "name": "length", "type": "VM_BufferIndex" },
      { "name": "alignment", "type": "I32" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_RefOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.buffer.clone",
    "summary": "Clones a buffer.",
    "description": "Clones a range of the source buffer to produce a mutable buffer with the\n    same contents.",
    "inputs": [
      { "name": "source_buffer", "type": "VM_RefOf" },
      { "name": "source_offset", "type": "VM_BufferIndex" },
      { "name": "length", "type": "VM_BufferIndex" },
      { "name": "alignment", "type": "I32" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_RefOf" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($source_buffer) `->` type($result)"
  },
  {
    "name": "vm.buffer.compare",
    "summary": "Compares a range of a buffer to another.",
    "description": "Returns 1 if the two ranges are bitwise equivalent, somewhat like memcmp.",
    "inputs": [
      { "name": "lhs_buffer", "type": "VM_RefOf" },
      { "name": "lhs_offset", "type": "VM_BufferIndex" },
      { "name": "rhs_buffer", "type": "VM_RefOf" },
      { "name": "rhs_offset", "type": "VM_BufferIndex" },
      { "name": "length", "type": "VM_BufferIndex" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs_buffer) `,` type($rhs_buffer)"
  },
  {
    "name": "vm.buffer.copy",
    "summary": "Copies a range of a buffer to another.",
    "description": "Copies a range of one buffer to another, like memcpy.",
    "inputs": [
      { "name": "source_buffer", "type": "VM_RefOf" },
      { "name": "source_offset", "type": "VM_BufferIndex" },
      { "name": "target_buffer", "type": "VM_RefOf" },
      { "name": "target_offset", "type": "VM_BufferIndex" },
      { "name": "length", "type": "VM_BufferIndex" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($source_buffer) `->` type($target_buffer)"
  },
  {
    "name": "vm.buffer.fill.f32",
    "summary": "Fills the buffer with the given repeating 32-bit value.",
    "description": "Fills an element range of the buffer with the given value, like memset.",
    "inputs": [
      { "name": "target_buffer", "type": "VM_RefOf" },
      { "name": "target_offset", "type": "VM_BufferIndex" },
      { "name": "length", "type": "VM_BufferIndex" },
      { "name": "value", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$target_buffer `,` $target_offset `,` $length `,` $value\n    attr-dict `:` type($value) `->` type($target_buffer)"
  },
  {
    "name": "vm.buffer.fill.f64",
    "summary": "Fills the buffer with the given repeating 64-bit value.",
    "description": "Fills an element range of the buffer with the given value, like memset.",
    "inputs": [
      { "name": "target_buffer", "type": "VM_RefOf" },
      { "name": "target_offset", "type": "VM_BufferIndex" },
      { "name": "length", "type": "VM_BufferIndex" },
      { "name": "value", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$target_buffer `,` $target_offset `,` $length `,` $value\n    attr-dict `:` type($value) `->` type($target_buffer)"
  },
  {
    "name": "vm.buffer.fill.i16",
    "summary": "Fills the buffer with the given repeating 16-bit value.",
    "description": "Fills an element range of the buffer with the given value, like memset.",
    "inputs": [
      { "name": "target_buffer", "type": "VM_RefOf" },
      { "name": "target_offset", "type": "VM_BufferIndex" },
      { "name": "length", "type": "VM_BufferIndex" },
      { "name": "value", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$target_buffer `,` $target_offset `,` $length `,` $value\n    attr-dict `:` type($value) `->` type($target_buffer)"
  },
  {
    "name": "vm.buffer.fill.i32",
    "summary": "Fills the buffer with the given repeating 32-bit value.",
    "description": "Fills an element range of the buffer with the given value, like memset.",
    "inputs": [
      { "name": "target_buffer", "type": "VM_RefOf" },
      { "name": "target_offset", "type": "VM_BufferIndex" },
      { "name": "length", "type": "VM_BufferIndex" },
      { "name": "value", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$target_buffer `,` $target_offset `,` $length `,` $value\n    attr-dict `:` type($value) `->` type($target_buffer)"
  },
  {
    "name": "vm.buffer.fill.i64",
    "summary": "Fills the buffer with the given repeating 64-bit value.",
    "description": "Fills an element range of the buffer with the given value, like memset.",
    "inputs": [
      { "name": "target_buffer", "type": "VM_RefOf" },
      { "name": "target_offset", "type": "VM_BufferIndex" },
      { "name": "length", "type": "VM_BufferIndex" },
      { "name": "value", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$target_buffer `,` $target_offset `,` $length `,` $value\n    attr-dict `:` type($value) `->` type($target_buffer)"
  },
  {
    "name": "vm.buffer.fill.i8",
    "summary": "Fills the buffer with the given repeating 8-bit value.",
    "description": "Fills an element range of the buffer with the given value, like memset.",
    "inputs": [
      { "name": "target_buffer", "type": "VM_RefOf" },
      { "name": "target_offset", "type": "VM_BufferIndex" },
      { "name": "length", "type": "VM_BufferIndex" },
      { "name": "value", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$target_buffer `,` $target_offset `,` $length `,` $value\n    attr-dict `:` type($value) `->` type($target_buffer)"
  },
  {
    "name": "vm.buffer.hash",
    "description": "Computes the SipHash-2-4 of the source buffer at the given offset for\n    |length| bytes using seed `0x0001020304...0e0f`.",
    "inputs": [
      { "name": "source_buffer", "type": "VM_RefOf" },
      { "name": "source_offset", "type": "VM_BufferIndex" },
      { "name": "length", "type": "VM_BufferIndex" }
    ],
    "outputs": [
      { "name": "result", "type": "I64" }
    ],
    "assemblyFormat": "$source_buffer `,` $source_offset `,` $length\n    attr-dict `:` type($source_buffer) `->` type($result)"
  },
  {
    "name": "vm.buffer.length",
    "summary": "Returns the byte length of a buffer.",
    "description": "Returns the total byte length of the given buffer. This is the exact value\n    as specified during buffer allocation though the underlying system buffer\n    may have additional padding.",
    "inputs": [
      { "name": "buffer", "type": "VM_RefOf" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_BufferIndex" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($buffer) `->` type($result)"
  },
  {
    "name": "vm.buffer.load.f32",
    "summary": "32-bit floating-point load.",
    "description": "Loads a value from the buffer at the given element offset.",
    "inputs": [
      { "name": "source_buffer", "type": "VM_RefOf" },
      { "name": "source_offset", "type": "VM_BufferIndex" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$source_buffer `[` $source_offset `]`\n    attr-dict `:` type($source_buffer) `->` type($result)"
  },
  {
    "name": "vm.buffer.load.f64",
    "summary": "64-bit floating-point load.",
    "description": "Loads a value from the buffer at the given element offset.",
    "inputs": [
      { "name": "source_buffer", "type": "VM_RefOf" },
      { "name": "source_offset", "type": "VM_BufferIndex" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$source_buffer `[` $source_offset `]`\n    attr-dict `:` type($source_buffer) `->` type($result)"
  },
  {
    "name": "vm.buffer.load.i16.s",
    "summary": "Signed 16-bit integer load.",
    "description": "Loads a value from the buffer at the given element offset.",
    "inputs": [
      { "name": "source_buffer", "type": "VM_RefOf" },
      { "name": "source_offset", "type": "VM_BufferIndex" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$source_buffer `[` $source_offset `]`\n    attr-dict `:` type($source_buffer) `->` type($result)"
  },
  {
    "name": "vm.buffer.load.i16.u",
    "summary": "Unsigned 16-bit integer load.",
    "description": "Loads a value from the buffer at the given element offset.",
    "inputs": [
      { "name": "source_buffer", "type": "VM_RefOf" },
      { "name": "source_offset", "type": "VM_BufferIndex" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$source_buffer `[` $source_offset `]`\n    attr-dict `:` type($source_buffer) `->` type($result)"
  },
  {
    "name": "vm.buffer.load.i32",
    "summary": "32-bit integer load.",
    "description": "Loads a value from the buffer at the given element offset.",
    "inputs": [
      { "name": "source_buffer", "type": "VM_RefOf" },
      { "name": "source_offset", "type": "VM_BufferIndex" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$source_buffer `[` $source_offset `]`\n    attr-dict `:` type($source_buffer) `->` type($result)"
  },
  {
    "name": "vm.buffer.load.i64",
    "summary": "64-bit integer load.",
    "description": "Loads a value from the buffer at the given element offset.",
    "inputs": [
      { "name": "source_buffer", "type": "VM_RefOf" },
      { "name": "source_offset", "type": "VM_BufferIndex" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$source_buffer `[` $source_offset `]`\n    attr-dict `:` type($source_buffer) `->` type($result)"
  },
  {
    "name": "vm.buffer.load.i8.s",
    "summary": "Signed 8-bit integer load.",
    "description": "Loads a value from the buffer at the given element offset.",
    "inputs": [
      { "name": "source_buffer", "type": "VM_RefOf" },
      { "name": "source_offset", "type": "VM_BufferIndex" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$source_buffer `[` $source_offset `]`\n    attr-dict `:` type($source_buffer) `->` type($result)"
  },
  {
    "name": "vm.buffer.load.i8.u",
    "summary": "Unsigned 8-bit integer load.",
    "description": "Loads a value from the buffer at the given element offset.",
    "inputs": [
      { "name": "source_buffer", "type": "VM_RefOf" },
      { "name": "source_offset", "type": "VM_BufferIndex" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$source_buffer `[` $source_offset `]`\n    attr-dict `:` type($source_buffer) `->` type($result)"
  },
  {
    "name": "vm.buffer.store.f32",
    "summary": "32-bit floating-point store.",
    "description": "Stores a value to the buffer at the given element offset.",
    "inputs": [
      { "name": "target_buffer", "type": "VM_RefOf" },
      { "name": "target_offset", "type": "VM_BufferIndex" },
      { "name": "value", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$value `,` $target_buffer `[` $target_offset `]`\n    attr-dict `:` type($value) `->` type($target_buffer)"
  },
  {
    "name": "vm.buffer.store.f64",
    "summary": "64-bit floating-point store.",
    "description": "Stores a value to the buffer at the given element offset.",
    "inputs": [
      { "name": "target_buffer", "type": "VM_RefOf" },
      { "name": "target_offset", "type": "VM_BufferIndex" },
      { "name": "value", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$value `,` $target_buffer `[` $target_offset `]`\n    attr-dict `:` type($value) `->` type($target_buffer)"
  },
  {
    "name": "vm.buffer.store.i16",
    "summary": "Unsigned 16-bit integer store.",
    "description": "Stores a value to the buffer at the given element offset.",
    "inputs": [
      { "name": "target_buffer", "type": "VM_RefOf" },
      { "name": "target_offset", "type": "VM_BufferIndex" },
      { "name": "value", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$value `,` $target_buffer `[` $target_offset `]`\n    attr-dict `:` type($value) `->` type($target_buffer)"
  },
  {
    "name": "vm.buffer.store.i32",
    "summary": "32-bit integer store.",
    "description": "Stores a value to the buffer at the given element offset.",
    "inputs": [
      { "name": "target_buffer", "type": "VM_RefOf" },
      { "name": "target_offset", "type": "VM_BufferIndex" },
      { "name": "value", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$value `,` $target_buffer `[` $target_offset `]`\n    attr-dict `:` type($value) `->` type($target_buffer)"
  },
  {
    "name": "vm.buffer.store.i64",
    "summary": "64-bit integer store.",
    "description": "Stores a value to the buffer at the given element offset.",
    "inputs": [
      { "name": "target_buffer", "type": "VM_RefOf" },
      { "name": "target_offset", "type": "VM_BufferIndex" },
      { "name": "value", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$value `,` $target_buffer `[` $target_offset `]`\n    attr-dict `:` type($value) `->` type($target_buffer)"
  },
  {
    "name": "vm.buffer.store.i8",
    "summary": "Unsigned 8-bit integer store.",
    "description": "Stores a value to the buffer at the given element offset.",
    "inputs": [
      { "name": "target_buffer", "type": "VM_RefOf" },
      { "name": "target_offset", "type": "VM_BufferIndex" },
      { "name": "value", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$value `,` $target_buffer `[` $target_offset `]`\n    attr-dict `:` type($value) `->` type($target_buffer)"
  },
  {
    "name": "vm.call",
    "summary": "Call operation.",
    "description": "Calls an internal VM function with the given arguments.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "callee", "type": "VM_FuncRefAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$callee `(` operands `)` attr-dict `:` functional-type(operands, results)"
  },
  {
    "name": "vm.call.variadic",
    "summary": "Call operation with variadic arguments.",
    "description": "Calls an internal VM function with the given arguments. One or more of the\n    arguments may be variadic, encoded as segmented sized operand lists.\n\n    Variadic arguments must be specified with a total count in the segment_sizes\n    attribute.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "callee", "type": "VM_FuncRefAttr" },
      { "name": "segment_sizes", "type": "SignlessIntElementsAttr" },
      { "name": "segment_types", "type": "TypeArrayAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "vm.cast.any.ref",
    "summary": "Casts from any ref to a specific ref type.",
    "description": "Performs a runtime cast of an opaque `!vm.ref<?>` to a specific `!vm.ref<T>`\n    and raises an error if the operand does not match the expected type.\n    Null refs can always be cast between types.",
    "inputs": [
      { "name": "operand", "type": "VM_AnyRef" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_AnyRef" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.cast.f32.si32",
    "summary": "Cast from a float-point value to a signed 32-bit integer.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.cast.f32.si64",
    "summary": "Cast from a float-point value to a signed 64-bit integer.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.cast.f32.ui32",
    "summary": "Cast from an float-point value to an unsigned 32-bit integer.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.cast.f32.ui64",
    "summary": "Cast from an float-point value to an unsigned 64-bit integer.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.cast.f64.si64",
    "summary": "Cast from a float-point value to a signed integer.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.cast.f64.ui64",
    "summary": "Cast from an float-point value to an unsigned integer.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.cast.ref.any",
    "summary": "Casts from a specific ref to any ref type.",
    "description": "Performs a compile-time widening cast of a specific `!vm.ref<T>` to an\n    opaque `!vm.ref<?>`.",
    "inputs": [
      { "name": "operand", "type": "VM_AnyRef" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_AnyRef" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.cast.si32.f32",
    "summary": "Cast from a signed integer to a float-point value.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.cast.si64.f32",
    "summary": "Cast from a signed integer to a float-point value.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.cast.si64.f64",
    "summary": "Cast from a signed integer to a float-point value.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.cast.ui32.f32",
    "summary": "Cast from an unsigned integer to a float-point value.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.cast.ui64.f32",
    "summary": "Cast from an unsigned integer to a float-point value.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.cast.ui64.f64",
    "summary": "Cast from an unsigned integer to a float-point value.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.ceil.f32",
    "summary": "Floating point ceiling operation.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.ceil.f64",
    "summary": "Floating point ceiling operation.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.check.eq",
    "summary": "Raises a global failure if the condition is true.",
    "description": "When the condition is true this signals a runtime failure that causes the\n    entire active invocation - and possibly *all* in-flight and pending\n    invocations - to fail. The status will be propagated back via the available\n    runtime error handling mechanisms such as semaphores or synchronous\n    invocation results.\n\n    This is implemented as a pseudo-op that transforms into a vm.cond_fail\n    operation.\n\n    ```\n    vm.check.eq %a, %b, \"a == b\" : i32\n    vm.check.nz %ref, \"!null\" : !vm.ref<?>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "VM_AnyType" },
      { "name": "rhs", "type": "VM_AnyType" }
    ],
    "attributes": [
      { "name": "message", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs (`,` $message^)? attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.check.ne",
    "summary": "Raises a global failure if the condition is true.",
    "description": "When the condition is true this signals a runtime failure that causes the\n    entire active invocation - and possibly *all* in-flight and pending\n    invocations - to fail. The status will be propagated back via the available\n    runtime error handling mechanisms such as semaphores or synchronous\n    invocation results.\n\n    This is implemented as a pseudo-op that transforms into a vm.cond_fail\n    operation.\n\n    ```\n    vm.check.eq %a, %b, \"a == b\" : i32\n    vm.check.nz %ref, \"!null\" : !vm.ref<?>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "VM_AnyType" },
      { "name": "rhs", "type": "VM_AnyType" }
    ],
    "attributes": [
      { "name": "message", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs (`,` $message^)? attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.check.nearly_eq",
    "summary": "Raises a global failure if the condition is true.",
    "description": "When the condition is true this signals a runtime failure that causes the\n    entire active invocation - and possibly *all* in-flight and pending\n    invocations - to fail. The status will be propagated back via the available\n    runtime error handling mechanisms such as semaphores or synchronous\n    invocation results.\n\n    This is implemented as a pseudo-op that transforms into a vm.cond_fail\n    operation.\n\n    ```\n    vm.check.eq %a, %b, \"a == b\" : i32\n    vm.check.nz %ref, \"!null\" : !vm.ref<?>\n    ```",
    "inputs": [
      { "name": "lhs", "type": "VM_AnyType" },
      { "name": "rhs", "type": "VM_AnyType" }
    ],
    "attributes": [
      { "name": "message", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$lhs `,` $rhs (`,` $message^)? attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.check.nz",
    "summary": "Raises a global failure if the condition is true.",
    "description": "When the condition is true this signals a runtime failure that causes the\n    entire active invocation - and possibly *all* in-flight and pending\n    invocations - to fail. The status will be propagated back via the available\n    runtime error handling mechanisms such as semaphores or synchronous\n    invocation results.\n\n    This is implemented as a pseudo-op that transforms into a vm.cond_fail\n    operation.\n\n    ```\n    vm.check.eq %a, %b, \"a == b\" : i32\n    vm.check.nz %ref, \"!null\" : !vm.ref<?>\n    ```",
    "inputs": [
      { "name": "value", "type": "VM_AnyType" }
    ],
    "attributes": [
      { "name": "message", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$value (`,` $message^)? attr-dict `:` type($value)"
  },
  {
    "name": "vm.cmp.eq.f32.near",
    "summary": "Near floating-point equality comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.eq.f32.o",
    "summary": "Ordered floating-point equality comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.eq.f32.u",
    "summary": "Unordered floating-point equality comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.eq.f64.near",
    "summary": "Near floating-point equality comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.eq.f64.o",
    "summary": "Ordered floating-point equality comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.eq.f64.u",
    "summary": "Unordered floating-point equality comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.eq.i32",
    "summary": "Integer equality comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.eq.i64",
    "summary": "Integer equality comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.eq.ref",
    "summary": "ref<T> equality comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.gt.f32.o",
    "summary": "Ordered floating-point greater-than comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.gt.f32.u",
    "summary": "Unordered floating-point greater-than comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.gt.f64.o",
    "summary": "Ordered floating-point greater-than comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.gt.f64.u",
    "summary": "Unordered floating-point greater-than comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.gt.i32.s",
    "summary": "Signed integer greater-than comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.gt.i32.u",
    "summary": "Unsigned integer greater-than comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.gt.i64.s",
    "summary": "Signed integer greater-than comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.gt.i64.u",
    "summary": "Unsigned integer greater-than comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.gte.f32.o",
    "summary": "Ordered floating-point greater-than-or-equal comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.gte.f32.u",
    "summary": "Unordered floating-point greater-than-or-equal comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.gte.f64.o",
    "summary": "Ordered floating-point greater-than-or-equal comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.gte.f64.u",
    "summary": "Unordered floating-point greater-than-or-equal comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.gte.i32.s",
    "summary": "Signed integer greater-than-or-equal comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.gte.i32.u",
    "summary": "Unsigned integer greater-than-or-equal comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.gte.i64.s",
    "summary": "Signed integer greater-than-or-equal comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.gte.i64.u",
    "summary": "Unsigned integer greater-than-or-equal comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.lt.f32.o",
    "summary": "Ordered floating-point less-than comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.lt.f32.u",
    "summary": "Unordered floating-point less-than comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.lt.f64.o",
    "summary": "Ordered floating-point less-than comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.lt.f64.u",
    "summary": "Unordered floating-point less-than comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.lt.i32.s",
    "summary": "Signed integer less-than comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.lt.i32.u",
    "summary": "Unsigned integer less-than comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.lt.i64.s",
    "summary": "Signed integer less-than comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.lt.i64.u",
    "summary": "Unsigned integer less-than comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.lte.f32.o",
    "summary": "Ordered floating-point less-than-or-equal comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.lte.f32.u",
    "summary": "Unordered floating-point less-than-or-equal comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.lte.f64.o",
    "summary": "Ordered floating-point less-than-or-equal comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.lte.f64.u",
    "summary": "Unordered floating-point less-than-or-equal comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.lte.i32.s",
    "summary": "Signed integer less-than-or-equal comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.lte.i32.u",
    "summary": "Unsigned integer less-than-or-equal comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.lte.i64.s",
    "summary": "Signed integer less-than-or-equal comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.lte.i64.u",
    "summary": "Unsigned integer less-than-or-equal comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.nan.f32",
    "summary": "Floating-point NaN comparison operation.",
    "description": "Returns 1 if the value is NaN.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand)"
  },
  {
    "name": "vm.cmp.nan.f64",
    "summary": "Floating-point NaN comparison operation.",
    "description": "Returns 1 if the value is NaN.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand)"
  },
  {
    "name": "vm.cmp.ne.f32.o",
    "summary": "Ordered floating-point inequality comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.ne.f32.u",
    "summary": "Unordered floating-point inequality comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.ne.f64.o",
    "summary": "Ordered floating-point inequality comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.ne.f64.u",
    "summary": "Unordered floating-point inequality comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.ne.i32",
    "summary": "Integer inequality comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.ne.i64",
    "summary": "Integer inequality comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.ne.ref",
    "summary": "ref<T> inequality comparison operation.",
    "description": "Compares two operands with the specified predicate.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($lhs)"
  },
  {
    "name": "vm.cmp.nz.f32.o",
    "summary": "Ordered floating-point non-zero comparison operation.",
    "description": "Compares the given floating-point operand for a non-zero value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($operand)"
  },
  {
    "name": "vm.cmp.nz.f32.u",
    "summary": "Unordered floating-point non-zero comparison operation.",
    "description": "Compares the given floating-point operand for a non-zero value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($operand)"
  },
  {
    "name": "vm.cmp.nz.f64.o",
    "summary": "Ordered floating-point non-zero comparison operation.",
    "description": "Compares the given floating-point operand for a non-zero value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($operand)"
  },
  {
    "name": "vm.cmp.nz.f64.u",
    "summary": "Unordered floating-point non-zero comparison operation.",
    "description": "Compares the given floating-point operand for a non-zero value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($operand)"
  },
  {
    "name": "vm.cmp.nz.i32",
    "summary": "Integer non-zero comparison operation.",
    "description": "Compares the given integer operand for a non-zero value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand)"
  },
  {
    "name": "vm.cmp.nz.i64",
    "summary": "Integer non-zero comparison operation.",
    "description": "Compares the given integer operand for a non-zero value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand)"
  },
  {
    "name": "vm.cmp.nz.ref",
    "summary": "ref<T> non-zero comparison operation.",
    "description": "Compares the given ref operand for a non-zero/null value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand)"
  },
  {
    "name": "vm.cond_br",
    "summary": "Conditional branch operation.",
    "description": "Represents a conditional branch operation that branches to one of the two\n    target blocks with the given set of arguments.\n\n    ```\n    ^bb0(...):\n      vm.cond_br %condition, ^bb1(%a), ^bb2(%b)\n    ^bb1(%blockArg1):\n      ...\n    ^bb2(%blockArg2):\n      ...\n   ```",
    "inputs": [
      { "name": "condition", "type": "VM_CondValue" },
      { "name": "trueDestOperands", "type": "Variadic" },
      { "name": "falseDestOperands", "type": "Variadic" }
    ],
    "successors": [
      {
        "name": "trueDest"
      },
      {
        "name": "falseDest"
      }
    ],
    "assemblyFormat": "$condition `,`\n    $trueDest (`(` $trueDestOperands^ `:` type($trueDestOperands) `)`)? `,`\n    $falseDest (`(` $falseDestOperands^ `:` type($falseDestOperands) `)`)?\n    attr-dict"
  },
  {
    "name": "vm.cond_break",
    "summary": "Conditional debug break operation.",
    "description": "Breaks into the attached debugger or asks for attaching a debugger if the\n    provided condition is true. After resuming (or if a debugger is not\n    attached) execution will continue at the target block.",
    "inputs": [
      { "name": "condition", "type": "VM_CondValue" },
      { "name": "destOperands", "type": "Variadic" }
    ],
    "successors": [
      {
        "name": "dest"
      }
    ],
    "assemblyFormat": "$condition `,` $dest (`(` $destOperands^ `:` type($destOperands) `)`)?\n    attr-dict"
  },
  {
    "name": "vm.cond_fail",
    "summary": "Raises a global failure if the condition is true.",
    "description": "When the condition is true this signals a runtime failure that causes the\n    entire active invocation - and possibly *all* in-flight and pending\n    invocations - to fail with the given status. The status will be propagated\n    back via the available runtime error handling mechanisms such as semaphores\n    or synchronous invocation results.\n\n    As the IREE execution model is deeply pipelined it's possible that failures\n    have a latency between when they are emitted and when the application can\n    observe the failure. It's also possible that other work that is in-flight\n    or pending when the failure occurs will complete.\n\n    This is implemented as a pseudo-op that transforms into a vm.fail operation\n    guarded by the condition.\n\n    ```\n    %nz = vm.cmp.nz.i32 %value : i32\n    %statusCode = vm.const.i32 9\n    vm.cond_fail %nz, %statusCode, \"expected non-zero\"\n    ```",
    "inputs": [
      { "name": "condition", "type": "VM_CondValue" },
      { "name": "status", "type": "Util_Status" }
    ],
    "attributes": [
      { "name": "message", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "vm.const.f32",
    "summary": "32-bit floating-point constant operation.",
    "description": "Defines a constant value that is treated as a scalar literal at runtime.",
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "attributes": [
      { "name": "value", "type": "VM_ConstantFloatValueAttr" }
    ],
    "assemblyFormat": "$value attr-dict"
  },
  {
    "name": "vm.const.f32.zero",
    "summary": "32-bit floating-point constant zero operation.",
    "description": "Defines a constant zero primitive.",
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "vm.const.f64",
    "summary": "64-bit floating-point constant operation.",
    "description": "Defines a constant value that is treated as a scalar literal at runtime.",
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "attributes": [
      { "name": "value", "type": "VM_ConstantFloatValueAttr" }
    ],
    "assemblyFormat": "$value attr-dict"
  },
  {
    "name": "vm.const.f64.zero",
    "summary": "64-bit floating-point constant zero operation.",
    "description": "Defines a constant zero primitive.",
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "vm.const.i32",
    "summary": "32-bit integer constant operation.",
    "description": "Defines a constant value that is treated as a scalar literal at runtime.",
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "attributes": [
      { "name": "value", "type": "VM_ConstantIntegerValueAttr" }
    ],
    "assemblyFormat": "$value attr-dict"
  },
  {
    "name": "vm.const.i32.zero",
    "summary": "32-bit integer constant zero operation.",
    "description": "Defines a constant zero primitive.",
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "vm.const.i64",
    "summary": "64-bit integer constant operation.",
    "description": "Defines a constant value that is treated as a scalar literal at runtime.",
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "attributes": [
      { "name": "value", "type": "VM_ConstantIntegerValueAttr" }
    ],
    "assemblyFormat": "$value attr-dict"
  },
  {
    "name": "vm.const.i64.zero",
    "summary": "64-bit integer constant zero operation.",
    "description": "Defines a constant zero primitive.",
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "vm.const.ref.rodata",
    "summary": "Constant rodata access operation.",
    "description": "Returns a reference to a read-only buffer.",
    "outputs": [
      { "name": "value", "type": "VM_RefOf" }
    ],
    "attributes": [
      { "name": "rodata", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$rodata attr-dict `:` type($value)"
  },
  {
    "name": "vm.const.ref.zero",
    "summary": "null ref constant operation.",
    "description": "Defines a constant null ref that can be used in comparisons and\n    initialization.",
    "outputs": [
      { "name": "result", "type": "VM_AnyRef" }
    ],
    "assemblyFormat": "`:` type($result) attr-dict"
  },
  {
    "name": "vm.cos.f32",
    "summary": "Cosine of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.cos.f64",
    "summary": "Cosine of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.ctlz.i32",
    "summary": "Counts the leading zeros in an integer value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.ctlz.i64",
    "summary": "Counts the leading zeros in an integer value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.div.f32",
    "summary": "Floating point division operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.div.f64",
    "summary": "Floating point division operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.div.i32.s",
    "summary": "Signed integer division operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.div.i32.u",
    "summary": "Unsigned integer division operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.div.i64.s",
    "summary": "Signed integer division operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.div.i64.u",
    "summary": "Unsigned integer division operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.erf.f32",
    "summary": "Computes the error function of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.erf.f64",
    "summary": "Computes the error function of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.exp.f32",
    "summary": "Base-e exponential of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.exp.f64",
    "summary": "Base-e exponential of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.exp2.f32",
    "summary": "Base-2 exponential of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.exp2.f64",
    "summary": "Base-2 exponential of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.expm1.f32",
    "summary": "Base-e exponential of the specified value minus 1.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.expm1.f64",
    "summary": "Base-e exponential of the specified value minus 1.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.export",
    "summary": "Exports a function from the module.",
    "description": "Specifies an exported function with an externally-visible alias. Multiple\n    exports can reference the same internal functions.",
    "attributes": [
      { "name": "function_ref", "type": "FlatSymbolRefAttr" },
      { "name": "export_name", "type": "StrAttr" },
      { "name": "ordinal", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "vm.ext.f32.f64",
    "summary": "Floating-point zero extend 32 bits to 64 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.ext.i16.i32.s",
    "summary": "Integer sign extend 16 bits to 32 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.ext.i16.i32.u",
    "summary": "Integer zero extend 16 bits to 32 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.ext.i16.i64.s",
    "summary": "Integer sign extend 16 bits to 64 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.ext.i16.i64.u",
    "summary": "Integer zero extend 16 bits to 64 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.ext.i32.i64.s",
    "summary": "Integer sign extend 32 bits to 64 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.ext.i32.i64.u",
    "summary": "Integer zero extend 32 bits to 64 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.ext.i8.i32.s",
    "summary": "Integer sign extend 8 bits to 32 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.ext.i8.i32.u",
    "summary": "Integer zero extend 8 bits to 32 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.ext.i8.i64.s",
    "summary": "Integer sign extend 8 bits to 64 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.ext.i8.i64.u",
    "summary": "Integer zero extend 8 bits to 64 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.fail",
    "summary": "Raises a global failure.",
    "description": "Signals a runtime failure that causes the entire active invocation - and\n    possibly *all* in-flight and pending invocations - to fail with the given\n    status. The status will be propagated back via the available runtime error\n    handling mechanisms such as semaphores or synchronous invocation results.\n\n    As the IREE execution model is deeply pipelined it's possible that failures\n    have a latency between when they are emitted and when the application can\n    observe the failure. It's also possible that other work that is in-flight\n    or pending when the failure occurs will complete.\n\n    ```\n    %statusCode = vm.const.i32 9\n    vm.fail %statusCode, \"oh no!\"\n    ```",
    "inputs": [
      { "name": "status", "type": "Util_Status" }
    ],
    "attributes": [
      { "name": "message", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$status (`,` $message^)? attr-dict"
  },
  {
    "name": "vm.floor.f32",
    "summary": "Floating point floor operation.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.floor.f64",
    "summary": "Floating point floor operation.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.fma.f32",
    "summary": "Floating point fused multiply-add operation (a*b+c).",
    "inputs": [
      { "name": "a", "type": "type" },
      { "name": "b", "type": "type" },
      { "name": "c", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.fma.f64",
    "summary": "Floating point fused multiply-add operation (a*b+c).",
    "inputs": [
      { "name": "a", "type": "type" },
      { "name": "b", "type": "type" },
      { "name": "c", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.fma.i32",
    "summary": "Integer fused-multiply add operation (a*b+c).",
    "inputs": [
      { "name": "a", "type": "type" },
      { "name": "b", "type": "type" },
      { "name": "c", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.fma.i64",
    "summary": "Integer fused-multiply add operation (a*b+c).",
    "inputs": [
      { "name": "a", "type": "type" },
      { "name": "b", "type": "type" },
      { "name": "c", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.func",
    "summary": "Function defined with VM control flow ops.",
    "description": "Represents a function containing VM ops and those of compatible dialects.\n    All flow control is performed by VM ops.",
    "attributes": [
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "ordinal", "type": "OptionalAttr" },
      { "name": "inlining_policy", "type": "OptionalAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "vm.global.address",
    "summary": "Returns an address reference to a global.",
    "description": "Returns an indirect address reference to the given global. During export the\n    address will be converted to the natural format of the global table (for\n    example, ordinals for refs and byte offsets for primitive types).",
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "global", "type": "VM_GlobalRefAttr" },
      { "name": "is_immutable", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`immutable` $is_immutable^)?\n    $global attr-dict `:` type($result)"
  },
  {
    "name": "vm.global.f32",
    "summary": "32-bit floating-point global declaration.",
    "description": "Defines a global value that is treated as a scalar literal at runtime.\n    Initialized to zero unless an initial value is specified.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "type", "type": "TypeAttr" },
      { "name": "is_mutable", "type": "UnitAttr" },
      { "name": "initial_value", "type": "OptionalAttr" },
      { "name": "inlining_policy", "type": "OptionalAttr" },
      { "name": "ordinal", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    (`mutable` $is_mutable^)?\n    $sym_name\n    attr-dict\n    custom<TypeOrAttr>($type, $initial_value)"
  },
  {
    "name": "vm.global.f64",
    "summary": "64-bit floating-point global declaration.",
    "description": "Defines a global value that is treated as a scalar literal at runtime.\n    Initialized to zero unless an initial value is specified.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "type", "type": "TypeAttr" },
      { "name": "is_mutable", "type": "UnitAttr" },
      { "name": "initial_value", "type": "OptionalAttr" },
      { "name": "inlining_policy", "type": "OptionalAttr" },
      { "name": "ordinal", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    (`mutable` $is_mutable^)?\n    $sym_name\n    attr-dict\n    custom<TypeOrAttr>($type, $initial_value)"
  },
  {
    "name": "vm.global.i32",
    "summary": "32-bit integer global declaration.",
    "description": "Defines a global value that is treated as a scalar literal at runtime.\n    Initialized to zero unless an initial value is specified.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "type", "type": "TypeAttr" },
      { "name": "is_mutable", "type": "UnitAttr" },
      { "name": "initial_value", "type": "OptionalAttr" },
      { "name": "inlining_policy", "type": "OptionalAttr" },
      { "name": "ordinal", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    (`mutable` $is_mutable^)?\n    $sym_name\n    attr-dict\n    custom<TypeOrAttr>($type, $initial_value)"
  },
  {
    "name": "vm.global.i64",
    "summary": "64-bit integer global declaration.",
    "description": "Defines a global value that is treated as a scalar literal at runtime.\n    Initialized to zero unless an initial value is specified.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "type", "type": "TypeAttr" },
      { "name": "is_mutable", "type": "UnitAttr" },
      { "name": "initial_value", "type": "OptionalAttr" },
      { "name": "inlining_policy", "type": "OptionalAttr" },
      { "name": "ordinal", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    (`mutable` $is_mutable^)?\n    $sym_name\n    attr-dict\n    custom<TypeOrAttr>($type, $initial_value)"
  },
  {
    "name": "vm.global.load.f32",
    "summary": "Global 32-bit floating-point load operation.",
    "description": "Loads the value of a global containing an primitive value.",
    "outputs": [
      { "name": "value", "type": "type" }
    ],
    "attributes": [
      { "name": "global", "type": "VM_GlobalRefAttr" },
      { "name": "is_immutable", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`immutable` $is_immutable^)?\n    $global attr-dict `:` type($value)"
  },
  {
    "name": "vm.global.load.f64",
    "summary": "Global 64-bit floating-point load operation.",
    "description": "Loads the value of a global containing an primitive value.",
    "outputs": [
      { "name": "value", "type": "type" }
    ],
    "attributes": [
      { "name": "global", "type": "VM_GlobalRefAttr" },
      { "name": "is_immutable", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`immutable` $is_immutable^)?\n    $global attr-dict `:` type($value)"
  },
  {
    "name": "vm.global.load.i32",
    "summary": "Global 32-bit integer load operation.",
    "description": "Loads the value of a global containing an primitive value.",
    "outputs": [
      { "name": "value", "type": "type" }
    ],
    "attributes": [
      { "name": "global", "type": "VM_GlobalRefAttr" },
      { "name": "is_immutable", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`immutable` $is_immutable^)?\n    $global attr-dict `:` type($value)"
  },
  {
    "name": "vm.global.load.i64",
    "summary": "Global 64-bit integer load operation.",
    "description": "Loads the value of a global containing an primitive value.",
    "outputs": [
      { "name": "value", "type": "type" }
    ],
    "attributes": [
      { "name": "global", "type": "VM_GlobalRefAttr" },
      { "name": "is_immutable", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`immutable` $is_immutable^)?\n    $global attr-dict `:` type($value)"
  },
  {
    "name": "vm.global.load.indirect.f32",
    "summary": "Global 32-bit floating-point load operation.",
    "description": "Loads the value of a global containing a primitive value.",
    "inputs": [
      { "name": "global", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "value", "type": "type" }
    ],
    "attributes": [
      { "name": "is_immutable", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`immutable` $is_immutable^)?\n    $global attr-dict `:` type($global) `->` type($value)"
  },
  {
    "name": "vm.global.load.indirect.f64",
    "summary": "Global 64-bit floating-point load operation.",
    "description": "Loads the value of a global containing a primitive value.",
    "inputs": [
      { "name": "global", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "value", "type": "type" }
    ],
    "attributes": [
      { "name": "is_immutable", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`immutable` $is_immutable^)?\n    $global attr-dict `:` type($global) `->` type($value)"
  },
  {
    "name": "vm.global.load.indirect.i32",
    "summary": "Global 32-bit integer load operation.",
    "description": "Loads the value of a global containing a primitive value.",
    "inputs": [
      { "name": "global", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "value", "type": "type" }
    ],
    "attributes": [
      { "name": "is_immutable", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`immutable` $is_immutable^)?\n    $global attr-dict `:` type($global) `->` type($value)"
  },
  {
    "name": "vm.global.load.indirect.i64",
    "summary": "Global 64-bit integer load operation.",
    "description": "Loads the value of a global containing a primitive value.",
    "inputs": [
      { "name": "global", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "value", "type": "type" }
    ],
    "attributes": [
      { "name": "is_immutable", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`immutable` $is_immutable^)?\n    $global attr-dict `:` type($global) `->` type($value)"
  },
  {
    "name": "vm.global.load.indirect.ref",
    "summary": "Global ref<T> load operation.",
    "description": "Loads the value of a global containing a ref of the given type.",
    "inputs": [
      { "name": "global", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "value", "type": "type" }
    ],
    "attributes": [
      { "name": "is_immutable", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`immutable` $is_immutable^)?\n    $global attr-dict `:` type($global) `->` type($value)"
  },
  {
    "name": "vm.global.load.ref",
    "summary": "Global ref<T> load operation.",
    "description": "Loads the value of a global containing a ref of the given type.",
    "outputs": [
      { "name": "value", "type": "type" }
    ],
    "attributes": [
      { "name": "global", "type": "VM_GlobalRefAttr" },
      { "name": "is_immutable", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`immutable` $is_immutable^)?\n    $global attr-dict `:` type($value)"
  },
  {
    "name": "vm.global.ref",
    "summary": "ref<T> global declaration.",
    "description": "Defines a global value that is a ref of a specific type. The global will\n    retain the ref object for the lifetime of the context or until the value is\n    replaced with a store or reset.\n    Initialized to null unless an initial value is specified.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "type", "type": "TypeAttr" },
      { "name": "is_mutable", "type": "UnitAttr" },
      { "name": "inlining_policy", "type": "OptionalAttr" },
      { "name": "ordinal", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    (`mutable` $is_mutable^)?\n    $sym_name\n    attr-dict\n    `:` $type"
  },
  {
    "name": "vm.global.store.f32",
    "summary": "Global 32-bit floating-point store operation.",
    "description": "Stores a primitive value value to a global.",
    "inputs": [
      { "name": "value", "type": "type" }
    ],
    "attributes": [
      { "name": "global", "type": "VM_GlobalRefAttr" }
    ],
    "assemblyFormat": "$value `,` $global attr-dict `:` type($value)"
  },
  {
    "name": "vm.global.store.f64",
    "summary": "Global 64-bit floating-point store operation.",
    "description": "Stores a primitive value value to a global.",
    "inputs": [
      { "name": "value", "type": "type" }
    ],
    "attributes": [
      { "name": "global", "type": "VM_GlobalRefAttr" }
    ],
    "assemblyFormat": "$value `,` $global attr-dict `:` type($value)"
  },
  {
    "name": "vm.global.store.i32",
    "summary": "Global 32-bit integer store operation.",
    "description": "Stores a primitive value value to a global.",
    "inputs": [
      { "name": "value", "type": "type" }
    ],
    "attributes": [
      { "name": "global", "type": "VM_GlobalRefAttr" }
    ],
    "assemblyFormat": "$value `,` $global attr-dict `:` type($value)"
  },
  {
    "name": "vm.global.store.i64",
    "summary": "Global 64-bit integer store operation.",
    "description": "Stores a primitive value value to a global.",
    "inputs": [
      { "name": "value", "type": "type" }
    ],
    "attributes": [
      { "name": "global", "type": "VM_GlobalRefAttr" }
    ],
    "assemblyFormat": "$value `,` $global attr-dict `:` type($value)"
  },
  {
    "name": "vm.global.store.indirect.f32",
    "summary": "Global 32-bit floating-point store operation.",
    "description": "Stores a primitive value to a global.",
    "inputs": [
      { "name": "value", "type": "type" },
      { "name": "global", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$value `,` $global attr-dict `:` type($value) `->` type($global)"
  },
  {
    "name": "vm.global.store.indirect.f64",
    "summary": "Global 64-bit floating-point store operation.",
    "description": "Stores a primitive value to a global.",
    "inputs": [
      { "name": "value", "type": "type" },
      { "name": "global", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$value `,` $global attr-dict `:` type($value) `->` type($global)"
  },
  {
    "name": "vm.global.store.indirect.i32",
    "summary": "Global 32-bit integer store operation.",
    "description": "Stores a primitive value to a global.",
    "inputs": [
      { "name": "value", "type": "type" },
      { "name": "global", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$value `,` $global attr-dict `:` type($value) `->` type($global)"
  },
  {
    "name": "vm.global.store.indirect.i64",
    "summary": "Global 64-bit integer store operation.",
    "description": "Stores a primitive value to a global.",
    "inputs": [
      { "name": "value", "type": "type" },
      { "name": "global", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$value `,` $global attr-dict `:` type($value) `->` type($global)"
  },
  {
    "name": "vm.global.store.indirect.ref",
    "summary": "Global ref<T> stores operation.",
    "description": "Stores a ref<T> to a global, retaining it until the global is reset.",
    "inputs": [
      { "name": "value", "type": "type" },
      { "name": "global", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$value `,` $global attr-dict `:` type($value) `->` type($global)"
  },
  {
    "name": "vm.global.store.ref",
    "summary": "Global ref<T> stores operation.",
    "description": "Stores a ref<T> to a global, retaining it until the global is reset.",
    "inputs": [
      { "name": "value", "type": "type" }
    ],
    "attributes": [
      { "name": "global", "type": "VM_GlobalRefAttr" }
    ],
    "assemblyFormat": "$value `,` $global attr-dict `:` type($value)"
  },
  {
    "name": "vm.import",
    "summary": "Imports a function from an external module.",
    "description": "Specifies a function that should be imported from either the runtime or\n    an external VM module.\n\n    Required imports can be declared with a minimum version of the module that\n    contains the import. The maximum declared minimum version of all required\n    imports from the module will become the required minimum version at runtime.\n\n    Optional imports not present at runtime will be invalid to call and whether\n    they were resolved can be queried with `vm.import.resolved`.",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "ordinal", "type": "OptionalAttr" },
      { "name": "is_optional", "type": "OptionalAttr" },
      { "name": "minimum_version", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "vm.import.resolved",
    "summary": "Returns true if an optional import was resolved at runtime.",
    "description": "Allows for checking whether a optional import was resolved at runtime. If\n    this returns false then attempting to call the imported function will result\n    in a failure at runtime.",
    "outputs": [
      { "name": "result", "type": "VM_CondValue" }
    ],
    "attributes": [
      { "name": "import", "type": "VM_FuncRefAttr" }
    ],
    "assemblyFormat": "$import attr-dict `:` type($result)"
  },
  {
    "name": "vm.initializer",
    "summary": "Global initialization function.",
    "description": "A function that is called in definition order upon module initialization.\n    Must not load any globals that are defined or initialized after it in the\n    module.",
    "attributes": [
      { "name": "function_type", "type": "TypeAttrOf" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ]
  },
  {
    "name": "vm.list.alloc",
    "summary": "Allocates a new empty list.",
    "description": "Allocates a new typed list with a minimum initial_capacity.",
    "inputs": [
      { "name": "initial_capacity", "type": "VM_ListIndex" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_AnyList" }
    ],
    "assemblyFormat": "operands attr-dict `:` `(` type($initial_capacity) `)` `->` type($result)"
  },
  {
    "name": "vm.list.get.f32",
    "summary": "Primitive type element accessor.",
    "description": "Returns the value of the element at the given index.",
    "inputs": [
      { "name": "list", "type": "VM_ListOf" },
      { "name": "index", "type": "VM_ListIndex" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` `(` type($list) `,` type($index) `)` `->` type($result)"
  },
  {
    "name": "vm.list.get.f64",
    "summary": "Primitive type element accessor.",
    "description": "Returns the value of the element at the given index.",
    "inputs": [
      { "name": "list", "type": "VM_ListOf" },
      { "name": "index", "type": "VM_ListIndex" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` `(` type($list) `,` type($index) `)` `->` type($result)"
  },
  {
    "name": "vm.list.get.i32",
    "summary": "Primitive type element accessor.",
    "description": "Returns the value of the element at the given index.",
    "inputs": [
      { "name": "list", "type": "VM_ListOf" },
      { "name": "index", "type": "VM_ListIndex" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` `(` type($list) `,` type($index) `)` `->` type($result)"
  },
  {
    "name": "vm.list.get.i64",
    "summary": "Primitive type element accessor.",
    "description": "Returns the value of the element at the given index.",
    "inputs": [
      { "name": "list", "type": "VM_ListOf" },
      { "name": "index", "type": "VM_ListIndex" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` `(` type($list) `,` type($index) `)` `->` type($result)"
  },
  {
    "name": "vm.list.get.ref",
    "summary": "ref type element accessor.",
    "description": "Returns the ref value of the element at the given index. Note that the value\n    may be null if the element is null or the type does not match.",
    "inputs": [
      { "name": "list", "type": "VM_AnyList" },
      { "name": "index", "type": "VM_ListIndex" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_AnyRef" }
    ],
    "assemblyFormat": "operands attr-dict `:` `(` type($list) `,` type($index) `)` `->` type($result)"
  },
  {
    "name": "vm.list.reserve",
    "summary": "Reserves capacity for list growth.",
    "description": "Reserves storage for at least minimum_capacity elements. If the list already\n    has at least the specified capacity the operation is ignored.",
    "inputs": [
      { "name": "list", "type": "VM_AnyList" },
      { "name": "minimum_capacity", "type": "VM_ListIndex" }
    ],
    "assemblyFormat": "operands attr-dict `:` `(` type($list) `,` type($minimum_capacity) `)`"
  },
  {
    "name": "vm.list.resize",
    "summary": "Resizes the list to a new count in elements.",
    "description": "Resizes the list to contain new_size elements. This will either truncate\n    the list if the existing size is greater than new_size or extend the list\n    with the default list value of 0 if storing primitives and null if refs.",
    "inputs": [
      { "name": "list", "type": "VM_AnyList" },
      { "name": "new_size", "type": "VM_ListIndex" }
    ],
    "assemblyFormat": "operands attr-dict `:` `(` type($list) `,` type($new_size) `)`"
  },
  {
    "name": "vm.list.set.f32",
    "summary": "Primitive type element mutator.",
    "description": "Sets the element at the given index to the new value.",
    "inputs": [
      { "name": "list", "type": "VM_ListOf" },
      { "name": "index", "type": "VM_ListIndex" },
      { "name": "value", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` `(` type($list) `,` type($index) `,` type($value) `)`"
  },
  {
    "name": "vm.list.set.f64",
    "summary": "Primitive type element mutator.",
    "description": "Sets the element at the given index to the new value.",
    "inputs": [
      { "name": "list", "type": "VM_ListOf" },
      { "name": "index", "type": "VM_ListIndex" },
      { "name": "value", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` `(` type($list) `,` type($index) `,` type($value) `)`"
  },
  {
    "name": "vm.list.set.i32",
    "summary": "Primitive type element mutator.",
    "description": "Sets the element at the given index to the new value.",
    "inputs": [
      { "name": "list", "type": "VM_ListOf" },
      { "name": "index", "type": "VM_ListIndex" },
      { "name": "value", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` `(` type($list) `,` type($index) `,` type($value) `)`"
  },
  {
    "name": "vm.list.set.i64",
    "summary": "Primitive type element mutator.",
    "description": "Sets the element at the given index to the new value.",
    "inputs": [
      { "name": "list", "type": "VM_ListOf" },
      { "name": "index", "type": "VM_ListIndex" },
      { "name": "value", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` `(` type($list) `,` type($index) `,` type($value) `)`"
  },
  {
    "name": "vm.list.set.ref",
    "summary": "ref type element mutator.",
    "description": "Sets the element at the given index to the new ref value (possibly null).",
    "inputs": [
      { "name": "list", "type": "VM_AnyList" },
      { "name": "index", "type": "VM_ListIndex" },
      { "name": "value", "type": "VM_AnyRef" }
    ],
    "assemblyFormat": "operands attr-dict `:` `(` type($list) `,` type($index) `,` type($value) `)`"
  },
  {
    "name": "vm.list.size",
    "summary": "The size of the list in elements.",
    "description": "Returns the current size of the list in elements.",
    "inputs": [
      { "name": "list", "type": "VM_AnyList" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_ListIndex" }
    ],
    "assemblyFormat": "operands attr-dict `:` `(` type($list) `)` `->` type($result)"
  },
  {
    "name": "vm.log.f32",
    "summary": "Base-e logarithm of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.log.f64",
    "summary": "Base-e logarithm of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.log10.f32",
    "summary": "Base-10 logarithm of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.log10.f64",
    "summary": "Base-10 logarithm of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.log1p.f32",
    "summary": "Natural logarithm of one plus the given value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.log1p.f64",
    "summary": "Natural logarithm of one plus the given value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.log2.f32",
    "summary": "Base-2 logarithm of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.log2.f64",
    "summary": "Base-2 logarithm of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.max.f32",
    "summary": "Floating point maximum operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.max.f64",
    "summary": "Floating point maximum operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.max.i32.s",
    "summary": "Signed integer maximum operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.max.i32.u",
    "summary": "Unsigned integer maximum operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.max.i64.s",
    "summary": "Signed integer maximum operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.max.i64.u",
    "summary": "Unsigned integer maximum operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.min.f32",
    "summary": "Floating point minimum operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.min.f64",
    "summary": "Floating point minimum operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.min.i32.s",
    "summary": "Signed integer minimum operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.min.i32.u",
    "summary": "Unsigned integer minimum operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.min.i64.s",
    "summary": "Signed integer minimum operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.min.i64.u",
    "summary": "Unsigned integer minimum operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.module",
    "summary": "Module containing VM functions and variables.",
    "description": "Top-level container for VM functions.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "ordinal_counts", "type": "OptionalAttr" },
      { "name": "version", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility)\n    $sym_name\n    attr-dict-with-keyword\n    regions"
  },
  {
    "name": "vm.module_terminator",
    "summary": "Terminator pseudo-op for the module op.",
    "assemblyFormat": "attr-dict"
  },
  {
    "name": "vm.mul.f32",
    "summary": "Floating point multiplication operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.mul.f64",
    "summary": "Floating point multiplication operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.mul.i32",
    "summary": "Integer multiplication operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.mul.i64",
    "summary": "Integer multiplication operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.neg.f32",
    "summary": "Floating point negation operation.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.neg.f64",
    "summary": "Floating point negation operation.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.not.i32",
    "summary": "Integer binary not operation.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.not.i64",
    "summary": "Integer binary not operation.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.or.i32",
    "summary": "Integer binary or operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.or.i64",
    "summary": "Integer binary or operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.pow.f32",
    "summary": "Floating point raised to the power of operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.pow.f64",
    "summary": "Floating point raised to the power of operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.print",
    "summary": "Message printing operation.",
    "description": "Prints the given string message and zero or more values.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "message", "type": "StrAttr" }
    ],
    "assemblyFormat": "$message `(` operands `)` attr-dict `:` type(operands)"
  },
  {
    "name": "vm.rem.f32",
    "summary": "Floating point remainder operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.rem.f64",
    "summary": "Floating point remainder operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.rem.i32.s",
    "summary": "Signed integer division remainder operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.rem.i32.u",
    "summary": "Unsigned integer division remainder operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.rem.i64.s",
    "summary": "Signed integer division remainder operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.rem.i64.u",
    "summary": "Unsigned integer division remainder operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.return",
    "summary": "Return operation.",
    "description": "Represents a return operation within a function.\n\n    ```\n    vm.func @foo(%0: i32, %1: f8) -> (i32, f8) {\n      vm.return %0, %1 : i32, f8\n    }\n    ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "vm.rodata",
    "summary": "Read-only data definition operation.",
    "description": "Defines a blob of read-only constant data that can be represented as a\n    ref. This can be used to store arbitrary data within modules such as\n    large constant buffers and other file contents.\n\n    Note that the data is reference counted as a way to track its usage once the\n    value leaves the module. For example, returning rodata from an exported\n    function must keep the data (possibly backed by mmap) valid for its entire\n    lifetime.\n\n    By default all rodata will be aligned in the final module output at a\n    16-byte granularity. An optional alignment can be specified to override the\n    default for cases where larger or smaller alignments are needed.",
    "attributes": [
      { "name": "sym_visibility", "type": "OptionalAttr" },
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "value", "type": "Util_AnySerializableAttr" },
      { "name": "alignment", "type": "OptionalAttr" },
      { "name": "ordinal", "type": "OptionalAttr" },
      { "name": "mime_type", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "custom<SymbolVisibility>($sym_visibility) $sym_name attr-dict $value"
  },
  {
    "name": "vm.rodata.inline",
    "summary": "Inlined constant rodata.",
    "description": "vm.rodata that can be embedded inline in functions. See vm.rodata for more\n    information.",
    "outputs": [
      { "name": "result", "type": "VM_RefOf" }
    ],
    "attributes": [
      { "name": "name", "type": "OptionalAttr" },
      { "name": "value", "type": "Util_AnySerializableAttr" },
      { "name": "alignment", "type": "OptionalAttr" },
      { "name": "mime_type", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "($name^)? attr-dict `:` type($result) `=` $value"
  },
  {
    "name": "vm.rodata.table.inline",
    "summary": "Inlined constant rodata table.",
    "description": "vm.rodata with another associated vm.rodata table specifying byte offsets\n    and sizes as a subview into the flattened data. The table is a flat array\n    of 32 or 64-bit integers storing (offset, size) in element order.\n\n    The optional alignment attribute applies to both the table and data rodata.\n    The data_alignment attribute can be used to specify an alignment for the\n    elements of the table, padding to the data alignment with zeros. The element\n    sizes reflect the unpadded attribute storage sizes.\n\n    See vm.rodata for more information.",
    "outputs": [
      { "name": "table_result", "type": "VM_RefOf" },
      { "name": "data_result", "type": "VM_RefOf" }
    ],
    "attributes": [
      { "name": "table_name", "type": "OptionalAttr" },
      { "name": "data_name", "type": "OptionalAttr" },
      { "name": "table_type", "type": "TypeAttrOf" },
      { "name": "data_array", "type": "Util_AnySerializableArrayAttr" },
      { "name": "alignment", "type": "OptionalAttr" },
      { "name": "data_alignment", "type": "OptionalAttr" },
      { "name": "mime_type", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$table_type attr-dict `:` type($table_result) `,` type($data_result) `=` $data_array"
  },
  {
    "name": "vm.round.f32",
    "summary": "Rounds the value to the nearest integer away from zero.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.round.f32.even",
    "summary": "Rounds the value to the nearest even integer.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.round.f64",
    "summary": "Rounds the value to the nearest integer away from zero.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.round.f64.even",
    "summary": "Rounds the value to the nearest even integer.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.rsqrt.f32",
    "summary": "Reciprocal of sqrt (1 / sqrt of the specified value).",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.rsqrt.f64",
    "summary": "Reciprocal of sqrt (1 / sqrt of the specified value).",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.select.f32",
    "summary": "Floating-point select operation.",
    "description": "Chooses one value based on a binary condition supplied as its first operand.\n    If the value of the condition is true the `true_value` operand is chosen,\n    otherwise the `false_value` operand is chosen. The true and false values\n    must have the same types. For example, the maximum operation is obtained by\n    combining \"select\" with \"cmpi\" as follows:\n\n    ```\n    %2 = vm.cmp.gt.i32.s %0, %1 : i32\n    %3 = vm.select.i32 %2, %0, %1 : i32\n    ```",
    "inputs": [
      { "name": "condition", "type": "VM_CondValue" },
      { "name": "true_value", "type": "type" },
      { "name": "false_value", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.select.f64",
    "summary": "Floating-point select operation.",
    "description": "Chooses one value based on a binary condition supplied as its first operand.\n    If the value of the condition is true the `true_value` operand is chosen,\n    otherwise the `false_value` operand is chosen. The true and false values\n    must have the same types. For example, the maximum operation is obtained by\n    combining \"select\" with \"cmpi\" as follows:\n\n    ```\n    %2 = vm.cmp.gt.i32.s %0, %1 : i32\n    %3 = vm.select.i32 %2, %0, %1 : i32\n    ```",
    "inputs": [
      { "name": "condition", "type": "VM_CondValue" },
      { "name": "true_value", "type": "type" },
      { "name": "false_value", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.select.i32",
    "summary": "Integer select operation.",
    "description": "Chooses one value based on a binary condition supplied as its first operand.\n    If the value of the condition is true the `true_value` operand is chosen,\n    otherwise the `false_value` operand is chosen. The true and false values\n    must have the same types. For example, the maximum operation is obtained by\n    combining \"select\" with \"cmpi\" as follows:\n\n    ```\n    %2 = vm.cmp.gt.i32.s %0, %1 : i32\n    %3 = vm.select.i32 %2, %0, %1 : i32\n    ```",
    "inputs": [
      { "name": "condition", "type": "VM_CondValue" },
      { "name": "true_value", "type": "type" },
      { "name": "false_value", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.select.i64",
    "summary": "Integer select operation.",
    "description": "Chooses one value based on a binary condition supplied as its first operand.\n    If the value of the condition is true the `true_value` operand is chosen,\n    otherwise the `false_value` operand is chosen. The true and false values\n    must have the same types. For example, the maximum operation is obtained by\n    combining \"select\" with \"cmpi\" as follows:\n\n    ```\n    %2 = vm.cmp.gt.i32.s %0, %1 : i32\n    %3 = vm.select.i32 %2, %0, %1 : i32\n    ```",
    "inputs": [
      { "name": "condition", "type": "VM_CondValue" },
      { "name": "true_value", "type": "type" },
      { "name": "false_value", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.select.ref",
    "summary": "ref<T> select operation.",
    "description": "Chooses one value based on a binary condition supplied as its first operand.\n    If the value of the condition is true the `true_value` operand is chosen,\n    otherwise the `false_value` operand is chosen.",
    "inputs": [
      { "name": "condition", "type": "VM_CondValue" },
      { "name": "true_value", "type": "VM_AnyRef" },
      { "name": "false_value", "type": "VM_AnyRef" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_AnyRef" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.shl.i32",
    "summary": "Integer shift left operation.",
    "description": "Shifts the operand in a direction by the number of bits specified.",
    "inputs": [
      { "name": "operand", "type": "type" },
      { "name": "amount", "type": "I32" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand `,` $amount attr-dict `:` type($operand)"
  },
  {
    "name": "vm.shl.i64",
    "summary": "Integer shift left operation.",
    "description": "Shifts the operand in a direction by the number of bits specified.",
    "inputs": [
      { "name": "operand", "type": "type" },
      { "name": "amount", "type": "I32" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand `,` $amount attr-dict `:` type($operand)"
  },
  {
    "name": "vm.shr.i32.s",
    "summary": "Signed integer (arithmetic) shift right operation.",
    "description": "Shifts the operand in a direction by the number of bits specified.",
    "inputs": [
      { "name": "operand", "type": "type" },
      { "name": "amount", "type": "I32" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand `,` $amount attr-dict `:` type($operand)"
  },
  {
    "name": "vm.shr.i32.u",
    "summary": "Unsigned integer (logical) shift right operation.",
    "description": "Shifts the operand in a direction by the number of bits specified.",
    "inputs": [
      { "name": "operand", "type": "type" },
      { "name": "amount", "type": "I32" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand `,` $amount attr-dict `:` type($operand)"
  },
  {
    "name": "vm.shr.i64.s",
    "summary": "Signed integer (arithmetic) shift right operation.",
    "description": "Shifts the operand in a direction by the number of bits specified.",
    "inputs": [
      { "name": "operand", "type": "type" },
      { "name": "amount", "type": "I32" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand `,` $amount attr-dict `:` type($operand)"
  },
  {
    "name": "vm.shr.i64.u",
    "summary": "Unsigned integer (logical) shift right operation.",
    "description": "Shifts the operand in a direction by the number of bits specified.",
    "inputs": [
      { "name": "operand", "type": "type" },
      { "name": "amount", "type": "I32" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand `,` $amount attr-dict `:` type($operand)"
  },
  {
    "name": "vm.sin.f32",
    "summary": "Sine of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.sin.f64",
    "summary": "Sine of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.sqrt.f32",
    "summary": "Sqrt of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.sqrt.f64",
    "summary": "Sqrt of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.sub.f32",
    "summary": "Floating point subtraction operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.sub.f64",
    "summary": "Floating point subtraction operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.sub.i32",
    "summary": "Integer subtract operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.sub.i64",
    "summary": "Integer subtract operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.switch.f32",
    "summary": "Floating-point switch operation.",
    "description": "Returns the value with the given `index` in `values` or `default_value` if\n    the index is out of bounds.\n\n    ```mlir\n    // Switch %index to cases of %c100/%c200/%c300 if index==0, ==1, ==2.\n    // If %index is out of range (<0 or >2) then default to %c5.\n    %0 = vm.switch.f32 %index[%c100, %c200, %c300] else %c5 : f32\n    ```",
    "inputs": [
      { "name": "index", "type": "VM_Index" },
      { "name": "default_value", "type": "type" },
      { "name": "values", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$index `[` $values `]` `else` $default_value attr-dict `:` type($result)"
  },
  {
    "name": "vm.switch.f64",
    "summary": "Floating-point switch operation.",
    "description": "Returns the value with the given `index` in `values` or `default_value` if\n    the index is out of bounds.\n\n    ```mlir\n    // Switch %index to cases of %c100/%c200/%c300 if index==0, ==1, ==2.\n    // If %index is out of range (<0 or >2) then default to %c5.\n    %0 = vm.switch.f32 %index[%c100, %c200, %c300] else %c5 : f32\n    ```",
    "inputs": [
      { "name": "index", "type": "VM_Index" },
      { "name": "default_value", "type": "type" },
      { "name": "values", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$index `[` $values `]` `else` $default_value attr-dict `:` type($result)"
  },
  {
    "name": "vm.switch.i32",
    "summary": "Integer switch operation.",
    "description": "Returns the value with the given `index` in `values` or `default_value` if\n    the index is out of bounds.\n\n    ```mlir\n    // Switch %index to cases of %c100/%c200/%c300 if index==0, ==1, ==2.\n    // If %index is out of range (<0 or >2) then default to %c5.\n    %0 = vm.switch.i32 %index[%c100, %c200, %c300] else %c5 : i32\n    ```",
    "inputs": [
      { "name": "index", "type": "VM_Index" },
      { "name": "default_value", "type": "type" },
      { "name": "values", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$index `[` $values `]` `else` $default_value attr-dict `:` type($result)"
  },
  {
    "name": "vm.switch.i64",
    "summary": "Integer switch operation.",
    "description": "Returns the value with the given `index` in `values` or `default_value` if\n    the index is out of bounds.\n\n    ```mlir\n    // Switch %index to cases of %c100/%c200/%c300 if index==0, ==1, ==2.\n    // If %index is out of range (<0 or >2) then default to %c5.\n    %0 = vm.switch.i32 %index[%c100, %c200, %c300] else %c5 : i32\n    ```",
    "inputs": [
      { "name": "index", "type": "VM_Index" },
      { "name": "default_value", "type": "type" },
      { "name": "values", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$index `[` $values `]` `else` $default_value attr-dict `:` type($result)"
  },
  {
    "name": "vm.switch.ref",
    "summary": "ref<T> switch operation.",
    "description": "Returns the value with the given `index` in `values` or `default_value` if\n    the index is out of bounds.\n\n    ```mlir\n    // Switch %arg0 to cases of %r0/%r1/%r2 if arg0==0, ==1, ==2.\n    // If %arg0 is out of range (<0 or >2) then default to %null.\n    %0 = vm.switch.ref %index[%r0, %r1, %r2] else %null : vm.ref<!foo>\n    ```",
    "inputs": [
      { "name": "index", "type": "VM_Index" },
      { "name": "default_value", "type": "VM_AnyRef" },
      { "name": "values", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "VM_AnyRef" }
    ]
  },
  {
    "name": "vm.tanh.f32",
    "summary": "Hyperbolic tangent of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.tanh.f64",
    "summary": "Hyperbolic tangent of the specified value.",
    "inputs": [
      { "name": "operand", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($result)"
  },
  {
    "name": "vm.trace",
    "summary": "Trace value(s) operation.",
    "description": "Traces one or more values at the time the operation is executed.\n    These values will be encoded into the active trace depending on the active\n    trace verbosity setting.",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "event_name", "type": "StrAttr" }
    ],
    "assemblyFormat": "$event_name `(` operands `)` attr-dict `:` type(operands)"
  },
  {
    "name": "vm.trunc.f64.f32",
    "summary": "Floating-point truncate to 32 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.trunc.i16.i8",
    "summary": "Integer truncate to 8 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.trunc.i32.i16",
    "summary": "Integer truncate to 16 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.trunc.i32.i8",
    "summary": "Integer truncate to 8 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.trunc.i64.i16",
    "summary": "Integer truncate to 16 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.trunc.i64.i32",
    "summary": "Integer truncate to 32 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.trunc.i64.i8",
    "summary": "Integer truncate to 8 bits.",
    "inputs": [
      { "name": "operand", "type": "src_type" }
    ],
    "outputs": [
      { "name": "result", "type": "dst_type" }
    ],
    "assemblyFormat": "$operand attr-dict `:` type($operand) `->` type($result)"
  },
  {
    "name": "vm.xor.i32",
    "summary": "Integer binary exclusive-or operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.xor.i64",
    "summary": "Integer binary exclusive-or operation.",
    "inputs": [
      { "name": "lhs", "type": "type" },
      { "name": "rhs", "type": "type" }
    ],
    "outputs": [
      { "name": "result", "type": "type" }
    ],
    "assemblyFormat": "operands attr-dict `:` type($result)"
  },
  {
    "name": "vm.yield",
    "summary": "Unconditional fiber yield operation.",
    "description": "Yields the fiber for some (likely short) amount of time. This can be used to\n    perform cooperative scheduling and ensure fair (enough) execution. Execution\n    resumes at the specified target branch.\n\n    ```\n    ^bb0:\n      vm.yield ^on_resume\n    ^on_resume:\n      ...\n   ```",
    "inputs": [
      { "name": "destOperands", "type": "Variadic" }
    ],
    "successors": [
      {
        "name": "dest"
      }
    ],
    "assemblyFormat": "$dest (`(` $destOperands^ `:` type($destOperands) `)`)? attr-dict"
  },
  {
    "name": "wasmssa.abs",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "src", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$src`:` type($src) attr-dict"
  },
  {
    "name": "wasmssa.add",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) attr-dict"
  },
  {
    "name": "wasmssa.and",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) attr-dict"
  },
  {
    "name": "wasmssa.block",
    "summary": "summaryStr",
    "description": "Defines a Wasm block, creating a new nested scope.\n  A block contains a body region and an optional list of input values.\n  Control can enter the block and later branch out to the block target.\n  Example:\n\n  ```mlir\n\n  wasmssa.block {\n\n    // instructions\n\n  } > ^successor",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "successors": [
      {
        "name": "target"
      }
    ],
    "assemblyFormat": "(`(`$inputs^`)` `:` type($inputs))? attr-dict  `:` $body `>` $target"
  },
  {
    "name": "wasmssa.block_return",
    "summary": "Return from the current block",
    "description": "Escape from the current nesting level and return the control flow to its successor.\n    Optionally, mark the arguments that should be transfered to the successor block.\n\n    This shouldn't be confused with branch operations that targets the label defined\n    by the nesting level operation.\n\n    For instance, a `wasmssa.block_return` in a loop will give back control to the\n    successor of the loop, where a `branch` targeting the loop will flow back to the entry block of the loop.\n\n    Example:\n\n    ```mlir\n      wasmssa.block_return\n    ```",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "assemblyFormat": "($inputs^ `:` type($inputs))? attr-dict"
  },
  {
    "name": "wasmssa.branch_if",
    "summary": "Jump to target level if condition has non-zero value",
    "description": "Jump to target level if the condition is has a non-zero value.\n\n     Example:\n\n     ```mlir\n     wasmssa.branch_if %a to level 0 with args(%b : i32) else ^bb1\n     ```",
    "inputs": [
      { "name": "condition", "type": "I32" },
      { "name": "inputs", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "exitLevel", "type": "UI32Attr" }
    ],
    "successors": [
      {
        "name": "elseSuccessor"
      }
    ],
    "assemblyFormat": "$condition `to` `level` $exitLevel (`with` `args`  `(`$inputs^ `:` type($inputs)`)`)?  `else` $elseSuccessor  attr-dict"
  },
  {
    "name": "wasmssa.call",
    "summary": "Calling a Wasm function",
    "description": "Emits a call to a defined function\n\n     Example:\n\n     ```mlir\n     %a = wasmssa.call @func_0 : () -> i32\n     ```",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "results", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "callee", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$callee (`(`$operands^`)`)? attr-dict `:` functional-type($operands, $results)"
  },
  {
    "name": "wasmssa.ceil",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "src", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$src`:` type($src) attr-dict"
  },
  {
    "name": "wasmssa.clz",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "src", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$src`:` type($src) attr-dict"
  },
  {
    "name": "wasmssa.const",
    "summary": "Operator that represents a constant value",
    "description": "Defines a constant value.\n\n     Example:\n\n     ```mlir\n     // Example of integer constant\n     %a = wasmssa.const 1 : i32\n\n     // Example of floating point constant\n     %b = wasmssa.const 9.000000e+00 : f64\n     ```",
    "outputs": [
      { "name": "result", "type": "WasmSSA_NumericType" }
    ],
    "attributes": [
      { "name": "value", "type": "TypedAttrInterface" }
    ],
    "assemblyFormat": "$value attr-dict"
  },
  {
    "name": "wasmssa.convert_s",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$input `:` type($input) `to` type($result)  attr-dict"
  },
  {
    "name": "wasmssa.convert_u",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$input `:` type($input) `to` type($result)  attr-dict"
  },
  {
    "name": "wasmssa.copysign",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) attr-dict"
  },
  {
    "name": "wasmssa.ctz",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "src", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$src`:` type($src) attr-dict"
  },
  {
    "name": "wasmssa.demote",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$input `:` type($input) `to` type($result)  attr-dict"
  },
  {
    "name": "wasmssa.div",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) attr-dict"
  },
  {
    "name": "wasmssa.div_si",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) attr-dict"
  },
  {
    "name": "wasmssa.div_ui",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) attr-dict"
  },
  {
    "name": "wasmssa.eq",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "I32" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) `->` type($result) attr-dict"
  },
  {
    "name": "wasmssa.eqz",
    "summary": "Check if the given value is equal to zero",
    "description": "Example:\n\n     ```mlir\n     %a = wasmssa.eqz %b : i64 -> i32\n     ```",
    "inputs": [
      { "name": "input", "type": "WasmSSA_IntegerType" }
    ],
    "outputs": [
      { "name": "result", "type": "I32" }
    ],
    "assemblyFormat": "$input`:` type($input) `->` type($result) attr-dict"
  },
  {
    "name": "wasmssa.extend",
    "description": "Extend low bytes of a value to fit a given width.\n  For instance, signed extension from 8 low bits of the 32-bits integer value\n  254 (0x000000FE) would produce the value -2 (0xFFFFFFFE).\n\n  This corresponds to the `extendnn` instruction of Wasm, which shouldn't be\n  confused with the `extend_inn` Wasm instruction, for which all input bits\n  are used and widened to wider output type.\n  In this operation, input and output types are the same.\n\n  Example:\n\n  ```mlir\n  %a = wasmssa.extend 16 low bits from %[[VAL_0]]: i64\n  ```",
    "inputs": [
      { "name": "input", "type": "WasmSSA_IntegerType" }
    ],
    "outputs": [
      { "name": "result", "type": "WasmSSA_IntegerType" }
    ],
    "attributes": [
      { "name": "bitsToTake", "type": "Builtin_IntegerAttr" }
    ],
    "assemblyFormat": "$bitsToTake `low` `bits` `from` $input `:` type($input) attr-dict"
  },
  {
    "name": "wasmssa.extend_i32_s",
    "summary": "Sign extend i32 to i64.",
    "description": "Example:\n\n     ```mlir\n     %a = wasmssa.extend_i32_s %b to i64\n    ```",
    "inputs": [
      { "name": "input", "type": "I32" }
    ],
    "outputs": [
      { "name": "result", "type": "I64" }
    ],
    "assemblyFormat": "$input `to` type($result)  attr-dict"
  },
  {
    "name": "wasmssa.extend_i32_u",
    "summary": "Zero extend i32 to i64.",
    "description": "Example:\n\n     ```mlir\n     %a = wasmssa.extend_i32_s %b to i64\n    ```",
    "inputs": [
      { "name": "input", "type": "I32" }
    ],
    "outputs": [
      { "name": "result", "type": "I64" }
    ],
    "assemblyFormat": "$input `to` type($result)  attr-dict"
  },
  {
    "name": "wasmssa.floor",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "src", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$src`:` type($src) attr-dict"
  },
  {
    "name": "wasmssa.func",
    "description": "Represents a Wasm function definition.\n\n    In Wasm function, locals and function arguments are interchangeable.\n    They are for instance both accessed using `local.get` instruction.\n\n    On the other hand, a function type is defined as a pair of tuples of Wasm value types.\n    To model this, the wasm.func operation has:\n\n    - A function type that represents the corresponding Wasm type (tuples of value types)\n\n    - Arguments of the entry block of type `!wasm<local T>`, with T the corresponding type\n     in the function type.\n\n    By default, `wasmssa.func` have nested visibility. Functions exported by the module\n    are marked with the exported attribute. This gives them public visibility.\n\n     Example:\n\n     ```mlir\n     // Internal function with no arguments that returns a float32\n     wasmssa.func @my_f32_func() -> f32\n\n     // Exported function with no arguments that returns a float32\n     wasmssa.func exported @my_f32_func() -> f32\n\n     // A function that takes a local ref argument\n     wasmssa.func @i64_wrap(%a: !wasmssa<local ref to i64>) -> i32\n     ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "functionType", "type": "WasmSSA_FuncTypeAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" },
      { "name": "exported", "type": "UnitAttr" }
    ]
  },
  {
    "name": "wasmssa.ge",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "I32" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) `->` type($result) attr-dict"
  },
  {
    "name": "wasmssa.ge_si",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "I32" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) `->` type($result) attr-dict"
  },
  {
    "name": "wasmssa.ge_ui",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "I32" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) `->` type($result) attr-dict"
  },
  {
    "name": "wasmssa.global",
    "summary": "WebAssembly global value",
    "description": "WebAssembly global variable.\n    Body contains the initialization instructions for the variable value.\n    The body must contain only instructions considered `const` in a webassembly context,\n    such as `wasmssa.const` or `global.get`.\n\n    By default, `wasmssa.global` have nested visibility. Global exported by the module\n    are marked with the exported attribute. This gives them public visibility.\n\n    Example:\n\n    ```mlir\n    // Define module_global_var, an internal mutable i32 global variable equal to 10.\n    wasmssa.global @module_global_var i32 mutable : {\n          %[[VAL_0:.*]] = wasmssa.const 10 : i32\n          wasmssa.return %[[VAL_0]] : i32\n    }\n\n    // Define global_var, an exported constant i32 global variable equal to 42.\n    wasmssa.global @global_var i32 : {\n          %[[VAL_0:.*]] = wasmssa.const 42 : i32\n          wasmssa.return %[[VAL_0]] : i32\n    }\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "type", "type": "WasmSSA_ValTypeAttr" },
      { "name": "isMutable", "type": "UnitAttr" },
      { "name": "exported", "type": "UnitAttr" }
    ]
  },
  {
    "name": "wasmssa.global_get",
    "summary": "Returns the value of the global passed as argument.",
    "description": "Retrieves the value of the global passed as argument and stores it in a\n     variable\n\n     Example:\n\n     ```mlir\n     // Gets the value of `@global_0` and stores its value in %a\n     %a = wasmssa.global_get @global_0 : i32\n     ```",
    "outputs": [
      { "name": "global_val", "type": "WasmSSA_ValType" }
    ],
    "attributes": [
      { "name": "global", "type": "FlatSymbolRefAttr" }
    ],
    "assemblyFormat": "$global attr-dict `:` type($global_val)"
  },
  {
    "name": "wasmssa.gt",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "I32" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) `->` type($result) attr-dict"
  },
  {
    "name": "wasmssa.gt_si",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "I32" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) `->` type($result) attr-dict"
  },
  {
    "name": "wasmssa.gt_ui",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "I32" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) `->` type($result) attr-dict"
  },
  {
    "name": "wasmssa.if",
    "summary": "Execute the if region if condition value is non-zero, the else region otherwise.",
    "description": "Execute the if region if the condition is non-zero. Otherwise the else region is executed.\n    The else region can be empty but must return the same datatype as the if region.\n    If clauses can be nested.\n\n     Example:\n\n     ```mlir\n     // Runs the if clause is %a is non-zero\n     wasmssa.if %a {\n        // Execute if %a is non-zero\n     } else {\n        // else clause\n     }\n     ```",
    "inputs": [
      { "name": "condition", "type": "I32" },
      { "name": "inputs", "type": "Variadic" }
    ],
    "successors": [
      {
        "name": "target"
      }
    ],
    "assemblyFormat": "$condition (`(`$inputs^`)` `:` type($inputs))? attr-dict  `:` $if custom<ElseRegion>($else) `>` $target"
  },
  {
    "name": "wasmssa.import_func",
    "summary": "Importing a function variable",
    "description": "Imports a function from another module\n\n     Example:\n\n     ```mlir\n     // Imports foo(i32) -> () from the module my_module\n     wasmssa.import_func \"foo\" from \"my_module\" as @func_0 {sym_visibility = \"nested\", type = (i32) -> ()}\n     ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "moduleName", "type": "StrAttr" },
      { "name": "importName", "type": "StrAttr" },
      { "name": "type", "type": "WasmSSA_FuncTypeAttr" },
      { "name": "arg_attrs", "type": "OptionalAttr" },
      { "name": "res_attrs", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$importName `from` $moduleName `as` $sym_name attr-dict"
  },
  {
    "name": "wasmssa.import_global",
    "summary": "Importing a global variable",
    "description": "Imports a global from another module\n\n     Example:\n\n     ```mlir\n     // Imports the \"glob\" i32 global from the module my_module as \"global_0\"\n     wasmssa.import_global \"glob\" from \"my_module\" as @global_0 nested : i32\n     ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "moduleName", "type": "StrAttr" },
      { "name": "importName", "type": "StrAttr" },
      { "name": "type", "type": "WasmSSA_ValTypeAttr" },
      { "name": "isMutable", "type": "UnitAttr" }
    ]
  },
  {
    "name": "wasmssa.import_mem",
    "summary": "Importing a memory",
    "description": "Import a memory from another module.\n\n     Example:\n\n     ```mlir\n     // Import the memory `mem` from `my_module` as @mem_0\n     wasmssa.import_mem \"mem\" from \"my_module\" as @mem_0 {limits = !wasmssa<limit[2:]>}\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "moduleName", "type": "StrAttr" },
      { "name": "importName", "type": "StrAttr" },
      { "name": "limits", "type": "WasmSSA_LimitTypeAttr" }
    ],
    "assemblyFormat": "$importName `from` $moduleName `as` $sym_name attr-dict"
  },
  {
    "name": "wasmssa.import_table",
    "summary": "Importing a table",
    "description": "Import a table from another module.\n\n     Example:\n\n     ```mlir\n     // Import the table `table` from `my_module` as @table_0\n     wasmssa.import_table \"table\" from \"my_module\" as @table_0 {type = !wasmssa<tabletype !wasmssa.funcref [2:]>}\n    ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "moduleName", "type": "StrAttr" },
      { "name": "importName", "type": "StrAttr" },
      { "name": "type", "type": "WasmSSA_TableTypeAttr" }
    ],
    "assemblyFormat": "$importName `from` $moduleName `as` $sym_name attr-dict"
  },
  {
    "name": "wasmssa.le",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "I32" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) `->` type($result) attr-dict"
  },
  {
    "name": "wasmssa.le_si",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "I32" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) `->` type($result) attr-dict"
  },
  {
    "name": "wasmssa.le_ui",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "I32" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) `->` type($result) attr-dict"
  },
  {
    "name": "wasmssa.local",
    "summary": "Declaration of local variable",
    "description": "Declares a local variable\n\n     Example:\n\n     ```mlir\n     // Declares `%a`, a float32 local\n     %a = wasmssa.local of type f32\n     ```",
    "outputs": [
      { "name": "result", "type": "WasmSSA_LocalRef" }
    ],
    "attributes": [
      { "name": "type", "type": "WasmSSA_ValTypeAttr" }
    ],
    "assemblyFormat": "`of` `type` $type attr-dict"
  },
  {
    "name": "wasmssa.local_get",
    "summary": "Set local to value and return the operand.",
    "description": "Gets the value of a local variable and returns a reference to it.\n\n     Example:\n\n     ```mlir\n     // Retrieves a reference to `%a`, a float32 local\n     %b = wasmssa.local_get %a : ref to f32\n     ```",
    "inputs": [
      { "name": "localVar", "type": "WasmSSA_LocalRef" }
    ],
    "outputs": [
      { "name": "result", "type": "WasmSSA_ValType" }
    ],
    "assemblyFormat": "$localVar `:` type($localVar) attr-dict"
  },
  {
    "name": "wasmssa.local_set",
    "summary": "Set local to given value",
    "description": "Sets the value of a local variable.\n\n     Example:\n\n     ```mlir\n     // Sets `%d`, to the value of `%c`\n     wasmssa.local_set %d :  ref to i32 to %c : i32\n     ```",
    "inputs": [
      { "name": "localVar", "type": "WasmSSA_LocalRef" },
      { "name": "value", "type": "WasmSSA_ValType" }
    ],
    "assemblyFormat": "$localVar `:` type($localVar) `to` $value `:` type($value) attr-dict"
  },
  {
    "name": "wasmssa.local_tee",
    "summary": "Set local to value and return the operand.",
    "description": "Sets the value of a local variable and returns it.\n\n     Example:\n\n     ```mlir\n     // Sets `%b`, to the value of `%c` and returns it in %a\n      %a = wasmssa.local_tee %b :  ref to i32 to %c : i32\n     ```",
    "inputs": [
      { "name": "localVar", "type": "WasmSSA_LocalRef" },
      { "name": "value", "type": "WasmSSA_ValType" }
    ],
    "outputs": [
      { "name": "result", "type": "WasmSSA_ValType" }
    ],
    "assemblyFormat": "$localVar `:` type($localVar) `to` $value `:` type($value) attr-dict"
  },
  {
    "name": "wasmssa.loop",
    "summary": "summaryStr",
    "description": "Represents a Wasm loop construct. This defines a nesting level with\n  a label at the entry of the region.\n\n  Example:\n\n  ```mlir\n\n  wasmssa.loop {\n\n  } > ^successor",
    "inputs": [
      { "name": "inputs", "type": "Variadic" }
    ],
    "successors": [
      {
        "name": "target"
      }
    ],
    "assemblyFormat": "(`(`$inputs^`)` `:` type($inputs))? attr-dict  `:` $body `>` $target"
  },
  {
    "name": "wasmssa.lt",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "I32" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) `->` type($result) attr-dict"
  },
  {
    "name": "wasmssa.lt_si",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "I32" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) `->` type($result) attr-dict"
  },
  {
    "name": "wasmssa.lt_ui",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "I32" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) `->` type($result) attr-dict"
  },
  {
    "name": "wasmssa.max",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) attr-dict"
  },
  {
    "name": "wasmssa.memory",
    "summary": "WebAssembly memory definition",
    "description": "Define a memory to be used by the program.\n    Multiple memories can be defined in the same module.\n\n    By default, `wasmssa.memory` have nested visibility. Memory exported by\n    the module are marked with the exported attribute. This gives them public\n    visibility.\n\n     Example:\n\n     ```mlir\n     // Define the `mem_0` (internal)  memory with defined size bounds of [0:65536]\n     wasmssa.memory @mem_0 !wasmssa<limit[0:65536]>\n\n     // Define the `mem_1` exported  memory with minimal size of 512\n     wasmssa.memory exported @mem_1 !wasmssa<limit[512:]>\n     ```",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "limits", "type": "WasmSSA_LimitTypeAttr" },
      { "name": "exported", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`exported` $exported^)? $sym_name $limits attr-dict"
  },
  {
    "name": "wasmssa.min",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) attr-dict"
  },
  {
    "name": "wasmssa.mul",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) attr-dict"
  },
  {
    "name": "wasmssa.ne",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "I32" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) `->` type($result) attr-dict"
  },
  {
    "name": "wasmssa.neg",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "src", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$src`:` type($src) attr-dict"
  },
  {
    "name": "wasmssa.or",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) attr-dict"
  },
  {
    "name": "wasmssa.popcnt",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "src", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$src`:` type($src) attr-dict"
  },
  {
    "name": "wasmssa.promote",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$input `:` type($input) `to` type($result)  attr-dict"
  },
  {
    "name": "wasmssa.reinterpret",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$input `:` type($input) `as` type($result) attr-dict"
  },
  {
    "name": "wasmssa.rem_si",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) attr-dict"
  },
  {
    "name": "wasmssa.rem_ui",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) attr-dict"
  },
  {
    "name": "wasmssa.return",
    "summary": "Return from the current function frame",
    "inputs": [
      { "name": "operands", "type": "Variadic" }
    ],
    "assemblyFormat": "attr-dict ($operands^ `:` type($operands))?"
  },
  {
    "name": "wasmssa.rotl",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "val", "type": "WasmSSA_IntegerType" },
      { "name": "bits", "type": "WasmSSA_IntegerType" }
    ],
    "outputs": [
      { "name": "result", "type": "WasmSSA_IntegerType" }
    ],
    "assemblyFormat": "$val `by` $bits `bits` `:` type($val) attr-dict"
  },
  {
    "name": "wasmssa.rotr",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "val", "type": "WasmSSA_IntegerType" },
      { "name": "bits", "type": "WasmSSA_IntegerType" }
    ],
    "outputs": [
      { "name": "result", "type": "WasmSSA_IntegerType" }
    ],
    "assemblyFormat": "$val `by` $bits `bits` `:` type($val) attr-dict"
  },
  {
    "name": "wasmssa.shl",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "val", "type": "WasmSSA_IntegerType" },
      { "name": "bits", "type": "WasmSSA_IntegerType" }
    ],
    "outputs": [
      { "name": "result", "type": "WasmSSA_IntegerType" }
    ],
    "assemblyFormat": "$val `by` $bits `bits` `:` type($val) attr-dict"
  },
  {
    "name": "wasmssa.shr_s",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "val", "type": "WasmSSA_IntegerType" },
      { "name": "bits", "type": "WasmSSA_IntegerType" }
    ],
    "outputs": [
      { "name": "result", "type": "WasmSSA_IntegerType" }
    ],
    "assemblyFormat": "$val `by` $bits `bits` `:` type($val) attr-dict"
  },
  {
    "name": "wasmssa.shr_u",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "val", "type": "WasmSSA_IntegerType" },
      { "name": "bits", "type": "WasmSSA_IntegerType" }
    ],
    "outputs": [
      { "name": "result", "type": "WasmSSA_IntegerType" }
    ],
    "assemblyFormat": "$val `by` $bits `bits` `:` type($val) attr-dict"
  },
  {
    "name": "wasmssa.sqrt",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "src", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$src`:` type($src) attr-dict"
  },
  {
    "name": "wasmssa.sub",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) attr-dict"
  },
  {
    "name": "wasmssa.table",
    "summary": "WebAssembly table value",
    "attributes": [
      { "name": "sym_name", "type": "SymbolNameAttr" },
      { "name": "type", "type": "WasmSSA_TableTypeAttr" },
      { "name": "exported", "type": "UnitAttr" }
    ],
    "assemblyFormat": "(`exported` $exported^)? $sym_name $type attr-dict"
  },
  {
    "name": "wasmssa.trunc",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "src", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$src`:` type($src) attr-dict"
  },
  {
    "name": "wasmssa.wrap",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "input", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$input `:` type($input) `to` type($result)  attr-dict"
  },
  {
    "name": "wasmssa.xor",
    "summary": "summaryStr",
    "description": "descStr",
    "inputs": [
      { "name": "lhs", "type": "AnyTypeOf" },
      { "name": "rhs", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "result", "type": "AnyTypeOf" }
    ],
    "assemblyFormat": "$lhs $rhs `:` type($lhs) attr-dict"
  },
  {
    "name": "x86vector.avx.#mnemonic",
    "summary": "AVX: Convert packed BF16/F16 odd-indexed elements into packed F32 Data.",
    "description": "#### From the Intel Intrinsics Guide:\n\n    Convert packed BF16 or F16 (16-bit) floating-point odd-indexed elements stored at\n    memory locations starting at location `__A` to packed single-precision\n    (32-bit) floating-point elements, and store the results in `dst`.\n\n    Example:\n    ```mlir\n    %dst = x86vector.avx.cvt.packed.odd.indexed_to_f32 %a : memref<16xbf16> -> vector<8xf32>\n    %dst = x86vector.avx.cvt.packed.odd.indexed_to_f32 %a : memref<16xf16> -> vector<8xf32>\n    ```",
    "inputs": [
      { "name": "a", "type": "MemRefOf" }
    ],
    "outputs": [
      { "name": "dst", "type": "VectorOfLengthAndType" }
    ],
    "assemblyFormat": "$a  attr-dict`:` type($a)`->` type($dst)"
  },
  {
    "name": "x86vector.avx512.#mnemonic",
    "summary": "Convert packed F32 to packed BF16 Data.",
    "description": "The `convert_f32_to_bf16` op is an AVX512-BF16 specific op that can lower\n    to the proper LLVMAVX512BF16 operation `llvm.cvtneps2bf16` depending on\n    the width of MLIR vectors it is applied to.\n\n    #### From the Intel Intrinsics Guide:\n\n    Convert packed single-precision (32-bit) floating-point elements in `a` to\n    packed BF16 (16-bit) floating-point elements, and store the results in `dst`.\n\n    Example:\n    ```mlir\n    %dst = x86vector.avx512.cvt.packed.f32_to_bf16 %a : vector<8xf32> -> vector<8xbf16>\n    ```",
    "inputs": [
      { "name": "a", "type": "VectorOfLengthAndType" }
    ],
    "outputs": [
      { "name": "dst", "type": "VectorOfLengthAndType" }
    ],
    "assemblyFormat": "$a attr-dict `:` type($a) `->` type($dst)"
  },
  {
    "name": "xegpu.alloc_nbarrier",
    "summary": "It allocates a set of named barriers.",
    "description": "AllocNbarrier is to create a set of named barriers as\n  specified by `nbarrier_num`. Named barriers are workgroup level resources,\n    and are shared by all threads in the workgroup. For example, there are\n    up to 32 barriers (range 0-31) for each XeCore on PVC. A typical use case\n    is that a workgroup is partitioned into N subgroups of threads (N <= 32),\n    and each subgroup coordinating their work with a separate barrier with id\n    range from 0 to N respectively.",
    "attributes": [
      { "name": "nbarrier_num", "type": "I64Attr" }
    ],
    "assemblyFormat": "$nbarrier_num attr-dict"
  },
  {
    "name": "xegpu.atomic_rmw",
    "summary": "Atomic read-modify-write operation on the TensorDesc.",
    "description": "The `xegpu.atomic_rmw` operation provides a way to perform a read-modify-write\n    operation on the region described by the `TensorDesc` free from data races. The\n    `kind` enumeration specifies the modification to be performed, The `mask` operand\n    has the same shape with `TensorDesc`, and is used to enable or disable specific\n    data points of the `TensorDesc`. The `value` operand represents the new value to\n    be applied during the modification.",
    "inputs": [
      { "name": "tensorDesc", "type": "XeGPU_TensorDesc" },
      { "name": "mask", "type": "XeGPU_MaskType" },
      { "name": "value", "type": "XeGPU_ValueType" }
    ],
    "outputs": [
      { "name": "result", "type": "XeGPU_ValueType" }
    ],
    "attributes": [
      { "name": "kind", "type": "AtomicRMWKindAttr" }
    ],
    "assemblyFormat": "$kind $tensorDesc `,` $mask `,` $value attr-dict `:`\n    qualified(type($tensorDesc)) `,` type($mask) `,` type($value) `->` type($result)"
  },
  {
    "name": "xegpu.convert_layout",
    "summary": "Convert the layout of the input operand",
    "description": "`convert_layout` redistribute data across subgroups and/or work-items from the `input_layout` to\n      the `target_layout`. Both `input_layout` and `target_layout` must correspond to the same programming\n      scope, such as workgroup-level (wg) or subgroup-level (sg) code. This operation is not valid once\n      the IR is lowered to WI level because that is the end result of all distributions.",
    "inputs": [
      { "name": "source", "type": "XeGPU_VectorType" }
    ],
    "outputs": [
      { "name": "result", "type": "XeGPU_VectorType" }
    ],
    "attributes": [
      { "name": "input_layout", "type": "DistributeLayoutAttr" },
      { "name": "target_layout", "type": "DistributeLayoutAttr" }
    ],
    "assemblyFormat": "$source prop-dict attr-dict `:` type($source)"
  },
  {
    "name": "xegpu.create_mem_desc",
    "summary": "Create a memory descriptor.",
    "description": "Creates a memory descriptor from a shared local memory (SLM) buffer, and xegpu\n    specific memory layout. The resulting memory descriptor has to have the same size\n    as the underlying shared local memory.\n\n    Arguments:\n     - `source` : a 1D statically shaped memref with element type i8, representing the raw SLM buffer.\n    Results:\n     - `mem_desc` : the memory descriptor.",
    "inputs": [
      { "name": "source", "type": "StaticShared1DMemRefOf" }
    ],
    "outputs": [
      { "name": "mem_desc", "type": "XeGPU_MemDesc" }
    ],
    "assemblyFormat": "$source prop-dict attr-dict `` `:` type($source) `->` qualified(type($mem_desc))"
  },
  {
    "name": "xegpu.create_nd_tdesc",
    "summary": "Create nd-tensor descriptor operation",
    "description": "The \"create_nd_tdesc\" operation creates a TensorDescType which represents\n    a sub-view of a 1D/2D memory region inside the one or two innermost dimensions\n    of the source. (It can be extended to support n-D memory region if needed in\n    future). Elements in the subview continuous in each dimension. It encodes the\n    following important information for supporting Intel hardware features:\n\n    Arguments:\n    - `source`: an object representing (starting address/pointer of) a memory region.\n       It can be either a memref object, or simply a pointer represented by uint64_t type.\n       For the case of dynamic memrefs or pointer, the shape and layout information of the\n       memory region should be explicitly passed via `shape` and `strides` parameters.\n\n    - `offsets`: index values represents offsets from the \"source\" at the each dimension\n        at which the subview of the target memory will be created. It is encoded via\n        \"offsets\" and \"const_offsets\", such that it can accept various forms, such as,\n        operands (e.g., [%c0, %c]) and attributes (e.g., [2, 4]).\n\n    - `shape`: the shape information of the memory region pointed by the \"source\". It is\n         typically encoded via the MemRefType of the source, e.g., memref<4096x4096xf16>.\n        But if \"source\" is simply a pointer represented as uint64_t type, or a memref\n        type without shape information e.g., memref<?x?xf16>, the shape information has\n        to be explicitly passed via the \"shape\" and \"const_shape\" arguments.\n\n    - `strides`: the strides of the memory region pointed by the \"source\". Similar to shape,\n        it is typically encoded via the MemRefType of the source too. But if \"source\" is\n        simply a pointer represented as uint64_t type, or a memref type without shape\n        information e.g., memref<?x?xf16>, the strides information has to be explicitly\n        passed via the \"strides\" and \"const_strides\" argument.\n\n    Results:\n    - `res`: nd tensor descriptor\n\n    Example 1 (suppose the tensor shape inferred by the compiler is 8x16):\n    ```mlir\n    %0 = memref.alloc() : memref<1024x1024xf32>\n    %c0 = arith.constant 0 : index\n    %c1 = arith.constant 1 : index\n    %1 = xegpu.create_nd_tdesc %0[%c0, %c0]: memref<1024x1024xf32> -> TensorDesc<8x16xf32>\n    ```\n\n    Example 2 (suppose the tensor shape inferred by the compiler is 8x16):\n    ```mlir\n    %0 = memref.alloc(%h, %w) : memref<?x?xf32>\n    %c0 = arith.constant 0 : index\n    %c1 = arith.constant 1 : index\n    %1 = xegpu.create_nd_tdesc %0[%c0, %c0], [%h, %w], [%w, %c1]: memref<?x?xf32> -> TensorDesc<8x16xf32>\n    ```\n\n    Example 3 (suppose the tensor shape inferred by the compiler is 8x16):\n    ```mlir\n    %0 = ... : ui64\n    %c0 = arith.constant 0 : index\n    %c1 = arith.constant 1 : index\n    %1 = xegpu.create_nd_tdesc %0[%c0, %c0], [%h, %w], [%w, %c1]: ui64 -> TensorDesc<8x16xf32>\n    ```",
    "inputs": [
      { "name": "source", "type": "XeGPU_BaseAddrType" },
      { "name": "offsets", "type": "Variadic" },
      { "name": "shape", "type": "Variadic" },
      { "name": "strides", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "TensorDesc", "type": "XeGPU_TensorDesc" }
    ],
    "attributes": [
      { "name": "const_offsets", "type": "OptionalAttr" },
      { "name": "const_shape", "type": "OptionalAttr" },
      { "name": "const_strides", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$source ``\n    custom<OptionalDynamicIndexList>($offsets, $const_offsets)\n    (`,` `shape` `:` custom<DynamicIndexList>($shape, $const_shape)^\n     `,` `strides``:` custom<DynamicIndexList>($strides, $const_strides))?\n    attr-dict `:` type($source) `->` qualified(type($TensorDesc))"
  },
  {
    "name": "xegpu.create_tdesc",
    "summary": "create scattered tensor descriptors (TensorDesc).",
    "description": "\"create_tdesc\" is similar to \"create_nd_tdesc\" in terms that it creates\n    a Tensor Descriptor (TensorDescType) for a memory region. While \"create_nd_tdesc\"\n    is for creating continuous subviews, \"create_tdesc\" is for creating non-continuous\n    (scattered) subviews, allowing each work-item in a subgroup specifying their own offset.\n    It accepts the following parameters:\n\n    Arguments:\n    - `source`: a 1D memref or pointer (i64, i32, ui64, ui32) represents the flattened\n      memory object.\n    - `offsets`: a vector containing offsets of each access point. Its size\n      is fixed to the hardware supportted subgroup size, e.g., 16 on PVC,\n      implying each element in the vector corresponds to a work-item (SIMT lane)\n      in the subgroup.\n\n    Results:\n    - `res`: scattered tensor descriptor\n\n    The first dimension of the result TensorDesc corresponds to work-items, so it should\n    match the dimension of offsets. It may also has a second dimension corresponding to\n    the chunk_size if the chunk size is larger than 1.\n\n    Example 1: It assumes subgroup size is 4, and accesses a[0], a[16], a[32], a[64]\n    ```mlir\n    %a = memref.alloc() : memref<1024xf32>\n    %0 = arith.constant dense<[0, 16, 32, 64]> : vector<4xindex>\n    %1 = xegpu.create_tdesc %a, %0: memref<1024xf32>, vector<4xindex> -> TensorDesc<4xf32>\n    ```\n\n    Example 2: It assumes subgroup size is 4, and each workitem access 8 elements.\n               It will access totally 32 data elements: a[0:7], a[16:23], a[32:39], a[64:71]\n    ```mlir\n    %0 = memref.alloc() : memref<1024xf32>\n    %off = arith.constant dense<[0, 16, 32, 64]> : vector<4xindex>\n    %1 = xegpu.create_tdesc %0, %off : memref<1024xf32>, vector<4xindex>\n          -> TensorDesc<4x8xf32, #xegpu.scattered_tdesc_attr<chunk_size = 8>>\n    ```\n\n    Example 3: It is similar to Example 2, but there is some overlaps among workitems.\n               It accesses: a[0:7], a[4:11], a[8:15], a[12:19]\n    ```mlir\n    %0 = memref.alloc() : memref<1024xf32>\n    %off = arith.constant dense<[0, 4, 8, 12]> : vector<4xindex>\n    %1 = xegpu.create_tdesc %0, %off : memref<1024xf32>, vector<4xindex>\n          -> TensorDesc<4x8xf32, #xegpu.scattered_tdesc_attr<chunk_size = 8>>\n    ```",
    "inputs": [
      { "name": "source", "type": "XeGPU_GatherScatterBaseAddrType" },
      { "name": "offsets", "type": "XeGPU_OffsetType" }
    ],
    "outputs": [
      { "name": "TensorDesc", "type": "XeGPU_TensorDesc" }
    ],
    "assemblyFormat": "$source `,` $offsets attr-dict `:`  type($source) `,` type($offsets) `->` qualified(type($TensorDesc))"
  },
  {
    "name": "xegpu.dpas",
    "summary": "It performs mma computation",
    "description": "DPAS performs matrix multiplication on matrix A of `mxk`\n    size, B of `kxn` size, and accumulate on matrix C of `mxn` to the same size\n    matrix , `m=8`, `n=16` and `k=8 * 32/bit_width_of_elem_type`. So for fp16\n    data type, the matrices are `A: vector<8x16xf16>`, `B: vector<16x16xf16>`,\n    and `C/D: vector<8x16xf32>`. Besides the matrix size requirements, DPAS\n    also requires A and B to be loaded with the required data layout. Specially,\n    VNNI layout is required for B operand. It is achieved via adding `packed`\n    attribute to the `load_nd` operator.  Due to the VNNI transformation, B operands\n    can be represented as a 3D vector, with the last dimension representing the VNNI\n    factor, which is computed as `32/bit_width_of_elem_type`. Thus, `B: vector<16x16xf16>`\n    can be represented as `B: vector<8x16x2xf16>`.\n\n    In SIMT code, each work-item from a subgroup holds a data fragment for A, B, C and the result,\n    which are represented as 1D vectors. Please refer to [OpenCL Intel extentions]\n    (https://registry.khronos.org/OpenCL/extensions/intel/cl_intel_subgroup_matrix_multiply_accumulate.html)\n    for more details about the fragment distribution.\n\n    Note: on PVC, the hardware can perform load with VNNI transformation when data\n          element type is 16-bit or lower precision, taking 2 or 4 elements from\n          the first dimension and inserted into the newly added innermost dimension.",
    "inputs": [
      { "name": "lhs", "type": "XeGPU_DpasOprType" },
      { "name": "rhs", "type": "XeGPU_DpasOprType" },
      { "name": "acc", "type": "Optional" }
    ],
    "outputs": [
      { "name": "result", "type": "XeGPU_DpasResType" }
    ],
    "assemblyFormat": "$lhs `,` $rhs (`,` $acc^)? attr-dict `:` type($lhs)`,` type($rhs) (`,` type($acc)^)?  `->` type($result)"
  },
  {
    "name": "xegpu.fence",
    "summary": "It synchronizes memory accesses.",
    "description": "It synchronizes the memory access between\n    write and following read or write.\n    1. `Memory_kind` describes the memory kind. \"global\" means the global memory,\n        \"slm\" means the share local memory.\n    2. `Fence_scope` describes the scope of fence. \"Workgroup\" means that the scope would be\n        within each workgroup. \"GPU\" means the scope would be across workgroups within the GPU.",
    "attributes": [
      { "name": "memory_kind", "type": "XeGPU_MemorySpaceAttr" },
      { "name": "fence_scope", "type": "XeGPU_FenceScopeAttr" }
    ],
    "assemblyFormat": "`memory_kind` `=` `` $memory_kind `,` `fence_scope` `=` `` $fence_scope attr-dict"
  },
  {
    "name": "xegpu.init_nbarrier",
    "summary": "It assigns a named barrier to the current thread.",
    "description": "InitNbarrierOp assigns the named barrier with the specified\n      barrier ID (0~31) to the current thread. Multiple threads may bind to the\n      same named barrier, and the `participant_thread_num` specifies the total\n      number of threads associated with the nbarrier. It returns an object of\n      NbarrierType representing the barrier",
    "inputs": [
      { "name": "nbarrier_id", "type": "I8" },
      { "name": "participant_thread_num", "type": "I8" }
    ],
    "outputs": [
      { "name": "result", "type": "XeGPU_Nbarrier" }
    ],
    "assemblyFormat": "$nbarrier_id `,` $participant_thread_num attr-dict `:`\n    type($nbarrier_id) `,` type($participant_thread_num) `->` qualified(type($result))"
  },
  {
    "name": "xegpu.load",
    "summary": "load a set of scattered data points from memory.",
    "description": "It (aka. load) load data per each work-item. The output\n    describes the data being loaded at the subgroup level, so its size is\n    consistent with the number of work-items in a subgroup. When the chunk size\n    is larger than 2, the output vector is a 2D vector, with dim-0 correspoding\n    to work-items, and dim-1 corresponding to the chunk size loaded by each work-item.\n    The mask operand masks out memory access so that it is safe to pass out-of-boundary\n    addresses/offsets as long as they are masked. It applies to slots of SIMD lanes.\n\n    In SIMT mode, the result is a 1D vector that represents the data to be loaded by\n    each work-item. If size is not 1, size should be equal to the chunk size,\n\n    Arguments:\n    - `source`: represents the memory region to be loaded from, which can be either a\n        tensor_desc or a 1D memref or pointer (ui64, ui32, i64 or i32).\n        In case of tensor_desc, offsets come from the producer create_tdesc op.\n        tensor_desc cannot be used in SIMT mode.\n    - `offsets`: represents offsets from source. required if `source` in not a TensorDescType.\n        offsets is a vector of `index` type and vector length is either the subgroup size\n        or 1 in SIMT mode. scalar offset is also valid for SIMT mode.\n    - `mask`: is a vector of `i1` type, which is used to mask out the memory access.\n        mask is a vector of size equal to the subgroup size, or 1 in SIMT mode.\n        scalar mask is also valid for SIMT mode.\n    - `chunk_size`: (optional) represents contiguous number of elements to load from per work item.\n    - `l1_hint`, `l2_hint`, `l3_hint`: are optional cache hints for each level of cache.\n\n    Results:\n    - `res`: represents loaded data\n\n\n  Example 1:\n  ```mlir\n    %2 = xegpu.load %1, %0 <{l1_hint = #xegpu.cache_hint<cached>,\n                             l2_hint = #xegpu.cache_hint<uncached>,\n                             l3_hint = #xegpu.cache_hint<uncached>}>\n          : !xegpu.tensor_desc<16xf32, #xegpu.scatter_tdesc_attr<memory_space=global>>,\n            vector<16xi1> -> vector<16xf32>\n  ```\n\n  Example 2:\n  ```mlir\n    %2 = xegpu.load %1, %0 <{l1_hint = #xegpu.cache_hint<cached>,\n                             l2_hint = #xegpu.cache_hint<uncached>,\n                             l3_hint = #xegpu.cache_hint<uncached>}>\n          : !xegpu.tensor_desc<16x8xf32, #xegpu.scatter_tdesc_attr<memory_space=global, chunk_size=8>>,\n            vector<16xi1> -> vector<16x8xf32>\n  ```\n\n  Example 3:\n  A variant accepts memref as base pointer and an offset instead of scattered TensorTdesc.\n  It combines \"create scattered TensorTdesc\" and \"load with scattered TensorTdesc\".\n  The source operand could be a raw pointer (ui64, ui32, i64, i32). Please refer to create_tdesc\n  for the restriction of memref.\n  ```mlir\n    %a = memref.alloc() : memref<1024xf32>\n    %offsets = vector.step : vector<16xindex>\n    %mask = vector.constant_mask [16]: vector<16xi1>\n    %val = xegpu.load %a[%offsets], %mask {l1_hint = #xegpu.cache_hint<cached>,\n                           l2_hint = #xegpu.cache_hint<cached>,\n                           l3_hint = #xegpu.cache_hint<cached>}\n      : memref<1024xf32>, vector<16xi1>, vector<16xindex> -> vector<16xf32>\n  ```\n\n  Example 4 (SIMT mode):\n  SIMT mode only accepts the offsets variant. chunk_size can be inferred from result\n  type. In this example, chunk_size is 8.\n  ```mlir\n    %2 = xegpu.load %1[%2], %0 <{l1_hint = #xegpu.cache_hint<cached>,\n                             l2_hint = #xegpu.cache_hint<uncached>,\n                             l3_hint = #xegpu.cache_hint<uncached>}>\n          : memref<128xf32>, vector<1xindex>, vector<1xi1> -> vector<8xf32>\n  ```",
    "inputs": [
      { "name": "source", "type": "XeGPU_GatherScatterSourceType" },
      { "name": "offsets", "type": "Optional" },
      { "name": "mask", "type": "AnyTypeOf" }
    ],
    "outputs": [
      { "name": "value", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "chunk_size", "type": "OptionalAttr" },
      { "name": "l1_hint", "type": "OptionalAttr" },
      { "name": "l2_hint", "type": "OptionalAttr" },
      { "name": "l3_hint", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$source\n    (`[` $offsets^ `]`)? `,`\n    $mask prop-dict\n    attr-dict `:` type(operands) `->` type($value)"
  },
  {
    "name": "xegpu.load_matrix",
    "description": "This operation loads a 2D block of data from shared local memory (SLM) as specified\n    by the provided 2D `mem_desc`. Only 2D memory descriptors are supported; use the\n    subview operation to obtain a compatible 2D `mem_desc` from a higher-rank descriptor if needed.\n\n    Arguments:\n     - `mem_desc`: the memory descriptor identifying the SLM region.\n     - `offsets`: the coordinates within the matrix to read from.\n     - `subgroup_block_io`: [optional] An attribute indicating that the operation can be \n                 lowered to a subgroup block load. When this attribute is present, \n                 the offsets are subgroup-uniform across all lanes.\n     - `layout`: [optional] An attribute for guiding distributions among\n                 subgroups and/or work-items. It currently can accept either\n                 LayoutAttr or SliceAttr.\n    Results:\n     - `res`: the matrix elements loaded from SLM.",
    "inputs": [
      { "name": "mem_desc", "type": "XeGPU_MemDesc" },
      { "name": "offsets", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "res", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "const_offsets", "type": "DenseI64ArrayAttr" },
      { "name": "subgroup_block_io", "type": "OptionalAttr" },
      { "name": "layout", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$mem_desc `` custom<DynamicIndexList>($offsets, $const_offsets)\n    prop-dict attr-dict `` `:` type(operands) `->` type(results)"
  },
  {
    "name": "xegpu.load_nd",
    "summary": "loads a n-D block from memory (represented by TensorDesc)to registers (represented by vector)",
    "description": "LoadNdOp essentially mimics the hardware block read instruction to read\n    a block of data from memory to register. It takes a set of optional cache\n    hints for each level of cache, L1, L2 and L3. If hardware does not have a\n    correspoding cache, Corresponding cache hint attribute will be masked.\n    VNNI transformation is an hardware feature for Intel GPU, which is used to\n    do data packing during the load for B operand of matrix operation, if\n    the bit width of the data type is less then 32 bits, e.g., fp16. And\n    transpose is another Intel hardware feature, which will do transpose\n    operation when loading the data if the bit width of the data type is\n    fp32 or fp64. It implies that vnni and transpose cannot exit at the\n    same time. It is only available to 1D or 2D blocked tensor_desc.\n\n    In SIMT mode, result vector represents the data to be loaded by each work-item.\n\n    Example 1:\n    ```mlir\n      xegpu.load_nd %1 {transpose = [1, 0],\n                        l1_hint = #xegpu.cache_hint<cached>,\n                        l2_hint = #xegpu.cache_hint<uncached>,\n                        l3_hint = #xegpu.cache_hint<streaming>}\n              : !xegpu.tensor_desc<8x16xf32> -> vector<16x8xf32>\n    ```\n    Example 2 (SIMT mode):\n    ```mlir\n      xegpu.load_nd %1 {l1_hint = #xegpu.cache_hint<cached>,\n                        l2_hint = #xegpu.cache_hint<uncached>}>\n        : !xegpu.tensor_desc<8x16xf32> -> vector<8xf32>\n    ```",
    "inputs": [
      { "name": "TensorDesc", "type": "XeGPU_TensorDesc" },
      { "name": "offsets", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "value", "type": "XeGPU_ValueType" }
    ],
    "attributes": [
      { "name": "const_offsets", "type": "OptionalAttr" },
      { "name": "packed", "type": "OptionalAttr" },
      { "name": "transpose", "type": "OptionalAttr" },
      { "name": "l1_hint", "type": "OptionalAttr" },
      { "name": "l2_hint", "type": "OptionalAttr" },
      { "name": "l3_hint", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$TensorDesc ``\n    custom<OptionalDynamicIndexList>($offsets, $const_offsets)\n    prop-dict attr-dict `:` qualified(type($TensorDesc)) `->` type($value)"
  },
  {
    "name": "xegpu.nbarrier_arrive",
    "summary": "It signals the arrival at the named barrier.",
    "description": "NbarrierArriveOp signals the hardware (or other threads)\n    that the current thread has produced its data for the consumer threads. When\n    the hardware signalled by `participant_thread_num` threads for the named barrier,\n    it will notify the threads waiting for the named barrier to continue their work.",
    "inputs": [
      { "name": "nbarrier", "type": "XeGPU_Nbarrier" }
    ],
    "assemblyFormat": "$nbarrier attr-dict `:` qualified(type($nbarrier))"
  },
  {
    "name": "xegpu.nbarrier_wait",
    "summary": "It waits for a named barrier.",
    "description": "NbarrierWaitOp signals the hardware which named barrier\n    the current thread is waiting for, such that it can get notified when the\n    named barrier is completed.",
    "inputs": [
      { "name": "nbarrier", "type": "XeGPU_Nbarrier" }
    ],
    "assemblyFormat": "$nbarrier attr-dict `:` qualified(type($nbarrier))"
  },
  {
    "name": "xegpu.prefetch",
    "summary": "prefetches a set of scattered data points to cache",
    "description": "It issues instructions to prefetch a set of scattered data points\n    from memory to each level of the cache based on their cache policy.\n    As compared to prefetch_nd, which works on non-scattered TensorDesc,\n    it works on scattered TensorDesc instead.\n\n    Arguments:\n    - `source`: represents the memory region to be loaded from, which can be either a\n        tensor_desc or a 1D memref or pointer (ui64, ui32, i64 or i32).\n        In case of tensor_desc, offsets come from the producer create_tdesc op.\n        tensor_desc cannot be used in SIMT mode.\n    - `offsets`: represents offsets from source. required if `source` in not a TensorDescType.\n        offsets is a vector of `index` type and vector length is either the subgroup size\n        or 1 in SIMT mode. scalar offset is also valid for SIMT mode.\n    - `l1_hint`, `l2_hint`, `l3_hint`: are optional cache hints for each level of cache.\n    - `offset_align_byte`: required if `source` is a pointer. If `source` is not a pointer,\n        it is not allowed. Represents the alignment in bytes of each offset in offsets.\n\n    Example 1:\n    ```mlir\n      xegpu.prefetch %tdesc {l1_hint = #xegpu.cache_hint<cached>,\n                             l2_hint = #xegpu.cache_hint<cached>,\n                             l3_hint = #xegpu.cache_hint<cached>}\n        : !xegpu.tensor_desc<16xf16>\n    ```\n\n    Example 2:\n    A variant accepts memref as base pointer and an offset instead of scattered TensorTdesc.\n    It combines \"create scattered TensorTdesc\" and \"prefetch with scattered TensorTdesc\".\n    The source operand could be a raw pointer (ui64, ui32, i64, i32).\n    Please refer to create_tdesc for the restriction of memref.\n    ```mlir\n      %a = memref.alloc() : memref<1024xf32>\n      %0 = arith.constant dense<[0, 16, 32, 64]> : vector<4xindex>\n      xegpu.prefetch %a[%0] {l1_hint = #xegpu.cache_hint<cached>,\n                             l2_hint = #xegpu.cache_hint<cached>,\n                             l3_hint = #xegpu.cache_hint<cached>}\n        : memref<1024xf32>, vector<4xindex>\n    ```\n\n    Example 3 (SIMT mode):\n    SIMT mode only accepts the offsets variant.\n    ```mlir\n      xegpu.prefetch %0[%1] {l1_hint = #xegpu.cache_hint<cached>,\n                             l2_hint = #xegpu.cache_hint<cached>,\n                             l3_hint = #xegpu.cache_hint<cached>}\n        : memref<256xf32>, vector<1xindex>\n    ```\n\n    Example 4 (SIMT mode):\n    SIMT mode only accepts the offsets variant.\n    ```mlir\n      xegpu.prefetch %0[%1] {l1_hint = #xegpu.cache_hint<cached>,\n                             l2_hint = #xegpu.cache_hint<cached>,\n                             l3_hint = #xegpu.cache_hint<cached>,\n                             offset_align_byte = 2}\n        : i64, vector<1xindex>\n    ```",
    "inputs": [
      { "name": "source", "type": "XeGPU_GatherScatterSourceType" },
      { "name": "offsets", "type": "Optional" }
    ],
    "attributes": [
      { "name": "l1_hint", "type": "OptionalAttr" },
      { "name": "l2_hint", "type": "OptionalAttr" },
      { "name": "l3_hint", "type": "OptionalAttr" },
      { "name": "offset_align_byte", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$source\n    (`[` $offsets^ `]`)?\n    prop-dict\n    attr-dict `:` type(operands)"
  },
  {
    "name": "xegpu.prefetch_nd",
    "summary": "prefetches a n-D block to cache",
    "description": "It issues an instruction to prefetch a block of data from continuous\n    memory regions to each level of the cache based on their cache policy.\n\n    Example:\n    ```mlir\n      xegpu.prefetch_nd %tdesc {l1_hint = #xegpu.cache_hint<cached>,\n                                l2_hint = #xegpu.cache_hint<cached>,\n                                l3_hint = #xegpu.cache_hint<cached>}\n        : !xegpu.tensor_desc<8x16xf16>\n    ```",
    "inputs": [
      { "name": "TensorDesc", "type": "XeGPU_TensorDesc" },
      { "name": "offsets", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "const_offsets", "type": "OptionalAttr" },
      { "name": "l1_hint", "type": "OptionalAttr" },
      { "name": "l2_hint", "type": "OptionalAttr" },
      { "name": "l3_hint", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$TensorDesc ``\n    custom<OptionalDynamicIndexList>($offsets, $const_offsets)\n    prop-dict attr-dict `:` qualified(type($TensorDesc))"
  },
  {
    "name": "xegpu.store",
    "summary": "store data to scattered memory locations.",
    "description": "It (aka. store) stores data to scattered memory locations. The value is\n  typically a 1D vector. But when the chunk size of the TensorDesc is larger than 1, it will be\n  a 2D vector instead. For the later case, dim-1 of the value correspods to the simd lanes\n  and the dim-0 of the value corresponds to the chunk size stored per lane. So `store_scatter`\n  has transpose effect, which is similar to `load_gather`. Therefore, a transpose attribute is\n  introduced on purpose, making sure users are aware of this implicit transformation.\n\n  In SIMT mode, the result is a 1D vector that represents the data to be stored by\n  each work-item. If size is not 1, size should be equal to the chunk size.\n\n    Arguments:\n    - `value`: represents the data to be stored.\n    - `dest`: represents the memory region to be stored to, which can be either a\n        tensor_desc or a 1D memref or pointer (ui64, ui32, i64 or i32).\n        In case of tensor_desc, offsets come from the producer create_tdesc op.\n        tensor_desc cannot be used in SIMT mode.\n    - `offsets`: represents offsets from dest. required if `source` in not a TensorDescType.\n        offsets is a vector of `index` type and vector length is either the subgroup size\n        or 1 in SIMT mode. scalar offset is also valid for SIMT mode.\n    - `mask`: is a vector of `i1` type, which is used to mask out the memory access.\n        mask is a vector of size equal to the subgroup size, or 1 in SIMT mode.\n        scalar mask is also valid for SIMT mode.\n    - `chunk_size`: (optional) represents contiguous number of elements to store to per work item.\n    - `l1_hint`, `l2_hint`, `l3_hint`: are optional cache hints for each level of cache.\n\n  Example 1:\n  ```mlir\n    xegpu.store %0, %1, %2 <{l1_hint = #xegpu.cache_hint<uncached>,\n                             l2_hint = #xegpu.cache_hint<write_back>,\n                             l3_hint = #xegpu.cache_hint<write_through>}>\n          : vector<16xf32>, !xegpu.tensor_desc<16xf32, #xegpu.scattered_tdesc_attr<>>, vector<16xi1>\n  ```\n\n  Example 2:\n  ```mlir\n    xegpu.store %0, %1, %2 <{l1_hint = #xegpu.cache_hint<uncached>,\n                             l2_hint = #xegpu.cache_hint<write_back>,\n                             l3_hint = #xegpu.cache_hint<write_through>}>\n          : vector<16x8xf32>, !xegpu.tensor_desc<16x8xf32, #xegpu.scattered_tdesc_attr<chunk_size=8>>, vector<16xi1>\n  ```\n\n  Example 3:\n  A variant accepts memref as base pointer and an offset instead of scattered TensorTdesc.\n  It combines \"create scattered TensorTdesc\" and \"store with scattered TensorTdesc\".\n  The dest operand could be a raw pointer (uint64_t).\n  Please refer to create_tdesc for the restriction of memref.\n  ```mlir\n    %a = memref.alloc() : memref<1024xf32>\n    %val = arith.constant dense<0.0> : vector<16xf32>\n    %offsets = vector.step : vector<16xindex>\n    %mask = vector.constant_mask [16]: vector<16xi1>\n    xegpu.store %val, %a[%offsets], %mask {l1_hint = #xegpu.cache_hint<cached>,\n                           l2_hint = #xegpu.cache_hint<cached>,\n                           l3_hint = #xegpu.cache_hint<cached>}\n      : memref<1024xf32>, vector<16xi1>, vector<16xindex> -> vector<16xf32>\n  ```\n\n  Example 4 (SIMT mode):\n  SIMT mode only accepts the offsets variant. chunk_size can be inferred from value\n  type. In this example, chunk_size is 8.\n  ```mlir\n    xegpu.store %0, %1[%2], %3 <{l1_hint = #xegpu.cache_hint<uncached>,\n                             l2_hint = #xegpu.cache_hint<write_back>,\n                             l3_hint = #xegpu.cache_hint<write_through>}>\n          : vector<8xf32>, memref<256xf32>, vector<1xindex>, vector<1xi1>\n  ```",
    "inputs": [
      { "name": "value", "type": "AnyTypeOf" },
      { "name": "dest", "type": "XeGPU_GatherScatterSourceType" },
      { "name": "offsets", "type": "Optional" },
      { "name": "mask", "type": "AnyTypeOf" }
    ],
    "attributes": [
      { "name": "chunk_size", "type": "OptionalAttr" },
      { "name": "l1_hint", "type": "OptionalAttr" },
      { "name": "l2_hint", "type": "OptionalAttr" },
      { "name": "l3_hint", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$value `,`\n    $dest\n    (`[` $offsets^ `]`)? `,`\n    $mask\n    prop-dict\n    attr-dict `:`  type(operands)"
  },
  {
    "name": "xegpu.store_matrix",
    "description": "This operation stores a 2D `data` fragment into the shared local memory region\n    specified by a 2D `mem_desc`. Only 2D memory descriptors are supported; use the\n    subview operation to obtain a 2D `mem_desc` from a higher-rank descriptor if needed.\n\n    Arguments:\n     - `mem_desc`: the memory descriptor specifying the SLM region.\n     - `offsets`: the coordinates within the matrix where the data will be written.\n     - `data`: the values to be stored in the matrix.\n     - `subgroup_block_io`: [optional] An attribute indicating that the operation can be \n                 lowered to a subgroup block store. When this attribute is present, \n                 the offsets are subgroup-uniform across all lanes.     \n     - `layout`: [optional] An attribute for guiding distributions among\n                 subgroups and/or work-items. It currently can accept either\n                 LayoutAttr or SliceAttr.",
    "inputs": [
      { "name": "data", "type": "AnyTypeOf" },
      { "name": "mem_desc", "type": "XeGPU_MemDesc" },
      { "name": "offsets", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "const_offsets", "type": "DenseI64ArrayAttr" },
      { "name": "subgroup_block_io", "type": "OptionalAttr" },
      { "name": "layout", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$data `,` $mem_desc `` custom<DynamicIndexList>($offsets, $const_offsets)\n                          prop-dict attr-dict `` `:` type(operands)"
  },
  {
    "name": "xegpu.store_nd",
    "summary": "stores a n-D block register region back to memory, currently only supports 2D",
    "description": "StoreNdOp essentially mimics the hardware block write instruction io\n    write a block of data from register into the memory region as described\n    by the TensorDesc. It takes a set of optional cache hints for each level\n    of cache, L1, L2 and L3. If hardware does not have a correspoding cache,\n    Corresponding cache hint attribute will be masked.\n    It is only available to 1D or 2D blocked tensor_desc.\n\n    In SIMT mode, the input vector represents the data to be stored by each work-item.\n\n    Example 1:\n    ```mlir\n      xegpu.store_nd %3, %2 {l1_hint = #xegpu.cache_hint<uncached>,\n                             l2_hint = #xegpu.cache_hint<write_back>,\n                             l3_hint = #xegpu.cache_hint<write_through>}\n                             : vector<8x16xf16>, !xegpu.tensor_desc<8x16xf16>\n    ```\n    Example 2 (SIMT mode):\n    ```mlir\n      xegpu.store_nd %3, %2 {l1_hint = #xegpu.cache_hint<uncached>,\n                             l2_hint = #xegpu.cache_hint<write_back>,\n                             l3_hint = #xegpu.cache_hint<write_through>}\n                             : vector<8xf16>, !xegpu.tensor_desc<8x16xf16>\n    ```",
    "inputs": [
      { "name": "value", "type": "XeGPU_ValueType" },
      { "name": "TensorDesc", "type": "XeGPU_TensorDesc" },
      { "name": "offsets", "type": "Variadic" }
    ],
    "attributes": [
      { "name": "const_offsets", "type": "OptionalAttr" },
      { "name": "l1_hint", "type": "OptionalAttr" },
      { "name": "l2_hint", "type": "OptionalAttr" },
      { "name": "l3_hint", "type": "OptionalAttr" }
    ],
    "assemblyFormat": "$value `,`\n    $TensorDesc ``\n    custom<OptionalDynamicIndexList>($offsets, $const_offsets)\n    prop-dict attr-dict `:`  type($value) `,` qualified(type($TensorDesc))"
  },
  {
    "name": "xegpu.update_nd_offset",
    "summary": "It updates the offsets for the TensorDesc.",
    "description": "The op updates the offset of the given TensorDesc.\n    The offsets are relative offset to the current position in the number\n    of elements. It will result in a same type TensorDesc as the input.\n\n  Example:\n  ```\n    %2 = xegpu.update_nd_offset %1, [0, 16]: !xegpu.tensor_desc<8x16xf32>\n  ```",
    "inputs": [
      { "name": "TensorDesc", "type": "XeGPU_TensorDesc" },
      { "name": "offsets", "type": "Variadic" }
    ],
    "outputs": [
      { "name": "result", "type": "XeGPU_TensorDesc" }
    ],
    "attributes": [
      { "name": "const_offsets", "type": "DenseI64ArrayAttr" }
    ],
    "assemblyFormat": "$TensorDesc `,`\n    custom<DynamicIndexList>($offsets, $const_offsets)\n    attr-dict `:` qualified(type($result))"
  },
  {
    "name": "xegpu.update_offset",
    "summary": "It updates the offsets for the given tensor descriptor",
    "description": "It behaves similar to `update_nd_offset` in terms that\n    it updates offset of a TensorDesc, and the offsets are relative offset to\n    the current position in the number of elements. However, `update_nd_offset`\n    is to update the start point of a 2D block, so its offset constains two\n    elements representing the shift in each dimension. `update_offset` is to\n    update the offset per work-item, so its offsets contains values representing\n    shifts for each work-item.\n\n    Example:\n    ```mlir\n      %off = arith.constant dense<[32, 32, 32, 32]> : vector<4xindex>\n      %2 = xegpu.update_offset %1, %off :\n              !xegpu.tensor_desc<4x2xf32, #xegpu.scattered_tdesc_attr<chunk_size=2>>, vector<4xindex>\n    ```",
    "inputs": [
      { "name": "TensorDesc", "type": "XeGPU_TensorDesc" },
      { "name": "offsets", "type": "XeGPU_OffsetType" }
    ],
    "outputs": [
      { "name": "result", "type": "XeGPU_TensorDesc" }
    ],
    "assemblyFormat": "$TensorDesc `,` $offsets attr-dict `:` qualified(type($TensorDesc)) `,` type($offsets)"
  }
]